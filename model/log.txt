[12:18:36.673] 418 iterations per epoch
[12:18:38.657] iteration 1: total_loss: 1.607875, loss_sup: 1.475228, loss_mps: 0.062649, loss_cps: 0.069997
[12:18:38.808] iteration 2: total_loss: 1.344892, loss_sup: 1.224363, loss_mps: 0.057394, loss_cps: 0.063134
[12:18:38.952] iteration 3: total_loss: 1.293159, loss_sup: 1.196424, loss_mps: 0.046657, loss_cps: 0.050078
[12:18:39.095] iteration 4: total_loss: 1.029013, loss_sup: 0.948620, loss_mps: 0.038406, loss_cps: 0.041987
[12:18:39.238] iteration 5: total_loss: 1.216788, loss_sup: 1.141945, loss_mps: 0.035009, loss_cps: 0.039834
[12:18:39.380] iteration 6: total_loss: 0.888019, loss_sup: 0.813741, loss_mps: 0.032600, loss_cps: 0.041678
[12:18:39.521] iteration 7: total_loss: 1.033553, loss_sup: 0.961359, loss_mps: 0.031899, loss_cps: 0.040295
[12:18:39.663] iteration 8: total_loss: 0.970658, loss_sup: 0.911569, loss_mps: 0.027142, loss_cps: 0.031947
[12:18:39.804] iteration 9: total_loss: 0.743064, loss_sup: 0.685224, loss_mps: 0.026763, loss_cps: 0.031077
[12:18:39.951] iteration 10: total_loss: 0.630614, loss_sup: 0.586737, loss_mps: 0.020933, loss_cps: 0.022944
[12:18:40.094] iteration 11: total_loss: 0.958851, loss_sup: 0.920334, loss_mps: 0.018400, loss_cps: 0.020117
[12:18:40.238] iteration 12: total_loss: 0.948156, loss_sup: 0.912776, loss_mps: 0.016960, loss_cps: 0.018420
[12:18:40.380] iteration 13: total_loss: 1.165608, loss_sup: 1.123897, loss_mps: 0.019856, loss_cps: 0.021854
[12:18:40.521] iteration 14: total_loss: 0.967596, loss_sup: 0.920728, loss_mps: 0.022478, loss_cps: 0.024390
[12:18:40.664] iteration 15: total_loss: 0.913219, loss_sup: 0.861738, loss_mps: 0.025131, loss_cps: 0.026351
[12:18:40.806] iteration 16: total_loss: 0.942566, loss_sup: 0.881952, loss_mps: 0.029701, loss_cps: 0.030913
[12:18:40.948] iteration 17: total_loss: 0.840786, loss_sup: 0.763712, loss_mps: 0.037731, loss_cps: 0.039343
[12:18:41.090] iteration 18: total_loss: 1.118352, loss_sup: 1.047790, loss_mps: 0.033409, loss_cps: 0.037153
[12:18:41.234] iteration 19: total_loss: 1.242056, loss_sup: 1.185121, loss_mps: 0.027975, loss_cps: 0.028960
[12:18:41.376] iteration 20: total_loss: 0.894966, loss_sup: 0.839832, loss_mps: 0.026730, loss_cps: 0.028404
[12:18:41.518] iteration 21: total_loss: 1.049835, loss_sup: 0.994925, loss_mps: 0.026158, loss_cps: 0.028752
[12:18:41.659] iteration 22: total_loss: 0.633982, loss_sup: 0.577802, loss_mps: 0.026414, loss_cps: 0.029766
[12:18:41.800] iteration 23: total_loss: 0.884232, loss_sup: 0.830403, loss_mps: 0.024895, loss_cps: 0.028934
[12:18:41.944] iteration 24: total_loss: 1.181486, loss_sup: 1.125808, loss_mps: 0.025231, loss_cps: 0.030447
[12:18:42.086] iteration 25: total_loss: 0.808826, loss_sup: 0.757015, loss_mps: 0.023864, loss_cps: 0.027946
[12:18:42.228] iteration 26: total_loss: 0.945930, loss_sup: 0.894486, loss_mps: 0.023460, loss_cps: 0.027983
[12:18:42.371] iteration 27: total_loss: 0.897927, loss_sup: 0.844144, loss_mps: 0.024136, loss_cps: 0.029647
[12:18:42.512] iteration 28: total_loss: 0.834583, loss_sup: 0.779034, loss_mps: 0.026227, loss_cps: 0.029322
[12:18:42.655] iteration 29: total_loss: 0.777228, loss_sup: 0.728178, loss_mps: 0.023216, loss_cps: 0.025833
[12:18:42.798] iteration 30: total_loss: 1.221202, loss_sup: 1.169315, loss_mps: 0.024074, loss_cps: 0.027813
[12:18:42.940] iteration 31: total_loss: 0.885334, loss_sup: 0.830701, loss_mps: 0.025084, loss_cps: 0.029549
[12:18:43.083] iteration 32: total_loss: 1.018962, loss_sup: 0.959151, loss_mps: 0.027052, loss_cps: 0.032760
[12:18:43.225] iteration 33: total_loss: 0.817189, loss_sup: 0.751900, loss_mps: 0.030073, loss_cps: 0.035217
[12:18:43.367] iteration 34: total_loss: 1.150339, loss_sup: 1.094155, loss_mps: 0.026536, loss_cps: 0.029648
[12:18:43.510] iteration 35: total_loss: 0.746237, loss_sup: 0.688846, loss_mps: 0.027196, loss_cps: 0.030196
[12:18:43.652] iteration 36: total_loss: 0.659210, loss_sup: 0.601862, loss_mps: 0.027814, loss_cps: 0.029534
[12:18:43.794] iteration 37: total_loss: 0.918733, loss_sup: 0.864821, loss_mps: 0.026105, loss_cps: 0.027807
[12:18:43.938] iteration 38: total_loss: 1.051837, loss_sup: 0.994771, loss_mps: 0.027148, loss_cps: 0.029918
[12:18:44.082] iteration 39: total_loss: 0.956742, loss_sup: 0.901307, loss_mps: 0.026431, loss_cps: 0.029004
[12:18:44.225] iteration 40: total_loss: 0.879846, loss_sup: 0.826269, loss_mps: 0.025205, loss_cps: 0.028372
[12:18:44.368] iteration 41: total_loss: 0.948900, loss_sup: 0.896028, loss_mps: 0.024804, loss_cps: 0.028068
[12:18:44.512] iteration 42: total_loss: 1.028450, loss_sup: 0.974983, loss_mps: 0.025499, loss_cps: 0.027968
[12:18:44.656] iteration 43: total_loss: 0.716235, loss_sup: 0.667442, loss_mps: 0.023678, loss_cps: 0.025115
[12:18:44.799] iteration 44: total_loss: 0.945905, loss_sup: 0.893721, loss_mps: 0.025456, loss_cps: 0.026728
[12:18:44.941] iteration 45: total_loss: 0.913028, loss_sup: 0.864005, loss_mps: 0.024156, loss_cps: 0.024867
[12:18:45.084] iteration 46: total_loss: 0.656567, loss_sup: 0.611067, loss_mps: 0.022420, loss_cps: 0.023080
[12:18:45.227] iteration 47: total_loss: 0.713761, loss_sup: 0.664176, loss_mps: 0.024283, loss_cps: 0.025301
[12:18:45.369] iteration 48: total_loss: 1.309541, loss_sup: 1.268515, loss_mps: 0.020132, loss_cps: 0.020894
[12:18:45.512] iteration 49: total_loss: 0.798797, loss_sup: 0.755127, loss_mps: 0.021262, loss_cps: 0.022408
[12:18:45.656] iteration 50: total_loss: 0.812587, loss_sup: 0.767425, loss_mps: 0.021845, loss_cps: 0.023317
[12:18:45.800] iteration 51: total_loss: 0.972687, loss_sup: 0.928658, loss_mps: 0.021301, loss_cps: 0.022728
[12:18:45.943] iteration 52: total_loss: 0.803367, loss_sup: 0.754472, loss_mps: 0.023366, loss_cps: 0.025529
[12:18:46.088] iteration 53: total_loss: 0.906723, loss_sup: 0.855492, loss_mps: 0.023805, loss_cps: 0.027427
[12:18:46.230] iteration 54: total_loss: 0.949458, loss_sup: 0.897802, loss_mps: 0.024460, loss_cps: 0.027197
[12:18:46.377] iteration 55: total_loss: 1.044628, loss_sup: 0.990087, loss_mps: 0.025714, loss_cps: 0.028827
[12:18:46.523] iteration 56: total_loss: 1.204676, loss_sup: 1.153114, loss_mps: 0.024592, loss_cps: 0.026970
[12:18:46.666] iteration 57: total_loss: 0.829661, loss_sup: 0.771585, loss_mps: 0.027649, loss_cps: 0.030426
[12:18:46.809] iteration 58: total_loss: 1.003522, loss_sup: 0.943618, loss_mps: 0.029069, loss_cps: 0.030835
[12:18:46.952] iteration 59: total_loss: 0.752898, loss_sup: 0.690341, loss_mps: 0.029712, loss_cps: 0.032845
[12:18:47.097] iteration 60: total_loss: 1.015431, loss_sup: 0.959220, loss_mps: 0.026784, loss_cps: 0.029427
[12:18:47.240] iteration 61: total_loss: 0.832433, loss_sup: 0.779546, loss_mps: 0.025167, loss_cps: 0.027720
[12:18:47.382] iteration 62: total_loss: 0.889076, loss_sup: 0.838250, loss_mps: 0.024302, loss_cps: 0.026524
[12:18:47.524] iteration 63: total_loss: 0.938770, loss_sup: 0.884972, loss_mps: 0.025685, loss_cps: 0.028114
[12:18:47.667] iteration 64: total_loss: 0.831930, loss_sup: 0.784587, loss_mps: 0.022805, loss_cps: 0.024538
[12:18:47.810] iteration 65: total_loss: 0.681725, loss_sup: 0.631639, loss_mps: 0.023132, loss_cps: 0.026954
[12:18:47.953] iteration 66: total_loss: 0.776787, loss_sup: 0.723792, loss_mps: 0.024388, loss_cps: 0.028606
[12:18:48.096] iteration 67: total_loss: 1.002105, loss_sup: 0.957930, loss_mps: 0.020678, loss_cps: 0.023497
[12:18:48.239] iteration 68: total_loss: 0.603912, loss_sup: 0.551865, loss_mps: 0.023711, loss_cps: 0.028337
[12:18:48.385] iteration 69: total_loss: 0.604879, loss_sup: 0.559139, loss_mps: 0.020548, loss_cps: 0.025192
[12:18:48.527] iteration 70: total_loss: 1.059319, loss_sup: 1.014114, loss_mps: 0.020044, loss_cps: 0.025161
[12:18:48.676] iteration 71: total_loss: 0.862467, loss_sup: 0.817188, loss_mps: 0.020083, loss_cps: 0.025196
[12:18:48.823] iteration 72: total_loss: 0.972834, loss_sup: 0.922105, loss_mps: 0.022311, loss_cps: 0.028417
[12:18:48.966] iteration 73: total_loss: 0.959646, loss_sup: 0.911362, loss_mps: 0.021470, loss_cps: 0.026814
[12:18:49.109] iteration 74: total_loss: 0.898397, loss_sup: 0.847624, loss_mps: 0.023253, loss_cps: 0.027519
[12:18:49.251] iteration 75: total_loss: 0.981130, loss_sup: 0.935196, loss_mps: 0.020709, loss_cps: 0.025225
[12:18:49.396] iteration 76: total_loss: 0.909463, loss_sup: 0.857557, loss_mps: 0.024259, loss_cps: 0.027647
[12:18:49.539] iteration 77: total_loss: 0.844891, loss_sup: 0.794036, loss_mps: 0.024605, loss_cps: 0.026250
[12:18:49.681] iteration 78: total_loss: 0.930161, loss_sup: 0.880598, loss_mps: 0.023894, loss_cps: 0.025668
[12:18:49.824] iteration 79: total_loss: 0.809559, loss_sup: 0.754858, loss_mps: 0.026003, loss_cps: 0.028698
[12:18:49.970] iteration 80: total_loss: 0.692570, loss_sup: 0.640882, loss_mps: 0.024291, loss_cps: 0.027397
[12:18:50.113] iteration 81: total_loss: 0.726244, loss_sup: 0.681875, loss_mps: 0.021245, loss_cps: 0.023124
[12:18:50.255] iteration 82: total_loss: 1.149167, loss_sup: 1.104065, loss_mps: 0.021475, loss_cps: 0.023628
[12:18:50.400] iteration 83: total_loss: 0.885764, loss_sup: 0.840556, loss_mps: 0.021177, loss_cps: 0.024031
[12:18:50.545] iteration 84: total_loss: 1.151630, loss_sup: 1.101581, loss_mps: 0.022416, loss_cps: 0.027632
[12:18:50.688] iteration 85: total_loss: 0.692041, loss_sup: 0.640118, loss_mps: 0.023284, loss_cps: 0.028639
[12:18:50.831] iteration 86: total_loss: 0.950707, loss_sup: 0.897050, loss_mps: 0.023518, loss_cps: 0.030139
[12:18:50.977] iteration 87: total_loss: 0.938195, loss_sup: 0.884996, loss_mps: 0.023747, loss_cps: 0.029452
[12:18:51.120] iteration 88: total_loss: 0.526340, loss_sup: 0.477316, loss_mps: 0.022271, loss_cps: 0.026753
[12:18:51.266] iteration 89: total_loss: 0.725182, loss_sup: 0.679847, loss_mps: 0.020999, loss_cps: 0.024336
[12:18:51.409] iteration 90: total_loss: 0.679903, loss_sup: 0.629731, loss_mps: 0.022571, loss_cps: 0.027602
[12:18:51.553] iteration 91: total_loss: 0.957373, loss_sup: 0.913002, loss_mps: 0.020274, loss_cps: 0.024096
[12:18:51.697] iteration 92: total_loss: 0.680505, loss_sup: 0.632235, loss_mps: 0.021866, loss_cps: 0.026404
[12:18:51.839] iteration 93: total_loss: 0.776245, loss_sup: 0.725875, loss_mps: 0.022390, loss_cps: 0.027981
[12:18:51.987] iteration 94: total_loss: 0.691049, loss_sup: 0.644351, loss_mps: 0.021049, loss_cps: 0.025649
[12:18:52.131] iteration 95: total_loss: 0.735130, loss_sup: 0.688446, loss_mps: 0.020797, loss_cps: 0.025887
[12:18:52.281] iteration 96: total_loss: 0.908311, loss_sup: 0.858255, loss_mps: 0.022203, loss_cps: 0.027852
[12:18:52.425] iteration 97: total_loss: 0.753334, loss_sup: 0.703395, loss_mps: 0.021982, loss_cps: 0.027958
[12:18:52.569] iteration 98: total_loss: 1.403457, loss_sup: 1.367151, loss_mps: 0.016695, loss_cps: 0.019611
[12:18:52.713] iteration 99: total_loss: 0.784906, loss_sup: 0.746021, loss_mps: 0.018054, loss_cps: 0.020831
[12:18:52.856] iteration 100: total_loss: 0.649997, loss_sup: 0.607990, loss_mps: 0.019469, loss_cps: 0.022538
[12:18:52.856] Evaluation Started ==>
[12:19:04.306] ==> valid iteration 100: unet metrics: {'dc': 0.353351307338877, 'jc': 0.2591682912157635, 'pre': 0.3521286068555592, 'hd': 8.809092234116784}, ynet metrics: {'dc': 0.34064891622903115, 'jc': 0.25571778806519685, 'pre': 0.29794322610464014, 'hd': 9.943117706627786}.
[12:19:04.352] ==> New best valid dice for unet: 0.353351, at iteration 100
[12:19:04.458] ==> New best valid dice for ynet: 0.340649, at iteration 100
[12:19:04.459] Evaluation Finished!⏹️
[12:19:04.610] iteration 101: total_loss: 0.797253, loss_sup: 0.752915, loss_mps: 0.020510, loss_cps: 0.023828
[12:19:04.758] iteration 102: total_loss: 0.892604, loss_sup: 0.847845, loss_mps: 0.021624, loss_cps: 0.023134
[12:19:04.901] iteration 103: total_loss: 0.853904, loss_sup: 0.808816, loss_mps: 0.021432, loss_cps: 0.023656
[12:19:05.044] iteration 104: total_loss: 0.871982, loss_sup: 0.826099, loss_mps: 0.021465, loss_cps: 0.024418
[12:19:05.187] iteration 105: total_loss: 0.653823, loss_sup: 0.599687, loss_mps: 0.024070, loss_cps: 0.030067
[12:19:05.331] iteration 106: total_loss: 0.756017, loss_sup: 0.706514, loss_mps: 0.021717, loss_cps: 0.027786
[12:19:05.474] iteration 107: total_loss: 0.864713, loss_sup: 0.813004, loss_mps: 0.023457, loss_cps: 0.028252
[12:19:05.618] iteration 108: total_loss: 0.867207, loss_sup: 0.815370, loss_mps: 0.023288, loss_cps: 0.028549
[12:19:05.761] iteration 109: total_loss: 0.606650, loss_sup: 0.554025, loss_mps: 0.023651, loss_cps: 0.028974
[12:19:05.908] iteration 110: total_loss: 0.845139, loss_sup: 0.793680, loss_mps: 0.023591, loss_cps: 0.027868
[12:19:06.055] iteration 111: total_loss: 0.589772, loss_sup: 0.541449, loss_mps: 0.022488, loss_cps: 0.025835
[12:19:06.199] iteration 112: total_loss: 0.573512, loss_sup: 0.526722, loss_mps: 0.021903, loss_cps: 0.024888
[12:19:06.344] iteration 113: total_loss: 0.833676, loss_sup: 0.787769, loss_mps: 0.021353, loss_cps: 0.024554
[12:19:06.487] iteration 114: total_loss: 1.015643, loss_sup: 0.975526, loss_mps: 0.018774, loss_cps: 0.021343
[12:19:06.629] iteration 115: total_loss: 0.844976, loss_sup: 0.794465, loss_mps: 0.022631, loss_cps: 0.027880
[12:19:06.775] iteration 116: total_loss: 0.764142, loss_sup: 0.713975, loss_mps: 0.022226, loss_cps: 0.027941
[12:19:06.917] iteration 117: total_loss: 0.641624, loss_sup: 0.596394, loss_mps: 0.020815, loss_cps: 0.024416
[12:19:07.060] iteration 118: total_loss: 0.670963, loss_sup: 0.626506, loss_mps: 0.020539, loss_cps: 0.023917
[12:19:07.204] iteration 119: total_loss: 0.629764, loss_sup: 0.586688, loss_mps: 0.020042, loss_cps: 0.023034
[12:19:07.346] iteration 120: total_loss: 0.724801, loss_sup: 0.679589, loss_mps: 0.020662, loss_cps: 0.024550
[12:19:07.490] iteration 121: total_loss: 0.931420, loss_sup: 0.887442, loss_mps: 0.019657, loss_cps: 0.024321
[12:19:07.635] iteration 122: total_loss: 0.779395, loss_sup: 0.736031, loss_mps: 0.019314, loss_cps: 0.024051
[12:19:07.778] iteration 123: total_loss: 0.921062, loss_sup: 0.874888, loss_mps: 0.019971, loss_cps: 0.026204
[12:19:07.923] iteration 124: total_loss: 0.719851, loss_sup: 0.675359, loss_mps: 0.019782, loss_cps: 0.024709
[12:19:08.066] iteration 125: total_loss: 0.830212, loss_sup: 0.778217, loss_mps: 0.022197, loss_cps: 0.029799
[12:19:08.209] iteration 126: total_loss: 1.039083, loss_sup: 0.988296, loss_mps: 0.021758, loss_cps: 0.029029
[12:19:08.354] iteration 127: total_loss: 0.659761, loss_sup: 0.608818, loss_mps: 0.022325, loss_cps: 0.028618
[12:19:08.497] iteration 128: total_loss: 0.887717, loss_sup: 0.840648, loss_mps: 0.020781, loss_cps: 0.026288
[12:19:08.644] iteration 129: total_loss: 0.918099, loss_sup: 0.875937, loss_mps: 0.019354, loss_cps: 0.022808
[12:19:08.790] iteration 130: total_loss: 0.836229, loss_sup: 0.783316, loss_mps: 0.022993, loss_cps: 0.029920
[12:19:08.934] iteration 131: total_loss: 0.865069, loss_sup: 0.814605, loss_mps: 0.022332, loss_cps: 0.028132
[12:19:09.077] iteration 132: total_loss: 0.726836, loss_sup: 0.673469, loss_mps: 0.023698, loss_cps: 0.029669
[12:19:09.220] iteration 133: total_loss: 0.602107, loss_sup: 0.545460, loss_mps: 0.024929, loss_cps: 0.031719
[12:19:09.366] iteration 134: total_loss: 0.915402, loss_sup: 0.857585, loss_mps: 0.025606, loss_cps: 0.032211
[12:19:09.510] iteration 135: total_loss: 0.875929, loss_sup: 0.824106, loss_mps: 0.023879, loss_cps: 0.027944
[12:19:09.652] iteration 136: total_loss: 0.850551, loss_sup: 0.800183, loss_mps: 0.023302, loss_cps: 0.027065
[12:19:09.795] iteration 137: total_loss: 0.599789, loss_sup: 0.551313, loss_mps: 0.022529, loss_cps: 0.025947
[12:19:09.938] iteration 138: total_loss: 0.805653, loss_sup: 0.758310, loss_mps: 0.021290, loss_cps: 0.026053
[12:19:10.081] iteration 139: total_loss: 0.768097, loss_sup: 0.719791, loss_mps: 0.021831, loss_cps: 0.026474
[12:19:10.224] iteration 140: total_loss: 0.859478, loss_sup: 0.812300, loss_mps: 0.020847, loss_cps: 0.026332
[12:19:10.366] iteration 141: total_loss: 0.761551, loss_sup: 0.710261, loss_mps: 0.022345, loss_cps: 0.028945
[12:19:10.509] iteration 142: total_loss: 0.797628, loss_sup: 0.744546, loss_mps: 0.022351, loss_cps: 0.030732
[12:19:10.654] iteration 143: total_loss: 1.027801, loss_sup: 0.966521, loss_mps: 0.025569, loss_cps: 0.035711
[12:19:10.798] iteration 144: total_loss: 1.090793, loss_sup: 1.027177, loss_mps: 0.026251, loss_cps: 0.037365
[12:19:10.942] iteration 145: total_loss: 0.974955, loss_sup: 0.906312, loss_mps: 0.027950, loss_cps: 0.040692
[12:19:11.084] iteration 146: total_loss: 0.956007, loss_sup: 0.886521, loss_mps: 0.028130, loss_cps: 0.041356
[12:19:11.229] iteration 147: total_loss: 0.796576, loss_sup: 0.730761, loss_mps: 0.027466, loss_cps: 0.038348
[12:19:11.372] iteration 148: total_loss: 0.684724, loss_sup: 0.617624, loss_mps: 0.028146, loss_cps: 0.038955
[12:19:11.515] iteration 149: total_loss: 0.652911, loss_sup: 0.592484, loss_mps: 0.026056, loss_cps: 0.034371
[12:19:11.658] iteration 150: total_loss: 0.696188, loss_sup: 0.639010, loss_mps: 0.025812, loss_cps: 0.031367
[12:19:11.801] iteration 151: total_loss: 0.766109, loss_sup: 0.711365, loss_mps: 0.024998, loss_cps: 0.029746
[12:19:11.946] iteration 152: total_loss: 0.860408, loss_sup: 0.801188, loss_mps: 0.026515, loss_cps: 0.032705
[12:19:12.089] iteration 153: total_loss: 0.735317, loss_sup: 0.678420, loss_mps: 0.026360, loss_cps: 0.030537
[12:19:12.232] iteration 154: total_loss: 0.471386, loss_sup: 0.418523, loss_mps: 0.024679, loss_cps: 0.028184
[12:19:12.377] iteration 155: total_loss: 0.948051, loss_sup: 0.894295, loss_mps: 0.025089, loss_cps: 0.028667
[12:19:12.523] iteration 156: total_loss: 0.672327, loss_sup: 0.621514, loss_mps: 0.023593, loss_cps: 0.027219
[12:19:12.667] iteration 157: total_loss: 0.651034, loss_sup: 0.602477, loss_mps: 0.022494, loss_cps: 0.026063
[12:19:12.811] iteration 158: total_loss: 0.871831, loss_sup: 0.823389, loss_mps: 0.022355, loss_cps: 0.026087
[12:19:12.957] iteration 159: total_loss: 0.814853, loss_sup: 0.762820, loss_mps: 0.023542, loss_cps: 0.028491
[12:19:13.102] iteration 160: total_loss: 0.701567, loss_sup: 0.658627, loss_mps: 0.019994, loss_cps: 0.022947
[12:19:13.245] iteration 161: total_loss: 0.832349, loss_sup: 0.787346, loss_mps: 0.020458, loss_cps: 0.024545
[12:19:13.388] iteration 162: total_loss: 0.813565, loss_sup: 0.766966, loss_mps: 0.021514, loss_cps: 0.025085
[12:19:13.531] iteration 163: total_loss: 0.918112, loss_sup: 0.864062, loss_mps: 0.023311, loss_cps: 0.030739
[12:19:13.673] iteration 164: total_loss: 0.661997, loss_sup: 0.608407, loss_mps: 0.022555, loss_cps: 0.031034
[12:19:13.816] iteration 165: total_loss: 0.658882, loss_sup: 0.609919, loss_mps: 0.020845, loss_cps: 0.028118
[12:19:13.965] iteration 166: total_loss: 1.002593, loss_sup: 0.950249, loss_mps: 0.021989, loss_cps: 0.030355
[12:19:14.108] iteration 167: total_loss: 0.664570, loss_sup: 0.621411, loss_mps: 0.019706, loss_cps: 0.023453
[12:19:14.251] iteration 168: total_loss: 0.586992, loss_sup: 0.545105, loss_mps: 0.019321, loss_cps: 0.022565
[12:19:14.393] iteration 169: total_loss: 0.760105, loss_sup: 0.712638, loss_mps: 0.020593, loss_cps: 0.026874
[12:19:14.539] iteration 170: total_loss: 0.726980, loss_sup: 0.670688, loss_mps: 0.023233, loss_cps: 0.033060
[12:19:14.683] iteration 171: total_loss: 0.807190, loss_sup: 0.752381, loss_mps: 0.022513, loss_cps: 0.032296
[12:19:14.826] iteration 172: total_loss: 0.814437, loss_sup: 0.756631, loss_mps: 0.023515, loss_cps: 0.034290
[12:19:14.969] iteration 173: total_loss: 0.946538, loss_sup: 0.885757, loss_mps: 0.024270, loss_cps: 0.036511
[12:19:15.112] iteration 174: total_loss: 0.655773, loss_sup: 0.601966, loss_mps: 0.022110, loss_cps: 0.031696
[12:19:15.255] iteration 175: total_loss: 0.836356, loss_sup: 0.782308, loss_mps: 0.023301, loss_cps: 0.030747
[12:19:15.398] iteration 176: total_loss: 0.863417, loss_sup: 0.812323, loss_mps: 0.022883, loss_cps: 0.028211
[12:19:15.541] iteration 177: total_loss: 0.880133, loss_sup: 0.830971, loss_mps: 0.022066, loss_cps: 0.027096
[12:19:15.684] iteration 178: total_loss: 0.772269, loss_sup: 0.724166, loss_mps: 0.021908, loss_cps: 0.026194
[12:19:15.829] iteration 179: total_loss: 0.801223, loss_sup: 0.756406, loss_mps: 0.020840, loss_cps: 0.023976
[12:19:15.975] iteration 180: total_loss: 0.663493, loss_sup: 0.619271, loss_mps: 0.020552, loss_cps: 0.023669
[12:19:16.118] iteration 181: total_loss: 0.721857, loss_sup: 0.677410, loss_mps: 0.021010, loss_cps: 0.023437
[12:19:16.263] iteration 182: total_loss: 0.787847, loss_sup: 0.733616, loss_mps: 0.024479, loss_cps: 0.029752
[12:19:16.406] iteration 183: total_loss: 0.994909, loss_sup: 0.946077, loss_mps: 0.021634, loss_cps: 0.027197
[12:19:16.552] iteration 184: total_loss: 0.674628, loss_sup: 0.623807, loss_mps: 0.022972, loss_cps: 0.027849
[12:19:16.696] iteration 185: total_loss: 0.647819, loss_sup: 0.597094, loss_mps: 0.022452, loss_cps: 0.028273
[12:19:16.839] iteration 186: total_loss: 0.592678, loss_sup: 0.543211, loss_mps: 0.021622, loss_cps: 0.027845
[12:19:16.985] iteration 187: total_loss: 0.866163, loss_sup: 0.822281, loss_mps: 0.019526, loss_cps: 0.024357
[12:19:17.130] iteration 188: total_loss: 0.659783, loss_sup: 0.614942, loss_mps: 0.019891, loss_cps: 0.024951
[12:19:17.273] iteration 189: total_loss: 0.833255, loss_sup: 0.793093, loss_mps: 0.017495, loss_cps: 0.022667
[12:19:17.417] iteration 190: total_loss: 1.278525, loss_sup: 1.228024, loss_mps: 0.020476, loss_cps: 0.030025
[12:19:17.562] iteration 191: total_loss: 1.192487, loss_sup: 1.139378, loss_mps: 0.021323, loss_cps: 0.031786
[12:19:17.707] iteration 192: total_loss: 0.825877, loss_sup: 0.782861, loss_mps: 0.018272, loss_cps: 0.024744
[12:19:17.850] iteration 193: total_loss: 0.869210, loss_sup: 0.822365, loss_mps: 0.020346, loss_cps: 0.026499
[12:19:17.993] iteration 194: total_loss: 0.593629, loss_sup: 0.538692, loss_mps: 0.024143, loss_cps: 0.030794
[12:19:18.137] iteration 195: total_loss: 0.701134, loss_sup: 0.641881, loss_mps: 0.026509, loss_cps: 0.032743
[12:19:18.281] iteration 196: total_loss: 0.832301, loss_sup: 0.769908, loss_mps: 0.027541, loss_cps: 0.034851
[12:19:18.424] iteration 197: total_loss: 0.670353, loss_sup: 0.601477, loss_mps: 0.029983, loss_cps: 0.038892
[12:19:18.567] iteration 198: total_loss: 0.801601, loss_sup: 0.728476, loss_mps: 0.031619, loss_cps: 0.041506
[12:19:18.711] iteration 199: total_loss: 0.599106, loss_sup: 0.535207, loss_mps: 0.028185, loss_cps: 0.035714
[12:19:18.857] iteration 200: total_loss: 0.918838, loss_sup: 0.855173, loss_mps: 0.027543, loss_cps: 0.036121
[12:19:18.857] Evaluation Started ==>
[12:19:30.226] ==> valid iteration 200: unet metrics: {'dc': 0.14936816781328052, 'jc': 0.09977533011053394, 'pre': 0.3375068523720702, 'hd': 7.835934290468965}, ynet metrics: {'dc': 0.06469838166159282, 'jc': 0.04006368340248808, 'pre': 0.360067082713267, 'hd': 7.60204859882743}.
[12:19:30.228] Evaluation Finished!⏹️
[12:19:30.378] iteration 201: total_loss: 0.742588, loss_sup: 0.686307, loss_mps: 0.024332, loss_cps: 0.031949
[12:19:30.523] iteration 202: total_loss: 0.763731, loss_sup: 0.708892, loss_mps: 0.023925, loss_cps: 0.030913
[12:19:30.667] iteration 203: total_loss: 0.717516, loss_sup: 0.663413, loss_mps: 0.022962, loss_cps: 0.031141
[12:19:30.811] iteration 204: total_loss: 0.544059, loss_sup: 0.491147, loss_mps: 0.022038, loss_cps: 0.030874
[12:19:30.957] iteration 205: total_loss: 0.853902, loss_sup: 0.800107, loss_mps: 0.021932, loss_cps: 0.031863
[12:19:31.103] iteration 206: total_loss: 0.836819, loss_sup: 0.784480, loss_mps: 0.021632, loss_cps: 0.030707
[12:19:31.247] iteration 207: total_loss: 0.725789, loss_sup: 0.675088, loss_mps: 0.020758, loss_cps: 0.029943
[12:19:31.391] iteration 208: total_loss: 0.896860, loss_sup: 0.851380, loss_mps: 0.019256, loss_cps: 0.026224
[12:19:31.535] iteration 209: total_loss: 0.757527, loss_sup: 0.704650, loss_mps: 0.021074, loss_cps: 0.031803
[12:19:31.679] iteration 210: total_loss: 0.679584, loss_sup: 0.627085, loss_mps: 0.021374, loss_cps: 0.031125
[12:19:31.824] iteration 211: total_loss: 0.737522, loss_sup: 0.684777, loss_mps: 0.021284, loss_cps: 0.031461
[12:19:31.967] iteration 212: total_loss: 0.739476, loss_sup: 0.685984, loss_mps: 0.021629, loss_cps: 0.031863
[12:19:32.112] iteration 213: total_loss: 0.779644, loss_sup: 0.725643, loss_mps: 0.022641, loss_cps: 0.031361
[12:19:32.255] iteration 214: total_loss: 0.651382, loss_sup: 0.604083, loss_mps: 0.020390, loss_cps: 0.026909
[12:19:32.399] iteration 215: total_loss: 0.643178, loss_sup: 0.594298, loss_mps: 0.021270, loss_cps: 0.027610
[12:19:32.542] iteration 216: total_loss: 0.641509, loss_sup: 0.592228, loss_mps: 0.021563, loss_cps: 0.027719
[12:19:32.687] iteration 217: total_loss: 0.564010, loss_sup: 0.509403, loss_mps: 0.022765, loss_cps: 0.031842
[12:19:32.831] iteration 218: total_loss: 0.672560, loss_sup: 0.622642, loss_mps: 0.020597, loss_cps: 0.029321
[12:19:32.975] iteration 219: total_loss: 0.629532, loss_sup: 0.584971, loss_mps: 0.019086, loss_cps: 0.025475
[12:19:33.120] iteration 220: total_loss: 0.987268, loss_sup: 0.939976, loss_mps: 0.019654, loss_cps: 0.027638
[12:19:33.266] iteration 221: total_loss: 1.113254, loss_sup: 1.064364, loss_mps: 0.020418, loss_cps: 0.028472
[12:19:33.410] iteration 222: total_loss: 0.561313, loss_sup: 0.517437, loss_mps: 0.018461, loss_cps: 0.025415
[12:19:33.554] iteration 223: total_loss: 0.693283, loss_sup: 0.649187, loss_mps: 0.018859, loss_cps: 0.025237
[12:19:33.698] iteration 224: total_loss: 0.466041, loss_sup: 0.415406, loss_mps: 0.020721, loss_cps: 0.029913
[12:19:33.845] iteration 225: total_loss: 0.676036, loss_sup: 0.612552, loss_mps: 0.024964, loss_cps: 0.038520
[12:19:33.989] iteration 226: total_loss: 0.633528, loss_sup: 0.573569, loss_mps: 0.022779, loss_cps: 0.037180
[12:19:34.133] iteration 227: total_loss: 0.705251, loss_sup: 0.645773, loss_mps: 0.023556, loss_cps: 0.035922
[12:19:34.277] iteration 228: total_loss: 0.754681, loss_sup: 0.701231, loss_mps: 0.020980, loss_cps: 0.032470
[12:19:34.421] iteration 229: total_loss: 1.020394, loss_sup: 0.971250, loss_mps: 0.019668, loss_cps: 0.029475
[12:19:34.565] iteration 230: total_loss: 0.456724, loss_sup: 0.406421, loss_mps: 0.020179, loss_cps: 0.030124
[12:19:34.711] iteration 231: total_loss: 0.708846, loss_sup: 0.661288, loss_mps: 0.019459, loss_cps: 0.028099
[12:19:34.855] iteration 232: total_loss: 0.737092, loss_sup: 0.687572, loss_mps: 0.020037, loss_cps: 0.029483
[12:19:35.000] iteration 233: total_loss: 0.643690, loss_sup: 0.594580, loss_mps: 0.019371, loss_cps: 0.029739
[12:19:35.144] iteration 234: total_loss: 0.815250, loss_sup: 0.758969, loss_mps: 0.022391, loss_cps: 0.033890
[12:19:35.287] iteration 235: total_loss: 0.658684, loss_sup: 0.609262, loss_mps: 0.021066, loss_cps: 0.028357
[12:19:35.431] iteration 236: total_loss: 0.743470, loss_sup: 0.695005, loss_mps: 0.020666, loss_cps: 0.027799
[12:19:35.575] iteration 237: total_loss: 0.555151, loss_sup: 0.506277, loss_mps: 0.020630, loss_cps: 0.028244
[12:19:35.720] iteration 238: total_loss: 0.776309, loss_sup: 0.727789, loss_mps: 0.020106, loss_cps: 0.028415
[12:19:35.865] iteration 239: total_loss: 0.816611, loss_sup: 0.769860, loss_mps: 0.019695, loss_cps: 0.027056
[12:19:36.012] iteration 240: total_loss: 0.601266, loss_sup: 0.549488, loss_mps: 0.021113, loss_cps: 0.030665
[12:19:36.159] iteration 241: total_loss: 0.798072, loss_sup: 0.747343, loss_mps: 0.020799, loss_cps: 0.029930
[12:19:36.308] iteration 242: total_loss: 0.930394, loss_sup: 0.877206, loss_mps: 0.021599, loss_cps: 0.031588
[12:19:36.454] iteration 243: total_loss: 0.510973, loss_sup: 0.455733, loss_mps: 0.022330, loss_cps: 0.032910
[12:19:36.599] iteration 244: total_loss: 0.578458, loss_sup: 0.527907, loss_mps: 0.021474, loss_cps: 0.029077
[12:19:36.743] iteration 245: total_loss: 0.635094, loss_sup: 0.582325, loss_mps: 0.022119, loss_cps: 0.030650
[12:19:36.888] iteration 246: total_loss: 0.857058, loss_sup: 0.801820, loss_mps: 0.023235, loss_cps: 0.032003
[12:19:37.034] iteration 247: total_loss: 0.819899, loss_sup: 0.771864, loss_mps: 0.021138, loss_cps: 0.026898
[12:19:37.180] iteration 248: total_loss: 0.647092, loss_sup: 0.597475, loss_mps: 0.021929, loss_cps: 0.027687
[12:19:37.324] iteration 249: total_loss: 0.686658, loss_sup: 0.641440, loss_mps: 0.020480, loss_cps: 0.024738
[12:19:37.469] iteration 250: total_loss: 0.625355, loss_sup: 0.571770, loss_mps: 0.022992, loss_cps: 0.030593
[12:19:37.614] iteration 251: total_loss: 0.641888, loss_sup: 0.592041, loss_mps: 0.021537, loss_cps: 0.028310
[12:19:37.759] iteration 252: total_loss: 0.576084, loss_sup: 0.528124, loss_mps: 0.020667, loss_cps: 0.027292
[12:19:37.904] iteration 253: total_loss: 0.593354, loss_sup: 0.548042, loss_mps: 0.019802, loss_cps: 0.025510
[12:19:38.052] iteration 254: total_loss: 0.582649, loss_sup: 0.533785, loss_mps: 0.020857, loss_cps: 0.028007
[12:19:38.197] iteration 255: total_loss: 1.032901, loss_sup: 0.981028, loss_mps: 0.021255, loss_cps: 0.030618
[12:19:38.343] iteration 256: total_loss: 0.822405, loss_sup: 0.773553, loss_mps: 0.020493, loss_cps: 0.028359
[12:19:38.486] iteration 257: total_loss: 0.745470, loss_sup: 0.695727, loss_mps: 0.020662, loss_cps: 0.029081
[12:19:38.631] iteration 258: total_loss: 0.810334, loss_sup: 0.759233, loss_mps: 0.021034, loss_cps: 0.030066
[12:19:38.777] iteration 259: total_loss: 0.726915, loss_sup: 0.669908, loss_mps: 0.022924, loss_cps: 0.034082
[12:19:38.925] iteration 260: total_loss: 0.647455, loss_sup: 0.590961, loss_mps: 0.023218, loss_cps: 0.033276
[12:19:39.069] iteration 261: total_loss: 0.656811, loss_sup: 0.605745, loss_mps: 0.021650, loss_cps: 0.029415
[12:19:39.222] iteration 262: total_loss: 0.552628, loss_sup: 0.500544, loss_mps: 0.022116, loss_cps: 0.029967
[12:19:39.366] iteration 263: total_loss: 1.210198, loss_sup: 1.153618, loss_mps: 0.023205, loss_cps: 0.033375
[12:19:39.511] iteration 264: total_loss: 0.594146, loss_sup: 0.540308, loss_mps: 0.022379, loss_cps: 0.031459
[12:19:39.655] iteration 265: total_loss: 0.459279, loss_sup: 0.407235, loss_mps: 0.022055, loss_cps: 0.029989
[12:19:39.800] iteration 266: total_loss: 0.807281, loss_sup: 0.760941, loss_mps: 0.020211, loss_cps: 0.026129
[12:19:39.945] iteration 267: total_loss: 0.974843, loss_sup: 0.924990, loss_mps: 0.021062, loss_cps: 0.028791
[12:19:40.088] iteration 268: total_loss: 0.640256, loss_sup: 0.594604, loss_mps: 0.019686, loss_cps: 0.025966
[12:19:40.232] iteration 269: total_loss: 0.481746, loss_sup: 0.438879, loss_mps: 0.019029, loss_cps: 0.023838
[12:19:40.382] iteration 270: total_loss: 0.618732, loss_sup: 0.571781, loss_mps: 0.020098, loss_cps: 0.026853
[12:19:40.525] iteration 271: total_loss: 0.603245, loss_sup: 0.559711, loss_mps: 0.018937, loss_cps: 0.024596
[12:19:40.669] iteration 272: total_loss: 0.594168, loss_sup: 0.544428, loss_mps: 0.020638, loss_cps: 0.029102
[12:19:40.814] iteration 273: total_loss: 0.745815, loss_sup: 0.701488, loss_mps: 0.018737, loss_cps: 0.025591
[12:19:40.960] iteration 274: total_loss: 0.838300, loss_sup: 0.792166, loss_mps: 0.018918, loss_cps: 0.027216
[12:19:41.106] iteration 275: total_loss: 0.575735, loss_sup: 0.532478, loss_mps: 0.018083, loss_cps: 0.025174
[12:19:41.251] iteration 276: total_loss: 0.593127, loss_sup: 0.549661, loss_mps: 0.017894, loss_cps: 0.025572
[12:19:41.398] iteration 277: total_loss: 0.889603, loss_sup: 0.843516, loss_mps: 0.018847, loss_cps: 0.027240
[12:19:41.542] iteration 278: total_loss: 0.431430, loss_sup: 0.382027, loss_mps: 0.019952, loss_cps: 0.029451
[12:19:41.686] iteration 279: total_loss: 0.828641, loss_sup: 0.777626, loss_mps: 0.020545, loss_cps: 0.030470
[12:19:41.831] iteration 280: total_loss: 0.775140, loss_sup: 0.721318, loss_mps: 0.020905, loss_cps: 0.032917
[12:19:41.975] iteration 281: total_loss: 0.707601, loss_sup: 0.656425, loss_mps: 0.020429, loss_cps: 0.030746
[12:19:42.120] iteration 282: total_loss: 0.657983, loss_sup: 0.608689, loss_mps: 0.019639, loss_cps: 0.029655
[12:19:42.264] iteration 283: total_loss: 0.870500, loss_sup: 0.809042, loss_mps: 0.023340, loss_cps: 0.038119
[12:19:42.410] iteration 284: total_loss: 0.531138, loss_sup: 0.477033, loss_mps: 0.021043, loss_cps: 0.033062
[12:19:42.554] iteration 285: total_loss: 0.674316, loss_sup: 0.625540, loss_mps: 0.019448, loss_cps: 0.029328
[12:19:42.698] iteration 286: total_loss: 0.648819, loss_sup: 0.605880, loss_mps: 0.017801, loss_cps: 0.025138
[12:19:42.843] iteration 287: total_loss: 0.612967, loss_sup: 0.564130, loss_mps: 0.019888, loss_cps: 0.028949
[12:19:42.988] iteration 288: total_loss: 0.637941, loss_sup: 0.592481, loss_mps: 0.019143, loss_cps: 0.026317
[12:19:43.132] iteration 289: total_loss: 0.600260, loss_sup: 0.549395, loss_mps: 0.020643, loss_cps: 0.030223
[12:19:43.280] iteration 290: total_loss: 0.662101, loss_sup: 0.606035, loss_mps: 0.021890, loss_cps: 0.034175
[12:19:43.424] iteration 291: total_loss: 0.729492, loss_sup: 0.676534, loss_mps: 0.021699, loss_cps: 0.031260
[12:19:43.570] iteration 292: total_loss: 0.720952, loss_sup: 0.664867, loss_mps: 0.022377, loss_cps: 0.033707
[12:19:43.715] iteration 293: total_loss: 0.606936, loss_sup: 0.552147, loss_mps: 0.022605, loss_cps: 0.032184
[12:19:43.860] iteration 294: total_loss: 0.612691, loss_sup: 0.564321, loss_mps: 0.020323, loss_cps: 0.028047
[12:19:44.004] iteration 295: total_loss: 0.666859, loss_sup: 0.621185, loss_mps: 0.019579, loss_cps: 0.026094
[12:19:44.149] iteration 296: total_loss: 0.761190, loss_sup: 0.706817, loss_mps: 0.021871, loss_cps: 0.032502
[12:19:44.294] iteration 297: total_loss: 0.808663, loss_sup: 0.764620, loss_mps: 0.018689, loss_cps: 0.025354
[12:19:44.438] iteration 298: total_loss: 0.875391, loss_sup: 0.824447, loss_mps: 0.021144, loss_cps: 0.029800
[12:19:44.583] iteration 299: total_loss: 0.555189, loss_sup: 0.507977, loss_mps: 0.020307, loss_cps: 0.026904
[12:19:44.728] iteration 300: total_loss: 0.670819, loss_sup: 0.613676, loss_mps: 0.023552, loss_cps: 0.033591
[12:19:44.728] Evaluation Started ==>
[12:19:56.010] ==> valid iteration 300: unet metrics: {'dc': 0.2596031050711617, 'jc': 0.17955818633634885, 'pre': 0.3150841932457506, 'hd': 8.368187503574461}, ynet metrics: {'dc': 0.3551680133609905, 'jc': 0.2541651023714361, 'pre': 0.46780317263463506, 'hd': 7.688820141239065}.
[12:19:56.177] ==> New best valid dice for ynet: 0.355168, at iteration 300
[12:19:56.179] Evaluation Finished!⏹️
[12:19:56.332] iteration 301: total_loss: 0.852971, loss_sup: 0.795661, loss_mps: 0.023676, loss_cps: 0.033635
[12:19:56.482] iteration 302: total_loss: 0.741105, loss_sup: 0.683364, loss_mps: 0.023510, loss_cps: 0.034231
[12:19:56.626] iteration 303: total_loss: 0.660832, loss_sup: 0.604507, loss_mps: 0.022979, loss_cps: 0.033346
[12:19:56.769] iteration 304: total_loss: 0.702471, loss_sup: 0.649256, loss_mps: 0.022405, loss_cps: 0.030810
[12:19:56.915] iteration 305: total_loss: 0.763336, loss_sup: 0.707872, loss_mps: 0.022577, loss_cps: 0.032888
[12:19:57.059] iteration 306: total_loss: 0.628760, loss_sup: 0.577176, loss_mps: 0.022023, loss_cps: 0.029561
[12:19:57.203] iteration 307: total_loss: 0.849612, loss_sup: 0.793786, loss_mps: 0.023354, loss_cps: 0.032472
[12:19:57.346] iteration 308: total_loss: 0.642454, loss_sup: 0.594003, loss_mps: 0.021217, loss_cps: 0.027234
[12:19:57.490] iteration 309: total_loss: 0.761568, loss_sup: 0.707817, loss_mps: 0.022718, loss_cps: 0.031032
[12:19:57.633] iteration 310: total_loss: 0.743544, loss_sup: 0.691221, loss_mps: 0.022896, loss_cps: 0.029428
[12:19:57.778] iteration 311: total_loss: 0.508234, loss_sup: 0.459139, loss_mps: 0.021579, loss_cps: 0.027517
[12:19:57.923] iteration 312: total_loss: 0.748353, loss_sup: 0.690631, loss_mps: 0.024324, loss_cps: 0.033399
[12:19:58.067] iteration 313: total_loss: 0.544970, loss_sup: 0.488488, loss_mps: 0.023951, loss_cps: 0.032531
[12:19:58.211] iteration 314: total_loss: 0.786846, loss_sup: 0.733688, loss_mps: 0.022823, loss_cps: 0.030336
[12:19:58.355] iteration 315: total_loss: 0.673475, loss_sup: 0.616957, loss_mps: 0.024322, loss_cps: 0.032196
[12:19:58.499] iteration 316: total_loss: 0.672252, loss_sup: 0.615774, loss_mps: 0.023798, loss_cps: 0.032679
[12:19:58.647] iteration 317: total_loss: 0.603657, loss_sup: 0.544330, loss_mps: 0.024711, loss_cps: 0.034616
[12:19:58.791] iteration 318: total_loss: 0.805762, loss_sup: 0.750293, loss_mps: 0.023560, loss_cps: 0.031909
[12:19:58.935] iteration 319: total_loss: 0.740203, loss_sup: 0.681624, loss_mps: 0.024531, loss_cps: 0.034048
[12:19:59.079] iteration 320: total_loss: 0.697469, loss_sup: 0.644955, loss_mps: 0.022411, loss_cps: 0.030102
[12:19:59.222] iteration 321: total_loss: 0.583992, loss_sup: 0.536240, loss_mps: 0.020842, loss_cps: 0.026910
[12:19:59.366] iteration 322: total_loss: 0.979546, loss_sup: 0.927654, loss_mps: 0.022081, loss_cps: 0.029811
[12:19:59.510] iteration 323: total_loss: 0.580985, loss_sup: 0.532130, loss_mps: 0.020640, loss_cps: 0.028215
[12:19:59.656] iteration 324: total_loss: 0.624937, loss_sup: 0.574589, loss_mps: 0.020773, loss_cps: 0.029575
[12:19:59.800] iteration 325: total_loss: 0.675555, loss_sup: 0.622111, loss_mps: 0.022217, loss_cps: 0.031227
[12:19:59.944] iteration 326: total_loss: 0.847354, loss_sup: 0.795827, loss_mps: 0.021263, loss_cps: 0.030264
[12:20:00.088] iteration 327: total_loss: 0.772821, loss_sup: 0.726381, loss_mps: 0.019472, loss_cps: 0.026968
[12:20:00.233] iteration 328: total_loss: 0.712222, loss_sup: 0.662505, loss_mps: 0.020220, loss_cps: 0.029496
[12:20:00.377] iteration 329: total_loss: 0.656201, loss_sup: 0.609675, loss_mps: 0.019155, loss_cps: 0.027371
[12:20:00.523] iteration 330: total_loss: 0.618575, loss_sup: 0.577014, loss_mps: 0.017279, loss_cps: 0.024283
[12:20:00.667] iteration 331: total_loss: 0.714047, loss_sup: 0.662029, loss_mps: 0.020607, loss_cps: 0.031412
[12:20:00.812] iteration 332: total_loss: 0.732217, loss_sup: 0.681330, loss_mps: 0.020391, loss_cps: 0.030496
[12:20:00.957] iteration 333: total_loss: 0.559301, loss_sup: 0.511597, loss_mps: 0.019360, loss_cps: 0.028344
[12:20:01.101] iteration 334: total_loss: 0.579961, loss_sup: 0.534795, loss_mps: 0.018962, loss_cps: 0.026204
[12:20:01.247] iteration 335: total_loss: 0.609748, loss_sup: 0.570963, loss_mps: 0.017248, loss_cps: 0.021536
[12:20:01.392] iteration 336: total_loss: 1.005311, loss_sup: 0.963023, loss_mps: 0.017687, loss_cps: 0.024601
[12:20:01.537] iteration 337: total_loss: 0.540819, loss_sup: 0.495462, loss_mps: 0.018918, loss_cps: 0.026439
[12:20:01.681] iteration 338: total_loss: 0.524500, loss_sup: 0.474151, loss_mps: 0.020214, loss_cps: 0.030134
[12:20:01.827] iteration 339: total_loss: 0.605909, loss_sup: 0.548791, loss_mps: 0.022372, loss_cps: 0.034745
[12:20:01.973] iteration 340: total_loss: 0.838593, loss_sup: 0.778265, loss_mps: 0.023155, loss_cps: 0.037173
[12:20:02.118] iteration 341: total_loss: 0.784066, loss_sup: 0.727054, loss_mps: 0.021945, loss_cps: 0.035068
[12:20:02.263] iteration 342: total_loss: 0.710507, loss_sup: 0.652745, loss_mps: 0.022601, loss_cps: 0.035162
[12:20:02.410] iteration 343: total_loss: 0.792972, loss_sup: 0.735321, loss_mps: 0.022617, loss_cps: 0.035034
[12:20:02.555] iteration 344: total_loss: 0.532719, loss_sup: 0.476483, loss_mps: 0.022839, loss_cps: 0.033397
[12:20:02.701] iteration 345: total_loss: 0.634891, loss_sup: 0.586459, loss_mps: 0.020465, loss_cps: 0.027968
[12:20:02.845] iteration 346: total_loss: 0.963295, loss_sup: 0.901631, loss_mps: 0.024029, loss_cps: 0.037635
[12:20:02.995] iteration 347: total_loss: 0.681140, loss_sup: 0.629258, loss_mps: 0.021690, loss_cps: 0.030192
[12:20:03.144] iteration 348: total_loss: 0.702915, loss_sup: 0.647985, loss_mps: 0.022802, loss_cps: 0.032128
[12:20:03.288] iteration 349: total_loss: 0.453070, loss_sup: 0.405595, loss_mps: 0.020480, loss_cps: 0.026995
[12:20:03.432] iteration 350: total_loss: 0.563261, loss_sup: 0.511157, loss_mps: 0.021616, loss_cps: 0.030488
[12:20:03.577] iteration 351: total_loss: 0.549459, loss_sup: 0.500685, loss_mps: 0.020824, loss_cps: 0.027949
[12:20:03.726] iteration 352: total_loss: 0.668483, loss_sup: 0.617205, loss_mps: 0.021331, loss_cps: 0.029947
[12:20:03.871] iteration 353: total_loss: 0.557585, loss_sup: 0.510353, loss_mps: 0.019922, loss_cps: 0.027309
[12:20:04.014] iteration 354: total_loss: 1.052655, loss_sup: 1.002521, loss_mps: 0.020258, loss_cps: 0.029875
[12:20:04.158] iteration 355: total_loss: 0.454487, loss_sup: 0.408644, loss_mps: 0.018960, loss_cps: 0.026883
[12:20:04.304] iteration 356: total_loss: 0.793818, loss_sup: 0.745320, loss_mps: 0.019999, loss_cps: 0.028499
[12:20:04.448] iteration 357: total_loss: 0.833288, loss_sup: 0.787423, loss_mps: 0.019309, loss_cps: 0.026556
[12:20:04.592] iteration 358: total_loss: 0.681378, loss_sup: 0.635469, loss_mps: 0.019686, loss_cps: 0.026224
[12:20:04.737] iteration 359: total_loss: 0.512504, loss_sup: 0.464043, loss_mps: 0.020125, loss_cps: 0.028336
[12:20:04.881] iteration 360: total_loss: 0.705330, loss_sup: 0.657868, loss_mps: 0.019440, loss_cps: 0.028022
[12:20:05.026] iteration 361: total_loss: 0.675513, loss_sup: 0.619258, loss_mps: 0.022448, loss_cps: 0.033807
[12:20:05.170] iteration 362: total_loss: 0.453492, loss_sup: 0.395756, loss_mps: 0.022497, loss_cps: 0.035239
[12:20:05.314] iteration 363: total_loss: 0.686544, loss_sup: 0.633080, loss_mps: 0.021207, loss_cps: 0.032257
[12:20:05.458] iteration 364: total_loss: 0.526312, loss_sup: 0.479870, loss_mps: 0.019119, loss_cps: 0.027323
[12:20:05.605] iteration 365: total_loss: 0.748089, loss_sup: 0.692511, loss_mps: 0.021780, loss_cps: 0.033798
[12:20:05.750] iteration 366: total_loss: 0.555118, loss_sup: 0.497747, loss_mps: 0.021700, loss_cps: 0.035672
[12:20:05.895] iteration 367: total_loss: 0.580030, loss_sup: 0.528138, loss_mps: 0.020518, loss_cps: 0.031375
[12:20:06.040] iteration 368: total_loss: 0.708626, loss_sup: 0.657699, loss_mps: 0.020340, loss_cps: 0.030588
[12:20:06.185] iteration 369: total_loss: 0.643312, loss_sup: 0.595390, loss_mps: 0.019465, loss_cps: 0.028456
[12:20:06.330] iteration 370: total_loss: 0.590749, loss_sup: 0.532972, loss_mps: 0.021965, loss_cps: 0.035812
[12:20:06.474] iteration 371: total_loss: 0.483972, loss_sup: 0.426935, loss_mps: 0.021582, loss_cps: 0.035455
[12:20:06.618] iteration 372: total_loss: 0.860598, loss_sup: 0.802388, loss_mps: 0.022059, loss_cps: 0.036152
[12:20:06.763] iteration 373: total_loss: 0.798394, loss_sup: 0.736938, loss_mps: 0.023088, loss_cps: 0.038369
[12:20:06.907] iteration 374: total_loss: 0.635045, loss_sup: 0.584278, loss_mps: 0.020611, loss_cps: 0.030155
[12:20:07.052] iteration 375: total_loss: 0.461224, loss_sup: 0.408323, loss_mps: 0.021846, loss_cps: 0.031055
[12:20:07.196] iteration 376: total_loss: 0.554559, loss_sup: 0.498842, loss_mps: 0.023274, loss_cps: 0.032444
[12:20:07.340] iteration 377: total_loss: 0.786811, loss_sup: 0.734376, loss_mps: 0.022125, loss_cps: 0.030310
[12:20:07.485] iteration 378: total_loss: 0.744653, loss_sup: 0.691233, loss_mps: 0.022559, loss_cps: 0.030861
[12:20:07.629] iteration 379: total_loss: 0.576050, loss_sup: 0.525564, loss_mps: 0.021272, loss_cps: 0.029214
[12:20:07.776] iteration 380: total_loss: 0.554473, loss_sup: 0.503165, loss_mps: 0.021707, loss_cps: 0.029601
[12:20:07.920] iteration 381: total_loss: 0.448447, loss_sup: 0.398891, loss_mps: 0.021541, loss_cps: 0.028015
[12:20:08.064] iteration 382: total_loss: 0.677034, loss_sup: 0.627554, loss_mps: 0.021427, loss_cps: 0.028053
[12:20:08.208] iteration 383: total_loss: 0.485140, loss_sup: 0.435159, loss_mps: 0.020293, loss_cps: 0.029688
[12:20:08.352] iteration 384: total_loss: 0.565461, loss_sup: 0.516716, loss_mps: 0.020380, loss_cps: 0.028365
[12:20:08.496] iteration 385: total_loss: 0.817458, loss_sup: 0.760759, loss_mps: 0.022752, loss_cps: 0.033947
[12:20:08.641] iteration 386: total_loss: 0.618693, loss_sup: 0.572664, loss_mps: 0.019512, loss_cps: 0.026517
[12:20:08.786] iteration 387: total_loss: 0.619779, loss_sup: 0.569807, loss_mps: 0.020773, loss_cps: 0.029199
[12:20:08.930] iteration 388: total_loss: 0.667217, loss_sup: 0.621291, loss_mps: 0.019337, loss_cps: 0.026589
[12:20:09.075] iteration 389: total_loss: 0.818244, loss_sup: 0.765556, loss_mps: 0.021081, loss_cps: 0.031607
[12:20:09.219] iteration 390: total_loss: 0.844011, loss_sup: 0.796175, loss_mps: 0.019660, loss_cps: 0.028176
[12:20:09.363] iteration 391: total_loss: 0.729220, loss_sup: 0.684121, loss_mps: 0.018916, loss_cps: 0.026183
[12:20:09.507] iteration 392: total_loss: 0.515482, loss_sup: 0.469021, loss_mps: 0.019853, loss_cps: 0.026609
[12:20:09.651] iteration 393: total_loss: 0.439739, loss_sup: 0.397542, loss_mps: 0.018436, loss_cps: 0.023760
[12:20:09.795] iteration 394: total_loss: 0.548803, loss_sup: 0.502736, loss_mps: 0.019368, loss_cps: 0.026699
[12:20:09.939] iteration 395: total_loss: 0.479439, loss_sup: 0.438659, loss_mps: 0.018077, loss_cps: 0.022703
[12:20:10.083] iteration 396: total_loss: 0.644233, loss_sup: 0.605162, loss_mps: 0.017162, loss_cps: 0.021910
[12:20:10.229] iteration 397: total_loss: 0.556630, loss_sup: 0.518228, loss_mps: 0.016532, loss_cps: 0.021871
[12:20:10.374] iteration 398: total_loss: 0.634707, loss_sup: 0.594757, loss_mps: 0.017087, loss_cps: 0.022864
[12:20:10.519] iteration 399: total_loss: 0.809659, loss_sup: 0.773250, loss_mps: 0.015568, loss_cps: 0.020841
[12:20:10.664] iteration 400: total_loss: 0.670649, loss_sup: 0.625820, loss_mps: 0.018581, loss_cps: 0.026249
[12:20:10.664] Evaluation Started ==>
[12:20:22.090] ==> valid iteration 400: unet metrics: {'dc': 0.29501142125148055, 'jc': 0.21630284607685865, 'pre': 0.3471888950601097, 'hd': 8.051017593733356}, ynet metrics: {'dc': 0.36184753219834637, 'jc': 0.2666684782161462, 'pre': 0.34119053749586403, 'hd': 9.10212966771725}.
[12:20:22.257] ==> New best valid dice for ynet: 0.361848, at iteration 400
[12:20:22.259] Evaluation Finished!⏹️
[12:20:22.411] iteration 401: total_loss: 0.751323, loss_sup: 0.709403, loss_mps: 0.017827, loss_cps: 0.024094
[12:20:22.558] iteration 402: total_loss: 0.528143, loss_sup: 0.478665, loss_mps: 0.020460, loss_cps: 0.029019
[12:20:22.704] iteration 403: total_loss: 0.563676, loss_sup: 0.512656, loss_mps: 0.021217, loss_cps: 0.029803
[12:20:22.849] iteration 404: total_loss: 0.836646, loss_sup: 0.786762, loss_mps: 0.020955, loss_cps: 0.028929
[12:20:22.992] iteration 405: total_loss: 0.480830, loss_sup: 0.427318, loss_mps: 0.022414, loss_cps: 0.031099
[12:20:23.135] iteration 406: total_loss: 0.831441, loss_sup: 0.767396, loss_mps: 0.024521, loss_cps: 0.039524
[12:20:23.279] iteration 407: total_loss: 0.835323, loss_sup: 0.774434, loss_mps: 0.024431, loss_cps: 0.036458
[12:20:23.422] iteration 408: total_loss: 0.584807, loss_sup: 0.534429, loss_mps: 0.021053, loss_cps: 0.029325
[12:20:23.566] iteration 409: total_loss: 0.702184, loss_sup: 0.646118, loss_mps: 0.022144, loss_cps: 0.033922
[12:20:23.709] iteration 410: total_loss: 0.725705, loss_sup: 0.670639, loss_mps: 0.022495, loss_cps: 0.032571
[12:20:23.852] iteration 411: total_loss: 0.607898, loss_sup: 0.563344, loss_mps: 0.018651, loss_cps: 0.025903
[12:20:23.996] iteration 412: total_loss: 0.480528, loss_sup: 0.425879, loss_mps: 0.021511, loss_cps: 0.033138
[12:20:24.139] iteration 413: total_loss: 1.073359, loss_sup: 1.022889, loss_mps: 0.019996, loss_cps: 0.030473
[12:20:24.284] iteration 414: total_loss: 0.877236, loss_sup: 0.826847, loss_mps: 0.020470, loss_cps: 0.029918
[12:20:24.427] iteration 415: total_loss: 0.621632, loss_sup: 0.570553, loss_mps: 0.020910, loss_cps: 0.030169
[12:20:24.571] iteration 416: total_loss: 0.566757, loss_sup: 0.518717, loss_mps: 0.020011, loss_cps: 0.028028
[12:20:24.714] iteration 417: total_loss: 0.632337, loss_sup: 0.580942, loss_mps: 0.021429, loss_cps: 0.029966
[12:20:24.787] iteration 418: total_loss: 0.451427, loss_sup: 0.404346, loss_mps: 0.020485, loss_cps: 0.026596
[12:20:25.948] iteration 419: total_loss: 0.660106, loss_sup: 0.617662, loss_mps: 0.018622, loss_cps: 0.023822
[12:20:26.094] iteration 420: total_loss: 0.817329, loss_sup: 0.771150, loss_mps: 0.020010, loss_cps: 0.026169
[12:20:26.239] iteration 421: total_loss: 0.759652, loss_sup: 0.713813, loss_mps: 0.019745, loss_cps: 0.026095
[12:20:26.383] iteration 422: total_loss: 0.669538, loss_sup: 0.616885, loss_mps: 0.021864, loss_cps: 0.030788
[12:20:26.529] iteration 423: total_loss: 0.649260, loss_sup: 0.595616, loss_mps: 0.022733, loss_cps: 0.030911
[12:20:26.672] iteration 424: total_loss: 0.714851, loss_sup: 0.661865, loss_mps: 0.022070, loss_cps: 0.030917
[12:20:26.816] iteration 425: total_loss: 0.631661, loss_sup: 0.574830, loss_mps: 0.023115, loss_cps: 0.033717
[12:20:26.961] iteration 426: total_loss: 0.671614, loss_sup: 0.612106, loss_mps: 0.024397, loss_cps: 0.035111
[12:20:27.105] iteration 427: total_loss: 0.582023, loss_sup: 0.537584, loss_mps: 0.019842, loss_cps: 0.024597
[12:20:27.249] iteration 428: total_loss: 0.760156, loss_sup: 0.710331, loss_mps: 0.021348, loss_cps: 0.028477
[12:20:27.393] iteration 429: total_loss: 0.533335, loss_sup: 0.479662, loss_mps: 0.022537, loss_cps: 0.031136
[12:20:27.537] iteration 430: total_loss: 0.418149, loss_sup: 0.368223, loss_mps: 0.021227, loss_cps: 0.028700
[12:20:27.682] iteration 431: total_loss: 0.623146, loss_sup: 0.572324, loss_mps: 0.021136, loss_cps: 0.029686
[12:20:27.826] iteration 432: total_loss: 0.751402, loss_sup: 0.698129, loss_mps: 0.021517, loss_cps: 0.031756
[12:20:27.970] iteration 433: total_loss: 0.639527, loss_sup: 0.589830, loss_mps: 0.020648, loss_cps: 0.029050
[12:20:28.114] iteration 434: total_loss: 0.544726, loss_sup: 0.498726, loss_mps: 0.019325, loss_cps: 0.026675
[12:20:28.259] iteration 435: total_loss: 0.618944, loss_sup: 0.573388, loss_mps: 0.018707, loss_cps: 0.026850
[12:20:28.403] iteration 436: total_loss: 0.707680, loss_sup: 0.665491, loss_mps: 0.017452, loss_cps: 0.024738
[12:20:28.548] iteration 437: total_loss: 0.668224, loss_sup: 0.616092, loss_mps: 0.020846, loss_cps: 0.031286
[12:20:28.692] iteration 438: total_loss: 0.761786, loss_sup: 0.706144, loss_mps: 0.021614, loss_cps: 0.034028
[12:20:28.837] iteration 439: total_loss: 0.578442, loss_sup: 0.527963, loss_mps: 0.020781, loss_cps: 0.029698
[12:20:28.983] iteration 440: total_loss: 0.736415, loss_sup: 0.677152, loss_mps: 0.023337, loss_cps: 0.035926
[12:20:29.128] iteration 441: total_loss: 0.536778, loss_sup: 0.488572, loss_mps: 0.019530, loss_cps: 0.028676
[12:20:29.272] iteration 442: total_loss: 0.555172, loss_sup: 0.502279, loss_mps: 0.021858, loss_cps: 0.031035
[12:20:29.421] iteration 443: total_loss: 0.610540, loss_sup: 0.556628, loss_mps: 0.021618, loss_cps: 0.032294
[12:20:29.567] iteration 444: total_loss: 0.554733, loss_sup: 0.507612, loss_mps: 0.019362, loss_cps: 0.027759
[12:20:29.711] iteration 445: total_loss: 0.558151, loss_sup: 0.514353, loss_mps: 0.018179, loss_cps: 0.025619
[12:20:29.856] iteration 446: total_loss: 0.697716, loss_sup: 0.652822, loss_mps: 0.018866, loss_cps: 0.026028
[12:20:30.001] iteration 447: total_loss: 0.468477, loss_sup: 0.422580, loss_mps: 0.018837, loss_cps: 0.027060
[12:20:30.147] iteration 448: total_loss: 0.565140, loss_sup: 0.523921, loss_mps: 0.017302, loss_cps: 0.023917
[12:20:30.296] iteration 449: total_loss: 0.748661, loss_sup: 0.709042, loss_mps: 0.017130, loss_cps: 0.022489
[12:20:30.440] iteration 450: total_loss: 0.607030, loss_sup: 0.559136, loss_mps: 0.020154, loss_cps: 0.027740
[12:20:30.585] iteration 451: total_loss: 0.578166, loss_sup: 0.531546, loss_mps: 0.019197, loss_cps: 0.027423
[12:20:30.729] iteration 452: total_loss: 0.491550, loss_sup: 0.450533, loss_mps: 0.016911, loss_cps: 0.024107
[12:20:30.875] iteration 453: total_loss: 0.650508, loss_sup: 0.602881, loss_mps: 0.019664, loss_cps: 0.027963
[12:20:31.023] iteration 454: total_loss: 0.641929, loss_sup: 0.599008, loss_mps: 0.017865, loss_cps: 0.025056
[12:20:31.169] iteration 455: total_loss: 0.865828, loss_sup: 0.814392, loss_mps: 0.020603, loss_cps: 0.030834
[12:20:31.313] iteration 456: total_loss: 0.689531, loss_sup: 0.644883, loss_mps: 0.018757, loss_cps: 0.025890
[12:20:31.457] iteration 457: total_loss: 0.456191, loss_sup: 0.409409, loss_mps: 0.019333, loss_cps: 0.027449
[12:20:31.602] iteration 458: total_loss: 0.667967, loss_sup: 0.618686, loss_mps: 0.020105, loss_cps: 0.029176
[12:20:31.746] iteration 459: total_loss: 0.572232, loss_sup: 0.524848, loss_mps: 0.019199, loss_cps: 0.028185
[12:20:31.893] iteration 460: total_loss: 0.544949, loss_sup: 0.492146, loss_mps: 0.020517, loss_cps: 0.032287
[12:20:32.037] iteration 461: total_loss: 0.780052, loss_sup: 0.731166, loss_mps: 0.019673, loss_cps: 0.029213
[12:20:32.182] iteration 462: total_loss: 0.690163, loss_sup: 0.640322, loss_mps: 0.020411, loss_cps: 0.029430
[12:20:32.326] iteration 463: total_loss: 0.791444, loss_sup: 0.738651, loss_mps: 0.021355, loss_cps: 0.031438
[12:20:32.469] iteration 464: total_loss: 0.751418, loss_sup: 0.701434, loss_mps: 0.021195, loss_cps: 0.028788
[12:20:32.616] iteration 465: total_loss: 0.731366, loss_sup: 0.681132, loss_mps: 0.022054, loss_cps: 0.028180
[12:20:32.760] iteration 466: total_loss: 0.604915, loss_sup: 0.552575, loss_mps: 0.022039, loss_cps: 0.030300
[12:20:32.904] iteration 467: total_loss: 0.677023, loss_sup: 0.625436, loss_mps: 0.022625, loss_cps: 0.028962
[12:20:33.048] iteration 468: total_loss: 0.534173, loss_sup: 0.484569, loss_mps: 0.021697, loss_cps: 0.027906
[12:20:33.192] iteration 469: total_loss: 0.591431, loss_sup: 0.537218, loss_mps: 0.023081, loss_cps: 0.031132
[12:20:33.336] iteration 470: total_loss: 0.729709, loss_sup: 0.679672, loss_mps: 0.022861, loss_cps: 0.027176
[12:20:33.480] iteration 471: total_loss: 0.418954, loss_sup: 0.374054, loss_mps: 0.019710, loss_cps: 0.025190
[12:20:33.624] iteration 472: total_loss: 0.511138, loss_sup: 0.460290, loss_mps: 0.021511, loss_cps: 0.029336
[12:20:33.768] iteration 473: total_loss: 0.476668, loss_sup: 0.435235, loss_mps: 0.018345, loss_cps: 0.023088
[12:20:33.914] iteration 474: total_loss: 0.687667, loss_sup: 0.642145, loss_mps: 0.019786, loss_cps: 0.025735
[12:20:34.059] iteration 475: total_loss: 0.566052, loss_sup: 0.518655, loss_mps: 0.019849, loss_cps: 0.027548
[12:20:34.204] iteration 476: total_loss: 0.679172, loss_sup: 0.632242, loss_mps: 0.019271, loss_cps: 0.027660
[12:20:34.349] iteration 477: total_loss: 0.585233, loss_sup: 0.533890, loss_mps: 0.020191, loss_cps: 0.031152
[12:20:34.493] iteration 478: total_loss: 0.538307, loss_sup: 0.496267, loss_mps: 0.017719, loss_cps: 0.024321
[12:20:34.638] iteration 479: total_loss: 0.490296, loss_sup: 0.436308, loss_mps: 0.021537, loss_cps: 0.032452
[12:20:34.782] iteration 480: total_loss: 0.520346, loss_sup: 0.471211, loss_mps: 0.019819, loss_cps: 0.029315
[12:20:34.926] iteration 481: total_loss: 0.702517, loss_sup: 0.656463, loss_mps: 0.018676, loss_cps: 0.027378
[12:20:35.071] iteration 482: total_loss: 0.565364, loss_sup: 0.515720, loss_mps: 0.019142, loss_cps: 0.030502
[12:20:35.216] iteration 483: total_loss: 0.383553, loss_sup: 0.333539, loss_mps: 0.019513, loss_cps: 0.030502
[12:20:35.361] iteration 484: total_loss: 0.749715, loss_sup: 0.696900, loss_mps: 0.019855, loss_cps: 0.032960
[12:20:35.506] iteration 485: total_loss: 0.515045, loss_sup: 0.460755, loss_mps: 0.020421, loss_cps: 0.033868
[12:20:35.651] iteration 486: total_loss: 0.397515, loss_sup: 0.347102, loss_mps: 0.019219, loss_cps: 0.031194
[12:20:35.798] iteration 487: total_loss: 0.399507, loss_sup: 0.345250, loss_mps: 0.020681, loss_cps: 0.033576
[12:20:35.942] iteration 488: total_loss: 0.695464, loss_sup: 0.642227, loss_mps: 0.020344, loss_cps: 0.032894
[12:20:36.087] iteration 489: total_loss: 0.501123, loss_sup: 0.449181, loss_mps: 0.019267, loss_cps: 0.032675
[12:20:36.236] iteration 490: total_loss: 0.425665, loss_sup: 0.376500, loss_mps: 0.018910, loss_cps: 0.030255
[12:20:36.382] iteration 491: total_loss: 0.554509, loss_sup: 0.507060, loss_mps: 0.018209, loss_cps: 0.029240
[12:20:36.526] iteration 492: total_loss: 0.442759, loss_sup: 0.398918, loss_mps: 0.017179, loss_cps: 0.026661
[12:20:36.671] iteration 493: total_loss: 0.396118, loss_sup: 0.345678, loss_mps: 0.019313, loss_cps: 0.031127
[12:20:36.818] iteration 494: total_loss: 0.791949, loss_sup: 0.751086, loss_mps: 0.016502, loss_cps: 0.024361
[12:20:36.963] iteration 495: total_loss: 0.773254, loss_sup: 0.721209, loss_mps: 0.019455, loss_cps: 0.032590
[12:20:37.107] iteration 496: total_loss: 0.788949, loss_sup: 0.736162, loss_mps: 0.020353, loss_cps: 0.032434
[12:20:37.252] iteration 497: total_loss: 0.346477, loss_sup: 0.302124, loss_mps: 0.017461, loss_cps: 0.026892
[12:20:37.398] iteration 498: total_loss: 0.626285, loss_sup: 0.581581, loss_mps: 0.018030, loss_cps: 0.026673
[12:20:37.546] iteration 499: total_loss: 0.636819, loss_sup: 0.591631, loss_mps: 0.018214, loss_cps: 0.026974
[12:20:37.691] iteration 500: total_loss: 0.461171, loss_sup: 0.417550, loss_mps: 0.018105, loss_cps: 0.025516
[12:20:37.691] Evaluation Started ==>
[12:20:49.033] ==> valid iteration 500: unet metrics: {'dc': 0.35636906559513715, 'jc': 0.26259822513535397, 'pre': 0.3816000866894269, 'hd': 8.329802584462731}, ynet metrics: {'dc': 0.4060041441602509, 'jc': 0.30015778565176104, 'pre': 0.3759074461769601, 'hd': 9.057194733081388}.
[12:20:49.094] ==> New best valid dice for unet: 0.356369, at iteration 500
[12:20:49.260] ==> New best valid dice for ynet: 0.406004, at iteration 500
[12:20:49.262] Evaluation Finished!⏹️
[12:20:49.418] iteration 501: total_loss: 0.649076, loss_sup: 0.600415, loss_mps: 0.019514, loss_cps: 0.029147
[12:20:49.564] iteration 502: total_loss: 0.561984, loss_sup: 0.516551, loss_mps: 0.018665, loss_cps: 0.026769
[12:20:49.708] iteration 503: total_loss: 0.532449, loss_sup: 0.485447, loss_mps: 0.019569, loss_cps: 0.027433
[12:20:49.852] iteration 504: total_loss: 0.429097, loss_sup: 0.383737, loss_mps: 0.019425, loss_cps: 0.025935
[12:20:49.996] iteration 505: total_loss: 0.932025, loss_sup: 0.877727, loss_mps: 0.021700, loss_cps: 0.032598
[12:20:50.141] iteration 506: total_loss: 0.615715, loss_sup: 0.565107, loss_mps: 0.019773, loss_cps: 0.030835
[12:20:50.287] iteration 507: total_loss: 0.491741, loss_sup: 0.442794, loss_mps: 0.019607, loss_cps: 0.029340
[12:20:50.432] iteration 508: total_loss: 0.572573, loss_sup: 0.511315, loss_mps: 0.024053, loss_cps: 0.037205
[12:20:50.576] iteration 509: total_loss: 0.477737, loss_sup: 0.430837, loss_mps: 0.019391, loss_cps: 0.027508
[12:20:50.721] iteration 510: total_loss: 0.560039, loss_sup: 0.505654, loss_mps: 0.021443, loss_cps: 0.032941
[12:20:50.867] iteration 511: total_loss: 0.624223, loss_sup: 0.574019, loss_mps: 0.019927, loss_cps: 0.030277
[12:20:51.011] iteration 512: total_loss: 0.568525, loss_sup: 0.518489, loss_mps: 0.019807, loss_cps: 0.030229
[12:20:51.156] iteration 513: total_loss: 0.740472, loss_sup: 0.691528, loss_mps: 0.019680, loss_cps: 0.029264
[12:20:51.300] iteration 514: total_loss: 0.571518, loss_sup: 0.522591, loss_mps: 0.019577, loss_cps: 0.029351
[12:20:51.445] iteration 515: total_loss: 0.776193, loss_sup: 0.716339, loss_mps: 0.022552, loss_cps: 0.037302
[12:20:51.589] iteration 516: total_loss: 0.403985, loss_sup: 0.354668, loss_mps: 0.020034, loss_cps: 0.029284
[12:20:51.734] iteration 517: total_loss: 0.487747, loss_sup: 0.438665, loss_mps: 0.019566, loss_cps: 0.029515
[12:20:51.879] iteration 518: total_loss: 0.646646, loss_sup: 0.600934, loss_mps: 0.018688, loss_cps: 0.027024
[12:20:52.024] iteration 519: total_loss: 0.596985, loss_sup: 0.557974, loss_mps: 0.016611, loss_cps: 0.022400
[12:20:52.170] iteration 520: total_loss: 0.707865, loss_sup: 0.663038, loss_mps: 0.018278, loss_cps: 0.026549
[12:20:52.316] iteration 521: total_loss: 1.034845, loss_sup: 0.976441, loss_mps: 0.021727, loss_cps: 0.036677
[12:20:52.460] iteration 522: total_loss: 0.784485, loss_sup: 0.735306, loss_mps: 0.019547, loss_cps: 0.029632
[12:20:52.604] iteration 523: total_loss: 0.655936, loss_sup: 0.607184, loss_mps: 0.019997, loss_cps: 0.028755
[12:20:52.749] iteration 524: total_loss: 0.778612, loss_sup: 0.725288, loss_mps: 0.021686, loss_cps: 0.031639
[12:20:52.893] iteration 525: total_loss: 0.745093, loss_sup: 0.688112, loss_mps: 0.022869, loss_cps: 0.034113
[12:20:53.039] iteration 526: total_loss: 0.470560, loss_sup: 0.417244, loss_mps: 0.022020, loss_cps: 0.031296
[12:20:53.183] iteration 527: total_loss: 0.523023, loss_sup: 0.471117, loss_mps: 0.022290, loss_cps: 0.029616
[12:20:53.327] iteration 528: total_loss: 0.743044, loss_sup: 0.684694, loss_mps: 0.023998, loss_cps: 0.034352
[12:20:53.472] iteration 529: total_loss: 0.736988, loss_sup: 0.673619, loss_mps: 0.025600, loss_cps: 0.037769
[12:20:53.617] iteration 530: total_loss: 0.517765, loss_sup: 0.459464, loss_mps: 0.024023, loss_cps: 0.034278
[12:20:53.762] iteration 531: total_loss: 0.824704, loss_sup: 0.766681, loss_mps: 0.024435, loss_cps: 0.033588
[12:20:53.912] iteration 532: total_loss: 0.468332, loss_sup: 0.410704, loss_mps: 0.024540, loss_cps: 0.033089
[12:20:54.057] iteration 533: total_loss: 0.511431, loss_sup: 0.452793, loss_mps: 0.024647, loss_cps: 0.033991
[12:20:54.202] iteration 534: total_loss: 0.848673, loss_sup: 0.798234, loss_mps: 0.022089, loss_cps: 0.028350
[12:20:54.348] iteration 535: total_loss: 0.448716, loss_sup: 0.394509, loss_mps: 0.023346, loss_cps: 0.030860
[12:20:54.493] iteration 536: total_loss: 0.642654, loss_sup: 0.595840, loss_mps: 0.020476, loss_cps: 0.026339
[12:20:54.640] iteration 537: total_loss: 0.483959, loss_sup: 0.432170, loss_mps: 0.022356, loss_cps: 0.029433
[12:20:54.791] iteration 538: total_loss: 0.856091, loss_sup: 0.804231, loss_mps: 0.022471, loss_cps: 0.029389
[12:20:54.936] iteration 539: total_loss: 0.554037, loss_sup: 0.506648, loss_mps: 0.020875, loss_cps: 0.026514
[12:20:55.080] iteration 540: total_loss: 0.716195, loss_sup: 0.664636, loss_mps: 0.021649, loss_cps: 0.029910
[12:20:55.225] iteration 541: total_loss: 1.018668, loss_sup: 0.967562, loss_mps: 0.021618, loss_cps: 0.029488
[12:20:55.370] iteration 542: total_loss: 0.585841, loss_sup: 0.541003, loss_mps: 0.019668, loss_cps: 0.025171
[12:20:55.515] iteration 543: total_loss: 0.895424, loss_sup: 0.846239, loss_mps: 0.021539, loss_cps: 0.027645
[12:20:55.659] iteration 544: total_loss: 0.596631, loss_sup: 0.547332, loss_mps: 0.021515, loss_cps: 0.027784
[12:20:55.804] iteration 545: total_loss: 0.616323, loss_sup: 0.567258, loss_mps: 0.021592, loss_cps: 0.027473
[12:20:55.949] iteration 546: total_loss: 0.862331, loss_sup: 0.808776, loss_mps: 0.023404, loss_cps: 0.030152
[12:20:56.094] iteration 547: total_loss: 0.666399, loss_sup: 0.613347, loss_mps: 0.023093, loss_cps: 0.029959
[12:20:56.238] iteration 548: total_loss: 0.713654, loss_sup: 0.668898, loss_mps: 0.020288, loss_cps: 0.024467
[12:20:56.384] iteration 549: total_loss: 0.753970, loss_sup: 0.702155, loss_mps: 0.023055, loss_cps: 0.028760
[12:20:56.532] iteration 550: total_loss: 0.708785, loss_sup: 0.653880, loss_mps: 0.023775, loss_cps: 0.031130
[12:20:56.677] iteration 551: total_loss: 0.654075, loss_sup: 0.595218, loss_mps: 0.024982, loss_cps: 0.033875
[12:20:56.821] iteration 552: total_loss: 0.660063, loss_sup: 0.606527, loss_mps: 0.023432, loss_cps: 0.030104
[12:20:56.970] iteration 553: total_loss: 0.510512, loss_sup: 0.453033, loss_mps: 0.024855, loss_cps: 0.032624
[12:20:57.116] iteration 554: total_loss: 0.518375, loss_sup: 0.461338, loss_mps: 0.024326, loss_cps: 0.032711
[12:20:57.261] iteration 555: total_loss: 0.579775, loss_sup: 0.526239, loss_mps: 0.023067, loss_cps: 0.030469
[12:20:57.406] iteration 556: total_loss: 0.525814, loss_sup: 0.473858, loss_mps: 0.022304, loss_cps: 0.029653
[12:20:57.551] iteration 557: total_loss: 0.833417, loss_sup: 0.783228, loss_mps: 0.021124, loss_cps: 0.029065
[12:20:57.695] iteration 558: total_loss: 0.678038, loss_sup: 0.625479, loss_mps: 0.021761, loss_cps: 0.030799
[12:20:57.840] iteration 559: total_loss: 0.644606, loss_sup: 0.595232, loss_mps: 0.019872, loss_cps: 0.029502
[12:20:57.984] iteration 560: total_loss: 0.399500, loss_sup: 0.354504, loss_mps: 0.018813, loss_cps: 0.026183
[12:20:58.129] iteration 561: total_loss: 0.561461, loss_sup: 0.515906, loss_mps: 0.019027, loss_cps: 0.026528
[12:20:58.277] iteration 562: total_loss: 0.525814, loss_sup: 0.480552, loss_mps: 0.018926, loss_cps: 0.026335
[12:20:58.422] iteration 563: total_loss: 0.817985, loss_sup: 0.767384, loss_mps: 0.019991, loss_cps: 0.030610
[12:20:58.567] iteration 564: total_loss: 0.590813, loss_sup: 0.549465, loss_mps: 0.017634, loss_cps: 0.023714
[12:20:58.712] iteration 565: total_loss: 0.578755, loss_sup: 0.532366, loss_mps: 0.018735, loss_cps: 0.027654
[12:20:58.863] iteration 566: total_loss: 0.558934, loss_sup: 0.520199, loss_mps: 0.016880, loss_cps: 0.021854
[12:20:59.012] iteration 567: total_loss: 0.632491, loss_sup: 0.585566, loss_mps: 0.018691, loss_cps: 0.028234
[12:20:59.157] iteration 568: total_loss: 0.434082, loss_sup: 0.389639, loss_mps: 0.018635, loss_cps: 0.025808
[12:20:59.303] iteration 569: total_loss: 0.449715, loss_sup: 0.404278, loss_mps: 0.018985, loss_cps: 0.026453
[12:20:59.448] iteration 570: total_loss: 0.870986, loss_sup: 0.827616, loss_mps: 0.018488, loss_cps: 0.024883
[12:20:59.594] iteration 571: total_loss: 0.665925, loss_sup: 0.620147, loss_mps: 0.019665, loss_cps: 0.026113
[12:20:59.739] iteration 572: total_loss: 0.530339, loss_sup: 0.477088, loss_mps: 0.021439, loss_cps: 0.031812
[12:20:59.883] iteration 573: total_loss: 0.624266, loss_sup: 0.570982, loss_mps: 0.021721, loss_cps: 0.031562
[12:21:00.030] iteration 574: total_loss: 0.567000, loss_sup: 0.518288, loss_mps: 0.020604, loss_cps: 0.028108
[12:21:00.175] iteration 575: total_loss: 0.656202, loss_sup: 0.604318, loss_mps: 0.021416, loss_cps: 0.030469
[12:21:00.319] iteration 576: total_loss: 0.484269, loss_sup: 0.436972, loss_mps: 0.020447, loss_cps: 0.026851
[12:21:00.464] iteration 577: total_loss: 0.677531, loss_sup: 0.617155, loss_mps: 0.024230, loss_cps: 0.036147
[12:21:00.608] iteration 578: total_loss: 0.398683, loss_sup: 0.349063, loss_mps: 0.020812, loss_cps: 0.028807
[12:21:00.753] iteration 579: total_loss: 0.516124, loss_sup: 0.460344, loss_mps: 0.022481, loss_cps: 0.033299
[12:21:00.899] iteration 580: total_loss: 0.691674, loss_sup: 0.641529, loss_mps: 0.020531, loss_cps: 0.029615
[12:21:01.045] iteration 581: total_loss: 0.667595, loss_sup: 0.609631, loss_mps: 0.022902, loss_cps: 0.035062
[12:21:01.190] iteration 582: total_loss: 0.718864, loss_sup: 0.665680, loss_mps: 0.021486, loss_cps: 0.031698
[12:21:01.334] iteration 583: total_loss: 0.488526, loss_sup: 0.428000, loss_mps: 0.023064, loss_cps: 0.037462
[12:21:01.479] iteration 584: total_loss: 0.608177, loss_sup: 0.549262, loss_mps: 0.022342, loss_cps: 0.036573
[12:21:01.624] iteration 585: total_loss: 0.717136, loss_sup: 0.670346, loss_mps: 0.019288, loss_cps: 0.027502
[12:21:01.771] iteration 586: total_loss: 0.517092, loss_sup: 0.458146, loss_mps: 0.022776, loss_cps: 0.036170
[12:21:01.917] iteration 587: total_loss: 0.686119, loss_sup: 0.632822, loss_mps: 0.021246, loss_cps: 0.032051
[12:21:02.064] iteration 588: total_loss: 0.574265, loss_sup: 0.523974, loss_mps: 0.020489, loss_cps: 0.029802
[12:21:02.209] iteration 589: total_loss: 0.681073, loss_sup: 0.630576, loss_mps: 0.020631, loss_cps: 0.029866
[12:21:02.354] iteration 590: total_loss: 0.602648, loss_sup: 0.547437, loss_mps: 0.022155, loss_cps: 0.033055
[12:21:02.499] iteration 591: total_loss: 0.716810, loss_sup: 0.659541, loss_mps: 0.022988, loss_cps: 0.034280
[12:21:02.644] iteration 592: total_loss: 0.700854, loss_sup: 0.645756, loss_mps: 0.022175, loss_cps: 0.032923
[12:21:02.789] iteration 593: total_loss: 0.367865, loss_sup: 0.305790, loss_mps: 0.024105, loss_cps: 0.037970
[12:21:02.936] iteration 594: total_loss: 0.777099, loss_sup: 0.720791, loss_mps: 0.022825, loss_cps: 0.033483
[12:21:03.081] iteration 595: total_loss: 0.491920, loss_sup: 0.435584, loss_mps: 0.022996, loss_cps: 0.033340
[12:21:03.226] iteration 596: total_loss: 0.608337, loss_sup: 0.548339, loss_mps: 0.024612, loss_cps: 0.035385
[12:21:03.371] iteration 597: total_loss: 0.710937, loss_sup: 0.651587, loss_mps: 0.024399, loss_cps: 0.034951
[12:21:03.516] iteration 598: total_loss: 0.549646, loss_sup: 0.491409, loss_mps: 0.023418, loss_cps: 0.034820
[12:21:03.662] iteration 599: total_loss: 0.395005, loss_sup: 0.340603, loss_mps: 0.022305, loss_cps: 0.032098
[12:21:03.807] iteration 600: total_loss: 0.609777, loss_sup: 0.556775, loss_mps: 0.021060, loss_cps: 0.031941
[12:21:03.807] Evaluation Started ==>
[12:21:15.146] ==> valid iteration 600: unet metrics: {'dc': 0.33433615483526313, 'jc': 0.24285991646360364, 'pre': 0.5200710729116013, 'hd': 7.128841372626419}, ynet metrics: {'dc': 0.39402130385172013, 'jc': 0.2832379571658808, 'pre': 0.5062831409378502, 'hd': 7.7748599481243374}.
[12:21:15.149] Evaluation Finished!⏹️
[12:21:15.304] iteration 601: total_loss: 0.646575, loss_sup: 0.595795, loss_mps: 0.020439, loss_cps: 0.030340
[12:21:15.455] iteration 602: total_loss: 0.737800, loss_sup: 0.683854, loss_mps: 0.021378, loss_cps: 0.032569
[12:21:15.600] iteration 603: total_loss: 0.866876, loss_sup: 0.823333, loss_mps: 0.018393, loss_cps: 0.025149
[12:21:15.746] iteration 604: total_loss: 0.850801, loss_sup: 0.798381, loss_mps: 0.020647, loss_cps: 0.031774
[12:21:15.891] iteration 605: total_loss: 0.453482, loss_sup: 0.406097, loss_mps: 0.018963, loss_cps: 0.028423
[12:21:16.035] iteration 606: total_loss: 0.541107, loss_sup: 0.492787, loss_mps: 0.019272, loss_cps: 0.029048
[12:21:16.179] iteration 607: total_loss: 0.512285, loss_sup: 0.464259, loss_mps: 0.019278, loss_cps: 0.028748
[12:21:16.324] iteration 608: total_loss: 0.552191, loss_sup: 0.497927, loss_mps: 0.021181, loss_cps: 0.033083
[12:21:16.471] iteration 609: total_loss: 0.434702, loss_sup: 0.383921, loss_mps: 0.020488, loss_cps: 0.030293
[12:21:16.616] iteration 610: total_loss: 0.881931, loss_sup: 0.833166, loss_mps: 0.019355, loss_cps: 0.029410
[12:21:16.761] iteration 611: total_loss: 0.541235, loss_sup: 0.490711, loss_mps: 0.021049, loss_cps: 0.029475
[12:21:16.908] iteration 612: total_loss: 0.764218, loss_sup: 0.704132, loss_mps: 0.023329, loss_cps: 0.036757
[12:21:17.053] iteration 613: total_loss: 0.633421, loss_sup: 0.573713, loss_mps: 0.022736, loss_cps: 0.036972
[12:21:17.198] iteration 614: total_loss: 0.542011, loss_sup: 0.480992, loss_mps: 0.023474, loss_cps: 0.037545
[12:21:17.343] iteration 615: total_loss: 0.651502, loss_sup: 0.592584, loss_mps: 0.022968, loss_cps: 0.035950
[12:21:17.488] iteration 616: total_loss: 0.712290, loss_sup: 0.646129, loss_mps: 0.025340, loss_cps: 0.040821
[12:21:17.638] iteration 617: total_loss: 0.483097, loss_sup: 0.417736, loss_mps: 0.025295, loss_cps: 0.040067
[12:21:17.783] iteration 618: total_loss: 0.457135, loss_sup: 0.399192, loss_mps: 0.023573, loss_cps: 0.034370
[12:21:17.928] iteration 619: total_loss: 0.449350, loss_sup: 0.393389, loss_mps: 0.023067, loss_cps: 0.032894
[12:21:18.074] iteration 620: total_loss: 0.797648, loss_sup: 0.746000, loss_mps: 0.021392, loss_cps: 0.030257
[12:21:18.219] iteration 621: total_loss: 0.644135, loss_sup: 0.594368, loss_mps: 0.020837, loss_cps: 0.028929
[12:21:18.365] iteration 622: total_loss: 0.542332, loss_sup: 0.487691, loss_mps: 0.022310, loss_cps: 0.032331
[12:21:18.513] iteration 623: total_loss: 0.476578, loss_sup: 0.424351, loss_mps: 0.021468, loss_cps: 0.030760
[12:21:18.658] iteration 624: total_loss: 0.605976, loss_sup: 0.558118, loss_mps: 0.020544, loss_cps: 0.027313
[12:21:18.802] iteration 625: total_loss: 0.499768, loss_sup: 0.444719, loss_mps: 0.021455, loss_cps: 0.033594
[12:21:18.947] iteration 626: total_loss: 0.508242, loss_sup: 0.449292, loss_mps: 0.022968, loss_cps: 0.035982
[12:21:19.093] iteration 627: total_loss: 0.473408, loss_sup: 0.422000, loss_mps: 0.020386, loss_cps: 0.031022
[12:21:19.239] iteration 628: total_loss: 0.605870, loss_sup: 0.550887, loss_mps: 0.021270, loss_cps: 0.033713
[12:21:19.384] iteration 629: total_loss: 0.721813, loss_sup: 0.675129, loss_mps: 0.018754, loss_cps: 0.027929
[12:21:19.530] iteration 630: total_loss: 0.552314, loss_sup: 0.496862, loss_mps: 0.021372, loss_cps: 0.034079
[12:21:19.675] iteration 631: total_loss: 0.443795, loss_sup: 0.390480, loss_mps: 0.020601, loss_cps: 0.032714
[12:21:19.821] iteration 632: total_loss: 0.512433, loss_sup: 0.466654, loss_mps: 0.018030, loss_cps: 0.027749
[12:21:19.966] iteration 633: total_loss: 0.471666, loss_sup: 0.421229, loss_mps: 0.019766, loss_cps: 0.030671
[12:21:20.111] iteration 634: total_loss: 0.321280, loss_sup: 0.275225, loss_mps: 0.018028, loss_cps: 0.028027
[12:21:20.255] iteration 635: total_loss: 0.480228, loss_sup: 0.428119, loss_mps: 0.019689, loss_cps: 0.032419
[12:21:20.403] iteration 636: total_loss: 0.530442, loss_sup: 0.474965, loss_mps: 0.020971, loss_cps: 0.034506
[12:21:20.547] iteration 637: total_loss: 0.697861, loss_sup: 0.648000, loss_mps: 0.018776, loss_cps: 0.031085
[12:21:20.692] iteration 638: total_loss: 0.529588, loss_sup: 0.472114, loss_mps: 0.021067, loss_cps: 0.036407
[12:21:20.837] iteration 639: total_loss: 0.528836, loss_sup: 0.479654, loss_mps: 0.018418, loss_cps: 0.030764
[12:21:20.982] iteration 640: total_loss: 0.602799, loss_sup: 0.548827, loss_mps: 0.020549, loss_cps: 0.033423
[12:21:21.127] iteration 641: total_loss: 0.532600, loss_sup: 0.482544, loss_mps: 0.019507, loss_cps: 0.030549
[12:21:21.271] iteration 642: total_loss: 0.439353, loss_sup: 0.391878, loss_mps: 0.018209, loss_cps: 0.029267
[12:21:21.417] iteration 643: total_loss: 0.564104, loss_sup: 0.509940, loss_mps: 0.019879, loss_cps: 0.034284
[12:21:21.563] iteration 644: total_loss: 0.499746, loss_sup: 0.454601, loss_mps: 0.017788, loss_cps: 0.027356
[12:21:21.708] iteration 645: total_loss: 0.545128, loss_sup: 0.494695, loss_mps: 0.018804, loss_cps: 0.031629
[12:21:21.858] iteration 646: total_loss: 0.407048, loss_sup: 0.353651, loss_mps: 0.020128, loss_cps: 0.033269
[12:21:22.004] iteration 647: total_loss: 0.683704, loss_sup: 0.620250, loss_mps: 0.022798, loss_cps: 0.040656
[12:21:22.149] iteration 648: total_loss: 0.574918, loss_sup: 0.529888, loss_mps: 0.018415, loss_cps: 0.026614
[12:21:22.294] iteration 649: total_loss: 0.561913, loss_sup: 0.512546, loss_mps: 0.019861, loss_cps: 0.029507
[12:21:22.439] iteration 650: total_loss: 0.767394, loss_sup: 0.719908, loss_mps: 0.019492, loss_cps: 0.027994
[12:21:22.585] iteration 651: total_loss: 0.637042, loss_sup: 0.584728, loss_mps: 0.020792, loss_cps: 0.031521
[12:21:22.736] iteration 652: total_loss: 0.653118, loss_sup: 0.601004, loss_mps: 0.021007, loss_cps: 0.031108
[12:21:22.881] iteration 653: total_loss: 0.758504, loss_sup: 0.698357, loss_mps: 0.023248, loss_cps: 0.036898
[12:21:23.026] iteration 654: total_loss: 0.414694, loss_sup: 0.365586, loss_mps: 0.020101, loss_cps: 0.029007
[12:21:23.171] iteration 655: total_loss: 0.544021, loss_sup: 0.488584, loss_mps: 0.022055, loss_cps: 0.033381
[12:21:23.316] iteration 656: total_loss: 0.556332, loss_sup: 0.499838, loss_mps: 0.022415, loss_cps: 0.034079
[12:21:23.461] iteration 657: total_loss: 0.431316, loss_sup: 0.380158, loss_mps: 0.021253, loss_cps: 0.029906
[12:21:23.607] iteration 658: total_loss: 0.407301, loss_sup: 0.359071, loss_mps: 0.020028, loss_cps: 0.028202
[12:21:23.752] iteration 659: total_loss: 0.599514, loss_sup: 0.545777, loss_mps: 0.021150, loss_cps: 0.032587
[12:21:23.897] iteration 660: total_loss: 0.867852, loss_sup: 0.815466, loss_mps: 0.021106, loss_cps: 0.031280
[12:21:24.042] iteration 661: total_loss: 0.913735, loss_sup: 0.855840, loss_mps: 0.022233, loss_cps: 0.035662
[12:21:24.191] iteration 662: total_loss: 0.564172, loss_sup: 0.506725, loss_mps: 0.022075, loss_cps: 0.035371
[12:21:24.336] iteration 663: total_loss: 0.295700, loss_sup: 0.245461, loss_mps: 0.020324, loss_cps: 0.029915
[12:21:24.482] iteration 664: total_loss: 0.649655, loss_sup: 0.600921, loss_mps: 0.019648, loss_cps: 0.029085
[12:21:24.630] iteration 665: total_loss: 0.590416, loss_sup: 0.534754, loss_mps: 0.021365, loss_cps: 0.034298
[12:21:24.775] iteration 666: total_loss: 0.397002, loss_sup: 0.348006, loss_mps: 0.019954, loss_cps: 0.029042
[12:21:24.923] iteration 667: total_loss: 0.477678, loss_sup: 0.428167, loss_mps: 0.019331, loss_cps: 0.030180
[12:21:25.068] iteration 668: total_loss: 0.666602, loss_sup: 0.609326, loss_mps: 0.022120, loss_cps: 0.035157
[12:21:25.212] iteration 669: total_loss: 0.581045, loss_sup: 0.528002, loss_mps: 0.020178, loss_cps: 0.032864
[12:21:25.357] iteration 670: total_loss: 1.094273, loss_sup: 1.037547, loss_mps: 0.021524, loss_cps: 0.035203
[12:21:25.502] iteration 671: total_loss: 0.454775, loss_sup: 0.399017, loss_mps: 0.021662, loss_cps: 0.034095
[12:21:25.648] iteration 672: total_loss: 0.728866, loss_sup: 0.670177, loss_mps: 0.022706, loss_cps: 0.035983
[12:21:25.792] iteration 673: total_loss: 0.472744, loss_sup: 0.414939, loss_mps: 0.022759, loss_cps: 0.035045
[12:21:25.937] iteration 674: total_loss: 0.675788, loss_sup: 0.617693, loss_mps: 0.023044, loss_cps: 0.035051
[12:21:26.082] iteration 675: total_loss: 0.565727, loss_sup: 0.510170, loss_mps: 0.022353, loss_cps: 0.033204
[12:21:26.228] iteration 676: total_loss: 0.827794, loss_sup: 0.764859, loss_mps: 0.024620, loss_cps: 0.038315
[12:21:26.373] iteration 677: total_loss: 0.623658, loss_sup: 0.555531, loss_mps: 0.026039, loss_cps: 0.042089
[12:21:26.518] iteration 678: total_loss: 0.613119, loss_sup: 0.553051, loss_mps: 0.024194, loss_cps: 0.035874
[12:21:26.664] iteration 679: total_loss: 0.536064, loss_sup: 0.475819, loss_mps: 0.024613, loss_cps: 0.035631
[12:21:26.811] iteration 680: total_loss: 0.536164, loss_sup: 0.474866, loss_mps: 0.024921, loss_cps: 0.036377
[12:21:26.957] iteration 681: total_loss: 0.667636, loss_sup: 0.604666, loss_mps: 0.025476, loss_cps: 0.037495
[12:21:27.103] iteration 682: total_loss: 0.581534, loss_sup: 0.520152, loss_mps: 0.024646, loss_cps: 0.036736
[12:21:27.248] iteration 683: total_loss: 0.702869, loss_sup: 0.630160, loss_mps: 0.028027, loss_cps: 0.044682
[12:21:27.393] iteration 684: total_loss: 0.621146, loss_sup: 0.559013, loss_mps: 0.024274, loss_cps: 0.037858
[12:21:27.538] iteration 685: total_loss: 0.490145, loss_sup: 0.427693, loss_mps: 0.025291, loss_cps: 0.037161
[12:21:27.685] iteration 686: total_loss: 0.577220, loss_sup: 0.521252, loss_mps: 0.022769, loss_cps: 0.033198
[12:21:27.830] iteration 687: total_loss: 0.697990, loss_sup: 0.637616, loss_mps: 0.023516, loss_cps: 0.036858
[12:21:27.975] iteration 688: total_loss: 0.761878, loss_sup: 0.697835, loss_mps: 0.025154, loss_cps: 0.038889
[12:21:28.123] iteration 689: total_loss: 0.638826, loss_sup: 0.583144, loss_mps: 0.022698, loss_cps: 0.032985
[12:21:28.272] iteration 690: total_loss: 0.412645, loss_sup: 0.352304, loss_mps: 0.023475, loss_cps: 0.036866
[12:21:28.417] iteration 691: total_loss: 0.411909, loss_sup: 0.354949, loss_mps: 0.022678, loss_cps: 0.034282
[12:21:28.562] iteration 692: total_loss: 0.596996, loss_sup: 0.538005, loss_mps: 0.022860, loss_cps: 0.036132
[12:21:28.708] iteration 693: total_loss: 0.731133, loss_sup: 0.674144, loss_mps: 0.023121, loss_cps: 0.033868
[12:21:28.857] iteration 694: total_loss: 0.528068, loss_sup: 0.467191, loss_mps: 0.023738, loss_cps: 0.037138
[12:21:29.004] iteration 695: total_loss: 0.923020, loss_sup: 0.859946, loss_mps: 0.024509, loss_cps: 0.038565
[12:21:29.150] iteration 696: total_loss: 0.418114, loss_sup: 0.366743, loss_mps: 0.020821, loss_cps: 0.030551
[12:21:29.295] iteration 697: total_loss: 0.321353, loss_sup: 0.264124, loss_mps: 0.022798, loss_cps: 0.034430
[12:21:29.440] iteration 698: total_loss: 0.763031, loss_sup: 0.706192, loss_mps: 0.022939, loss_cps: 0.033899
[12:21:29.585] iteration 699: total_loss: 0.422970, loss_sup: 0.369064, loss_mps: 0.022063, loss_cps: 0.031843
[12:21:29.730] iteration 700: total_loss: 0.666873, loss_sup: 0.611409, loss_mps: 0.022934, loss_cps: 0.032531
[12:21:29.730] Evaluation Started ==>
[12:21:41.036] ==> valid iteration 700: unet metrics: {'dc': 0.2819440770445184, 'jc': 0.19762121363389057, 'pre': 0.40656905602869087, 'hd': 7.394781547039958}, ynet metrics: {'dc': 0.3666853127288541, 'jc': 0.2548343145563172, 'pre': 0.3730024401203528, 'hd': 9.063420322400377}.
[12:21:41.039] Evaluation Finished!⏹️
[12:21:41.188] iteration 701: total_loss: 0.459228, loss_sup: 0.401544, loss_mps: 0.023331, loss_cps: 0.034353
[12:21:41.335] iteration 702: total_loss: 0.556911, loss_sup: 0.500857, loss_mps: 0.023064, loss_cps: 0.032990
[12:21:41.480] iteration 703: total_loss: 0.421019, loss_sup: 0.368277, loss_mps: 0.022037, loss_cps: 0.030704
[12:21:41.624] iteration 704: total_loss: 0.792040, loss_sup: 0.726060, loss_mps: 0.025497, loss_cps: 0.040483
[12:21:41.768] iteration 705: total_loss: 0.489032, loss_sup: 0.433350, loss_mps: 0.022077, loss_cps: 0.033605
[12:21:41.913] iteration 706: total_loss: 0.473104, loss_sup: 0.415719, loss_mps: 0.022304, loss_cps: 0.035081
[12:21:42.057] iteration 707: total_loss: 0.732140, loss_sup: 0.668735, loss_mps: 0.023544, loss_cps: 0.039861
[12:21:42.202] iteration 708: total_loss: 0.664635, loss_sup: 0.602483, loss_mps: 0.022842, loss_cps: 0.039311
[12:21:42.346] iteration 709: total_loss: 0.642157, loss_sup: 0.588355, loss_mps: 0.021166, loss_cps: 0.032635
[12:21:42.491] iteration 710: total_loss: 0.592466, loss_sup: 0.545081, loss_mps: 0.018772, loss_cps: 0.028613
[12:21:42.638] iteration 711: total_loss: 0.882004, loss_sup: 0.836744, loss_mps: 0.019149, loss_cps: 0.026112
[12:21:42.782] iteration 712: total_loss: 0.531396, loss_sup: 0.478290, loss_mps: 0.021194, loss_cps: 0.031912
[12:21:42.926] iteration 713: total_loss: 0.727691, loss_sup: 0.674936, loss_mps: 0.021198, loss_cps: 0.031557
[12:21:43.071] iteration 714: total_loss: 0.397273, loss_sup: 0.339518, loss_mps: 0.022719, loss_cps: 0.035036
[12:21:43.215] iteration 715: total_loss: 0.459904, loss_sup: 0.401834, loss_mps: 0.023137, loss_cps: 0.034933
[12:21:43.361] iteration 716: total_loss: 0.684525, loss_sup: 0.632971, loss_mps: 0.021223, loss_cps: 0.030331
[12:21:43.506] iteration 717: total_loss: 0.674051, loss_sup: 0.616827, loss_mps: 0.023508, loss_cps: 0.033716
[12:21:43.651] iteration 718: total_loss: 0.487716, loss_sup: 0.435311, loss_mps: 0.022108, loss_cps: 0.030297
[12:21:43.801] iteration 719: total_loss: 0.435625, loss_sup: 0.382686, loss_mps: 0.021645, loss_cps: 0.031293
[12:21:43.946] iteration 720: total_loss: 0.540729, loss_sup: 0.492516, loss_mps: 0.020721, loss_cps: 0.027493
[12:21:44.091] iteration 721: total_loss: 0.696137, loss_sup: 0.648358, loss_mps: 0.020533, loss_cps: 0.027245
[12:21:44.239] iteration 722: total_loss: 1.081330, loss_sup: 1.037697, loss_mps: 0.019491, loss_cps: 0.024142
[12:21:44.384] iteration 723: total_loss: 0.759133, loss_sup: 0.706902, loss_mps: 0.022255, loss_cps: 0.029976
[12:21:44.530] iteration 724: total_loss: 0.717684, loss_sup: 0.664328, loss_mps: 0.022972, loss_cps: 0.030384
[12:21:44.676] iteration 725: total_loss: 0.868630, loss_sup: 0.813712, loss_mps: 0.023352, loss_cps: 0.031565
[12:21:44.821] iteration 726: total_loss: 0.656574, loss_sup: 0.597023, loss_mps: 0.025109, loss_cps: 0.034443
[12:21:44.967] iteration 727: total_loss: 0.829009, loss_sup: 0.770573, loss_mps: 0.025718, loss_cps: 0.032718
[12:21:45.113] iteration 728: total_loss: 0.812952, loss_sup: 0.748991, loss_mps: 0.028248, loss_cps: 0.035712
[12:21:45.259] iteration 729: total_loss: 0.734121, loss_sup: 0.662345, loss_mps: 0.030303, loss_cps: 0.041474
[12:21:45.405] iteration 730: total_loss: 0.609890, loss_sup: 0.549367, loss_mps: 0.027376, loss_cps: 0.033147
[12:21:45.551] iteration 731: total_loss: 0.698361, loss_sup: 0.636211, loss_mps: 0.028118, loss_cps: 0.034032
[12:21:45.696] iteration 732: total_loss: 0.557160, loss_sup: 0.492864, loss_mps: 0.029084, loss_cps: 0.035212
[12:21:45.841] iteration 733: total_loss: 0.546419, loss_sup: 0.488749, loss_mps: 0.026216, loss_cps: 0.031454
[12:21:45.989] iteration 734: total_loss: 0.667247, loss_sup: 0.597083, loss_mps: 0.030165, loss_cps: 0.040000
[12:21:46.134] iteration 735: total_loss: 0.577045, loss_sup: 0.512598, loss_mps: 0.028152, loss_cps: 0.036294
[12:21:46.284] iteration 736: total_loss: 0.602250, loss_sup: 0.546406, loss_mps: 0.025039, loss_cps: 0.030804
[12:21:46.429] iteration 737: total_loss: 0.766953, loss_sup: 0.708669, loss_mps: 0.025605, loss_cps: 0.032679
[12:21:46.574] iteration 738: total_loss: 0.549214, loss_sup: 0.493035, loss_mps: 0.024301, loss_cps: 0.031879
[12:21:46.719] iteration 739: total_loss: 0.640916, loss_sup: 0.580270, loss_mps: 0.024973, loss_cps: 0.035673
[12:21:46.864] iteration 740: total_loss: 0.630605, loss_sup: 0.568299, loss_mps: 0.025270, loss_cps: 0.037036
[12:21:47.009] iteration 741: total_loss: 0.508411, loss_sup: 0.454657, loss_mps: 0.022554, loss_cps: 0.031200
[12:21:47.156] iteration 742: total_loss: 0.909340, loss_sup: 0.862036, loss_mps: 0.020353, loss_cps: 0.026951
[12:21:47.301] iteration 743: total_loss: 0.968536, loss_sup: 0.904449, loss_mps: 0.025321, loss_cps: 0.038766
[12:21:47.446] iteration 744: total_loss: 0.565166, loss_sup: 0.511084, loss_mps: 0.022549, loss_cps: 0.031533
[12:21:47.591] iteration 745: total_loss: 0.494018, loss_sup: 0.443257, loss_mps: 0.021820, loss_cps: 0.028941
[12:21:47.737] iteration 746: total_loss: 0.501467, loss_sup: 0.445371, loss_mps: 0.022597, loss_cps: 0.033499
[12:21:47.882] iteration 747: total_loss: 0.680490, loss_sup: 0.617557, loss_mps: 0.025373, loss_cps: 0.037559
[12:21:48.027] iteration 748: total_loss: 0.569924, loss_sup: 0.514066, loss_mps: 0.023056, loss_cps: 0.032802
[12:21:48.173] iteration 749: total_loss: 0.532059, loss_sup: 0.480446, loss_mps: 0.022162, loss_cps: 0.029451
[12:21:48.318] iteration 750: total_loss: 0.476503, loss_sup: 0.421339, loss_mps: 0.023074, loss_cps: 0.032090
[12:21:48.463] iteration 751: total_loss: 0.599713, loss_sup: 0.549121, loss_mps: 0.021735, loss_cps: 0.028857
[12:21:48.608] iteration 752: total_loss: 0.749113, loss_sup: 0.696225, loss_mps: 0.022193, loss_cps: 0.030695
[12:21:48.753] iteration 753: total_loss: 0.523585, loss_sup: 0.467854, loss_mps: 0.022923, loss_cps: 0.032808
[12:21:48.900] iteration 754: total_loss: 0.707012, loss_sup: 0.652729, loss_mps: 0.022121, loss_cps: 0.032162
[12:21:49.045] iteration 755: total_loss: 0.543517, loss_sup: 0.490769, loss_mps: 0.021276, loss_cps: 0.031472
[12:21:49.191] iteration 756: total_loss: 0.603237, loss_sup: 0.549658, loss_mps: 0.022161, loss_cps: 0.031418
[12:21:49.343] iteration 757: total_loss: 0.506541, loss_sup: 0.455207, loss_mps: 0.021655, loss_cps: 0.029679
[12:21:49.489] iteration 758: total_loss: 0.497903, loss_sup: 0.436433, loss_mps: 0.023776, loss_cps: 0.037695
[12:21:49.635] iteration 759: total_loss: 0.690669, loss_sup: 0.630341, loss_mps: 0.023961, loss_cps: 0.036368
[12:21:49.782] iteration 760: total_loss: 0.424331, loss_sup: 0.369108, loss_mps: 0.021764, loss_cps: 0.033459
[12:21:49.929] iteration 761: total_loss: 0.579741, loss_sup: 0.519284, loss_mps: 0.023340, loss_cps: 0.037117
[12:21:50.079] iteration 762: total_loss: 0.546667, loss_sup: 0.495430, loss_mps: 0.020831, loss_cps: 0.030406
[12:21:50.225] iteration 763: total_loss: 0.715559, loss_sup: 0.649101, loss_mps: 0.025060, loss_cps: 0.041398
[12:21:50.371] iteration 764: total_loss: 0.559184, loss_sup: 0.502428, loss_mps: 0.022566, loss_cps: 0.034189
[12:21:50.520] iteration 765: total_loss: 0.621249, loss_sup: 0.571692, loss_mps: 0.020438, loss_cps: 0.029119
[12:21:50.666] iteration 766: total_loss: 0.461272, loss_sup: 0.407037, loss_mps: 0.021793, loss_cps: 0.032442
[12:21:50.811] iteration 767: total_loss: 0.403173, loss_sup: 0.349574, loss_mps: 0.021252, loss_cps: 0.032347
[12:21:50.959] iteration 768: total_loss: 0.715581, loss_sup: 0.661385, loss_mps: 0.021344, loss_cps: 0.032851
[12:21:51.104] iteration 769: total_loss: 0.446169, loss_sup: 0.393760, loss_mps: 0.020572, loss_cps: 0.031837
[12:21:51.251] iteration 770: total_loss: 0.755058, loss_sup: 0.700071, loss_mps: 0.021329, loss_cps: 0.033658
[12:21:51.402] iteration 771: total_loss: 0.421031, loss_sup: 0.367064, loss_mps: 0.020836, loss_cps: 0.033131
[12:21:51.549] iteration 772: total_loss: 0.630758, loss_sup: 0.567385, loss_mps: 0.023913, loss_cps: 0.039460
[12:21:51.695] iteration 773: total_loss: 0.998631, loss_sup: 0.933048, loss_mps: 0.024667, loss_cps: 0.040916
[12:21:51.841] iteration 774: total_loss: 0.631073, loss_sup: 0.571160, loss_mps: 0.022405, loss_cps: 0.037508
[12:21:51.987] iteration 775: total_loss: 0.431971, loss_sup: 0.372114, loss_mps: 0.022856, loss_cps: 0.037000
[12:21:52.132] iteration 776: total_loss: 0.459824, loss_sup: 0.402965, loss_mps: 0.021653, loss_cps: 0.035206
[12:21:52.281] iteration 777: total_loss: 0.510018, loss_sup: 0.451678, loss_mps: 0.022681, loss_cps: 0.035660
[12:21:52.429] iteration 778: total_loss: 0.625039, loss_sup: 0.567322, loss_mps: 0.022709, loss_cps: 0.035008
[12:21:52.575] iteration 779: total_loss: 0.502405, loss_sup: 0.449155, loss_mps: 0.021744, loss_cps: 0.031507
[12:21:52.722] iteration 780: total_loss: 0.718881, loss_sup: 0.659956, loss_mps: 0.023204, loss_cps: 0.035721
[12:21:52.868] iteration 781: total_loss: 0.462584, loss_sup: 0.407543, loss_mps: 0.021839, loss_cps: 0.033202
[12:21:53.015] iteration 782: total_loss: 0.670487, loss_sup: 0.616771, loss_mps: 0.021707, loss_cps: 0.032009
[12:21:53.161] iteration 783: total_loss: 0.581367, loss_sup: 0.520549, loss_mps: 0.023228, loss_cps: 0.037590
[12:21:53.307] iteration 784: total_loss: 0.460382, loss_sup: 0.411170, loss_mps: 0.019781, loss_cps: 0.029432
[12:21:53.457] iteration 785: total_loss: 0.388322, loss_sup: 0.340655, loss_mps: 0.019919, loss_cps: 0.027749
[12:21:53.602] iteration 786: total_loss: 0.482657, loss_sup: 0.430310, loss_mps: 0.020371, loss_cps: 0.031976
[12:21:53.749] iteration 787: total_loss: 0.337449, loss_sup: 0.290734, loss_mps: 0.019362, loss_cps: 0.027353
[12:21:53.896] iteration 788: total_loss: 0.663575, loss_sup: 0.611930, loss_mps: 0.020456, loss_cps: 0.031190
[12:21:54.041] iteration 789: total_loss: 0.662278, loss_sup: 0.610663, loss_mps: 0.021046, loss_cps: 0.030568
[12:21:54.186] iteration 790: total_loss: 0.328057, loss_sup: 0.279772, loss_mps: 0.019517, loss_cps: 0.028768
[12:21:54.333] iteration 791: total_loss: 0.708745, loss_sup: 0.659379, loss_mps: 0.019789, loss_cps: 0.029577
[12:21:54.478] iteration 792: total_loss: 0.657303, loss_sup: 0.606503, loss_mps: 0.020230, loss_cps: 0.030570
[12:21:54.624] iteration 793: total_loss: 0.394075, loss_sup: 0.350323, loss_mps: 0.018448, loss_cps: 0.025304
[12:21:54.770] iteration 794: total_loss: 0.493430, loss_sup: 0.443478, loss_mps: 0.020125, loss_cps: 0.029827
[12:21:54.916] iteration 795: total_loss: 0.559991, loss_sup: 0.507609, loss_mps: 0.021064, loss_cps: 0.031317
[12:21:55.064] iteration 796: total_loss: 0.690115, loss_sup: 0.631340, loss_mps: 0.022684, loss_cps: 0.036090
[12:21:55.211] iteration 797: total_loss: 0.641242, loss_sup: 0.584855, loss_mps: 0.021171, loss_cps: 0.035216
[12:21:55.358] iteration 798: total_loss: 0.406508, loss_sup: 0.354346, loss_mps: 0.020144, loss_cps: 0.032018
[12:21:55.504] iteration 799: total_loss: 0.448327, loss_sup: 0.387541, loss_mps: 0.022623, loss_cps: 0.038163
[12:21:55.649] iteration 800: total_loss: 0.598325, loss_sup: 0.541236, loss_mps: 0.021479, loss_cps: 0.035610
[12:21:55.650] Evaluation Started ==>
[12:22:06.956] ==> valid iteration 800: unet metrics: {'dc': 0.3132580701858728, 'jc': 0.22197070600045485, 'pre': 0.4092095650700162, 'hd': 7.376315313544083}, ynet metrics: {'dc': 0.3906526114838181, 'jc': 0.2791382648208796, 'pre': 0.46809175884462817, 'hd': 8.338967517354279}.
[12:22:06.958] Evaluation Finished!⏹️
[12:22:07.108] iteration 801: total_loss: 0.408781, loss_sup: 0.352099, loss_mps: 0.021562, loss_cps: 0.035120
[12:22:07.255] iteration 802: total_loss: 0.406086, loss_sup: 0.349591, loss_mps: 0.021103, loss_cps: 0.035392
[12:22:07.400] iteration 803: total_loss: 0.443478, loss_sup: 0.388666, loss_mps: 0.020764, loss_cps: 0.034048
[12:22:07.545] iteration 804: total_loss: 0.516233, loss_sup: 0.457913, loss_mps: 0.021557, loss_cps: 0.036763
[12:22:07.691] iteration 805: total_loss: 0.533785, loss_sup: 0.467409, loss_mps: 0.023767, loss_cps: 0.042609
[12:22:07.836] iteration 806: total_loss: 0.621779, loss_sup: 0.563674, loss_mps: 0.021676, loss_cps: 0.036429
[12:22:07.981] iteration 807: total_loss: 0.662770, loss_sup: 0.609644, loss_mps: 0.020462, loss_cps: 0.032664
[12:22:08.127] iteration 808: total_loss: 0.656541, loss_sup: 0.603556, loss_mps: 0.020633, loss_cps: 0.032351
[12:22:08.272] iteration 809: total_loss: 0.514445, loss_sup: 0.453212, loss_mps: 0.023037, loss_cps: 0.038196
[12:22:08.417] iteration 810: total_loss: 0.617107, loss_sup: 0.555067, loss_mps: 0.023449, loss_cps: 0.038591
[12:22:08.562] iteration 811: total_loss: 0.398398, loss_sup: 0.351149, loss_mps: 0.019588, loss_cps: 0.027661
[12:22:08.708] iteration 812: total_loss: 0.342777, loss_sup: 0.287073, loss_mps: 0.022007, loss_cps: 0.033698
[12:22:08.853] iteration 813: total_loss: 0.524738, loss_sup: 0.470706, loss_mps: 0.021345, loss_cps: 0.032688
[12:22:09.001] iteration 814: total_loss: 0.467976, loss_sup: 0.419932, loss_mps: 0.019619, loss_cps: 0.028425
[12:22:09.147] iteration 815: total_loss: 0.431870, loss_sup: 0.388150, loss_mps: 0.018450, loss_cps: 0.025270
[12:22:09.293] iteration 816: total_loss: 0.677359, loss_sup: 0.632046, loss_mps: 0.018801, loss_cps: 0.026512
[12:22:09.439] iteration 817: total_loss: 0.619741, loss_sup: 0.568328, loss_mps: 0.020836, loss_cps: 0.030578
[12:22:09.585] iteration 818: total_loss: 0.494594, loss_sup: 0.444096, loss_mps: 0.020583, loss_cps: 0.029915
[12:22:09.730] iteration 819: total_loss: 0.414884, loss_sup: 0.362134, loss_mps: 0.021446, loss_cps: 0.031305
[12:22:09.876] iteration 820: total_loss: 0.540493, loss_sup: 0.492363, loss_mps: 0.019610, loss_cps: 0.028520
[12:22:10.023] iteration 821: total_loss: 0.892953, loss_sup: 0.835032, loss_mps: 0.021952, loss_cps: 0.035969
[12:22:10.168] iteration 822: total_loss: 0.321460, loss_sup: 0.267866, loss_mps: 0.021136, loss_cps: 0.032459
[12:22:10.315] iteration 823: total_loss: 0.468135, loss_sup: 0.412693, loss_mps: 0.022146, loss_cps: 0.033296
[12:22:10.460] iteration 824: total_loss: 0.918014, loss_sup: 0.869490, loss_mps: 0.019897, loss_cps: 0.028627
[12:22:10.605] iteration 825: total_loss: 0.305575, loss_sup: 0.252624, loss_mps: 0.021448, loss_cps: 0.031503
[12:22:10.751] iteration 826: total_loss: 0.467393, loss_sup: 0.403901, loss_mps: 0.024504, loss_cps: 0.038988
[12:22:10.897] iteration 827: total_loss: 0.437940, loss_sup: 0.380907, loss_mps: 0.022808, loss_cps: 0.034225
[12:22:11.041] iteration 828: total_loss: 0.550579, loss_sup: 0.497160, loss_mps: 0.021953, loss_cps: 0.031466
[12:22:11.186] iteration 829: total_loss: 0.592003, loss_sup: 0.528528, loss_mps: 0.024532, loss_cps: 0.038944
[12:22:11.333] iteration 830: total_loss: 0.497350, loss_sup: 0.435489, loss_mps: 0.024478, loss_cps: 0.037382
[12:22:11.478] iteration 831: total_loss: 0.369731, loss_sup: 0.314622, loss_mps: 0.021858, loss_cps: 0.033251
[12:22:11.623] iteration 832: total_loss: 0.609235, loss_sup: 0.549978, loss_mps: 0.023183, loss_cps: 0.036075
[12:22:11.768] iteration 833: total_loss: 0.346804, loss_sup: 0.292029, loss_mps: 0.021652, loss_cps: 0.033122
[12:22:11.913] iteration 834: total_loss: 0.699540, loss_sup: 0.645009, loss_mps: 0.021994, loss_cps: 0.032537
[12:22:12.061] iteration 835: total_loss: 0.539244, loss_sup: 0.493597, loss_mps: 0.019211, loss_cps: 0.026435
[12:22:12.126] iteration 836: total_loss: 0.502842, loss_sup: 0.448676, loss_mps: 0.021199, loss_cps: 0.032967
[12:22:13.317] iteration 837: total_loss: 0.372695, loss_sup: 0.327683, loss_mps: 0.018234, loss_cps: 0.026778
[12:22:13.464] iteration 838: total_loss: 0.467632, loss_sup: 0.422279, loss_mps: 0.018305, loss_cps: 0.027048
[12:22:13.610] iteration 839: total_loss: 0.231015, loss_sup: 0.191222, loss_mps: 0.016322, loss_cps: 0.023471
[12:22:13.756] iteration 840: total_loss: 0.881612, loss_sup: 0.829916, loss_mps: 0.019603, loss_cps: 0.032092
[12:22:13.910] iteration 841: total_loss: 0.481138, loss_sup: 0.430861, loss_mps: 0.019078, loss_cps: 0.031199
[12:22:14.055] iteration 842: total_loss: 0.494481, loss_sup: 0.448883, loss_mps: 0.017642, loss_cps: 0.027956
[12:22:14.200] iteration 843: total_loss: 0.660447, loss_sup: 0.612621, loss_mps: 0.018898, loss_cps: 0.028928
[12:22:14.345] iteration 844: total_loss: 0.543502, loss_sup: 0.489765, loss_mps: 0.020076, loss_cps: 0.033661
[12:22:14.492] iteration 845: total_loss: 0.572361, loss_sup: 0.514904, loss_mps: 0.021447, loss_cps: 0.036009
[12:22:14.637] iteration 846: total_loss: 0.576562, loss_sup: 0.524199, loss_mps: 0.020489, loss_cps: 0.031874
[12:22:14.783] iteration 847: total_loss: 0.555841, loss_sup: 0.496687, loss_mps: 0.022228, loss_cps: 0.036926
[12:22:14.928] iteration 848: total_loss: 0.483486, loss_sup: 0.428789, loss_mps: 0.021488, loss_cps: 0.033209
[12:22:15.073] iteration 849: total_loss: 0.767977, loss_sup: 0.713375, loss_mps: 0.020867, loss_cps: 0.033734
[12:22:15.218] iteration 850: total_loss: 0.397452, loss_sup: 0.344981, loss_mps: 0.020836, loss_cps: 0.031635
[12:22:15.364] iteration 851: total_loss: 0.505610, loss_sup: 0.441313, loss_mps: 0.024099, loss_cps: 0.040198
[12:22:15.510] iteration 852: total_loss: 0.569062, loss_sup: 0.511428, loss_mps: 0.023534, loss_cps: 0.034100
[12:22:15.655] iteration 853: total_loss: 0.527751, loss_sup: 0.471451, loss_mps: 0.022912, loss_cps: 0.033388
[12:22:15.800] iteration 854: total_loss: 0.491701, loss_sup: 0.433274, loss_mps: 0.023093, loss_cps: 0.035334
[12:22:15.946] iteration 855: total_loss: 0.624390, loss_sup: 0.565654, loss_mps: 0.023452, loss_cps: 0.035284
[12:22:16.092] iteration 856: total_loss: 0.470693, loss_sup: 0.404252, loss_mps: 0.026211, loss_cps: 0.040229
[12:22:16.237] iteration 857: total_loss: 0.675353, loss_sup: 0.624231, loss_mps: 0.021632, loss_cps: 0.029489
[12:22:16.382] iteration 858: total_loss: 0.522422, loss_sup: 0.465222, loss_mps: 0.023326, loss_cps: 0.033874
[12:22:16.528] iteration 859: total_loss: 0.519104, loss_sup: 0.460731, loss_mps: 0.022930, loss_cps: 0.035443
[12:22:16.675] iteration 860: total_loss: 0.718292, loss_sup: 0.660232, loss_mps: 0.023545, loss_cps: 0.034515
[12:22:16.821] iteration 861: total_loss: 0.405991, loss_sup: 0.358237, loss_mps: 0.020082, loss_cps: 0.027672
[12:22:16.966] iteration 862: total_loss: 0.522026, loss_sup: 0.470299, loss_mps: 0.021423, loss_cps: 0.030304
[12:22:17.112] iteration 863: total_loss: 0.491526, loss_sup: 0.441812, loss_mps: 0.021118, loss_cps: 0.028596
[12:22:17.261] iteration 864: total_loss: 0.389823, loss_sup: 0.344362, loss_mps: 0.019739, loss_cps: 0.025722
[12:22:17.407] iteration 865: total_loss: 0.634280, loss_sup: 0.579777, loss_mps: 0.021082, loss_cps: 0.033420
[12:22:17.555] iteration 866: total_loss: 0.379783, loss_sup: 0.321890, loss_mps: 0.022506, loss_cps: 0.035386
[12:22:17.701] iteration 867: total_loss: 0.431739, loss_sup: 0.381429, loss_mps: 0.019759, loss_cps: 0.030552
[12:22:17.847] iteration 868: total_loss: 0.482176, loss_sup: 0.426249, loss_mps: 0.020937, loss_cps: 0.034990
[12:22:17.993] iteration 869: total_loss: 0.607490, loss_sup: 0.557608, loss_mps: 0.019573, loss_cps: 0.030309
[12:22:18.138] iteration 870: total_loss: 0.429003, loss_sup: 0.374837, loss_mps: 0.020385, loss_cps: 0.033781
[12:22:18.286] iteration 871: total_loss: 0.279218, loss_sup: 0.239179, loss_mps: 0.016506, loss_cps: 0.023533
[12:22:18.431] iteration 872: total_loss: 0.501079, loss_sup: 0.443572, loss_mps: 0.021472, loss_cps: 0.036035
[12:22:18.576] iteration 873: total_loss: 0.520454, loss_sup: 0.455022, loss_mps: 0.023712, loss_cps: 0.041721
[12:22:18.721] iteration 874: total_loss: 0.449076, loss_sup: 0.396146, loss_mps: 0.019838, loss_cps: 0.033092
[12:22:18.868] iteration 875: total_loss: 0.467484, loss_sup: 0.417433, loss_mps: 0.019409, loss_cps: 0.030642
[12:22:19.013] iteration 876: total_loss: 0.504108, loss_sup: 0.443962, loss_mps: 0.021937, loss_cps: 0.038208
[12:22:19.158] iteration 877: total_loss: 0.410994, loss_sup: 0.357810, loss_mps: 0.019837, loss_cps: 0.033347
[12:22:19.304] iteration 878: total_loss: 0.702000, loss_sup: 0.648099, loss_mps: 0.020099, loss_cps: 0.033803
[12:22:19.449] iteration 879: total_loss: 1.251913, loss_sup: 1.197296, loss_mps: 0.020429, loss_cps: 0.034188
[12:22:19.594] iteration 880: total_loss: 0.595560, loss_sup: 0.547232, loss_mps: 0.018763, loss_cps: 0.029565
[12:22:19.739] iteration 881: total_loss: 0.399321, loss_sup: 0.346853, loss_mps: 0.019971, loss_cps: 0.032497
[12:22:19.884] iteration 882: total_loss: 0.454550, loss_sup: 0.396923, loss_mps: 0.022159, loss_cps: 0.035469
[12:22:20.030] iteration 883: total_loss: 0.578061, loss_sup: 0.528491, loss_mps: 0.019635, loss_cps: 0.029936
[12:22:20.175] iteration 884: total_loss: 0.347390, loss_sup: 0.300209, loss_mps: 0.019166, loss_cps: 0.028014
[12:22:20.326] iteration 885: total_loss: 0.914325, loss_sup: 0.865583, loss_mps: 0.019822, loss_cps: 0.028920
[12:22:20.471] iteration 886: total_loss: 0.330625, loss_sup: 0.280658, loss_mps: 0.020602, loss_cps: 0.029365
[12:22:20.616] iteration 887: total_loss: 0.445502, loss_sup: 0.395218, loss_mps: 0.020762, loss_cps: 0.029522
[12:22:20.762] iteration 888: total_loss: 0.632420, loss_sup: 0.587182, loss_mps: 0.019650, loss_cps: 0.025588
[12:22:20.907] iteration 889: total_loss: 0.562560, loss_sup: 0.501989, loss_mps: 0.024841, loss_cps: 0.035730
[12:22:21.053] iteration 890: total_loss: 0.693875, loss_sup: 0.635944, loss_mps: 0.023456, loss_cps: 0.034475
[12:22:21.198] iteration 891: total_loss: 0.727479, loss_sup: 0.668129, loss_mps: 0.023974, loss_cps: 0.035376
[12:22:21.344] iteration 892: total_loss: 0.704030, loss_sup: 0.638281, loss_mps: 0.025945, loss_cps: 0.039804
[12:22:21.491] iteration 893: total_loss: 0.583409, loss_sup: 0.514675, loss_mps: 0.026928, loss_cps: 0.041807
[12:22:21.638] iteration 894: total_loss: 0.562714, loss_sup: 0.498815, loss_mps: 0.025618, loss_cps: 0.038281
[12:22:21.784] iteration 895: total_loss: 0.862736, loss_sup: 0.801852, loss_mps: 0.024765, loss_cps: 0.036119
[12:22:21.930] iteration 896: total_loss: 0.534867, loss_sup: 0.471243, loss_mps: 0.026136, loss_cps: 0.037488
[12:22:22.076] iteration 897: total_loss: 0.702424, loss_sup: 0.635780, loss_mps: 0.026561, loss_cps: 0.040083
[12:22:22.222] iteration 898: total_loss: 0.566310, loss_sup: 0.500798, loss_mps: 0.025838, loss_cps: 0.039673
[12:22:22.368] iteration 899: total_loss: 0.475801, loss_sup: 0.415945, loss_mps: 0.024299, loss_cps: 0.035557
[12:22:22.516] iteration 900: total_loss: 0.544205, loss_sup: 0.482618, loss_mps: 0.025916, loss_cps: 0.035671
[12:22:22.516] Evaluation Started ==>
[12:22:33.840] ==> valid iteration 900: unet metrics: {'dc': 0.4226784335636636, 'jc': 0.30670145028770296, 'pre': 0.47552529267515625, 'hd': 7.937711533867864}, ynet metrics: {'dc': 0.4240578262323439, 'jc': 0.31109867080656556, 'pre': 0.47686666713599396, 'hd': 8.112857293718218}.
[12:22:33.902] ==> New best valid dice for unet: 0.422678, at iteration 900
[12:22:34.064] ==> New best valid dice for ynet: 0.424058, at iteration 900
[12:22:34.065] Evaluation Finished!⏹️
[12:22:34.218] iteration 901: total_loss: 0.498988, loss_sup: 0.438425, loss_mps: 0.025608, loss_cps: 0.034955
[12:22:34.364] iteration 902: total_loss: 0.401471, loss_sup: 0.345092, loss_mps: 0.023607, loss_cps: 0.032772
[12:22:34.509] iteration 903: total_loss: 0.642176, loss_sup: 0.584193, loss_mps: 0.024002, loss_cps: 0.033981
[12:22:34.654] iteration 904: total_loss: 0.624303, loss_sup: 0.562366, loss_mps: 0.024788, loss_cps: 0.037149
[12:22:34.798] iteration 905: total_loss: 0.386812, loss_sup: 0.326839, loss_mps: 0.023836, loss_cps: 0.036137
[12:22:34.944] iteration 906: total_loss: 0.706645, loss_sup: 0.651831, loss_mps: 0.022451, loss_cps: 0.032362
[12:22:35.088] iteration 907: total_loss: 0.562538, loss_sup: 0.504692, loss_mps: 0.023403, loss_cps: 0.034443
[12:22:35.232] iteration 908: total_loss: 0.528307, loss_sup: 0.476329, loss_mps: 0.021180, loss_cps: 0.030798
[12:22:35.377] iteration 909: total_loss: 0.281138, loss_sup: 0.232490, loss_mps: 0.020572, loss_cps: 0.028076
[12:22:35.523] iteration 910: total_loss: 0.499328, loss_sup: 0.449227, loss_mps: 0.020977, loss_cps: 0.029123
[12:22:35.667] iteration 911: total_loss: 0.757187, loss_sup: 0.692428, loss_mps: 0.024142, loss_cps: 0.040617
[12:22:35.813] iteration 912: total_loss: 0.425152, loss_sup: 0.372318, loss_mps: 0.020879, loss_cps: 0.031955
[12:22:35.959] iteration 913: total_loss: 0.550673, loss_sup: 0.497910, loss_mps: 0.021421, loss_cps: 0.031342
[12:22:36.104] iteration 914: total_loss: 0.428538, loss_sup: 0.374364, loss_mps: 0.021225, loss_cps: 0.032948
[12:22:36.252] iteration 915: total_loss: 0.459998, loss_sup: 0.401255, loss_mps: 0.022265, loss_cps: 0.036478
[12:22:36.397] iteration 916: total_loss: 0.659880, loss_sup: 0.595690, loss_mps: 0.023670, loss_cps: 0.040520
[12:22:36.542] iteration 917: total_loss: 0.663313, loss_sup: 0.613504, loss_mps: 0.020028, loss_cps: 0.029781
[12:22:36.695] iteration 918: total_loss: 0.448471, loss_sup: 0.399450, loss_mps: 0.019709, loss_cps: 0.029312
[12:22:36.842] iteration 919: total_loss: 0.682813, loss_sup: 0.630115, loss_mps: 0.021108, loss_cps: 0.031590
[12:22:36.988] iteration 920: total_loss: 0.421843, loss_sup: 0.366420, loss_mps: 0.021044, loss_cps: 0.034379
[12:22:37.134] iteration 921: total_loss: 0.654792, loss_sup: 0.587273, loss_mps: 0.024093, loss_cps: 0.043427
[12:22:37.283] iteration 922: total_loss: 0.427745, loss_sup: 0.376366, loss_mps: 0.019874, loss_cps: 0.031506
[12:22:37.429] iteration 923: total_loss: 0.241871, loss_sup: 0.196057, loss_mps: 0.018181, loss_cps: 0.027634
[12:22:37.574] iteration 924: total_loss: 0.442965, loss_sup: 0.379933, loss_mps: 0.023648, loss_cps: 0.039385
[12:22:37.719] iteration 925: total_loss: 0.221719, loss_sup: 0.178177, loss_mps: 0.017626, loss_cps: 0.025915
[12:22:37.865] iteration 926: total_loss: 0.409337, loss_sup: 0.358821, loss_mps: 0.019743, loss_cps: 0.030773
[12:22:38.010] iteration 927: total_loss: 0.413694, loss_sup: 0.364804, loss_mps: 0.018752, loss_cps: 0.030138
[12:22:38.156] iteration 928: total_loss: 0.696085, loss_sup: 0.640265, loss_mps: 0.021111, loss_cps: 0.034709
[12:22:38.304] iteration 929: total_loss: 0.446411, loss_sup: 0.394794, loss_mps: 0.019561, loss_cps: 0.032056
[12:22:38.449] iteration 930: total_loss: 0.423443, loss_sup: 0.368832, loss_mps: 0.020815, loss_cps: 0.033796
[12:22:38.594] iteration 931: total_loss: 0.465230, loss_sup: 0.416081, loss_mps: 0.019149, loss_cps: 0.030001
[12:22:38.740] iteration 932: total_loss: 0.534580, loss_sup: 0.491108, loss_mps: 0.017697, loss_cps: 0.025775
[12:22:38.885] iteration 933: total_loss: 0.427680, loss_sup: 0.382415, loss_mps: 0.018200, loss_cps: 0.027065
[12:22:39.030] iteration 934: total_loss: 0.571481, loss_sup: 0.513008, loss_mps: 0.021747, loss_cps: 0.036727
[12:22:39.175] iteration 935: total_loss: 0.778918, loss_sup: 0.723489, loss_mps: 0.021058, loss_cps: 0.034372
[12:22:39.321] iteration 936: total_loss: 0.605412, loss_sup: 0.547118, loss_mps: 0.021674, loss_cps: 0.036621
[12:22:39.468] iteration 937: total_loss: 0.460530, loss_sup: 0.404091, loss_mps: 0.021715, loss_cps: 0.034724
[12:22:39.615] iteration 938: total_loss: 0.616027, loss_sup: 0.557545, loss_mps: 0.022484, loss_cps: 0.035999
[12:22:39.761] iteration 939: total_loss: 0.500287, loss_sup: 0.447178, loss_mps: 0.020691, loss_cps: 0.032418
[12:22:39.907] iteration 940: total_loss: 0.324619, loss_sup: 0.268414, loss_mps: 0.021904, loss_cps: 0.034301
[12:22:40.052] iteration 941: total_loss: 0.420109, loss_sup: 0.359324, loss_mps: 0.023207, loss_cps: 0.037578
[12:22:40.198] iteration 942: total_loss: 0.388342, loss_sup: 0.330648, loss_mps: 0.022416, loss_cps: 0.035278
[12:22:40.343] iteration 943: total_loss: 0.477949, loss_sup: 0.425065, loss_mps: 0.021071, loss_cps: 0.031813
[12:22:40.488] iteration 944: total_loss: 0.587630, loss_sup: 0.522300, loss_mps: 0.024193, loss_cps: 0.041138
[12:22:40.634] iteration 945: total_loss: 0.525042, loss_sup: 0.462739, loss_mps: 0.024074, loss_cps: 0.038228
[12:22:40.781] iteration 946: total_loss: 0.471824, loss_sup: 0.411849, loss_mps: 0.023002, loss_cps: 0.036974
[12:22:40.927] iteration 947: total_loss: 0.633681, loss_sup: 0.571640, loss_mps: 0.023562, loss_cps: 0.038479
[12:22:41.072] iteration 948: total_loss: 0.642173, loss_sup: 0.580480, loss_mps: 0.023065, loss_cps: 0.038628
[12:22:41.217] iteration 949: total_loss: 0.500502, loss_sup: 0.454280, loss_mps: 0.019539, loss_cps: 0.026682
[12:22:41.362] iteration 950: total_loss: 0.539793, loss_sup: 0.479416, loss_mps: 0.023215, loss_cps: 0.037162
[12:22:41.509] iteration 951: total_loss: 0.470944, loss_sup: 0.408200, loss_mps: 0.023770, loss_cps: 0.038974
[12:22:41.655] iteration 952: total_loss: 0.398967, loss_sup: 0.338658, loss_mps: 0.023588, loss_cps: 0.036722
[12:22:41.801] iteration 953: total_loss: 0.504190, loss_sup: 0.443723, loss_mps: 0.023050, loss_cps: 0.037416
[12:22:41.947] iteration 954: total_loss: 0.672699, loss_sup: 0.620478, loss_mps: 0.020862, loss_cps: 0.031359
[12:22:42.092] iteration 955: total_loss: 0.373258, loss_sup: 0.311573, loss_mps: 0.024180, loss_cps: 0.037505
[12:22:42.243] iteration 956: total_loss: 0.475387, loss_sup: 0.423441, loss_mps: 0.020977, loss_cps: 0.030969
[12:22:42.391] iteration 957: total_loss: 0.655536, loss_sup: 0.597823, loss_mps: 0.022376, loss_cps: 0.035337
[12:22:42.540] iteration 958: total_loss: 0.442656, loss_sup: 0.387965, loss_mps: 0.020872, loss_cps: 0.033819
[12:22:42.686] iteration 959: total_loss: 0.631895, loss_sup: 0.575364, loss_mps: 0.022140, loss_cps: 0.034391
[12:22:42.832] iteration 960: total_loss: 0.552727, loss_sup: 0.490684, loss_mps: 0.023415, loss_cps: 0.038627
[12:22:42.978] iteration 961: total_loss: 0.426715, loss_sup: 0.370819, loss_mps: 0.021562, loss_cps: 0.034335
[12:22:43.124] iteration 962: total_loss: 0.368133, loss_sup: 0.316682, loss_mps: 0.020726, loss_cps: 0.030725
[12:22:43.269] iteration 963: total_loss: 0.371274, loss_sup: 0.317280, loss_mps: 0.020483, loss_cps: 0.033511
[12:22:43.416] iteration 964: total_loss: 0.337211, loss_sup: 0.279058, loss_mps: 0.021819, loss_cps: 0.036335
[12:22:43.562] iteration 965: total_loss: 0.581877, loss_sup: 0.527954, loss_mps: 0.021090, loss_cps: 0.032833
[12:22:43.707] iteration 966: total_loss: 0.370084, loss_sup: 0.313626, loss_mps: 0.021513, loss_cps: 0.034945
[12:22:43.851] iteration 967: total_loss: 0.445867, loss_sup: 0.384930, loss_mps: 0.022887, loss_cps: 0.038051
[12:22:43.997] iteration 968: total_loss: 0.643968, loss_sup: 0.588317, loss_mps: 0.021291, loss_cps: 0.034359
[12:22:44.143] iteration 969: total_loss: 0.490947, loss_sup: 0.433844, loss_mps: 0.021241, loss_cps: 0.035863
[12:22:44.289] iteration 970: total_loss: 0.705967, loss_sup: 0.659382, loss_mps: 0.018378, loss_cps: 0.028207
[12:22:44.435] iteration 971: total_loss: 0.546295, loss_sup: 0.492508, loss_mps: 0.020611, loss_cps: 0.033175
[12:22:44.582] iteration 972: total_loss: 0.833088, loss_sup: 0.770519, loss_mps: 0.022355, loss_cps: 0.040214
[12:22:44.730] iteration 973: total_loss: 0.334739, loss_sup: 0.284484, loss_mps: 0.019779, loss_cps: 0.030476
[12:22:44.877] iteration 974: total_loss: 0.714439, loss_sup: 0.654088, loss_mps: 0.022681, loss_cps: 0.037670
[12:22:45.022] iteration 975: total_loss: 0.532655, loss_sup: 0.467178, loss_mps: 0.023640, loss_cps: 0.041837
[12:22:45.167] iteration 976: total_loss: 0.685122, loss_sup: 0.623463, loss_mps: 0.023856, loss_cps: 0.037804
[12:22:45.314] iteration 977: total_loss: 0.581927, loss_sup: 0.514180, loss_mps: 0.025059, loss_cps: 0.042688
[12:22:45.460] iteration 978: total_loss: 0.583549, loss_sup: 0.526573, loss_mps: 0.022715, loss_cps: 0.034262
[12:22:45.605] iteration 979: total_loss: 0.620389, loss_sup: 0.562317, loss_mps: 0.022794, loss_cps: 0.035278
[12:22:45.751] iteration 980: total_loss: 0.710515, loss_sup: 0.657534, loss_mps: 0.021525, loss_cps: 0.031457
[12:22:45.897] iteration 981: total_loss: 0.492158, loss_sup: 0.436035, loss_mps: 0.024067, loss_cps: 0.032056
[12:22:46.043] iteration 982: total_loss: 0.486952, loss_sup: 0.429485, loss_mps: 0.023642, loss_cps: 0.033825
[12:22:46.189] iteration 983: total_loss: 0.395313, loss_sup: 0.336606, loss_mps: 0.023883, loss_cps: 0.034823
[12:22:46.336] iteration 984: total_loss: 0.479860, loss_sup: 0.427709, loss_mps: 0.022122, loss_cps: 0.030030
[12:22:46.485] iteration 985: total_loss: 0.446609, loss_sup: 0.392735, loss_mps: 0.022717, loss_cps: 0.031157
[12:22:46.631] iteration 986: total_loss: 0.364806, loss_sup: 0.310816, loss_mps: 0.022971, loss_cps: 0.031019
[12:22:46.777] iteration 987: total_loss: 0.550366, loss_sup: 0.491445, loss_mps: 0.024179, loss_cps: 0.034742
[12:22:46.922] iteration 988: total_loss: 0.679000, loss_sup: 0.620058, loss_mps: 0.024317, loss_cps: 0.034625
[12:22:47.068] iteration 989: total_loss: 0.662191, loss_sup: 0.611806, loss_mps: 0.021087, loss_cps: 0.029298
[12:22:47.214] iteration 990: total_loss: 1.052997, loss_sup: 1.002135, loss_mps: 0.022032, loss_cps: 0.028831
[12:22:47.361] iteration 991: total_loss: 0.661350, loss_sup: 0.609551, loss_mps: 0.022250, loss_cps: 0.029549
[12:22:47.506] iteration 992: total_loss: 0.587722, loss_sup: 0.536488, loss_mps: 0.021915, loss_cps: 0.029319
[12:22:47.655] iteration 993: total_loss: 0.542800, loss_sup: 0.481474, loss_mps: 0.024797, loss_cps: 0.036530
[12:22:47.801] iteration 994: total_loss: 0.661174, loss_sup: 0.600848, loss_mps: 0.024226, loss_cps: 0.036100
[12:22:47.946] iteration 995: total_loss: 0.648539, loss_sup: 0.593827, loss_mps: 0.023161, loss_cps: 0.031551
[12:22:48.097] iteration 996: total_loss: 0.590616, loss_sup: 0.531499, loss_mps: 0.024060, loss_cps: 0.035058
[12:22:48.242] iteration 997: total_loss: 0.492140, loss_sup: 0.438280, loss_mps: 0.023613, loss_cps: 0.030247
[12:22:48.389] iteration 998: total_loss: 0.756183, loss_sup: 0.691141, loss_mps: 0.026247, loss_cps: 0.038796
[12:22:48.535] iteration 999: total_loss: 0.517949, loss_sup: 0.461154, loss_mps: 0.023740, loss_cps: 0.033055
[12:22:48.681] iteration 1000: total_loss: 0.798930, loss_sup: 0.732239, loss_mps: 0.026380, loss_cps: 0.040311
[12:22:48.681] Evaluation Started ==>
[12:22:59.995] ==> valid iteration 1000: unet metrics: {'dc': 0.32917521053685883, 'jc': 0.2358016824520694, 'pre': 0.515944712938663, 'hd': 7.157026018139659}, ynet metrics: {'dc': 0.37404889547725123, 'jc': 0.26963553180472166, 'pre': 0.5370056594673017, 'hd': 7.498300507723475}.
[12:22:59.997] Evaluation Finished!⏹️
[12:23:00.149] iteration 1001: total_loss: 0.669745, loss_sup: 0.612007, loss_mps: 0.024117, loss_cps: 0.033621
[12:23:00.297] iteration 1002: total_loss: 0.732690, loss_sup: 0.673220, loss_mps: 0.023977, loss_cps: 0.035493
[12:23:00.442] iteration 1003: total_loss: 0.484079, loss_sup: 0.421519, loss_mps: 0.025345, loss_cps: 0.037216
[12:23:00.587] iteration 1004: total_loss: 0.436204, loss_sup: 0.380996, loss_mps: 0.023721, loss_cps: 0.031487
[12:23:00.733] iteration 1005: total_loss: 0.388833, loss_sup: 0.340178, loss_mps: 0.020945, loss_cps: 0.027710
[12:23:00.878] iteration 1006: total_loss: 0.370272, loss_sup: 0.309858, loss_mps: 0.024862, loss_cps: 0.035551
[12:23:01.022] iteration 1007: total_loss: 0.538458, loss_sup: 0.486150, loss_mps: 0.022409, loss_cps: 0.029899
[12:23:01.167] iteration 1008: total_loss: 0.626944, loss_sup: 0.576728, loss_mps: 0.021446, loss_cps: 0.028769
[12:23:01.312] iteration 1009: total_loss: 0.468708, loss_sup: 0.416264, loss_mps: 0.021940, loss_cps: 0.030504
[12:23:01.457] iteration 1010: total_loss: 0.334440, loss_sup: 0.286049, loss_mps: 0.020824, loss_cps: 0.027567
[12:23:01.603] iteration 1011: total_loss: 0.622071, loss_sup: 0.569828, loss_mps: 0.021332, loss_cps: 0.030911
[12:23:01.748] iteration 1012: total_loss: 0.456102, loss_sup: 0.403183, loss_mps: 0.021072, loss_cps: 0.031847
[12:23:01.894] iteration 1013: total_loss: 0.526534, loss_sup: 0.474631, loss_mps: 0.021277, loss_cps: 0.030626
[12:23:02.039] iteration 1014: total_loss: 0.573141, loss_sup: 0.511338, loss_mps: 0.023642, loss_cps: 0.038161
[12:23:02.185] iteration 1015: total_loss: 0.686365, loss_sup: 0.637509, loss_mps: 0.020172, loss_cps: 0.028683
[12:23:02.331] iteration 1016: total_loss: 0.609115, loss_sup: 0.553914, loss_mps: 0.021919, loss_cps: 0.033282
[12:23:02.476] iteration 1017: total_loss: 0.645449, loss_sup: 0.582724, loss_mps: 0.024331, loss_cps: 0.038394
[12:23:02.621] iteration 1018: total_loss: 0.435551, loss_sup: 0.376650, loss_mps: 0.022748, loss_cps: 0.036154
[12:23:02.771] iteration 1019: total_loss: 0.490559, loss_sup: 0.438730, loss_mps: 0.021205, loss_cps: 0.030624
[12:23:02.916] iteration 1020: total_loss: 0.478854, loss_sup: 0.426727, loss_mps: 0.021481, loss_cps: 0.030646
[12:23:03.062] iteration 1021: total_loss: 0.607716, loss_sup: 0.560513, loss_mps: 0.019016, loss_cps: 0.028187
[12:23:03.208] iteration 1022: total_loss: 0.435932, loss_sup: 0.378911, loss_mps: 0.022620, loss_cps: 0.034400
[12:23:03.355] iteration 1023: total_loss: 0.261420, loss_sup: 0.212164, loss_mps: 0.019980, loss_cps: 0.029276
[12:23:03.500] iteration 1024: total_loss: 0.377163, loss_sup: 0.318668, loss_mps: 0.022983, loss_cps: 0.035512
[12:23:03.645] iteration 1025: total_loss: 0.433525, loss_sup: 0.369061, loss_mps: 0.024351, loss_cps: 0.040113
[12:23:03.791] iteration 1026: total_loss: 0.817017, loss_sup: 0.758874, loss_mps: 0.022596, loss_cps: 0.035547
[12:23:03.936] iteration 1027: total_loss: 0.318316, loss_sup: 0.256051, loss_mps: 0.023836, loss_cps: 0.038429
[12:23:04.081] iteration 1028: total_loss: 0.633732, loss_sup: 0.576062, loss_mps: 0.021995, loss_cps: 0.035676
[12:23:04.227] iteration 1029: total_loss: 0.440513, loss_sup: 0.388246, loss_mps: 0.020589, loss_cps: 0.031678
[12:23:04.375] iteration 1030: total_loss: 0.558885, loss_sup: 0.502612, loss_mps: 0.021924, loss_cps: 0.034349
[12:23:04.520] iteration 1031: total_loss: 0.354364, loss_sup: 0.302596, loss_mps: 0.021604, loss_cps: 0.030164
[12:23:04.665] iteration 1032: total_loss: 0.579782, loss_sup: 0.524957, loss_mps: 0.022063, loss_cps: 0.032762
[12:23:04.811] iteration 1033: total_loss: 0.502681, loss_sup: 0.444553, loss_mps: 0.023208, loss_cps: 0.034920
[12:23:04.956] iteration 1034: total_loss: 0.593794, loss_sup: 0.544442, loss_mps: 0.020189, loss_cps: 0.029162
[12:23:05.101] iteration 1035: total_loss: 0.631283, loss_sup: 0.574132, loss_mps: 0.022656, loss_cps: 0.034495
[12:23:05.247] iteration 1036: total_loss: 0.583765, loss_sup: 0.524129, loss_mps: 0.023432, loss_cps: 0.036204
[12:23:05.393] iteration 1037: total_loss: 0.298675, loss_sup: 0.247978, loss_mps: 0.021231, loss_cps: 0.029466
[12:23:05.538] iteration 1038: total_loss: 0.318472, loss_sup: 0.260302, loss_mps: 0.023309, loss_cps: 0.034861
[12:23:05.683] iteration 1039: total_loss: 0.567719, loss_sup: 0.513927, loss_mps: 0.021998, loss_cps: 0.031794
[12:23:05.829] iteration 1040: total_loss: 0.351857, loss_sup: 0.292307, loss_mps: 0.023236, loss_cps: 0.036313
[12:23:05.974] iteration 1041: total_loss: 0.346988, loss_sup: 0.279962, loss_mps: 0.025382, loss_cps: 0.041644
[12:23:06.120] iteration 1042: total_loss: 0.535777, loss_sup: 0.484647, loss_mps: 0.021226, loss_cps: 0.029903
[12:23:06.266] iteration 1043: total_loss: 0.832530, loss_sup: 0.778269, loss_mps: 0.021384, loss_cps: 0.032878
[12:23:06.412] iteration 1044: total_loss: 0.544339, loss_sup: 0.481650, loss_mps: 0.023201, loss_cps: 0.039487
[12:23:06.559] iteration 1045: total_loss: 0.707094, loss_sup: 0.640147, loss_mps: 0.025209, loss_cps: 0.041738
[12:23:06.707] iteration 1046: total_loss: 0.592003, loss_sup: 0.523002, loss_mps: 0.025085, loss_cps: 0.043916
[12:23:06.855] iteration 1047: total_loss: 0.739919, loss_sup: 0.681391, loss_mps: 0.022486, loss_cps: 0.036042
[12:23:07.000] iteration 1048: total_loss: 0.348949, loss_sup: 0.302481, loss_mps: 0.019858, loss_cps: 0.026611
[12:23:07.148] iteration 1049: total_loss: 0.387006, loss_sup: 0.328711, loss_mps: 0.022706, loss_cps: 0.035590
[12:23:07.294] iteration 1050: total_loss: 0.426874, loss_sup: 0.373582, loss_mps: 0.021488, loss_cps: 0.031804
[12:23:07.439] iteration 1051: total_loss: 0.424596, loss_sup: 0.371368, loss_mps: 0.021663, loss_cps: 0.031565
[12:23:07.586] iteration 1052: total_loss: 0.383274, loss_sup: 0.332215, loss_mps: 0.020394, loss_cps: 0.030665
[12:23:07.732] iteration 1053: total_loss: 0.461673, loss_sup: 0.410181, loss_mps: 0.020722, loss_cps: 0.030769
[12:23:07.879] iteration 1054: total_loss: 0.549326, loss_sup: 0.504353, loss_mps: 0.019223, loss_cps: 0.025750
[12:23:08.025] iteration 1055: total_loss: 1.008357, loss_sup: 0.953725, loss_mps: 0.021783, loss_cps: 0.032849
[12:23:08.171] iteration 1056: total_loss: 0.476124, loss_sup: 0.427211, loss_mps: 0.020424, loss_cps: 0.028488
[12:23:08.318] iteration 1057: total_loss: 0.446297, loss_sup: 0.387444, loss_mps: 0.023917, loss_cps: 0.034936
[12:23:08.464] iteration 1058: total_loss: 0.374097, loss_sup: 0.318037, loss_mps: 0.022794, loss_cps: 0.033265
[12:23:08.613] iteration 1059: total_loss: 0.571324, loss_sup: 0.512918, loss_mps: 0.023320, loss_cps: 0.035086
[12:23:08.758] iteration 1060: total_loss: 0.560175, loss_sup: 0.503099, loss_mps: 0.023369, loss_cps: 0.033706
[12:23:08.904] iteration 1061: total_loss: 0.463228, loss_sup: 0.395279, loss_mps: 0.026254, loss_cps: 0.041696
[12:23:09.049] iteration 1062: total_loss: 0.520629, loss_sup: 0.451152, loss_mps: 0.026374, loss_cps: 0.043103
[12:23:09.195] iteration 1063: total_loss: 0.467367, loss_sup: 0.406368, loss_mps: 0.023827, loss_cps: 0.037173
[12:23:09.340] iteration 1064: total_loss: 0.543269, loss_sup: 0.482468, loss_mps: 0.024008, loss_cps: 0.036793
[12:23:09.487] iteration 1065: total_loss: 0.491572, loss_sup: 0.425982, loss_mps: 0.025536, loss_cps: 0.040054
[12:23:09.635] iteration 1066: total_loss: 0.669253, loss_sup: 0.610622, loss_mps: 0.023152, loss_cps: 0.035479
[12:23:09.781] iteration 1067: total_loss: 0.568255, loss_sup: 0.507549, loss_mps: 0.023262, loss_cps: 0.037444
[12:23:09.927] iteration 1068: total_loss: 0.813210, loss_sup: 0.746046, loss_mps: 0.024867, loss_cps: 0.042297
[12:23:10.073] iteration 1069: total_loss: 0.473491, loss_sup: 0.418828, loss_mps: 0.021892, loss_cps: 0.032771
[12:23:10.219] iteration 1070: total_loss: 0.625061, loss_sup: 0.560078, loss_mps: 0.024555, loss_cps: 0.040428
[12:23:10.366] iteration 1071: total_loss: 0.312094, loss_sup: 0.256428, loss_mps: 0.022070, loss_cps: 0.033595
[12:23:10.512] iteration 1072: total_loss: 0.562690, loss_sup: 0.494454, loss_mps: 0.025433, loss_cps: 0.042803
[12:23:10.658] iteration 1073: total_loss: 0.606973, loss_sup: 0.546205, loss_mps: 0.023079, loss_cps: 0.037689
[12:23:10.806] iteration 1074: total_loss: 0.327012, loss_sup: 0.268815, loss_mps: 0.022516, loss_cps: 0.035681
[12:23:10.955] iteration 1075: total_loss: 0.687485, loss_sup: 0.619449, loss_mps: 0.024806, loss_cps: 0.043230
[12:23:11.102] iteration 1076: total_loss: 0.456520, loss_sup: 0.395622, loss_mps: 0.023600, loss_cps: 0.037299
[12:23:11.248] iteration 1077: total_loss: 0.412820, loss_sup: 0.360173, loss_mps: 0.020770, loss_cps: 0.031876
[12:23:11.394] iteration 1078: total_loss: 0.407707, loss_sup: 0.352479, loss_mps: 0.021798, loss_cps: 0.033430
[12:23:11.540] iteration 1079: total_loss: 0.652764, loss_sup: 0.596484, loss_mps: 0.021710, loss_cps: 0.034570
[12:23:11.687] iteration 1080: total_loss: 0.589766, loss_sup: 0.538219, loss_mps: 0.020416, loss_cps: 0.031131
[12:23:11.833] iteration 1081: total_loss: 0.530674, loss_sup: 0.469708, loss_mps: 0.023469, loss_cps: 0.037497
[12:23:11.981] iteration 1082: total_loss: 0.585817, loss_sup: 0.529195, loss_mps: 0.021881, loss_cps: 0.034740
[12:23:12.127] iteration 1083: total_loss: 0.526165, loss_sup: 0.465553, loss_mps: 0.023208, loss_cps: 0.037404
[12:23:12.272] iteration 1084: total_loss: 0.588626, loss_sup: 0.533439, loss_mps: 0.022235, loss_cps: 0.032952
[12:23:12.419] iteration 1085: total_loss: 0.477283, loss_sup: 0.421277, loss_mps: 0.021713, loss_cps: 0.034293
[12:23:12.565] iteration 1086: total_loss: 0.482097, loss_sup: 0.423530, loss_mps: 0.022244, loss_cps: 0.036322
[12:23:12.711] iteration 1087: total_loss: 0.605809, loss_sup: 0.548716, loss_mps: 0.022008, loss_cps: 0.035085
[12:23:12.857] iteration 1088: total_loss: 0.468341, loss_sup: 0.421442, loss_mps: 0.018947, loss_cps: 0.027952
[12:23:13.002] iteration 1089: total_loss: 0.437102, loss_sup: 0.388789, loss_mps: 0.019265, loss_cps: 0.029048
[12:23:13.147] iteration 1090: total_loss: 0.639572, loss_sup: 0.579500, loss_mps: 0.022914, loss_cps: 0.037157
[12:23:13.293] iteration 1091: total_loss: 0.401777, loss_sup: 0.351205, loss_mps: 0.020018, loss_cps: 0.030555
[12:23:13.439] iteration 1092: total_loss: 0.477284, loss_sup: 0.427723, loss_mps: 0.019646, loss_cps: 0.029915
[12:23:13.584] iteration 1093: total_loss: 0.626938, loss_sup: 0.568357, loss_mps: 0.022500, loss_cps: 0.036081
[12:23:13.730] iteration 1094: total_loss: 0.609746, loss_sup: 0.554267, loss_mps: 0.021876, loss_cps: 0.033602
[12:23:13.877] iteration 1095: total_loss: 0.494166, loss_sup: 0.443695, loss_mps: 0.020772, loss_cps: 0.029699
[12:23:14.022] iteration 1096: total_loss: 0.703880, loss_sup: 0.641979, loss_mps: 0.023991, loss_cps: 0.037910
[12:23:14.170] iteration 1097: total_loss: 0.404156, loss_sup: 0.347277, loss_mps: 0.022591, loss_cps: 0.034289
[12:23:14.316] iteration 1098: total_loss: 0.441313, loss_sup: 0.385131, loss_mps: 0.022708, loss_cps: 0.033474
[12:23:14.461] iteration 1099: total_loss: 0.448474, loss_sup: 0.393795, loss_mps: 0.022280, loss_cps: 0.032399
[12:23:14.607] iteration 1100: total_loss: 0.351272, loss_sup: 0.296904, loss_mps: 0.022613, loss_cps: 0.031755
[12:23:14.607] Evaluation Started ==>
[12:23:25.901] ==> valid iteration 1100: unet metrics: {'dc': 0.4050556123683731, 'jc': 0.2996772746536117, 'pre': 0.5059101257287245, 'hd': 7.247555186435486}, ynet metrics: {'dc': 0.38784197388481223, 'jc': 0.2857759128715814, 'pre': 0.44711309659417725, 'hd': 7.78862229414761}.
[12:23:25.904] Evaluation Finished!⏹️
[12:23:26.057] iteration 1101: total_loss: 0.488941, loss_sup: 0.432912, loss_mps: 0.022991, loss_cps: 0.033038
[12:23:26.207] iteration 1102: total_loss: 0.667791, loss_sup: 0.608762, loss_mps: 0.023987, loss_cps: 0.035042
[12:23:26.352] iteration 1103: total_loss: 0.410755, loss_sup: 0.362696, loss_mps: 0.020248, loss_cps: 0.027812
[12:23:26.497] iteration 1104: total_loss: 0.303425, loss_sup: 0.255529, loss_mps: 0.020349, loss_cps: 0.027548
[12:23:26.642] iteration 1105: total_loss: 0.352975, loss_sup: 0.306959, loss_mps: 0.018956, loss_cps: 0.027059
[12:23:26.787] iteration 1106: total_loss: 0.449926, loss_sup: 0.404839, loss_mps: 0.018796, loss_cps: 0.026290
[12:23:26.933] iteration 1107: total_loss: 0.507075, loss_sup: 0.450737, loss_mps: 0.021523, loss_cps: 0.034815
[12:23:27.078] iteration 1108: total_loss: 0.408967, loss_sup: 0.353777, loss_mps: 0.022045, loss_cps: 0.033145
[12:23:27.223] iteration 1109: total_loss: 0.364618, loss_sup: 0.314857, loss_mps: 0.019874, loss_cps: 0.029887
[12:23:27.368] iteration 1110: total_loss: 0.484787, loss_sup: 0.431959, loss_mps: 0.020236, loss_cps: 0.032592
[12:23:27.514] iteration 1111: total_loss: 0.507487, loss_sup: 0.451848, loss_mps: 0.021102, loss_cps: 0.034536
[12:23:27.667] iteration 1112: total_loss: 0.559891, loss_sup: 0.505115, loss_mps: 0.021293, loss_cps: 0.033483
[12:23:27.812] iteration 1113: total_loss: 0.517617, loss_sup: 0.459794, loss_mps: 0.022160, loss_cps: 0.035663
[12:23:27.958] iteration 1114: total_loss: 0.328973, loss_sup: 0.275553, loss_mps: 0.020690, loss_cps: 0.032730
[12:23:28.104] iteration 1115: total_loss: 0.589950, loss_sup: 0.535549, loss_mps: 0.020826, loss_cps: 0.033576
[12:23:28.250] iteration 1116: total_loss: 0.334913, loss_sup: 0.279311, loss_mps: 0.020654, loss_cps: 0.034949
[12:23:28.394] iteration 1117: total_loss: 0.347501, loss_sup: 0.296637, loss_mps: 0.019785, loss_cps: 0.031079
[12:23:28.539] iteration 1118: total_loss: 0.546335, loss_sup: 0.481227, loss_mps: 0.024012, loss_cps: 0.041096
[12:23:28.686] iteration 1119: total_loss: 0.480769, loss_sup: 0.426938, loss_mps: 0.020620, loss_cps: 0.033211
[12:23:28.832] iteration 1120: total_loss: 0.336933, loss_sup: 0.278486, loss_mps: 0.021697, loss_cps: 0.036750
[12:23:28.977] iteration 1121: total_loss: 0.541344, loss_sup: 0.491710, loss_mps: 0.019181, loss_cps: 0.030452
[12:23:29.123] iteration 1122: total_loss: 0.340779, loss_sup: 0.290215, loss_mps: 0.019711, loss_cps: 0.030853
[12:23:29.273] iteration 1123: total_loss: 0.685109, loss_sup: 0.611277, loss_mps: 0.026099, loss_cps: 0.047733
[12:23:29.418] iteration 1124: total_loss: 0.526466, loss_sup: 0.469990, loss_mps: 0.021365, loss_cps: 0.035111
[12:23:29.563] iteration 1125: total_loss: 0.510236, loss_sup: 0.449102, loss_mps: 0.022401, loss_cps: 0.038734
[12:23:29.709] iteration 1126: total_loss: 0.368054, loss_sup: 0.313074, loss_mps: 0.020634, loss_cps: 0.034346
[12:23:29.854] iteration 1127: total_loss: 0.314959, loss_sup: 0.249573, loss_mps: 0.024143, loss_cps: 0.041243
[12:23:29.999] iteration 1128: total_loss: 0.536888, loss_sup: 0.482128, loss_mps: 0.021401, loss_cps: 0.033359
[12:23:30.145] iteration 1129: total_loss: 0.525398, loss_sup: 0.463484, loss_mps: 0.022939, loss_cps: 0.038975
[12:23:30.290] iteration 1130: total_loss: 0.700853, loss_sup: 0.638571, loss_mps: 0.023498, loss_cps: 0.038784
[12:23:30.436] iteration 1131: total_loss: 0.394251, loss_sup: 0.337209, loss_mps: 0.022182, loss_cps: 0.034859
[12:23:30.581] iteration 1132: total_loss: 0.374708, loss_sup: 0.313315, loss_mps: 0.023465, loss_cps: 0.037928
[12:23:30.727] iteration 1133: total_loss: 0.464350, loss_sup: 0.391702, loss_mps: 0.026314, loss_cps: 0.046333
[12:23:30.872] iteration 1134: total_loss: 0.315427, loss_sup: 0.266061, loss_mps: 0.019799, loss_cps: 0.029567
[12:23:31.017] iteration 1135: total_loss: 0.418713, loss_sup: 0.369540, loss_mps: 0.020065, loss_cps: 0.029108
[12:23:31.163] iteration 1136: total_loss: 0.416965, loss_sup: 0.361697, loss_mps: 0.021672, loss_cps: 0.033596
[12:23:31.308] iteration 1137: total_loss: 0.625109, loss_sup: 0.564873, loss_mps: 0.022792, loss_cps: 0.037444
[12:23:31.453] iteration 1138: total_loss: 0.330266, loss_sup: 0.276937, loss_mps: 0.021223, loss_cps: 0.032105
[12:23:31.599] iteration 1139: total_loss: 0.373061, loss_sup: 0.312823, loss_mps: 0.023434, loss_cps: 0.036804
[12:23:31.746] iteration 1140: total_loss: 0.677848, loss_sup: 0.612161, loss_mps: 0.024431, loss_cps: 0.041256
[12:23:31.893] iteration 1141: total_loss: 0.651221, loss_sup: 0.595666, loss_mps: 0.021956, loss_cps: 0.033599
[12:23:32.039] iteration 1142: total_loss: 0.389660, loss_sup: 0.340284, loss_mps: 0.020270, loss_cps: 0.029106
[12:23:32.185] iteration 1143: total_loss: 0.606632, loss_sup: 0.555697, loss_mps: 0.021059, loss_cps: 0.029875
[12:23:32.330] iteration 1144: total_loss: 0.457074, loss_sup: 0.404390, loss_mps: 0.020757, loss_cps: 0.031927
[12:23:32.479] iteration 1145: total_loss: 0.660976, loss_sup: 0.606623, loss_mps: 0.021683, loss_cps: 0.032670
[12:23:32.626] iteration 1146: total_loss: 0.435623, loss_sup: 0.382270, loss_mps: 0.021538, loss_cps: 0.031816
[12:23:32.771] iteration 1147: total_loss: 0.323984, loss_sup: 0.269106, loss_mps: 0.021719, loss_cps: 0.033159
[12:23:32.916] iteration 1148: total_loss: 0.460326, loss_sup: 0.411132, loss_mps: 0.020266, loss_cps: 0.028928
[12:23:33.062] iteration 1149: total_loss: 0.513444, loss_sup: 0.456800, loss_mps: 0.022546, loss_cps: 0.034097
[12:23:33.207] iteration 1150: total_loss: 0.560149, loss_sup: 0.510109, loss_mps: 0.020479, loss_cps: 0.029562
[12:23:33.353] iteration 1151: total_loss: 0.400601, loss_sup: 0.350767, loss_mps: 0.020107, loss_cps: 0.029727
[12:23:33.499] iteration 1152: total_loss: 0.523013, loss_sup: 0.472745, loss_mps: 0.019933, loss_cps: 0.030336
[12:23:33.645] iteration 1153: total_loss: 0.352826, loss_sup: 0.290802, loss_mps: 0.022850, loss_cps: 0.039174
[12:23:33.791] iteration 1154: total_loss: 0.664215, loss_sup: 0.607505, loss_mps: 0.021635, loss_cps: 0.035075
[12:23:33.937] iteration 1155: total_loss: 0.389125, loss_sup: 0.336332, loss_mps: 0.020860, loss_cps: 0.031934
[12:23:34.086] iteration 1156: total_loss: 0.336885, loss_sup: 0.282878, loss_mps: 0.021149, loss_cps: 0.032858
[12:23:34.237] iteration 1157: total_loss: 0.395716, loss_sup: 0.347401, loss_mps: 0.020053, loss_cps: 0.028261
[12:23:34.382] iteration 1158: total_loss: 0.467093, loss_sup: 0.411467, loss_mps: 0.021564, loss_cps: 0.034062
[12:23:34.528] iteration 1159: total_loss: 0.302882, loss_sup: 0.256088, loss_mps: 0.019446, loss_cps: 0.027348
[12:23:34.673] iteration 1160: total_loss: 0.756217, loss_sup: 0.698922, loss_mps: 0.021807, loss_cps: 0.035488
[12:23:34.821] iteration 1161: total_loss: 0.590702, loss_sup: 0.529628, loss_mps: 0.022563, loss_cps: 0.038512
[12:23:34.967] iteration 1162: total_loss: 0.366740, loss_sup: 0.301549, loss_mps: 0.023837, loss_cps: 0.041355
[12:23:35.112] iteration 1163: total_loss: 0.450266, loss_sup: 0.389878, loss_mps: 0.021866, loss_cps: 0.038522
[12:23:35.258] iteration 1164: total_loss: 0.406577, loss_sup: 0.345043, loss_mps: 0.022413, loss_cps: 0.039120
[12:23:35.404] iteration 1165: total_loss: 0.505021, loss_sup: 0.450346, loss_mps: 0.020960, loss_cps: 0.033716
[12:23:35.549] iteration 1166: total_loss: 0.454395, loss_sup: 0.403113, loss_mps: 0.019801, loss_cps: 0.031481
[12:23:35.695] iteration 1167: total_loss: 0.375292, loss_sup: 0.326038, loss_mps: 0.018772, loss_cps: 0.030482
[12:23:35.840] iteration 1168: total_loss: 0.422776, loss_sup: 0.370876, loss_mps: 0.020281, loss_cps: 0.031620
[12:23:35.986] iteration 1169: total_loss: 0.398999, loss_sup: 0.350693, loss_mps: 0.019374, loss_cps: 0.028933
[12:23:36.132] iteration 1170: total_loss: 0.520384, loss_sup: 0.468625, loss_mps: 0.020306, loss_cps: 0.031453
[12:23:36.278] iteration 1171: total_loss: 0.687161, loss_sup: 0.635337, loss_mps: 0.020071, loss_cps: 0.031753
[12:23:36.424] iteration 1172: total_loss: 0.323324, loss_sup: 0.271743, loss_mps: 0.019969, loss_cps: 0.031612
[12:23:36.570] iteration 1173: total_loss: 0.525142, loss_sup: 0.477336, loss_mps: 0.019005, loss_cps: 0.028801
[12:23:36.715] iteration 1174: total_loss: 0.726924, loss_sup: 0.682377, loss_mps: 0.018365, loss_cps: 0.026182
[12:23:36.861] iteration 1175: total_loss: 0.341729, loss_sup: 0.293670, loss_mps: 0.019311, loss_cps: 0.028748
[12:23:37.007] iteration 1176: total_loss: 0.619243, loss_sup: 0.571172, loss_mps: 0.019305, loss_cps: 0.028765
[12:23:37.152] iteration 1177: total_loss: 0.376733, loss_sup: 0.318572, loss_mps: 0.022281, loss_cps: 0.035881
[12:23:37.299] iteration 1178: total_loss: 0.188718, loss_sup: 0.141524, loss_mps: 0.019228, loss_cps: 0.027967
[12:23:37.446] iteration 1179: total_loss: 0.375627, loss_sup: 0.323509, loss_mps: 0.020055, loss_cps: 0.032063
[12:23:37.591] iteration 1180: total_loss: 0.701400, loss_sup: 0.646603, loss_mps: 0.021123, loss_cps: 0.033674
[12:23:37.737] iteration 1181: total_loss: 0.245269, loss_sup: 0.188635, loss_mps: 0.021962, loss_cps: 0.034672
[12:23:37.884] iteration 1182: total_loss: 0.601493, loss_sup: 0.551371, loss_mps: 0.019996, loss_cps: 0.030127
[12:23:38.030] iteration 1183: total_loss: 0.339065, loss_sup: 0.285533, loss_mps: 0.020932, loss_cps: 0.032601
[12:23:38.176] iteration 1184: total_loss: 0.576972, loss_sup: 0.528269, loss_mps: 0.019576, loss_cps: 0.029128
[12:23:38.321] iteration 1185: total_loss: 0.450936, loss_sup: 0.403760, loss_mps: 0.018754, loss_cps: 0.028422
[12:23:38.467] iteration 1186: total_loss: 0.356818, loss_sup: 0.301354, loss_mps: 0.021846, loss_cps: 0.033619
[12:23:38.613] iteration 1187: total_loss: 0.892966, loss_sup: 0.825980, loss_mps: 0.024369, loss_cps: 0.042617
[12:23:38.760] iteration 1188: total_loss: 0.579862, loss_sup: 0.527575, loss_mps: 0.020187, loss_cps: 0.032100
[12:23:38.906] iteration 1189: total_loss: 0.253543, loss_sup: 0.198975, loss_mps: 0.021105, loss_cps: 0.033463
[12:23:39.052] iteration 1190: total_loss: 0.280006, loss_sup: 0.222560, loss_mps: 0.022256, loss_cps: 0.035191
[12:23:39.198] iteration 1191: total_loss: 0.440276, loss_sup: 0.374996, loss_mps: 0.023995, loss_cps: 0.041285
[12:23:39.343] iteration 1192: total_loss: 0.471784, loss_sup: 0.415460, loss_mps: 0.021149, loss_cps: 0.035176
[12:23:39.489] iteration 1193: total_loss: 0.271299, loss_sup: 0.218943, loss_mps: 0.020635, loss_cps: 0.031721
[12:23:39.635] iteration 1194: total_loss: 0.703189, loss_sup: 0.628545, loss_mps: 0.026795, loss_cps: 0.047849
[12:23:39.782] iteration 1195: total_loss: 0.412670, loss_sup: 0.339923, loss_mps: 0.025915, loss_cps: 0.046832
[12:23:39.928] iteration 1196: total_loss: 0.525577, loss_sup: 0.457393, loss_mps: 0.024213, loss_cps: 0.043971
[12:23:40.074] iteration 1197: total_loss: 0.650101, loss_sup: 0.581137, loss_mps: 0.024886, loss_cps: 0.044077
[12:23:40.221] iteration 1198: total_loss: 0.389407, loss_sup: 0.328903, loss_mps: 0.022774, loss_cps: 0.037730
[12:23:40.366] iteration 1199: total_loss: 0.561799, loss_sup: 0.505067, loss_mps: 0.021516, loss_cps: 0.035215
[12:23:40.512] iteration 1200: total_loss: 0.602468, loss_sup: 0.545339, loss_mps: 0.022508, loss_cps: 0.034621
[12:23:40.512] Evaluation Started ==>
[12:23:51.865] ==> valid iteration 1200: unet metrics: {'dc': 0.4017409266465421, 'jc': 0.29483835773275313, 'pre': 0.45352840760108337, 'hd': 7.814763799584491}, ynet metrics: {'dc': 0.3866658327573363, 'jc': 0.28235481534709744, 'pre': 0.3990023908122335, 'hd': 8.692122604718021}.
[12:23:51.866] Evaluation Finished!⏹️
[12:23:52.018] iteration 1201: total_loss: 0.323431, loss_sup: 0.257626, loss_mps: 0.024279, loss_cps: 0.041525
[12:23:52.166] iteration 1202: total_loss: 0.612717, loss_sup: 0.540903, loss_mps: 0.026407, loss_cps: 0.045406
[12:23:52.312] iteration 1203: total_loss: 0.505127, loss_sup: 0.445513, loss_mps: 0.023117, loss_cps: 0.036498
[12:23:52.462] iteration 1204: total_loss: 0.514385, loss_sup: 0.451844, loss_mps: 0.024193, loss_cps: 0.038348
[12:23:52.608] iteration 1205: total_loss: 0.564703, loss_sup: 0.504162, loss_mps: 0.023391, loss_cps: 0.037150
[12:23:52.756] iteration 1206: total_loss: 0.463855, loss_sup: 0.403936, loss_mps: 0.023252, loss_cps: 0.036667
[12:23:52.901] iteration 1207: total_loss: 0.741329, loss_sup: 0.682469, loss_mps: 0.023436, loss_cps: 0.035424
[12:23:53.047] iteration 1208: total_loss: 0.599377, loss_sup: 0.537181, loss_mps: 0.024831, loss_cps: 0.037364
[12:23:53.197] iteration 1209: total_loss: 0.528257, loss_sup: 0.470840, loss_mps: 0.023754, loss_cps: 0.033663
[12:23:53.343] iteration 1210: total_loss: 0.608664, loss_sup: 0.541273, loss_mps: 0.026518, loss_cps: 0.040874
[12:23:53.489] iteration 1211: total_loss: 0.521303, loss_sup: 0.458630, loss_mps: 0.025096, loss_cps: 0.037577
[12:23:53.635] iteration 1212: total_loss: 0.814557, loss_sup: 0.756126, loss_mps: 0.024096, loss_cps: 0.034335
[12:23:53.780] iteration 1213: total_loss: 0.540407, loss_sup: 0.482530, loss_mps: 0.024088, loss_cps: 0.033790
[12:23:53.925] iteration 1214: total_loss: 0.458415, loss_sup: 0.396580, loss_mps: 0.024748, loss_cps: 0.037088
[12:23:54.070] iteration 1215: total_loss: 0.405137, loss_sup: 0.344253, loss_mps: 0.024628, loss_cps: 0.036256
[12:23:54.215] iteration 1216: total_loss: 0.637862, loss_sup: 0.574987, loss_mps: 0.025304, loss_cps: 0.037571
[12:23:54.361] iteration 1217: total_loss: 0.503440, loss_sup: 0.436828, loss_mps: 0.026414, loss_cps: 0.040198
[12:23:54.507] iteration 1218: total_loss: 0.319315, loss_sup: 0.262231, loss_mps: 0.022509, loss_cps: 0.034576
[12:23:54.653] iteration 1219: total_loss: 0.577787, loss_sup: 0.518091, loss_mps: 0.023812, loss_cps: 0.035884
[12:23:54.801] iteration 1220: total_loss: 0.580794, loss_sup: 0.517796, loss_mps: 0.024381, loss_cps: 0.038617
[12:23:54.946] iteration 1221: total_loss: 0.344028, loss_sup: 0.285083, loss_mps: 0.023617, loss_cps: 0.035327
[12:23:55.094] iteration 1222: total_loss: 0.364011, loss_sup: 0.309334, loss_mps: 0.022444, loss_cps: 0.032232
[12:23:55.239] iteration 1223: total_loss: 0.394005, loss_sup: 0.342367, loss_mps: 0.021271, loss_cps: 0.030367
[12:23:55.389] iteration 1224: total_loss: 0.258876, loss_sup: 0.202552, loss_mps: 0.021710, loss_cps: 0.034614
[12:23:55.535] iteration 1225: total_loss: 0.730156, loss_sup: 0.659819, loss_mps: 0.025792, loss_cps: 0.044545
[12:23:55.681] iteration 1226: total_loss: 0.599801, loss_sup: 0.551121, loss_mps: 0.019556, loss_cps: 0.029124
[12:23:55.826] iteration 1227: total_loss: 0.673355, loss_sup: 0.618323, loss_mps: 0.020381, loss_cps: 0.034650
[12:23:55.972] iteration 1228: total_loss: 0.820723, loss_sup: 0.749165, loss_mps: 0.026031, loss_cps: 0.045527
[12:23:56.122] iteration 1229: total_loss: 0.606349, loss_sup: 0.555740, loss_mps: 0.020016, loss_cps: 0.030593
[12:23:56.267] iteration 1230: total_loss: 0.372842, loss_sup: 0.313395, loss_mps: 0.021586, loss_cps: 0.037861
[12:23:56.413] iteration 1231: total_loss: 0.677319, loss_sup: 0.622203, loss_mps: 0.021612, loss_cps: 0.033504
[12:23:56.559] iteration 1232: total_loss: 0.478585, loss_sup: 0.423486, loss_mps: 0.021205, loss_cps: 0.033893
[12:23:56.704] iteration 1233: total_loss: 0.395375, loss_sup: 0.341148, loss_mps: 0.021519, loss_cps: 0.032707
[12:23:56.850] iteration 1234: total_loss: 0.288198, loss_sup: 0.229472, loss_mps: 0.022379, loss_cps: 0.036347
[12:23:56.995] iteration 1235: total_loss: 0.586243, loss_sup: 0.531251, loss_mps: 0.021151, loss_cps: 0.033841
[12:23:57.146] iteration 1236: total_loss: 0.373672, loss_sup: 0.313216, loss_mps: 0.023593, loss_cps: 0.036864
[12:23:57.292] iteration 1237: total_loss: 0.388415, loss_sup: 0.332954, loss_mps: 0.022600, loss_cps: 0.032861
[12:23:57.438] iteration 1238: total_loss: 0.565435, loss_sup: 0.497113, loss_mps: 0.025798, loss_cps: 0.042523
[12:23:57.584] iteration 1239: total_loss: 0.430273, loss_sup: 0.363167, loss_mps: 0.025212, loss_cps: 0.041893
[12:23:57.730] iteration 1240: total_loss: 0.478763, loss_sup: 0.417602, loss_mps: 0.023238, loss_cps: 0.037922
[12:23:57.875] iteration 1241: total_loss: 0.623918, loss_sup: 0.561500, loss_mps: 0.024347, loss_cps: 0.038071
[12:23:58.021] iteration 1242: total_loss: 0.748873, loss_sup: 0.662573, loss_mps: 0.030810, loss_cps: 0.055490
[12:23:58.166] iteration 1243: total_loss: 0.476783, loss_sup: 0.413653, loss_mps: 0.024554, loss_cps: 0.038577
[12:23:58.312] iteration 1244: total_loss: 0.763299, loss_sup: 0.692113, loss_mps: 0.027111, loss_cps: 0.044075
[12:23:58.457] iteration 1245: total_loss: 0.555888, loss_sup: 0.495950, loss_mps: 0.023507, loss_cps: 0.036430
[12:23:58.603] iteration 1246: total_loss: 0.442551, loss_sup: 0.371007, loss_mps: 0.027293, loss_cps: 0.044251
[12:23:58.748] iteration 1247: total_loss: 0.425822, loss_sup: 0.360873, loss_mps: 0.025379, loss_cps: 0.039569
[12:23:58.894] iteration 1248: total_loss: 0.359342, loss_sup: 0.305865, loss_mps: 0.021616, loss_cps: 0.031860
[12:23:59.039] iteration 1249: total_loss: 0.515233, loss_sup: 0.460828, loss_mps: 0.022264, loss_cps: 0.032140
[12:23:59.184] iteration 1250: total_loss: 0.472239, loss_sup: 0.417429, loss_mps: 0.021837, loss_cps: 0.032974
[12:23:59.330] iteration 1251: total_loss: 0.603398, loss_sup: 0.539893, loss_mps: 0.024912, loss_cps: 0.038592
[12:23:59.476] iteration 1252: total_loss: 0.733141, loss_sup: 0.672682, loss_mps: 0.023975, loss_cps: 0.036484
[12:23:59.621] iteration 1253: total_loss: 0.476202, loss_sup: 0.416991, loss_mps: 0.023703, loss_cps: 0.035509
[12:23:59.686] iteration 1254: total_loss: 0.692733, loss_sup: 0.616143, loss_mps: 0.028358, loss_cps: 0.048232
[12:24:00.882] iteration 1255: total_loss: 0.440847, loss_sup: 0.378763, loss_mps: 0.024163, loss_cps: 0.037921
[12:24:01.030] iteration 1256: total_loss: 0.372598, loss_sup: 0.322926, loss_mps: 0.020602, loss_cps: 0.029070
[12:24:01.177] iteration 1257: total_loss: 0.543550, loss_sup: 0.472936, loss_mps: 0.026807, loss_cps: 0.043807
[12:24:01.324] iteration 1258: total_loss: 0.463851, loss_sup: 0.405401, loss_mps: 0.022871, loss_cps: 0.035580
[12:24:01.472] iteration 1259: total_loss: 0.419698, loss_sup: 0.360842, loss_mps: 0.022890, loss_cps: 0.035966
[12:24:01.618] iteration 1260: total_loss: 0.521113, loss_sup: 0.457214, loss_mps: 0.024560, loss_cps: 0.039339
[12:24:01.763] iteration 1261: total_loss: 0.417781, loss_sup: 0.360398, loss_mps: 0.022439, loss_cps: 0.034944
[12:24:01.908] iteration 1262: total_loss: 0.534772, loss_sup: 0.471078, loss_mps: 0.023947, loss_cps: 0.039747
[12:24:02.058] iteration 1263: total_loss: 0.403540, loss_sup: 0.349308, loss_mps: 0.021593, loss_cps: 0.032640
[12:24:02.203] iteration 1264: total_loss: 0.349045, loss_sup: 0.280643, loss_mps: 0.025036, loss_cps: 0.043366
[12:24:02.348] iteration 1265: total_loss: 0.453593, loss_sup: 0.389539, loss_mps: 0.024335, loss_cps: 0.039719
[12:24:02.494] iteration 1266: total_loss: 0.451171, loss_sup: 0.381329, loss_mps: 0.025355, loss_cps: 0.044487
[12:24:02.640] iteration 1267: total_loss: 0.555253, loss_sup: 0.484124, loss_mps: 0.026134, loss_cps: 0.044995
[12:24:02.785] iteration 1268: total_loss: 0.510670, loss_sup: 0.453312, loss_mps: 0.021580, loss_cps: 0.035778
[12:24:02.938] iteration 1269: total_loss: 0.462755, loss_sup: 0.405186, loss_mps: 0.021814, loss_cps: 0.035755
[12:24:03.084] iteration 1270: total_loss: 0.243496, loss_sup: 0.190146, loss_mps: 0.020761, loss_cps: 0.032589
[12:24:03.229] iteration 1271: total_loss: 0.692864, loss_sup: 0.636571, loss_mps: 0.021409, loss_cps: 0.034884
[12:24:03.377] iteration 1272: total_loss: 0.675516, loss_sup: 0.623951, loss_mps: 0.020732, loss_cps: 0.030833
[12:24:03.525] iteration 1273: total_loss: 0.495579, loss_sup: 0.437416, loss_mps: 0.022783, loss_cps: 0.035380
[12:24:03.675] iteration 1274: total_loss: 0.539911, loss_sup: 0.485872, loss_mps: 0.021583, loss_cps: 0.032456
[12:24:03.821] iteration 1275: total_loss: 0.386660, loss_sup: 0.330416, loss_mps: 0.022607, loss_cps: 0.033636
[12:24:03.968] iteration 1276: total_loss: 0.448411, loss_sup: 0.391935, loss_mps: 0.022734, loss_cps: 0.033742
[12:24:04.115] iteration 1277: total_loss: 0.547104, loss_sup: 0.482992, loss_mps: 0.024839, loss_cps: 0.039273
[12:24:04.261] iteration 1278: total_loss: 0.431506, loss_sup: 0.374126, loss_mps: 0.022345, loss_cps: 0.035035
[12:24:04.408] iteration 1279: total_loss: 0.416846, loss_sup: 0.362276, loss_mps: 0.021947, loss_cps: 0.032623
[12:24:04.554] iteration 1280: total_loss: 0.370571, loss_sup: 0.305311, loss_mps: 0.025389, loss_cps: 0.039870
[12:24:04.699] iteration 1281: total_loss: 0.386932, loss_sup: 0.331127, loss_mps: 0.022390, loss_cps: 0.033415
[12:24:04.846] iteration 1282: total_loss: 0.536499, loss_sup: 0.480567, loss_mps: 0.022878, loss_cps: 0.033054
[12:24:04.992] iteration 1283: total_loss: 0.422691, loss_sup: 0.367451, loss_mps: 0.022305, loss_cps: 0.032935
[12:24:05.138] iteration 1284: total_loss: 0.420826, loss_sup: 0.352807, loss_mps: 0.026049, loss_cps: 0.041970
[12:24:05.284] iteration 1285: total_loss: 0.431587, loss_sup: 0.377183, loss_mps: 0.021749, loss_cps: 0.032656
[12:24:05.431] iteration 1286: total_loss: 0.519705, loss_sup: 0.466930, loss_mps: 0.021180, loss_cps: 0.031595
[12:24:05.578] iteration 1287: total_loss: 0.402083, loss_sup: 0.348577, loss_mps: 0.022075, loss_cps: 0.031431
[12:24:05.723] iteration 1288: total_loss: 0.396309, loss_sup: 0.339632, loss_mps: 0.022344, loss_cps: 0.034332
[12:24:05.870] iteration 1289: total_loss: 0.732209, loss_sup: 0.675755, loss_mps: 0.022096, loss_cps: 0.034359
[12:24:06.015] iteration 1290: total_loss: 0.551401, loss_sup: 0.490624, loss_mps: 0.023086, loss_cps: 0.037691
[12:24:06.161] iteration 1291: total_loss: 0.462300, loss_sup: 0.411243, loss_mps: 0.020894, loss_cps: 0.030164
[12:24:06.307] iteration 1292: total_loss: 0.336974, loss_sup: 0.279548, loss_mps: 0.022116, loss_cps: 0.035310
[12:24:06.453] iteration 1293: total_loss: 0.386041, loss_sup: 0.335604, loss_mps: 0.020862, loss_cps: 0.029575
[12:24:06.602] iteration 1294: total_loss: 0.575794, loss_sup: 0.525356, loss_mps: 0.020575, loss_cps: 0.029863
[12:24:06.748] iteration 1295: total_loss: 0.407828, loss_sup: 0.356269, loss_mps: 0.020537, loss_cps: 0.031023
[12:24:06.893] iteration 1296: total_loss: 0.328298, loss_sup: 0.269012, loss_mps: 0.023073, loss_cps: 0.036214
[12:24:07.039] iteration 1297: total_loss: 0.554133, loss_sup: 0.492408, loss_mps: 0.023349, loss_cps: 0.038375
[12:24:07.186] iteration 1298: total_loss: 0.403043, loss_sup: 0.346925, loss_mps: 0.022739, loss_cps: 0.033378
[12:24:07.333] iteration 1299: total_loss: 0.395740, loss_sup: 0.342649, loss_mps: 0.021586, loss_cps: 0.031505
[12:24:07.478] iteration 1300: total_loss: 0.426243, loss_sup: 0.359603, loss_mps: 0.025544, loss_cps: 0.041095
[12:24:07.478] Evaluation Started ==>
[12:24:18.792] ==> valid iteration 1300: unet metrics: {'dc': 0.41507745526945145, 'jc': 0.3020581938146763, 'pre': 0.4823804258050305, 'hd': 7.738205554297588}, ynet metrics: {'dc': 0.3991317029930366, 'jc': 0.29013748747451523, 'pre': 0.4403058799333907, 'hd': 8.181427427540195}.
[12:24:18.794] Evaluation Finished!⏹️
[12:24:18.949] iteration 1301: total_loss: 0.462409, loss_sup: 0.407922, loss_mps: 0.021511, loss_cps: 0.032976
[12:24:19.096] iteration 1302: total_loss: 0.364960, loss_sup: 0.300571, loss_mps: 0.024652, loss_cps: 0.039737
[12:24:19.241] iteration 1303: total_loss: 0.571289, loss_sup: 0.513288, loss_mps: 0.022985, loss_cps: 0.035016
[12:24:19.386] iteration 1304: total_loss: 0.395148, loss_sup: 0.328592, loss_mps: 0.024559, loss_cps: 0.041997
[12:24:19.531] iteration 1305: total_loss: 0.324550, loss_sup: 0.258972, loss_mps: 0.024721, loss_cps: 0.040857
[12:24:19.680] iteration 1306: total_loss: 0.403282, loss_sup: 0.347193, loss_mps: 0.022192, loss_cps: 0.033898
[12:24:19.828] iteration 1307: total_loss: 0.495483, loss_sup: 0.425608, loss_mps: 0.026333, loss_cps: 0.043542
[12:24:19.973] iteration 1308: total_loss: 0.487956, loss_sup: 0.424460, loss_mps: 0.024332, loss_cps: 0.039165
[12:24:20.119] iteration 1309: total_loss: 0.500542, loss_sup: 0.430960, loss_mps: 0.025905, loss_cps: 0.043677
[12:24:20.265] iteration 1310: total_loss: 0.359945, loss_sup: 0.303692, loss_mps: 0.022085, loss_cps: 0.034169
[12:24:20.410] iteration 1311: total_loss: 0.650443, loss_sup: 0.601441, loss_mps: 0.020625, loss_cps: 0.028376
[12:24:20.555] iteration 1312: total_loss: 0.548977, loss_sup: 0.478117, loss_mps: 0.026790, loss_cps: 0.044070
[12:24:20.703] iteration 1313: total_loss: 0.503225, loss_sup: 0.444848, loss_mps: 0.023119, loss_cps: 0.035258
[12:24:20.849] iteration 1314: total_loss: 0.494379, loss_sup: 0.435125, loss_mps: 0.022063, loss_cps: 0.037190
[12:24:20.995] iteration 1315: total_loss: 0.487042, loss_sup: 0.431620, loss_mps: 0.021791, loss_cps: 0.033631
[12:24:21.140] iteration 1316: total_loss: 0.547780, loss_sup: 0.495021, loss_mps: 0.020811, loss_cps: 0.031948
[12:24:21.285] iteration 1317: total_loss: 0.299356, loss_sup: 0.253319, loss_mps: 0.018727, loss_cps: 0.027311
[12:24:21.431] iteration 1318: total_loss: 0.354531, loss_sup: 0.296419, loss_mps: 0.022488, loss_cps: 0.035623
[12:24:21.577] iteration 1319: total_loss: 0.302847, loss_sup: 0.252664, loss_mps: 0.019606, loss_cps: 0.030576
[12:24:21.724] iteration 1320: total_loss: 0.338430, loss_sup: 0.295598, loss_mps: 0.017860, loss_cps: 0.024972
[12:24:21.874] iteration 1321: total_loss: 0.489599, loss_sup: 0.439288, loss_mps: 0.020090, loss_cps: 0.030220
[12:24:22.019] iteration 1322: total_loss: 0.447086, loss_sup: 0.396981, loss_mps: 0.019484, loss_cps: 0.030622
[12:24:22.164] iteration 1323: total_loss: 0.351245, loss_sup: 0.292377, loss_mps: 0.021624, loss_cps: 0.037244
[12:24:22.310] iteration 1324: total_loss: 0.535662, loss_sup: 0.477398, loss_mps: 0.021844, loss_cps: 0.036420
[12:24:22.456] iteration 1325: total_loss: 0.463139, loss_sup: 0.409294, loss_mps: 0.020840, loss_cps: 0.033006
[12:24:22.601] iteration 1326: total_loss: 0.359441, loss_sup: 0.308714, loss_mps: 0.019683, loss_cps: 0.031044
[12:24:22.749] iteration 1327: total_loss: 0.462747, loss_sup: 0.396383, loss_mps: 0.023839, loss_cps: 0.042525
[12:24:22.896] iteration 1328: total_loss: 0.592176, loss_sup: 0.539737, loss_mps: 0.020261, loss_cps: 0.032178
[12:24:23.042] iteration 1329: total_loss: 0.368300, loss_sup: 0.312114, loss_mps: 0.020973, loss_cps: 0.035214
[12:24:23.188] iteration 1330: total_loss: 0.446123, loss_sup: 0.389886, loss_mps: 0.021579, loss_cps: 0.034658
[12:24:23.333] iteration 1331: total_loss: 0.678100, loss_sup: 0.614671, loss_mps: 0.023352, loss_cps: 0.040078
[12:24:23.479] iteration 1332: total_loss: 0.376957, loss_sup: 0.318081, loss_mps: 0.022410, loss_cps: 0.036467
[12:24:23.626] iteration 1333: total_loss: 0.510813, loss_sup: 0.443582, loss_mps: 0.024485, loss_cps: 0.042746
[12:24:23.772] iteration 1334: total_loss: 0.447555, loss_sup: 0.392883, loss_mps: 0.021329, loss_cps: 0.033343
[12:24:23.920] iteration 1335: total_loss: 0.610303, loss_sup: 0.556190, loss_mps: 0.021551, loss_cps: 0.032561
[12:24:24.066] iteration 1336: total_loss: 0.504885, loss_sup: 0.446530, loss_mps: 0.021863, loss_cps: 0.036492
[12:24:24.213] iteration 1337: total_loss: 0.343437, loss_sup: 0.279546, loss_mps: 0.023367, loss_cps: 0.040524
[12:24:24.362] iteration 1338: total_loss: 0.399026, loss_sup: 0.326994, loss_mps: 0.026049, loss_cps: 0.045983
[12:24:24.511] iteration 1339: total_loss: 0.666784, loss_sup: 0.616252, loss_mps: 0.020404, loss_cps: 0.030127
[12:24:24.658] iteration 1340: total_loss: 0.426491, loss_sup: 0.360764, loss_mps: 0.025033, loss_cps: 0.040694
[12:24:24.803] iteration 1341: total_loss: 0.564670, loss_sup: 0.503187, loss_mps: 0.024760, loss_cps: 0.036724
[12:24:24.949] iteration 1342: total_loss: 0.482831, loss_sup: 0.429159, loss_mps: 0.021652, loss_cps: 0.032021
[12:24:25.095] iteration 1343: total_loss: 0.317800, loss_sup: 0.257968, loss_mps: 0.023459, loss_cps: 0.036373
[12:24:25.241] iteration 1344: total_loss: 0.425943, loss_sup: 0.380126, loss_mps: 0.019827, loss_cps: 0.025990
[12:24:25.387] iteration 1345: total_loss: 0.533049, loss_sup: 0.474022, loss_mps: 0.023161, loss_cps: 0.035867
[12:24:25.535] iteration 1346: total_loss: 0.393093, loss_sup: 0.330353, loss_mps: 0.024243, loss_cps: 0.038496
[12:24:25.681] iteration 1347: total_loss: 0.491054, loss_sup: 0.435092, loss_mps: 0.022355, loss_cps: 0.033608
[12:24:25.828] iteration 1348: total_loss: 0.560589, loss_sup: 0.508457, loss_mps: 0.020663, loss_cps: 0.031469
[12:24:25.973] iteration 1349: total_loss: 0.444901, loss_sup: 0.387732, loss_mps: 0.022436, loss_cps: 0.034732
[12:24:26.119] iteration 1350: total_loss: 0.421638, loss_sup: 0.361521, loss_mps: 0.022506, loss_cps: 0.037611
[12:24:26.265] iteration 1351: total_loss: 0.470159, loss_sup: 0.409766, loss_mps: 0.023529, loss_cps: 0.036864
[12:24:26.410] iteration 1352: total_loss: 0.395067, loss_sup: 0.324145, loss_mps: 0.025135, loss_cps: 0.045787
[12:24:26.556] iteration 1353: total_loss: 0.427823, loss_sup: 0.364097, loss_mps: 0.023324, loss_cps: 0.040402
[12:24:26.702] iteration 1354: total_loss: 0.463305, loss_sup: 0.394028, loss_mps: 0.025105, loss_cps: 0.044171
[12:24:26.847] iteration 1355: total_loss: 0.698570, loss_sup: 0.637560, loss_mps: 0.022258, loss_cps: 0.038752
[12:24:26.993] iteration 1356: total_loss: 0.763460, loss_sup: 0.695252, loss_mps: 0.024278, loss_cps: 0.043929
[12:24:27.138] iteration 1357: total_loss: 0.554392, loss_sup: 0.500411, loss_mps: 0.020957, loss_cps: 0.033024
[12:24:27.284] iteration 1358: total_loss: 0.488594, loss_sup: 0.423355, loss_mps: 0.023947, loss_cps: 0.041292
[12:24:27.429] iteration 1359: total_loss: 0.671161, loss_sup: 0.606003, loss_mps: 0.024719, loss_cps: 0.040439
[12:24:27.575] iteration 1360: total_loss: 0.400506, loss_sup: 0.336808, loss_mps: 0.023988, loss_cps: 0.039710
[12:24:27.720] iteration 1361: total_loss: 0.615412, loss_sup: 0.552331, loss_mps: 0.023241, loss_cps: 0.039839
[12:24:27.867] iteration 1362: total_loss: 0.507854, loss_sup: 0.453517, loss_mps: 0.021398, loss_cps: 0.032939
[12:24:28.013] iteration 1363: total_loss: 0.287079, loss_sup: 0.227545, loss_mps: 0.022511, loss_cps: 0.037023
[12:24:28.159] iteration 1364: total_loss: 0.447907, loss_sup: 0.379259, loss_mps: 0.026112, loss_cps: 0.042536
[12:24:28.305] iteration 1365: total_loss: 0.503207, loss_sup: 0.433243, loss_mps: 0.026317, loss_cps: 0.043647
[12:24:28.451] iteration 1366: total_loss: 0.640913, loss_sup: 0.558796, loss_mps: 0.029177, loss_cps: 0.052940
[12:24:28.596] iteration 1367: total_loss: 0.489900, loss_sup: 0.425849, loss_mps: 0.025529, loss_cps: 0.038522
[12:24:28.741] iteration 1368: total_loss: 0.543833, loss_sup: 0.482413, loss_mps: 0.025076, loss_cps: 0.036344
[12:24:28.887] iteration 1369: total_loss: 0.417037, loss_sup: 0.357684, loss_mps: 0.023198, loss_cps: 0.036155
[12:24:29.033] iteration 1370: total_loss: 0.443159, loss_sup: 0.377415, loss_mps: 0.025185, loss_cps: 0.040559
[12:24:29.178] iteration 1371: total_loss: 0.419938, loss_sup: 0.358196, loss_mps: 0.024663, loss_cps: 0.037079
[12:24:29.323] iteration 1372: total_loss: 0.531541, loss_sup: 0.465810, loss_mps: 0.025065, loss_cps: 0.040665
[12:24:29.468] iteration 1373: total_loss: 0.460602, loss_sup: 0.395460, loss_mps: 0.024449, loss_cps: 0.040693
[12:24:29.614] iteration 1374: total_loss: 0.522548, loss_sup: 0.454034, loss_mps: 0.026398, loss_cps: 0.042115
[12:24:29.761] iteration 1375: total_loss: 0.417620, loss_sup: 0.361969, loss_mps: 0.022534, loss_cps: 0.033117
[12:24:29.906] iteration 1376: total_loss: 0.438350, loss_sup: 0.375703, loss_mps: 0.024070, loss_cps: 0.038578
[12:24:30.052] iteration 1377: total_loss: 0.435852, loss_sup: 0.370712, loss_mps: 0.024522, loss_cps: 0.040618
[12:24:30.199] iteration 1378: total_loss: 0.467337, loss_sup: 0.409362, loss_mps: 0.022078, loss_cps: 0.035896
[12:24:30.345] iteration 1379: total_loss: 0.336300, loss_sup: 0.286290, loss_mps: 0.020176, loss_cps: 0.029833
[12:24:30.490] iteration 1380: total_loss: 0.267188, loss_sup: 0.217261, loss_mps: 0.019691, loss_cps: 0.030237
[12:24:30.636] iteration 1381: total_loss: 0.429764, loss_sup: 0.373563, loss_mps: 0.022553, loss_cps: 0.033648
[12:24:30.782] iteration 1382: total_loss: 0.450399, loss_sup: 0.397108, loss_mps: 0.020626, loss_cps: 0.032665
[12:24:30.927] iteration 1383: total_loss: 0.640386, loss_sup: 0.591047, loss_mps: 0.020036, loss_cps: 0.029303
[12:24:31.074] iteration 1384: total_loss: 0.731524, loss_sup: 0.668469, loss_mps: 0.023243, loss_cps: 0.039812
[12:24:31.220] iteration 1385: total_loss: 0.472345, loss_sup: 0.406710, loss_mps: 0.024719, loss_cps: 0.040916
[12:24:31.367] iteration 1386: total_loss: 0.397130, loss_sup: 0.331460, loss_mps: 0.024760, loss_cps: 0.040910
[12:24:31.514] iteration 1387: total_loss: 0.456417, loss_sup: 0.391291, loss_mps: 0.023945, loss_cps: 0.041182
[12:24:31.662] iteration 1388: total_loss: 0.592263, loss_sup: 0.539511, loss_mps: 0.020881, loss_cps: 0.031871
[12:24:31.809] iteration 1389: total_loss: 0.300258, loss_sup: 0.250772, loss_mps: 0.020977, loss_cps: 0.028509
[12:24:31.955] iteration 1390: total_loss: 0.438783, loss_sup: 0.389313, loss_mps: 0.020157, loss_cps: 0.029314
[12:24:32.103] iteration 1391: total_loss: 0.473740, loss_sup: 0.413913, loss_mps: 0.022588, loss_cps: 0.037238
[12:24:32.249] iteration 1392: total_loss: 0.516335, loss_sup: 0.462883, loss_mps: 0.021499, loss_cps: 0.031954
[12:24:32.394] iteration 1393: total_loss: 0.423241, loss_sup: 0.370995, loss_mps: 0.021658, loss_cps: 0.030587
[12:24:32.541] iteration 1394: total_loss: 0.310725, loss_sup: 0.256481, loss_mps: 0.021570, loss_cps: 0.032674
[12:24:32.687] iteration 1395: total_loss: 0.403304, loss_sup: 0.355983, loss_mps: 0.019944, loss_cps: 0.027377
[12:24:32.833] iteration 1396: total_loss: 0.387025, loss_sup: 0.329140, loss_mps: 0.023049, loss_cps: 0.034835
[12:24:32.979] iteration 1397: total_loss: 0.565422, loss_sup: 0.502716, loss_mps: 0.024319, loss_cps: 0.038387
[12:24:33.126] iteration 1398: total_loss: 0.486334, loss_sup: 0.431579, loss_mps: 0.021959, loss_cps: 0.032796
[12:24:33.271] iteration 1399: total_loss: 0.250702, loss_sup: 0.191021, loss_mps: 0.022245, loss_cps: 0.037437
[12:24:33.417] iteration 1400: total_loss: 0.636158, loss_sup: 0.562030, loss_mps: 0.026814, loss_cps: 0.047314
[12:24:33.417] Evaluation Started ==>
[12:24:44.817] ==> valid iteration 1400: unet metrics: {'dc': 0.4526720336876179, 'jc': 0.33044116305335486, 'pre': 0.4783361081737905, 'hd': 7.999140156831893}, ynet metrics: {'dc': 0.4559847687290826, 'jc': 0.33656772479056346, 'pre': 0.5007850833809727, 'hd': 7.750595414345897}.
[12:24:44.877] ==> New best valid dice for unet: 0.452672, at iteration 1400
[12:24:45.038] ==> New best valid dice for ynet: 0.455985, at iteration 1400
[12:24:45.040] Evaluation Finished!⏹️
[12:24:45.192] iteration 1401: total_loss: 0.317888, loss_sup: 0.264849, loss_mps: 0.020597, loss_cps: 0.032442
[12:24:45.339] iteration 1402: total_loss: 0.417036, loss_sup: 0.350369, loss_mps: 0.024324, loss_cps: 0.042342
[12:24:45.484] iteration 1403: total_loss: 0.364771, loss_sup: 0.301688, loss_mps: 0.023738, loss_cps: 0.039346
[12:24:45.631] iteration 1404: total_loss: 0.491372, loss_sup: 0.428691, loss_mps: 0.022813, loss_cps: 0.039868
[12:24:45.776] iteration 1405: total_loss: 0.540083, loss_sup: 0.479446, loss_mps: 0.022640, loss_cps: 0.037998
[12:24:45.921] iteration 1406: total_loss: 0.427705, loss_sup: 0.369053, loss_mps: 0.021863, loss_cps: 0.036789
[12:24:46.067] iteration 1407: total_loss: 0.352978, loss_sup: 0.302604, loss_mps: 0.020058, loss_cps: 0.030315
[12:24:46.212] iteration 1408: total_loss: 0.271053, loss_sup: 0.211091, loss_mps: 0.022456, loss_cps: 0.037506
[12:24:46.357] iteration 1409: total_loss: 0.459006, loss_sup: 0.396356, loss_mps: 0.023175, loss_cps: 0.039475
[12:24:46.507] iteration 1410: total_loss: 0.463325, loss_sup: 0.397068, loss_mps: 0.024330, loss_cps: 0.041927
[12:24:46.653] iteration 1411: total_loss: 0.451941, loss_sup: 0.390915, loss_mps: 0.023325, loss_cps: 0.037701
[12:24:46.798] iteration 1412: total_loss: 0.246242, loss_sup: 0.187313, loss_mps: 0.021991, loss_cps: 0.036938
[12:24:46.943] iteration 1413: total_loss: 0.456539, loss_sup: 0.403680, loss_mps: 0.020767, loss_cps: 0.032093
[12:24:47.088] iteration 1414: total_loss: 0.527732, loss_sup: 0.461870, loss_mps: 0.024125, loss_cps: 0.041737
[12:24:47.233] iteration 1415: total_loss: 0.308678, loss_sup: 0.248957, loss_mps: 0.022328, loss_cps: 0.037393
[12:24:47.378] iteration 1416: total_loss: 0.759311, loss_sup: 0.685815, loss_mps: 0.026159, loss_cps: 0.047337
[12:24:47.523] iteration 1417: total_loss: 0.544836, loss_sup: 0.486732, loss_mps: 0.021824, loss_cps: 0.036280
[12:24:47.668] iteration 1418: total_loss: 0.540518, loss_sup: 0.477648, loss_mps: 0.023655, loss_cps: 0.039216
[12:24:47.813] iteration 1419: total_loss: 0.476587, loss_sup: 0.420760, loss_mps: 0.022315, loss_cps: 0.033513
[12:24:47.959] iteration 1420: total_loss: 0.431689, loss_sup: 0.369476, loss_mps: 0.023286, loss_cps: 0.038927
[12:24:48.104] iteration 1421: total_loss: 0.553142, loss_sup: 0.479063, loss_mps: 0.027039, loss_cps: 0.047040
[12:24:48.250] iteration 1422: total_loss: 0.596193, loss_sup: 0.533673, loss_mps: 0.024407, loss_cps: 0.038112
[12:24:48.395] iteration 1423: total_loss: 0.487211, loss_sup: 0.425344, loss_mps: 0.024508, loss_cps: 0.037359
[12:24:48.541] iteration 1424: total_loss: 0.509811, loss_sup: 0.438479, loss_mps: 0.026885, loss_cps: 0.044447
[12:24:48.686] iteration 1425: total_loss: 0.512200, loss_sup: 0.436839, loss_mps: 0.027860, loss_cps: 0.047501
[12:24:48.831] iteration 1426: total_loss: 0.274835, loss_sup: 0.207011, loss_mps: 0.026111, loss_cps: 0.041712
[12:24:48.977] iteration 1427: total_loss: 0.476006, loss_sup: 0.406647, loss_mps: 0.026038, loss_cps: 0.043321
[12:24:49.123] iteration 1428: total_loss: 0.638277, loss_sup: 0.566346, loss_mps: 0.026909, loss_cps: 0.045022
[12:24:49.269] iteration 1429: total_loss: 0.431225, loss_sup: 0.375755, loss_mps: 0.021935, loss_cps: 0.033535
[12:24:49.414] iteration 1430: total_loss: 0.565825, loss_sup: 0.502545, loss_mps: 0.024886, loss_cps: 0.038394
[12:24:49.560] iteration 1431: total_loss: 0.594013, loss_sup: 0.528225, loss_mps: 0.025055, loss_cps: 0.040733
[12:24:49.706] iteration 1432: total_loss: 0.308724, loss_sup: 0.252221, loss_mps: 0.022812, loss_cps: 0.033691
[12:24:49.851] iteration 1433: total_loss: 0.614392, loss_sup: 0.554543, loss_mps: 0.023352, loss_cps: 0.036497
[12:24:49.998] iteration 1434: total_loss: 0.601665, loss_sup: 0.542335, loss_mps: 0.023680, loss_cps: 0.035649
[12:24:50.145] iteration 1435: total_loss: 0.567110, loss_sup: 0.511107, loss_mps: 0.022342, loss_cps: 0.033661
[12:24:50.292] iteration 1436: total_loss: 0.487132, loss_sup: 0.428557, loss_mps: 0.023922, loss_cps: 0.034653
[12:24:50.437] iteration 1437: total_loss: 0.387655, loss_sup: 0.334443, loss_mps: 0.021949, loss_cps: 0.031263
[12:24:50.583] iteration 1438: total_loss: 0.498665, loss_sup: 0.441074, loss_mps: 0.023524, loss_cps: 0.034067
[12:24:50.729] iteration 1439: total_loss: 0.386008, loss_sup: 0.319550, loss_mps: 0.026616, loss_cps: 0.039842
[12:24:50.875] iteration 1440: total_loss: 0.389479, loss_sup: 0.331061, loss_mps: 0.023741, loss_cps: 0.034678
[12:24:51.021] iteration 1441: total_loss: 0.440072, loss_sup: 0.379920, loss_mps: 0.024157, loss_cps: 0.035995
[12:24:51.167] iteration 1442: total_loss: 0.412438, loss_sup: 0.348733, loss_mps: 0.024948, loss_cps: 0.038757
[12:24:51.313] iteration 1443: total_loss: 0.364525, loss_sup: 0.299956, loss_mps: 0.026034, loss_cps: 0.038535
[12:24:51.459] iteration 1444: total_loss: 0.350909, loss_sup: 0.306007, loss_mps: 0.019284, loss_cps: 0.025617
[12:24:51.605] iteration 1445: total_loss: 0.511753, loss_sup: 0.451629, loss_mps: 0.024271, loss_cps: 0.035854
[12:24:51.751] iteration 1446: total_loss: 0.669363, loss_sup: 0.612276, loss_mps: 0.022717, loss_cps: 0.034370
[12:24:51.897] iteration 1447: total_loss: 0.448719, loss_sup: 0.374124, loss_mps: 0.027619, loss_cps: 0.046976
[12:24:52.045] iteration 1448: total_loss: 0.509051, loss_sup: 0.445554, loss_mps: 0.024693, loss_cps: 0.038805
[12:24:52.194] iteration 1449: total_loss: 0.285717, loss_sup: 0.232137, loss_mps: 0.021766, loss_cps: 0.031813
[12:24:52.342] iteration 1450: total_loss: 0.247279, loss_sup: 0.199418, loss_mps: 0.020053, loss_cps: 0.027808
[12:24:52.489] iteration 1451: total_loss: 0.385977, loss_sup: 0.330273, loss_mps: 0.022664, loss_cps: 0.033040
[12:24:52.634] iteration 1452: total_loss: 0.588255, loss_sup: 0.527565, loss_mps: 0.023761, loss_cps: 0.036929
[12:24:52.782] iteration 1453: total_loss: 0.360058, loss_sup: 0.310676, loss_mps: 0.020098, loss_cps: 0.029284
[12:24:52.930] iteration 1454: total_loss: 0.374041, loss_sup: 0.316825, loss_mps: 0.022477, loss_cps: 0.034739
[12:24:53.076] iteration 1455: total_loss: 0.313669, loss_sup: 0.259160, loss_mps: 0.021672, loss_cps: 0.032837
[12:24:53.222] iteration 1456: total_loss: 0.796607, loss_sup: 0.736019, loss_mps: 0.023305, loss_cps: 0.037283
[12:24:53.367] iteration 1457: total_loss: 0.588054, loss_sup: 0.514629, loss_mps: 0.026928, loss_cps: 0.046497
[12:24:53.513] iteration 1458: total_loss: 0.409389, loss_sup: 0.341822, loss_mps: 0.025882, loss_cps: 0.041685
[12:24:53.661] iteration 1459: total_loss: 0.444276, loss_sup: 0.375912, loss_mps: 0.024846, loss_cps: 0.043518
[12:24:53.808] iteration 1460: total_loss: 0.384376, loss_sup: 0.314466, loss_mps: 0.025468, loss_cps: 0.044442
[12:24:53.954] iteration 1461: total_loss: 0.410114, loss_sup: 0.347987, loss_mps: 0.023393, loss_cps: 0.038734
[12:24:54.101] iteration 1462: total_loss: 0.263235, loss_sup: 0.215006, loss_mps: 0.019622, loss_cps: 0.028607
[12:24:54.247] iteration 1463: total_loss: 0.305681, loss_sup: 0.248199, loss_mps: 0.022403, loss_cps: 0.035079
[12:24:54.393] iteration 1464: total_loss: 0.757765, loss_sup: 0.693143, loss_mps: 0.024062, loss_cps: 0.040560
[12:24:54.541] iteration 1465: total_loss: 0.579874, loss_sup: 0.510392, loss_mps: 0.025811, loss_cps: 0.043671
[12:24:54.687] iteration 1466: total_loss: 0.306662, loss_sup: 0.240787, loss_mps: 0.024274, loss_cps: 0.041601
[12:24:54.833] iteration 1467: total_loss: 0.358236, loss_sup: 0.293044, loss_mps: 0.023960, loss_cps: 0.041232
[12:24:54.979] iteration 1468: total_loss: 0.515952, loss_sup: 0.445204, loss_mps: 0.025765, loss_cps: 0.044983
[12:24:55.126] iteration 1469: total_loss: 0.498367, loss_sup: 0.428662, loss_mps: 0.025914, loss_cps: 0.043791
[12:24:55.272] iteration 1470: total_loss: 0.473892, loss_sup: 0.417161, loss_mps: 0.022463, loss_cps: 0.034267
[12:24:55.417] iteration 1471: total_loss: 0.459827, loss_sup: 0.396116, loss_mps: 0.024919, loss_cps: 0.038793
[12:24:55.563] iteration 1472: total_loss: 0.791668, loss_sup: 0.720230, loss_mps: 0.026460, loss_cps: 0.044977
[12:24:55.710] iteration 1473: total_loss: 0.247393, loss_sup: 0.185400, loss_mps: 0.024393, loss_cps: 0.037600
[12:24:55.858] iteration 1474: total_loss: 0.338655, loss_sup: 0.271885, loss_mps: 0.025681, loss_cps: 0.041088
[12:24:56.004] iteration 1475: total_loss: 0.512737, loss_sup: 0.448527, loss_mps: 0.024712, loss_cps: 0.039498
[12:24:56.150] iteration 1476: total_loss: 0.385873, loss_sup: 0.332251, loss_mps: 0.021496, loss_cps: 0.032126
[12:24:56.296] iteration 1477: total_loss: 0.786673, loss_sup: 0.720747, loss_mps: 0.025103, loss_cps: 0.040823
[12:24:56.443] iteration 1478: total_loss: 0.500844, loss_sup: 0.444465, loss_mps: 0.022785, loss_cps: 0.033593
[12:24:56.590] iteration 1479: total_loss: 0.458632, loss_sup: 0.397172, loss_mps: 0.023826, loss_cps: 0.037634
[12:24:56.742] iteration 1480: total_loss: 0.605586, loss_sup: 0.541535, loss_mps: 0.024562, loss_cps: 0.039489
[12:24:56.888] iteration 1481: total_loss: 0.506300, loss_sup: 0.452381, loss_mps: 0.021563, loss_cps: 0.032357
[12:24:57.034] iteration 1482: total_loss: 0.326381, loss_sup: 0.269254, loss_mps: 0.023121, loss_cps: 0.034007
[12:24:57.180] iteration 1483: total_loss: 0.373467, loss_sup: 0.308112, loss_mps: 0.025205, loss_cps: 0.040149
[12:24:57.326] iteration 1484: total_loss: 0.520110, loss_sup: 0.443064, loss_mps: 0.028891, loss_cps: 0.048156
[12:24:57.472] iteration 1485: total_loss: 0.508296, loss_sup: 0.445671, loss_mps: 0.024334, loss_cps: 0.038291
[12:24:57.622] iteration 1486: total_loss: 0.360540, loss_sup: 0.301073, loss_mps: 0.023089, loss_cps: 0.036379
[12:24:57.768] iteration 1487: total_loss: 0.300629, loss_sup: 0.244831, loss_mps: 0.023016, loss_cps: 0.032782
[12:24:57.921] iteration 1488: total_loss: 0.472358, loss_sup: 0.406426, loss_mps: 0.025412, loss_cps: 0.040519
[12:24:58.068] iteration 1489: total_loss: 0.331685, loss_sup: 0.266931, loss_mps: 0.025171, loss_cps: 0.039583
[12:24:58.217] iteration 1490: total_loss: 0.323922, loss_sup: 0.269137, loss_mps: 0.022497, loss_cps: 0.032288
[12:24:58.363] iteration 1491: total_loss: 0.430896, loss_sup: 0.374459, loss_mps: 0.022686, loss_cps: 0.033751
[12:24:58.508] iteration 1492: total_loss: 0.440137, loss_sup: 0.377979, loss_mps: 0.024556, loss_cps: 0.037602
[12:24:58.655] iteration 1493: total_loss: 0.478032, loss_sup: 0.419090, loss_mps: 0.023178, loss_cps: 0.035765
[12:24:58.805] iteration 1494: total_loss: 0.303454, loss_sup: 0.239422, loss_mps: 0.024840, loss_cps: 0.039192
[12:24:58.951] iteration 1495: total_loss: 0.491235, loss_sup: 0.427888, loss_mps: 0.023798, loss_cps: 0.039549
[12:24:59.100] iteration 1496: total_loss: 0.379084, loss_sup: 0.323627, loss_mps: 0.022427, loss_cps: 0.033030
[12:24:59.246] iteration 1497: total_loss: 0.252759, loss_sup: 0.200214, loss_mps: 0.020903, loss_cps: 0.031643
[12:24:59.393] iteration 1498: total_loss: 0.320896, loss_sup: 0.264580, loss_mps: 0.022049, loss_cps: 0.034267
[12:24:59.540] iteration 1499: total_loss: 0.628554, loss_sup: 0.570322, loss_mps: 0.023016, loss_cps: 0.035217
[12:24:59.685] iteration 1500: total_loss: 0.614563, loss_sup: 0.560764, loss_mps: 0.021117, loss_cps: 0.032682
[12:24:59.685] Evaluation Started ==>
[12:25:11.010] ==> valid iteration 1500: unet metrics: {'dc': 0.4577787212381614, 'jc': 0.33927219531255387, 'pre': 0.5701885369615293, 'hd': 7.162432256898457}, ynet metrics: {'dc': 0.46136552845796674, 'jc': 0.34291926985863574, 'pre': 0.4811693731586597, 'hd': 8.036163581829998}.
[12:25:11.074] ==> New best valid dice for unet: 0.457779, at iteration 1500
[12:25:11.238] ==> New best valid dice for ynet: 0.461366, at iteration 1500
[12:25:11.240] Evaluation Finished!⏹️
[12:25:11.394] iteration 1501: total_loss: 0.267475, loss_sup: 0.209504, loss_mps: 0.022064, loss_cps: 0.035906
[12:25:11.541] iteration 1502: total_loss: 0.403219, loss_sup: 0.348683, loss_mps: 0.020751, loss_cps: 0.033785
[12:25:11.687] iteration 1503: total_loss: 0.446885, loss_sup: 0.377343, loss_mps: 0.025892, loss_cps: 0.043649
[12:25:11.832] iteration 1504: total_loss: 0.337882, loss_sup: 0.276347, loss_mps: 0.022770, loss_cps: 0.038765
[12:25:11.976] iteration 1505: total_loss: 0.421081, loss_sup: 0.374335, loss_mps: 0.019303, loss_cps: 0.027443
[12:25:12.122] iteration 1506: total_loss: 0.356706, loss_sup: 0.302564, loss_mps: 0.021276, loss_cps: 0.032866
[12:25:12.268] iteration 1507: total_loss: 0.534171, loss_sup: 0.461695, loss_mps: 0.026737, loss_cps: 0.045739
[12:25:12.413] iteration 1508: total_loss: 0.382617, loss_sup: 0.317679, loss_mps: 0.024418, loss_cps: 0.040520
[12:25:12.558] iteration 1509: total_loss: 0.212033, loss_sup: 0.159555, loss_mps: 0.020381, loss_cps: 0.032097
[12:25:12.704] iteration 1510: total_loss: 0.416013, loss_sup: 0.344702, loss_mps: 0.025205, loss_cps: 0.046107
[12:25:12.848] iteration 1511: total_loss: 0.387330, loss_sup: 0.327866, loss_mps: 0.022631, loss_cps: 0.036833
[12:25:12.994] iteration 1512: total_loss: 0.442699, loss_sup: 0.372157, loss_mps: 0.025316, loss_cps: 0.045226
[12:25:13.143] iteration 1513: total_loss: 0.569294, loss_sup: 0.504704, loss_mps: 0.023952, loss_cps: 0.040638
[12:25:13.288] iteration 1514: total_loss: 0.401883, loss_sup: 0.347563, loss_mps: 0.020346, loss_cps: 0.033975
[12:25:13.435] iteration 1515: total_loss: 0.357600, loss_sup: 0.306631, loss_mps: 0.020003, loss_cps: 0.030966
[12:25:13.581] iteration 1516: total_loss: 0.311722, loss_sup: 0.253076, loss_mps: 0.021882, loss_cps: 0.036764
[12:25:13.727] iteration 1517: total_loss: 0.540873, loss_sup: 0.483288, loss_mps: 0.021949, loss_cps: 0.035636
[12:25:13.872] iteration 1518: total_loss: 0.387728, loss_sup: 0.332909, loss_mps: 0.020752, loss_cps: 0.034067
[12:25:14.017] iteration 1519: total_loss: 0.249022, loss_sup: 0.196436, loss_mps: 0.019999, loss_cps: 0.032586
[12:25:14.165] iteration 1520: total_loss: 0.538470, loss_sup: 0.475725, loss_mps: 0.022733, loss_cps: 0.040012
[12:25:14.311] iteration 1521: total_loss: 0.322010, loss_sup: 0.265224, loss_mps: 0.021788, loss_cps: 0.034998
[12:25:14.457] iteration 1522: total_loss: 0.440280, loss_sup: 0.379955, loss_mps: 0.022082, loss_cps: 0.038243
[12:25:14.602] iteration 1523: total_loss: 0.696178, loss_sup: 0.617199, loss_mps: 0.027810, loss_cps: 0.051169
[12:25:14.748] iteration 1524: total_loss: 0.319460, loss_sup: 0.257611, loss_mps: 0.022937, loss_cps: 0.038912
[12:25:14.894] iteration 1525: total_loss: 0.329211, loss_sup: 0.269598, loss_mps: 0.022605, loss_cps: 0.037008
[12:25:15.043] iteration 1526: total_loss: 0.420326, loss_sup: 0.346990, loss_mps: 0.025843, loss_cps: 0.047493
[12:25:15.190] iteration 1527: total_loss: 0.432348, loss_sup: 0.369533, loss_mps: 0.023306, loss_cps: 0.039509
[12:25:15.336] iteration 1528: total_loss: 0.554559, loss_sup: 0.489362, loss_mps: 0.023751, loss_cps: 0.041446
[12:25:15.482] iteration 1529: total_loss: 0.483032, loss_sup: 0.406420, loss_mps: 0.026786, loss_cps: 0.049827
[12:25:15.628] iteration 1530: total_loss: 0.457278, loss_sup: 0.389504, loss_mps: 0.024462, loss_cps: 0.043312
[12:25:15.773] iteration 1531: total_loss: 0.297115, loss_sup: 0.241715, loss_mps: 0.020895, loss_cps: 0.034504
[12:25:15.920] iteration 1532: total_loss: 0.433453, loss_sup: 0.371444, loss_mps: 0.022683, loss_cps: 0.039326
[12:25:16.065] iteration 1533: total_loss: 0.361437, loss_sup: 0.303891, loss_mps: 0.021778, loss_cps: 0.035768
[12:25:16.212] iteration 1534: total_loss: 0.354923, loss_sup: 0.298083, loss_mps: 0.022130, loss_cps: 0.034710
[12:25:16.357] iteration 1535: total_loss: 0.563895, loss_sup: 0.512057, loss_mps: 0.021025, loss_cps: 0.030813
[12:25:16.507] iteration 1536: total_loss: 0.439772, loss_sup: 0.379126, loss_mps: 0.023112, loss_cps: 0.037533
[12:25:16.655] iteration 1537: total_loss: 0.261737, loss_sup: 0.210828, loss_mps: 0.020120, loss_cps: 0.030790
[12:25:16.802] iteration 1538: total_loss: 0.699124, loss_sup: 0.648431, loss_mps: 0.020194, loss_cps: 0.030499
[12:25:16.947] iteration 1539: total_loss: 0.612383, loss_sup: 0.538829, loss_mps: 0.026677, loss_cps: 0.046877
[12:25:17.093] iteration 1540: total_loss: 0.270134, loss_sup: 0.215144, loss_mps: 0.021456, loss_cps: 0.033535
[12:25:17.239] iteration 1541: total_loss: 0.603235, loss_sup: 0.534891, loss_mps: 0.024772, loss_cps: 0.043572
[12:25:17.385] iteration 1542: total_loss: 0.578104, loss_sup: 0.520003, loss_mps: 0.022674, loss_cps: 0.035427
[12:25:17.531] iteration 1543: total_loss: 0.500112, loss_sup: 0.433199, loss_mps: 0.025327, loss_cps: 0.041586
[12:25:17.677] iteration 1544: total_loss: 0.233951, loss_sup: 0.178748, loss_mps: 0.021991, loss_cps: 0.033212
[12:25:17.822] iteration 1545: total_loss: 0.393722, loss_sup: 0.345438, loss_mps: 0.020049, loss_cps: 0.028236
[12:25:17.967] iteration 1546: total_loss: 0.290984, loss_sup: 0.241010, loss_mps: 0.020325, loss_cps: 0.029649
[12:25:18.113] iteration 1547: total_loss: 0.495415, loss_sup: 0.423683, loss_mps: 0.026271, loss_cps: 0.045462
[12:25:18.258] iteration 1548: total_loss: 0.446109, loss_sup: 0.393007, loss_mps: 0.020951, loss_cps: 0.032152
[12:25:18.404] iteration 1549: total_loss: 0.609749, loss_sup: 0.551204, loss_mps: 0.023083, loss_cps: 0.035463
[12:25:18.550] iteration 1550: total_loss: 0.550249, loss_sup: 0.475932, loss_mps: 0.027355, loss_cps: 0.046963
[12:25:18.695] iteration 1551: total_loss: 0.591240, loss_sup: 0.532435, loss_mps: 0.023173, loss_cps: 0.035631
[12:25:18.841] iteration 1552: total_loss: 0.330276, loss_sup: 0.271099, loss_mps: 0.022636, loss_cps: 0.036540
[12:25:18.986] iteration 1553: total_loss: 0.572198, loss_sup: 0.506247, loss_mps: 0.025702, loss_cps: 0.040249
[12:25:19.131] iteration 1554: total_loss: 0.404845, loss_sup: 0.341819, loss_mps: 0.024790, loss_cps: 0.038236
[12:25:19.279] iteration 1555: total_loss: 0.343896, loss_sup: 0.289408, loss_mps: 0.021988, loss_cps: 0.032500
[12:25:19.424] iteration 1556: total_loss: 0.373462, loss_sup: 0.312877, loss_mps: 0.023434, loss_cps: 0.037152
[12:25:19.570] iteration 1557: total_loss: 0.361098, loss_sup: 0.307588, loss_mps: 0.021326, loss_cps: 0.032183
[12:25:19.716] iteration 1558: total_loss: 0.707273, loss_sup: 0.648939, loss_mps: 0.022644, loss_cps: 0.035690
[12:25:19.861] iteration 1559: total_loss: 0.439645, loss_sup: 0.385355, loss_mps: 0.021753, loss_cps: 0.032538
[12:25:20.008] iteration 1560: total_loss: 0.387534, loss_sup: 0.330015, loss_mps: 0.022693, loss_cps: 0.034827
[12:25:20.154] iteration 1561: total_loss: 0.305697, loss_sup: 0.241094, loss_mps: 0.024459, loss_cps: 0.040145
[12:25:20.300] iteration 1562: total_loss: 0.444407, loss_sup: 0.382214, loss_mps: 0.023924, loss_cps: 0.038270
[12:25:20.446] iteration 1563: total_loss: 0.232997, loss_sup: 0.173686, loss_mps: 0.022767, loss_cps: 0.036544
[12:25:20.593] iteration 1564: total_loss: 0.472309, loss_sup: 0.406932, loss_mps: 0.024468, loss_cps: 0.040910
[12:25:20.738] iteration 1565: total_loss: 0.392738, loss_sup: 0.335146, loss_mps: 0.021876, loss_cps: 0.035716
[12:25:20.884] iteration 1566: total_loss: 0.379493, loss_sup: 0.324600, loss_mps: 0.021520, loss_cps: 0.033373
[12:25:21.030] iteration 1567: total_loss: 0.209144, loss_sup: 0.147182, loss_mps: 0.022906, loss_cps: 0.039056
[12:25:21.175] iteration 1568: total_loss: 0.592143, loss_sup: 0.538064, loss_mps: 0.021175, loss_cps: 0.032905
[12:25:21.321] iteration 1569: total_loss: 0.241532, loss_sup: 0.192344, loss_mps: 0.019865, loss_cps: 0.029322
[12:25:21.467] iteration 1570: total_loss: 0.434127, loss_sup: 0.379566, loss_mps: 0.021445, loss_cps: 0.033116
[12:25:21.612] iteration 1571: total_loss: 0.390605, loss_sup: 0.335125, loss_mps: 0.021699, loss_cps: 0.033781
[12:25:21.758] iteration 1572: total_loss: 0.417765, loss_sup: 0.356790, loss_mps: 0.022850, loss_cps: 0.038126
[12:25:21.904] iteration 1573: total_loss: 0.595154, loss_sup: 0.527459, loss_mps: 0.024247, loss_cps: 0.043449
[12:25:22.050] iteration 1574: total_loss: 0.305694, loss_sup: 0.252084, loss_mps: 0.020678, loss_cps: 0.032932
[12:25:22.195] iteration 1575: total_loss: 0.397853, loss_sup: 0.330857, loss_mps: 0.024210, loss_cps: 0.042787
[12:25:22.341] iteration 1576: total_loss: 0.383516, loss_sup: 0.336425, loss_mps: 0.019257, loss_cps: 0.027833
[12:25:22.487] iteration 1577: total_loss: 0.416990, loss_sup: 0.360175, loss_mps: 0.021147, loss_cps: 0.035668
[12:25:22.636] iteration 1578: total_loss: 0.392447, loss_sup: 0.350255, loss_mps: 0.017923, loss_cps: 0.024269
[12:25:22.782] iteration 1579: total_loss: 0.469107, loss_sup: 0.407433, loss_mps: 0.022681, loss_cps: 0.038993
[12:25:22.928] iteration 1580: total_loss: 0.675788, loss_sup: 0.612095, loss_mps: 0.023195, loss_cps: 0.040498
[12:25:23.073] iteration 1581: total_loss: 0.343681, loss_sup: 0.291068, loss_mps: 0.020531, loss_cps: 0.032081
[12:25:23.220] iteration 1582: total_loss: 0.698699, loss_sup: 0.621019, loss_mps: 0.027620, loss_cps: 0.050059
[12:25:23.365] iteration 1583: total_loss: 0.447145, loss_sup: 0.367453, loss_mps: 0.027417, loss_cps: 0.052276
[12:25:23.511] iteration 1584: total_loss: 0.449481, loss_sup: 0.378832, loss_mps: 0.025016, loss_cps: 0.045633
[12:25:23.656] iteration 1585: total_loss: 0.373654, loss_sup: 0.311629, loss_mps: 0.023110, loss_cps: 0.038914
[12:25:23.805] iteration 1586: total_loss: 0.511154, loss_sup: 0.436732, loss_mps: 0.026834, loss_cps: 0.047589
[12:25:23.951] iteration 1587: total_loss: 0.646917, loss_sup: 0.589533, loss_mps: 0.022611, loss_cps: 0.034772
[12:25:24.096] iteration 1588: total_loss: 0.447958, loss_sup: 0.383055, loss_mps: 0.024889, loss_cps: 0.040014
[12:25:24.243] iteration 1589: total_loss: 0.211838, loss_sup: 0.155798, loss_mps: 0.022162, loss_cps: 0.033878
[12:25:24.389] iteration 1590: total_loss: 0.506657, loss_sup: 0.427553, loss_mps: 0.028393, loss_cps: 0.050711
[12:25:24.534] iteration 1591: total_loss: 0.574408, loss_sup: 0.511323, loss_mps: 0.024308, loss_cps: 0.038777
[12:25:24.680] iteration 1592: total_loss: 0.372678, loss_sup: 0.309990, loss_mps: 0.024515, loss_cps: 0.038172
[12:25:24.826] iteration 1593: total_loss: 0.546781, loss_sup: 0.486133, loss_mps: 0.023770, loss_cps: 0.036879
[12:25:24.973] iteration 1594: total_loss: 0.595068, loss_sup: 0.533564, loss_mps: 0.023956, loss_cps: 0.037549
[12:25:25.119] iteration 1595: total_loss: 0.379651, loss_sup: 0.317672, loss_mps: 0.023999, loss_cps: 0.037980
[12:25:25.265] iteration 1596: total_loss: 0.616660, loss_sup: 0.552979, loss_mps: 0.023843, loss_cps: 0.039839
[12:25:25.411] iteration 1597: total_loss: 0.351227, loss_sup: 0.287254, loss_mps: 0.024860, loss_cps: 0.039114
[12:25:25.557] iteration 1598: total_loss: 0.297062, loss_sup: 0.238473, loss_mps: 0.022680, loss_cps: 0.035909
[12:25:25.703] iteration 1599: total_loss: 0.329450, loss_sup: 0.269323, loss_mps: 0.023453, loss_cps: 0.036674
[12:25:25.850] iteration 1600: total_loss: 0.356577, loss_sup: 0.301204, loss_mps: 0.021728, loss_cps: 0.033645
[12:25:25.850] Evaluation Started ==>
[12:25:37.280] ==> valid iteration 1600: unet metrics: {'dc': 0.42989575784405076, 'jc': 0.313922237105232, 'pre': 0.4703189637561563, 'hd': 7.88166141673564}, ynet metrics: {'dc': 0.43840452090024995, 'jc': 0.32711704518505313, 'pre': 0.5081110482688137, 'hd': 7.942092736589932}.
[12:25:37.282] Evaluation Finished!⏹️
[12:25:37.434] iteration 1601: total_loss: 0.486837, loss_sup: 0.410962, loss_mps: 0.027643, loss_cps: 0.048232
[12:25:37.584] iteration 1602: total_loss: 0.379762, loss_sup: 0.315776, loss_mps: 0.024270, loss_cps: 0.039717
[12:25:37.732] iteration 1603: total_loss: 0.300186, loss_sup: 0.226503, loss_mps: 0.026989, loss_cps: 0.046693
[12:25:37.878] iteration 1604: total_loss: 0.290886, loss_sup: 0.242415, loss_mps: 0.019290, loss_cps: 0.029181
[12:25:38.024] iteration 1605: total_loss: 0.463260, loss_sup: 0.413552, loss_mps: 0.019498, loss_cps: 0.030209
[12:25:38.170] iteration 1606: total_loss: 0.320602, loss_sup: 0.257101, loss_mps: 0.023486, loss_cps: 0.040015
[12:25:38.315] iteration 1607: total_loss: 0.320426, loss_sup: 0.270825, loss_mps: 0.020019, loss_cps: 0.029582
[12:25:38.460] iteration 1608: total_loss: 0.274922, loss_sup: 0.219513, loss_mps: 0.020822, loss_cps: 0.034587
[12:25:38.605] iteration 1609: total_loss: 0.479211, loss_sup: 0.410708, loss_mps: 0.024088, loss_cps: 0.044415
[12:25:38.750] iteration 1610: total_loss: 0.372706, loss_sup: 0.301625, loss_mps: 0.024842, loss_cps: 0.046239
[12:25:38.896] iteration 1611: total_loss: 0.618554, loss_sup: 0.552868, loss_mps: 0.023340, loss_cps: 0.042346
[12:25:39.041] iteration 1612: total_loss: 0.426465, loss_sup: 0.346769, loss_mps: 0.027505, loss_cps: 0.052191
[12:25:39.189] iteration 1613: total_loss: 0.217334, loss_sup: 0.169892, loss_mps: 0.018474, loss_cps: 0.028968
[12:25:39.334] iteration 1614: total_loss: 0.443476, loss_sup: 0.399165, loss_mps: 0.017928, loss_cps: 0.026382
[12:25:39.479] iteration 1615: total_loss: 0.317358, loss_sup: 0.248411, loss_mps: 0.024156, loss_cps: 0.044792
[12:25:39.625] iteration 1616: total_loss: 0.461286, loss_sup: 0.391046, loss_mps: 0.024879, loss_cps: 0.045361
[12:25:39.771] iteration 1617: total_loss: 0.317276, loss_sup: 0.257624, loss_mps: 0.021750, loss_cps: 0.037902
[12:25:39.919] iteration 1618: total_loss: 0.322373, loss_sup: 0.262926, loss_mps: 0.022564, loss_cps: 0.036883
[12:25:40.066] iteration 1619: total_loss: 0.361384, loss_sup: 0.295614, loss_mps: 0.023933, loss_cps: 0.041837
[12:25:40.211] iteration 1620: total_loss: 0.406779, loss_sup: 0.337608, loss_mps: 0.024655, loss_cps: 0.044516
[12:25:40.358] iteration 1621: total_loss: 0.481072, loss_sup: 0.413009, loss_mps: 0.024728, loss_cps: 0.043335
[12:25:40.504] iteration 1622: total_loss: 0.618998, loss_sup: 0.564764, loss_mps: 0.020948, loss_cps: 0.033287
[12:25:40.649] iteration 1623: total_loss: 0.365577, loss_sup: 0.312276, loss_mps: 0.020830, loss_cps: 0.032471
[12:25:40.795] iteration 1624: total_loss: 0.345504, loss_sup: 0.282049, loss_mps: 0.024085, loss_cps: 0.039370
[12:25:40.942] iteration 1625: total_loss: 0.327983, loss_sup: 0.272143, loss_mps: 0.022025, loss_cps: 0.033816
[12:25:41.089] iteration 1626: total_loss: 0.460322, loss_sup: 0.411403, loss_mps: 0.019596, loss_cps: 0.029323
[12:25:41.238] iteration 1627: total_loss: 0.565559, loss_sup: 0.500798, loss_mps: 0.024062, loss_cps: 0.040698
[12:25:41.384] iteration 1628: total_loss: 0.472047, loss_sup: 0.417265, loss_mps: 0.020732, loss_cps: 0.034050
[12:25:41.529] iteration 1629: total_loss: 0.518399, loss_sup: 0.465194, loss_mps: 0.020281, loss_cps: 0.032924
[12:25:41.674] iteration 1630: total_loss: 0.543374, loss_sup: 0.478485, loss_mps: 0.024520, loss_cps: 0.040369
[12:25:41.820] iteration 1631: total_loss: 0.553836, loss_sup: 0.490905, loss_mps: 0.024340, loss_cps: 0.038590
[12:25:41.965] iteration 1632: total_loss: 0.247127, loss_sup: 0.181968, loss_mps: 0.024255, loss_cps: 0.040904
[12:25:42.114] iteration 1633: total_loss: 0.286870, loss_sup: 0.223818, loss_mps: 0.023718, loss_cps: 0.039334
[12:25:42.263] iteration 1634: total_loss: 0.302179, loss_sup: 0.247435, loss_mps: 0.022181, loss_cps: 0.032563
[12:25:42.409] iteration 1635: total_loss: 0.696021, loss_sup: 0.636773, loss_mps: 0.023397, loss_cps: 0.035852
[12:25:42.555] iteration 1636: total_loss: 0.389741, loss_sup: 0.331484, loss_mps: 0.022953, loss_cps: 0.035305
[12:25:42.700] iteration 1637: total_loss: 0.299020, loss_sup: 0.248260, loss_mps: 0.020855, loss_cps: 0.029904
[12:25:42.851] iteration 1638: total_loss: 0.507771, loss_sup: 0.443124, loss_mps: 0.024139, loss_cps: 0.040507
[12:25:42.996] iteration 1639: total_loss: 0.406324, loss_sup: 0.353561, loss_mps: 0.021057, loss_cps: 0.031707
[12:25:43.142] iteration 1640: total_loss: 0.502897, loss_sup: 0.453090, loss_mps: 0.019873, loss_cps: 0.029934
[12:25:43.288] iteration 1641: total_loss: 0.631865, loss_sup: 0.565055, loss_mps: 0.024663, loss_cps: 0.042147
[12:25:43.436] iteration 1642: total_loss: 0.378650, loss_sup: 0.327957, loss_mps: 0.020430, loss_cps: 0.030263
[12:25:43.582] iteration 1643: total_loss: 0.945334, loss_sup: 0.887861, loss_mps: 0.022742, loss_cps: 0.034731
[12:25:43.728] iteration 1644: total_loss: 0.455762, loss_sup: 0.385277, loss_mps: 0.026685, loss_cps: 0.043800
[12:25:43.875] iteration 1645: total_loss: 0.369361, loss_sup: 0.305967, loss_mps: 0.024569, loss_cps: 0.038826
[12:25:44.020] iteration 1646: total_loss: 0.419864, loss_sup: 0.358438, loss_mps: 0.024540, loss_cps: 0.036886
[12:25:44.166] iteration 1647: total_loss: 0.594723, loss_sup: 0.529112, loss_mps: 0.026199, loss_cps: 0.039412
[12:25:44.311] iteration 1648: total_loss: 0.605613, loss_sup: 0.542776, loss_mps: 0.025678, loss_cps: 0.037158
[12:25:44.458] iteration 1649: total_loss: 0.374804, loss_sup: 0.310943, loss_mps: 0.025629, loss_cps: 0.038233
[12:25:44.604] iteration 1650: total_loss: 0.393485, loss_sup: 0.340895, loss_mps: 0.022483, loss_cps: 0.030107
[12:25:44.749] iteration 1651: total_loss: 0.377003, loss_sup: 0.317668, loss_mps: 0.023687, loss_cps: 0.035647
[12:25:44.896] iteration 1652: total_loss: 0.351236, loss_sup: 0.293625, loss_mps: 0.023491, loss_cps: 0.034120
[12:25:45.041] iteration 1653: total_loss: 0.495561, loss_sup: 0.427661, loss_mps: 0.026999, loss_cps: 0.040901
[12:25:45.190] iteration 1654: total_loss: 0.470495, loss_sup: 0.401578, loss_mps: 0.027022, loss_cps: 0.041895
[12:25:45.336] iteration 1655: total_loss: 0.355602, loss_sup: 0.291989, loss_mps: 0.025905, loss_cps: 0.037708
[12:25:45.482] iteration 1656: total_loss: 0.363249, loss_sup: 0.315701, loss_mps: 0.020483, loss_cps: 0.027065
[12:25:45.628] iteration 1657: total_loss: 0.330337, loss_sup: 0.272947, loss_mps: 0.023135, loss_cps: 0.034255
[12:25:45.774] iteration 1658: total_loss: 0.406909, loss_sup: 0.350478, loss_mps: 0.022659, loss_cps: 0.033772
[12:25:45.922] iteration 1659: total_loss: 0.786319, loss_sup: 0.720403, loss_mps: 0.025595, loss_cps: 0.040321
[12:25:46.067] iteration 1660: total_loss: 0.324812, loss_sup: 0.264776, loss_mps: 0.023210, loss_cps: 0.036826
[12:25:46.212] iteration 1661: total_loss: 0.365463, loss_sup: 0.308149, loss_mps: 0.022154, loss_cps: 0.035160
[12:25:46.358] iteration 1662: total_loss: 0.519423, loss_sup: 0.457578, loss_mps: 0.023585, loss_cps: 0.038260
[12:25:46.502] iteration 1663: total_loss: 0.288075, loss_sup: 0.232807, loss_mps: 0.021755, loss_cps: 0.033513
[12:25:46.647] iteration 1664: total_loss: 0.668823, loss_sup: 0.613945, loss_mps: 0.021307, loss_cps: 0.033571
[12:25:46.793] iteration 1665: total_loss: 0.494824, loss_sup: 0.425876, loss_mps: 0.025263, loss_cps: 0.043685
[12:25:46.938] iteration 1666: total_loss: 0.462920, loss_sup: 0.403734, loss_mps: 0.023094, loss_cps: 0.036092
[12:25:47.083] iteration 1667: total_loss: 0.427146, loss_sup: 0.373336, loss_mps: 0.021090, loss_cps: 0.032721
[12:25:47.229] iteration 1668: total_loss: 0.325984, loss_sup: 0.273614, loss_mps: 0.020923, loss_cps: 0.031447
[12:25:47.374] iteration 1669: total_loss: 0.315050, loss_sup: 0.267743, loss_mps: 0.019575, loss_cps: 0.027732
[12:25:47.520] iteration 1670: total_loss: 0.377104, loss_sup: 0.322672, loss_mps: 0.021152, loss_cps: 0.033281
[12:25:47.666] iteration 1671: total_loss: 0.285028, loss_sup: 0.228194, loss_mps: 0.021803, loss_cps: 0.035031
[12:25:47.726] iteration 1672: total_loss: 0.677167, loss_sup: 0.625769, loss_mps: 0.020193, loss_cps: 0.031204
[12:25:48.940] iteration 1673: total_loss: 0.318008, loss_sup: 0.264392, loss_mps: 0.021412, loss_cps: 0.032204
[12:25:49.089] iteration 1674: total_loss: 0.790718, loss_sup: 0.719332, loss_mps: 0.026115, loss_cps: 0.045271
[12:25:49.236] iteration 1675: total_loss: 0.537230, loss_sup: 0.468269, loss_mps: 0.025978, loss_cps: 0.042984
[12:25:49.383] iteration 1676: total_loss: 0.425474, loss_sup: 0.355619, loss_mps: 0.025419, loss_cps: 0.044436
[12:25:49.531] iteration 1677: total_loss: 0.503024, loss_sup: 0.440744, loss_mps: 0.023495, loss_cps: 0.038786
[12:25:49.678] iteration 1678: total_loss: 0.790854, loss_sup: 0.718275, loss_mps: 0.026682, loss_cps: 0.045897
[12:25:49.825] iteration 1679: total_loss: 0.334710, loss_sup: 0.278352, loss_mps: 0.021860, loss_cps: 0.034498
[12:25:49.975] iteration 1680: total_loss: 0.536987, loss_sup: 0.466727, loss_mps: 0.025881, loss_cps: 0.044378
[12:25:50.123] iteration 1681: total_loss: 0.643671, loss_sup: 0.574753, loss_mps: 0.026738, loss_cps: 0.042180
[12:25:50.270] iteration 1682: total_loss: 0.649158, loss_sup: 0.556417, loss_mps: 0.033447, loss_cps: 0.059295
[12:25:50.416] iteration 1683: total_loss: 0.436672, loss_sup: 0.372844, loss_mps: 0.025040, loss_cps: 0.038787
[12:25:50.562] iteration 1684: total_loss: 0.330866, loss_sup: 0.260504, loss_mps: 0.026139, loss_cps: 0.044224
[12:25:50.708] iteration 1685: total_loss: 0.479669, loss_sup: 0.399707, loss_mps: 0.030115, loss_cps: 0.049847
[12:25:50.855] iteration 1686: total_loss: 0.317663, loss_sup: 0.258019, loss_mps: 0.023548, loss_cps: 0.036096
[12:25:51.002] iteration 1687: total_loss: 0.410305, loss_sup: 0.351872, loss_mps: 0.023702, loss_cps: 0.034731
[12:25:51.148] iteration 1688: total_loss: 0.539021, loss_sup: 0.475441, loss_mps: 0.025768, loss_cps: 0.037813
[12:25:51.295] iteration 1689: total_loss: 0.406040, loss_sup: 0.336194, loss_mps: 0.026170, loss_cps: 0.043675
[12:25:51.441] iteration 1690: total_loss: 0.266692, loss_sup: 0.189162, loss_mps: 0.028984, loss_cps: 0.048545
[12:25:51.589] iteration 1691: total_loss: 0.196999, loss_sup: 0.141887, loss_mps: 0.022239, loss_cps: 0.032873
[12:25:51.735] iteration 1692: total_loss: 0.387936, loss_sup: 0.318587, loss_mps: 0.026722, loss_cps: 0.042627
[12:25:51.882] iteration 1693: total_loss: 0.361801, loss_sup: 0.300668, loss_mps: 0.023983, loss_cps: 0.037150
[12:25:52.029] iteration 1694: total_loss: 0.583102, loss_sup: 0.519579, loss_mps: 0.024000, loss_cps: 0.039522
[12:25:52.176] iteration 1695: total_loss: 0.371471, loss_sup: 0.317256, loss_mps: 0.021936, loss_cps: 0.032279
[12:25:52.323] iteration 1696: total_loss: 0.394661, loss_sup: 0.326724, loss_mps: 0.025229, loss_cps: 0.042709
[12:25:52.469] iteration 1697: total_loss: 0.464943, loss_sup: 0.409817, loss_mps: 0.022332, loss_cps: 0.032794
[12:25:52.615] iteration 1698: total_loss: 0.344958, loss_sup: 0.290859, loss_mps: 0.021506, loss_cps: 0.032592
[12:25:52.762] iteration 1699: total_loss: 0.503648, loss_sup: 0.437769, loss_mps: 0.024682, loss_cps: 0.041197
[12:25:52.908] iteration 1700: total_loss: 0.358406, loss_sup: 0.301123, loss_mps: 0.022335, loss_cps: 0.034948
[12:25:52.908] Evaluation Started ==>
[12:26:04.330] ==> valid iteration 1700: unet metrics: {'dc': 0.4546271976609471, 'jc': 0.3373764295953286, 'pre': 0.4633635009527804, 'hd': 7.959096425345649}, ynet metrics: {'dc': 0.3912216601895097, 'jc': 0.28532880150631607, 'pre': 0.4077913917624378, 'hd': 8.475388699705798}.
[12:26:04.332] Evaluation Finished!⏹️
[12:26:04.486] iteration 1701: total_loss: 0.638368, loss_sup: 0.572716, loss_mps: 0.026066, loss_cps: 0.039586
[12:26:04.634] iteration 1702: total_loss: 0.722056, loss_sup: 0.654710, loss_mps: 0.026921, loss_cps: 0.040425
[12:26:04.779] iteration 1703: total_loss: 0.653617, loss_sup: 0.584763, loss_mps: 0.026260, loss_cps: 0.042594
[12:26:04.924] iteration 1704: total_loss: 0.345794, loss_sup: 0.288661, loss_mps: 0.022795, loss_cps: 0.034338
[12:26:05.068] iteration 1705: total_loss: 0.303110, loss_sup: 0.237411, loss_mps: 0.024715, loss_cps: 0.040984
[12:26:05.213] iteration 1706: total_loss: 0.445039, loss_sup: 0.378123, loss_mps: 0.025620, loss_cps: 0.041296
[12:26:05.360] iteration 1707: total_loss: 0.473874, loss_sup: 0.421718, loss_mps: 0.021638, loss_cps: 0.030518
[12:26:05.505] iteration 1708: total_loss: 0.482971, loss_sup: 0.407112, loss_mps: 0.027706, loss_cps: 0.048153
[12:26:05.650] iteration 1709: total_loss: 0.322915, loss_sup: 0.253809, loss_mps: 0.025735, loss_cps: 0.043372
[12:26:05.795] iteration 1710: total_loss: 0.471744, loss_sup: 0.401814, loss_mps: 0.026733, loss_cps: 0.043196
[12:26:05.940] iteration 1711: total_loss: 0.472071, loss_sup: 0.399860, loss_mps: 0.026353, loss_cps: 0.045858
[12:26:06.086] iteration 1712: total_loss: 0.379960, loss_sup: 0.320513, loss_mps: 0.022781, loss_cps: 0.036666
[12:26:06.232] iteration 1713: total_loss: 0.348504, loss_sup: 0.292700, loss_mps: 0.021696, loss_cps: 0.034108
[12:26:06.380] iteration 1714: total_loss: 0.250047, loss_sup: 0.197195, loss_mps: 0.020580, loss_cps: 0.032272
[12:26:06.526] iteration 1715: total_loss: 0.359344, loss_sup: 0.298370, loss_mps: 0.023636, loss_cps: 0.037338
[12:26:06.671] iteration 1716: total_loss: 0.407783, loss_sup: 0.353837, loss_mps: 0.021200, loss_cps: 0.032747
[12:26:06.817] iteration 1717: total_loss: 0.375043, loss_sup: 0.317262, loss_mps: 0.021979, loss_cps: 0.035803
[12:26:06.962] iteration 1718: total_loss: 0.278790, loss_sup: 0.224621, loss_mps: 0.021562, loss_cps: 0.032607
[12:26:07.108] iteration 1719: total_loss: 0.386749, loss_sup: 0.324479, loss_mps: 0.023851, loss_cps: 0.038419
[12:26:07.255] iteration 1720: total_loss: 0.397298, loss_sup: 0.351073, loss_mps: 0.019303, loss_cps: 0.026922
[12:26:07.400] iteration 1721: total_loss: 0.584275, loss_sup: 0.514899, loss_mps: 0.025099, loss_cps: 0.044277
[12:26:07.546] iteration 1722: total_loss: 0.308917, loss_sup: 0.244436, loss_mps: 0.023994, loss_cps: 0.040487
[12:26:07.692] iteration 1723: total_loss: 0.352982, loss_sup: 0.285471, loss_mps: 0.024854, loss_cps: 0.042657
[12:26:07.838] iteration 1724: total_loss: 0.292513, loss_sup: 0.239918, loss_mps: 0.019585, loss_cps: 0.033011
[12:26:07.983] iteration 1725: total_loss: 0.721935, loss_sup: 0.645594, loss_mps: 0.025870, loss_cps: 0.050471
[12:26:08.129] iteration 1726: total_loss: 0.257500, loss_sup: 0.199129, loss_mps: 0.021164, loss_cps: 0.037207
[12:26:08.276] iteration 1727: total_loss: 0.437416, loss_sup: 0.362789, loss_mps: 0.026198, loss_cps: 0.048429
[12:26:08.421] iteration 1728: total_loss: 0.629232, loss_sup: 0.556750, loss_mps: 0.025491, loss_cps: 0.046991
[12:26:08.567] iteration 1729: total_loss: 0.374384, loss_sup: 0.322726, loss_mps: 0.020060, loss_cps: 0.031598
[12:26:08.712] iteration 1730: total_loss: 0.604513, loss_sup: 0.548933, loss_mps: 0.020650, loss_cps: 0.034931
[12:26:08.858] iteration 1731: total_loss: 0.329042, loss_sup: 0.262292, loss_mps: 0.024198, loss_cps: 0.042551
[12:26:09.005] iteration 1732: total_loss: 0.327452, loss_sup: 0.275906, loss_mps: 0.019708, loss_cps: 0.031838
[12:26:09.150] iteration 1733: total_loss: 0.510836, loss_sup: 0.455187, loss_mps: 0.020916, loss_cps: 0.034733
[12:26:09.296] iteration 1734: total_loss: 0.385787, loss_sup: 0.335172, loss_mps: 0.019360, loss_cps: 0.031256
[12:26:09.441] iteration 1735: total_loss: 0.527047, loss_sup: 0.475029, loss_mps: 0.020209, loss_cps: 0.031809
[12:26:09.589] iteration 1736: total_loss: 0.554836, loss_sup: 0.488247, loss_mps: 0.024189, loss_cps: 0.042400
[12:26:09.734] iteration 1737: total_loss: 0.214414, loss_sup: 0.162531, loss_mps: 0.020334, loss_cps: 0.031550
[12:26:09.880] iteration 1738: total_loss: 0.462023, loss_sup: 0.379549, loss_mps: 0.028889, loss_cps: 0.053585
[12:26:10.027] iteration 1739: total_loss: 0.802748, loss_sup: 0.715239, loss_mps: 0.030671, loss_cps: 0.056838
[12:26:10.174] iteration 1740: total_loss: 0.522561, loss_sup: 0.445139, loss_mps: 0.027453, loss_cps: 0.049969
[12:26:10.323] iteration 1741: total_loss: 0.509176, loss_sup: 0.446407, loss_mps: 0.024483, loss_cps: 0.038287
[12:26:10.469] iteration 1742: total_loss: 0.458923, loss_sup: 0.385437, loss_mps: 0.027762, loss_cps: 0.045724
[12:26:10.614] iteration 1743: total_loss: 0.409283, loss_sup: 0.338134, loss_mps: 0.026747, loss_cps: 0.044403
[12:26:10.760] iteration 1744: total_loss: 0.335511, loss_sup: 0.255728, loss_mps: 0.029575, loss_cps: 0.050207
[12:26:10.907] iteration 1745: total_loss: 0.386074, loss_sup: 0.315349, loss_mps: 0.027136, loss_cps: 0.043589
[12:26:11.052] iteration 1746: total_loss: 0.490613, loss_sup: 0.425725, loss_mps: 0.025660, loss_cps: 0.039228
[12:26:11.198] iteration 1747: total_loss: 0.442574, loss_sup: 0.375751, loss_mps: 0.025934, loss_cps: 0.040889
[12:26:11.344] iteration 1748: total_loss: 0.475620, loss_sup: 0.405444, loss_mps: 0.026969, loss_cps: 0.043207
[12:26:11.490] iteration 1749: total_loss: 0.335121, loss_sup: 0.264702, loss_mps: 0.027329, loss_cps: 0.043090
[12:26:11.635] iteration 1750: total_loss: 0.251670, loss_sup: 0.191862, loss_mps: 0.024609, loss_cps: 0.035200
[12:26:11.781] iteration 1751: total_loss: 0.520164, loss_sup: 0.450616, loss_mps: 0.027540, loss_cps: 0.042008
[12:26:11.927] iteration 1752: total_loss: 0.428387, loss_sup: 0.372977, loss_mps: 0.023010, loss_cps: 0.032401
[12:26:12.073] iteration 1753: total_loss: 0.569607, loss_sup: 0.510035, loss_mps: 0.024740, loss_cps: 0.034832
[12:26:12.218] iteration 1754: total_loss: 0.635963, loss_sup: 0.581005, loss_mps: 0.024236, loss_cps: 0.030722
[12:26:12.364] iteration 1755: total_loss: 0.583971, loss_sup: 0.518840, loss_mps: 0.026113, loss_cps: 0.039018
[12:26:12.509] iteration 1756: total_loss: 0.361743, loss_sup: 0.307718, loss_mps: 0.023007, loss_cps: 0.031018
[12:26:12.655] iteration 1757: total_loss: 0.346790, loss_sup: 0.289701, loss_mps: 0.024346, loss_cps: 0.032742
[12:26:12.801] iteration 1758: total_loss: 0.499977, loss_sup: 0.436591, loss_mps: 0.025980, loss_cps: 0.037405
[12:26:12.947] iteration 1759: total_loss: 0.607570, loss_sup: 0.540245, loss_mps: 0.027620, loss_cps: 0.039706
[12:26:13.093] iteration 1760: total_loss: 0.604481, loss_sup: 0.535759, loss_mps: 0.028106, loss_cps: 0.040616
[12:26:13.238] iteration 1761: total_loss: 0.290554, loss_sup: 0.219233, loss_mps: 0.029031, loss_cps: 0.042291
[12:26:13.384] iteration 1762: total_loss: 0.530841, loss_sup: 0.463896, loss_mps: 0.027205, loss_cps: 0.039739
[12:26:13.529] iteration 1763: total_loss: 0.395466, loss_sup: 0.330906, loss_mps: 0.027377, loss_cps: 0.037183
[12:26:13.676] iteration 1764: total_loss: 0.366807, loss_sup: 0.293764, loss_mps: 0.028715, loss_cps: 0.044328
[12:26:13.821] iteration 1765: total_loss: 0.343813, loss_sup: 0.267470, loss_mps: 0.029864, loss_cps: 0.046480
[12:26:13.967] iteration 1766: total_loss: 0.371292, loss_sup: 0.301430, loss_mps: 0.027328, loss_cps: 0.042535
[12:26:14.113] iteration 1767: total_loss: 0.422460, loss_sup: 0.352800, loss_mps: 0.027375, loss_cps: 0.042285
[12:26:14.258] iteration 1768: total_loss: 0.415310, loss_sup: 0.353283, loss_mps: 0.024562, loss_cps: 0.037466
[12:26:14.405] iteration 1769: total_loss: 0.327007, loss_sup: 0.266212, loss_mps: 0.025203, loss_cps: 0.035592
[12:26:14.551] iteration 1770: total_loss: 0.224962, loss_sup: 0.164278, loss_mps: 0.024321, loss_cps: 0.036362
[12:26:14.698] iteration 1771: total_loss: 0.284124, loss_sup: 0.218120, loss_mps: 0.025449, loss_cps: 0.040554
[12:26:14.848] iteration 1772: total_loss: 0.626356, loss_sup: 0.565498, loss_mps: 0.024006, loss_cps: 0.036853
[12:26:14.995] iteration 1773: total_loss: 0.672547, loss_sup: 0.595929, loss_mps: 0.027825, loss_cps: 0.048793
[12:26:15.140] iteration 1774: total_loss: 0.323721, loss_sup: 0.268100, loss_mps: 0.021846, loss_cps: 0.033775
[12:26:15.286] iteration 1775: total_loss: 0.371075, loss_sup: 0.308991, loss_mps: 0.023890, loss_cps: 0.038194
[12:26:15.433] iteration 1776: total_loss: 0.755861, loss_sup: 0.690492, loss_mps: 0.024971, loss_cps: 0.040398
[12:26:15.578] iteration 1777: total_loss: 0.387440, loss_sup: 0.320243, loss_mps: 0.024398, loss_cps: 0.042800
[12:26:15.723] iteration 1778: total_loss: 0.774162, loss_sup: 0.713853, loss_mps: 0.022671, loss_cps: 0.037638
[12:26:15.869] iteration 1779: total_loss: 0.294446, loss_sup: 0.247983, loss_mps: 0.019140, loss_cps: 0.027323
[12:26:16.014] iteration 1780: total_loss: 0.379613, loss_sup: 0.301235, loss_mps: 0.028097, loss_cps: 0.050282
[12:26:16.160] iteration 1781: total_loss: 0.306296, loss_sup: 0.238804, loss_mps: 0.024787, loss_cps: 0.042706
[12:26:16.306] iteration 1782: total_loss: 0.386591, loss_sup: 0.328340, loss_mps: 0.023109, loss_cps: 0.035142
[12:26:16.452] iteration 1783: total_loss: 0.413656, loss_sup: 0.351795, loss_mps: 0.023644, loss_cps: 0.038218
[12:26:16.598] iteration 1784: total_loss: 0.267822, loss_sup: 0.206903, loss_mps: 0.023372, loss_cps: 0.037547
[12:26:16.745] iteration 1785: total_loss: 0.258033, loss_sup: 0.212354, loss_mps: 0.018470, loss_cps: 0.027210
[12:26:16.891] iteration 1786: total_loss: 0.389584, loss_sup: 0.330997, loss_mps: 0.022890, loss_cps: 0.035696
[12:26:17.040] iteration 1787: total_loss: 0.400243, loss_sup: 0.337819, loss_mps: 0.023704, loss_cps: 0.038720
[12:26:17.189] iteration 1788: total_loss: 0.442246, loss_sup: 0.388478, loss_mps: 0.021373, loss_cps: 0.032395
[12:26:17.336] iteration 1789: total_loss: 0.349988, loss_sup: 0.289580, loss_mps: 0.022554, loss_cps: 0.037855
[12:26:17.481] iteration 1790: total_loss: 0.431890, loss_sup: 0.379222, loss_mps: 0.021006, loss_cps: 0.031661
[12:26:17.628] iteration 1791: total_loss: 0.329656, loss_sup: 0.270818, loss_mps: 0.022915, loss_cps: 0.035923
[12:26:17.773] iteration 1792: total_loss: 0.402709, loss_sup: 0.339593, loss_mps: 0.023453, loss_cps: 0.039662
[12:26:17.919] iteration 1793: total_loss: 0.179115, loss_sup: 0.127184, loss_mps: 0.020490, loss_cps: 0.031441
[12:26:18.065] iteration 1794: total_loss: 0.548338, loss_sup: 0.485214, loss_mps: 0.023382, loss_cps: 0.039742
[12:26:18.212] iteration 1795: total_loss: 0.808549, loss_sup: 0.747280, loss_mps: 0.023238, loss_cps: 0.038032
[12:26:18.358] iteration 1796: total_loss: 0.545159, loss_sup: 0.475004, loss_mps: 0.025518, loss_cps: 0.044636
[12:26:18.504] iteration 1797: total_loss: 0.445248, loss_sup: 0.390290, loss_mps: 0.021294, loss_cps: 0.033664
[12:26:18.652] iteration 1798: total_loss: 0.262617, loss_sup: 0.199443, loss_mps: 0.023429, loss_cps: 0.039745
[12:26:18.800] iteration 1799: total_loss: 0.335399, loss_sup: 0.286211, loss_mps: 0.020114, loss_cps: 0.029074
[12:26:18.947] iteration 1800: total_loss: 0.317303, loss_sup: 0.268989, loss_mps: 0.019854, loss_cps: 0.028460
[12:26:18.947] Evaluation Started ==>
[12:26:30.483] ==> valid iteration 1800: unet metrics: {'dc': 0.4610863644704477, 'jc': 0.34391838560898275, 'pre': 0.5135625969259336, 'hd': 7.615602586804157}, ynet metrics: {'dc': 0.4350345316641616, 'jc': 0.32124730711708216, 'pre': 0.5037227019366404, 'hd': 7.640286096773623}.
[12:26:30.543] ==> New best valid dice for unet: 0.461086, at iteration 1800
[12:26:30.545] Evaluation Finished!⏹️
[12:26:30.695] iteration 1801: total_loss: 0.629722, loss_sup: 0.562069, loss_mps: 0.024964, loss_cps: 0.042689
[12:26:30.844] iteration 1802: total_loss: 0.288627, loss_sup: 0.229152, loss_mps: 0.023144, loss_cps: 0.036331
[12:26:30.990] iteration 1803: total_loss: 0.414892, loss_sup: 0.358725, loss_mps: 0.022306, loss_cps: 0.033862
[12:26:31.136] iteration 1804: total_loss: 0.442459, loss_sup: 0.389061, loss_mps: 0.021022, loss_cps: 0.032376
[12:26:31.281] iteration 1805: total_loss: 0.309735, loss_sup: 0.241940, loss_mps: 0.025671, loss_cps: 0.042124
[12:26:31.426] iteration 1806: total_loss: 0.444070, loss_sup: 0.385509, loss_mps: 0.023078, loss_cps: 0.035483
[12:26:31.573] iteration 1807: total_loss: 0.327906, loss_sup: 0.267237, loss_mps: 0.023502, loss_cps: 0.037167
[12:26:31.718] iteration 1808: total_loss: 0.476553, loss_sup: 0.403395, loss_mps: 0.026840, loss_cps: 0.046319
[12:26:31.863] iteration 1809: total_loss: 0.685769, loss_sup: 0.627460, loss_mps: 0.022774, loss_cps: 0.035535
[12:26:32.011] iteration 1810: total_loss: 0.443109, loss_sup: 0.378806, loss_mps: 0.024697, loss_cps: 0.039606
[12:26:32.157] iteration 1811: total_loss: 0.303881, loss_sup: 0.244534, loss_mps: 0.023234, loss_cps: 0.036113
[12:26:32.302] iteration 1812: total_loss: 0.587925, loss_sup: 0.533478, loss_mps: 0.021675, loss_cps: 0.032772
[12:26:32.447] iteration 1813: total_loss: 0.467564, loss_sup: 0.399884, loss_mps: 0.026159, loss_cps: 0.041521
[12:26:32.592] iteration 1814: total_loss: 0.446571, loss_sup: 0.375734, loss_mps: 0.025833, loss_cps: 0.045005
[12:26:32.737] iteration 1815: total_loss: 0.451041, loss_sup: 0.388015, loss_mps: 0.024049, loss_cps: 0.038976
[12:26:32.883] iteration 1816: total_loss: 0.337124, loss_sup: 0.277036, loss_mps: 0.024266, loss_cps: 0.035823
[12:26:33.030] iteration 1817: total_loss: 0.653873, loss_sup: 0.592474, loss_mps: 0.024130, loss_cps: 0.037269
[12:26:33.176] iteration 1818: total_loss: 0.315530, loss_sup: 0.241618, loss_mps: 0.027502, loss_cps: 0.046410
[12:26:33.321] iteration 1819: total_loss: 0.288912, loss_sup: 0.223947, loss_mps: 0.024151, loss_cps: 0.040815
[12:26:33.467] iteration 1820: total_loss: 0.367440, loss_sup: 0.306629, loss_mps: 0.023701, loss_cps: 0.037110
[12:26:33.613] iteration 1821: total_loss: 0.466645, loss_sup: 0.388340, loss_mps: 0.027949, loss_cps: 0.050355
[12:26:33.762] iteration 1822: total_loss: 0.491830, loss_sup: 0.415073, loss_mps: 0.027788, loss_cps: 0.048969
[12:26:33.910] iteration 1823: total_loss: 0.428161, loss_sup: 0.371005, loss_mps: 0.022118, loss_cps: 0.035038
[12:26:34.060] iteration 1824: total_loss: 0.491443, loss_sup: 0.423242, loss_mps: 0.025735, loss_cps: 0.042466
[12:26:34.206] iteration 1825: total_loss: 0.224485, loss_sup: 0.164688, loss_mps: 0.023291, loss_cps: 0.036506
[12:26:34.352] iteration 1826: total_loss: 0.402757, loss_sup: 0.320211, loss_mps: 0.029791, loss_cps: 0.052756
[12:26:34.498] iteration 1827: total_loss: 0.462811, loss_sup: 0.397693, loss_mps: 0.024404, loss_cps: 0.040715
[12:26:34.644] iteration 1828: total_loss: 0.285274, loss_sup: 0.219514, loss_mps: 0.025327, loss_cps: 0.040433
[12:26:34.792] iteration 1829: total_loss: 0.383767, loss_sup: 0.314883, loss_mps: 0.026011, loss_cps: 0.042872
[12:26:34.937] iteration 1830: total_loss: 0.320125, loss_sup: 0.250447, loss_mps: 0.026341, loss_cps: 0.043337
[12:26:35.084] iteration 1831: total_loss: 0.445812, loss_sup: 0.391669, loss_mps: 0.021978, loss_cps: 0.032165
[12:26:35.232] iteration 1832: total_loss: 0.251162, loss_sup: 0.190446, loss_mps: 0.023272, loss_cps: 0.037445
[12:26:35.378] iteration 1833: total_loss: 0.458375, loss_sup: 0.398475, loss_mps: 0.022572, loss_cps: 0.037328
[12:26:35.524] iteration 1834: total_loss: 0.318026, loss_sup: 0.257387, loss_mps: 0.023118, loss_cps: 0.037521
[12:26:35.670] iteration 1835: total_loss: 0.418054, loss_sup: 0.357427, loss_mps: 0.023793, loss_cps: 0.036834
[12:26:35.820] iteration 1836: total_loss: 0.257984, loss_sup: 0.182569, loss_mps: 0.027483, loss_cps: 0.047932
[12:26:35.967] iteration 1837: total_loss: 0.145934, loss_sup: 0.100916, loss_mps: 0.018589, loss_cps: 0.026428
[12:26:36.113] iteration 1838: total_loss: 0.364446, loss_sup: 0.311399, loss_mps: 0.020761, loss_cps: 0.032286
[12:26:36.259] iteration 1839: total_loss: 0.379287, loss_sup: 0.314377, loss_mps: 0.023467, loss_cps: 0.041443
[12:26:36.407] iteration 1840: total_loss: 0.260289, loss_sup: 0.192357, loss_mps: 0.024860, loss_cps: 0.043072
[12:26:36.554] iteration 1841: total_loss: 0.229406, loss_sup: 0.163891, loss_mps: 0.024412, loss_cps: 0.041103
[12:26:36.700] iteration 1842: total_loss: 0.358492, loss_sup: 0.288197, loss_mps: 0.025695, loss_cps: 0.044601
[12:26:36.846] iteration 1843: total_loss: 0.556599, loss_sup: 0.492023, loss_mps: 0.023813, loss_cps: 0.040763
[12:26:36.994] iteration 1844: total_loss: 0.357800, loss_sup: 0.294090, loss_mps: 0.023689, loss_cps: 0.040021
[12:26:37.140] iteration 1845: total_loss: 0.556308, loss_sup: 0.507167, loss_mps: 0.019742, loss_cps: 0.029399
[12:26:37.290] iteration 1846: total_loss: 0.663430, loss_sup: 0.599762, loss_mps: 0.023753, loss_cps: 0.039916
[12:26:37.435] iteration 1847: total_loss: 0.229602, loss_sup: 0.171259, loss_mps: 0.022393, loss_cps: 0.035951
[12:26:37.581] iteration 1848: total_loss: 0.526896, loss_sup: 0.472187, loss_mps: 0.021117, loss_cps: 0.033592
[12:26:37.729] iteration 1849: total_loss: 0.696137, loss_sup: 0.624074, loss_mps: 0.026259, loss_cps: 0.045804
[12:26:37.874] iteration 1850: total_loss: 0.431928, loss_sup: 0.374274, loss_mps: 0.022466, loss_cps: 0.035188
[12:26:38.020] iteration 1851: total_loss: 0.321046, loss_sup: 0.247845, loss_mps: 0.027752, loss_cps: 0.045449
[12:26:38.165] iteration 1852: total_loss: 0.463576, loss_sup: 0.393804, loss_mps: 0.026351, loss_cps: 0.043420
[12:26:38.312] iteration 1853: total_loss: 0.297092, loss_sup: 0.237666, loss_mps: 0.023673, loss_cps: 0.035754
[12:26:38.458] iteration 1854: total_loss: 0.353948, loss_sup: 0.294390, loss_mps: 0.023832, loss_cps: 0.035726
[12:26:38.603] iteration 1855: total_loss: 0.461128, loss_sup: 0.381186, loss_mps: 0.030110, loss_cps: 0.049832
[12:26:38.749] iteration 1856: total_loss: 0.396643, loss_sup: 0.325091, loss_mps: 0.026899, loss_cps: 0.044652
[12:26:38.898] iteration 1857: total_loss: 0.340253, loss_sup: 0.282441, loss_mps: 0.024004, loss_cps: 0.033809
[12:26:39.044] iteration 1858: total_loss: 0.404913, loss_sup: 0.345294, loss_mps: 0.022482, loss_cps: 0.037137
[12:26:39.190] iteration 1859: total_loss: 0.315109, loss_sup: 0.257077, loss_mps: 0.022819, loss_cps: 0.035214
[12:26:39.335] iteration 1860: total_loss: 0.368012, loss_sup: 0.303655, loss_mps: 0.025258, loss_cps: 0.039099
[12:26:39.483] iteration 1861: total_loss: 0.278595, loss_sup: 0.203342, loss_mps: 0.027874, loss_cps: 0.047378
[12:26:39.629] iteration 1862: total_loss: 0.207829, loss_sup: 0.142601, loss_mps: 0.024742, loss_cps: 0.040485
[12:26:39.775] iteration 1863: total_loss: 0.392403, loss_sup: 0.328729, loss_mps: 0.024067, loss_cps: 0.039608
[12:26:39.922] iteration 1864: total_loss: 0.255786, loss_sup: 0.191543, loss_mps: 0.024480, loss_cps: 0.039763
[12:26:40.068] iteration 1865: total_loss: 0.256127, loss_sup: 0.193396, loss_mps: 0.023684, loss_cps: 0.039047
[12:26:40.214] iteration 1866: total_loss: 0.479229, loss_sup: 0.415683, loss_mps: 0.024782, loss_cps: 0.038764
[12:26:40.359] iteration 1867: total_loss: 0.349860, loss_sup: 0.284481, loss_mps: 0.024507, loss_cps: 0.040872
[12:26:40.505] iteration 1868: total_loss: 0.597981, loss_sup: 0.530561, loss_mps: 0.024794, loss_cps: 0.042626
[12:26:40.653] iteration 1869: total_loss: 0.437257, loss_sup: 0.370364, loss_mps: 0.024101, loss_cps: 0.042792
[12:26:40.798] iteration 1870: total_loss: 0.440139, loss_sup: 0.368071, loss_mps: 0.025044, loss_cps: 0.047025
[12:26:40.944] iteration 1871: total_loss: 0.450220, loss_sup: 0.380673, loss_mps: 0.024692, loss_cps: 0.044856
[12:26:41.090] iteration 1872: total_loss: 0.637698, loss_sup: 0.566249, loss_mps: 0.025521, loss_cps: 0.045928
[12:26:41.236] iteration 1873: total_loss: 0.341623, loss_sup: 0.280066, loss_mps: 0.023700, loss_cps: 0.037857
[12:26:41.384] iteration 1874: total_loss: 0.516431, loss_sup: 0.443444, loss_mps: 0.025908, loss_cps: 0.047079
[12:26:41.529] iteration 1875: total_loss: 0.363449, loss_sup: 0.308822, loss_mps: 0.020739, loss_cps: 0.033889
[12:26:41.675] iteration 1876: total_loss: 0.353382, loss_sup: 0.300504, loss_mps: 0.020719, loss_cps: 0.032159
[12:26:41.820] iteration 1877: total_loss: 0.343353, loss_sup: 0.275171, loss_mps: 0.024755, loss_cps: 0.043427
[12:26:41.966] iteration 1878: total_loss: 0.452937, loss_sup: 0.384857, loss_mps: 0.024857, loss_cps: 0.043223
[12:26:42.111] iteration 1879: total_loss: 0.416055, loss_sup: 0.355531, loss_mps: 0.022000, loss_cps: 0.038523
[12:26:42.259] iteration 1880: total_loss: 0.381266, loss_sup: 0.311031, loss_mps: 0.025870, loss_cps: 0.044366
[12:26:42.405] iteration 1881: total_loss: 0.375462, loss_sup: 0.303910, loss_mps: 0.026090, loss_cps: 0.045461
[12:26:42.551] iteration 1882: total_loss: 0.292308, loss_sup: 0.231110, loss_mps: 0.023658, loss_cps: 0.037540
[12:26:42.696] iteration 1883: total_loss: 0.381244, loss_sup: 0.313219, loss_mps: 0.025723, loss_cps: 0.042302
[12:26:42.842] iteration 1884: total_loss: 0.481104, loss_sup: 0.414725, loss_mps: 0.024649, loss_cps: 0.041730
[12:26:42.987] iteration 1885: total_loss: 0.376122, loss_sup: 0.306967, loss_mps: 0.024889, loss_cps: 0.044266
[12:26:43.133] iteration 1886: total_loss: 0.603948, loss_sup: 0.537360, loss_mps: 0.023620, loss_cps: 0.042968
[12:26:43.279] iteration 1887: total_loss: 0.800613, loss_sup: 0.747331, loss_mps: 0.020954, loss_cps: 0.032328
[12:26:43.425] iteration 1888: total_loss: 0.362121, loss_sup: 0.296677, loss_mps: 0.024954, loss_cps: 0.040490
[12:26:43.570] iteration 1889: total_loss: 0.379198, loss_sup: 0.314574, loss_mps: 0.023706, loss_cps: 0.040918
[12:26:43.716] iteration 1890: total_loss: 0.199582, loss_sup: 0.146227, loss_mps: 0.021636, loss_cps: 0.031719
[12:26:43.862] iteration 1891: total_loss: 0.479346, loss_sup: 0.417552, loss_mps: 0.023600, loss_cps: 0.038193
[12:26:44.007] iteration 1892: total_loss: 0.211251, loss_sup: 0.157900, loss_mps: 0.021655, loss_cps: 0.031696
[12:26:44.155] iteration 1893: total_loss: 0.540693, loss_sup: 0.478738, loss_mps: 0.024360, loss_cps: 0.037595
[12:26:44.302] iteration 1894: total_loss: 0.772788, loss_sup: 0.700427, loss_mps: 0.026867, loss_cps: 0.045495
[12:26:44.448] iteration 1895: total_loss: 0.623775, loss_sup: 0.552762, loss_mps: 0.026715, loss_cps: 0.044298
[12:26:44.596] iteration 1896: total_loss: 0.526409, loss_sup: 0.452770, loss_mps: 0.027445, loss_cps: 0.046193
[12:26:44.743] iteration 1897: total_loss: 0.335738, loss_sup: 0.276105, loss_mps: 0.023441, loss_cps: 0.036191
[12:26:44.888] iteration 1898: total_loss: 0.437624, loss_sup: 0.363469, loss_mps: 0.028190, loss_cps: 0.045965
[12:26:45.034] iteration 1899: total_loss: 0.436436, loss_sup: 0.365132, loss_mps: 0.028454, loss_cps: 0.042850
[12:26:45.180] iteration 1900: total_loss: 0.439597, loss_sup: 0.381327, loss_mps: 0.023859, loss_cps: 0.034412
[12:26:45.180] Evaluation Started ==>
[12:26:56.638] ==> valid iteration 1900: unet metrics: {'dc': 0.4395747260094691, 'jc': 0.3296443399255719, 'pre': 0.5692943417463918, 'hd': 7.023247548493534}, ynet metrics: {'dc': 0.39548349355711354, 'jc': 0.29443973685805896, 'pre': 0.491485724957581, 'hd': 7.42388066271346}.
[12:26:56.640] Evaluation Finished!⏹️
[12:26:56.796] iteration 1901: total_loss: 0.454359, loss_sup: 0.386219, loss_mps: 0.027184, loss_cps: 0.040956
[12:26:56.946] iteration 1902: total_loss: 0.335769, loss_sup: 0.262985, loss_mps: 0.027606, loss_cps: 0.045177
[12:26:57.093] iteration 1903: total_loss: 0.323918, loss_sup: 0.259751, loss_mps: 0.025985, loss_cps: 0.038182
[12:26:57.238] iteration 1904: total_loss: 0.278609, loss_sup: 0.217406, loss_mps: 0.025490, loss_cps: 0.035712
[12:26:57.383] iteration 1905: total_loss: 0.390932, loss_sup: 0.326405, loss_mps: 0.026394, loss_cps: 0.038132
[12:26:57.527] iteration 1906: total_loss: 0.280801, loss_sup: 0.204742, loss_mps: 0.030068, loss_cps: 0.045991
[12:26:57.674] iteration 1907: total_loss: 0.308801, loss_sup: 0.239409, loss_mps: 0.026560, loss_cps: 0.042832
[12:26:57.821] iteration 1908: total_loss: 0.347098, loss_sup: 0.284047, loss_mps: 0.025189, loss_cps: 0.037862
[12:26:57.967] iteration 1909: total_loss: 0.404870, loss_sup: 0.331209, loss_mps: 0.027845, loss_cps: 0.045815
[12:26:58.112] iteration 1910: total_loss: 0.367302, loss_sup: 0.311371, loss_mps: 0.022708, loss_cps: 0.033223
[12:26:58.257] iteration 1911: total_loss: 0.634468, loss_sup: 0.557881, loss_mps: 0.028591, loss_cps: 0.047995
[12:26:58.403] iteration 1912: total_loss: 0.365934, loss_sup: 0.276482, loss_mps: 0.032040, loss_cps: 0.057412
[12:26:58.548] iteration 1913: total_loss: 0.424871, loss_sup: 0.349505, loss_mps: 0.027462, loss_cps: 0.047905
[12:26:58.693] iteration 1914: total_loss: 0.421596, loss_sup: 0.353769, loss_mps: 0.025364, loss_cps: 0.042464
[12:26:58.838] iteration 1915: total_loss: 0.233695, loss_sup: 0.169535, loss_mps: 0.024586, loss_cps: 0.039575
[12:26:58.983] iteration 1916: total_loss: 0.473345, loss_sup: 0.396733, loss_mps: 0.028976, loss_cps: 0.047635
[12:26:59.128] iteration 1917: total_loss: 0.357640, loss_sup: 0.294194, loss_mps: 0.024182, loss_cps: 0.039264
[12:26:59.275] iteration 1918: total_loss: 0.261422, loss_sup: 0.203500, loss_mps: 0.022801, loss_cps: 0.035120
[12:26:59.421] iteration 1919: total_loss: 0.486816, loss_sup: 0.425127, loss_mps: 0.023344, loss_cps: 0.038345
[12:26:59.568] iteration 1920: total_loss: 0.419341, loss_sup: 0.335261, loss_mps: 0.029700, loss_cps: 0.054380
[12:26:59.714] iteration 1921: total_loss: 0.305327, loss_sup: 0.245757, loss_mps: 0.023521, loss_cps: 0.036049
[12:26:59.861] iteration 1922: total_loss: 0.210371, loss_sup: 0.151871, loss_mps: 0.022829, loss_cps: 0.035671
[12:27:00.011] iteration 1923: total_loss: 0.450980, loss_sup: 0.383601, loss_mps: 0.025106, loss_cps: 0.042273
[12:27:00.158] iteration 1924: total_loss: 0.611183, loss_sup: 0.553502, loss_mps: 0.022802, loss_cps: 0.034880
[12:27:00.304] iteration 1925: total_loss: 0.874627, loss_sup: 0.800913, loss_mps: 0.027094, loss_cps: 0.046619
[12:27:00.449] iteration 1926: total_loss: 0.744637, loss_sup: 0.679876, loss_mps: 0.024498, loss_cps: 0.040264
[12:27:00.595] iteration 1927: total_loss: 0.280772, loss_sup: 0.225047, loss_mps: 0.021630, loss_cps: 0.034095
[12:27:00.744] iteration 1928: total_loss: 0.437422, loss_sup: 0.376400, loss_mps: 0.024107, loss_cps: 0.036915
[12:27:00.889] iteration 1929: total_loss: 0.464157, loss_sup: 0.390368, loss_mps: 0.027948, loss_cps: 0.045841
[12:27:01.034] iteration 1930: total_loss: 0.572479, loss_sup: 0.511629, loss_mps: 0.023861, loss_cps: 0.036988
[12:27:01.179] iteration 1931: total_loss: 0.404166, loss_sup: 0.336583, loss_mps: 0.026650, loss_cps: 0.040933
[12:27:01.325] iteration 1932: total_loss: 0.542869, loss_sup: 0.471529, loss_mps: 0.027750, loss_cps: 0.043590
[12:27:01.470] iteration 1933: total_loss: 0.323437, loss_sup: 0.266468, loss_mps: 0.023346, loss_cps: 0.033623
[12:27:01.616] iteration 1934: total_loss: 0.607561, loss_sup: 0.542614, loss_mps: 0.025094, loss_cps: 0.039852
[12:27:01.763] iteration 1935: total_loss: 0.475857, loss_sup: 0.395466, loss_mps: 0.030325, loss_cps: 0.050065
[12:27:01.909] iteration 1936: total_loss: 0.297177, loss_sup: 0.233984, loss_mps: 0.025165, loss_cps: 0.038028
[12:27:02.054] iteration 1937: total_loss: 0.606788, loss_sup: 0.531804, loss_mps: 0.029112, loss_cps: 0.045873
[12:27:02.201] iteration 1938: total_loss: 0.402082, loss_sup: 0.335205, loss_mps: 0.026139, loss_cps: 0.040738
[12:27:02.346] iteration 1939: total_loss: 0.312582, loss_sup: 0.246564, loss_mps: 0.026414, loss_cps: 0.039604
[12:27:02.494] iteration 1940: total_loss: 0.364834, loss_sup: 0.297379, loss_mps: 0.026053, loss_cps: 0.041402
[12:27:02.640] iteration 1941: total_loss: 0.308636, loss_sup: 0.255819, loss_mps: 0.022687, loss_cps: 0.030130
[12:27:02.786] iteration 1942: total_loss: 0.562753, loss_sup: 0.495372, loss_mps: 0.026220, loss_cps: 0.041162
[12:27:02.932] iteration 1943: total_loss: 0.423168, loss_sup: 0.343217, loss_mps: 0.029775, loss_cps: 0.050176
[12:27:03.078] iteration 1944: total_loss: 0.335525, loss_sup: 0.278585, loss_mps: 0.023800, loss_cps: 0.033140
[12:27:03.226] iteration 1945: total_loss: 0.453008, loss_sup: 0.380553, loss_mps: 0.027681, loss_cps: 0.044774
[12:27:03.372] iteration 1946: total_loss: 0.243201, loss_sup: 0.178008, loss_mps: 0.024926, loss_cps: 0.040267
[12:27:03.517] iteration 1947: total_loss: 0.474275, loss_sup: 0.409048, loss_mps: 0.025037, loss_cps: 0.040191
[12:27:03.663] iteration 1948: total_loss: 0.203273, loss_sup: 0.145063, loss_mps: 0.023509, loss_cps: 0.034701
[12:27:03.810] iteration 1949: total_loss: 0.613447, loss_sup: 0.547906, loss_mps: 0.025301, loss_cps: 0.040240
[12:27:03.956] iteration 1950: total_loss: 0.284979, loss_sup: 0.227527, loss_mps: 0.022240, loss_cps: 0.035212
[12:27:04.101] iteration 1951: total_loss: 0.239886, loss_sup: 0.180800, loss_mps: 0.022720, loss_cps: 0.036366
[12:27:04.246] iteration 1952: total_loss: 0.290093, loss_sup: 0.218859, loss_mps: 0.026006, loss_cps: 0.045227
[12:27:04.395] iteration 1953: total_loss: 0.327472, loss_sup: 0.265751, loss_mps: 0.022930, loss_cps: 0.038791
[12:27:04.541] iteration 1954: total_loss: 0.314763, loss_sup: 0.242029, loss_mps: 0.026425, loss_cps: 0.046309
[12:27:04.686] iteration 1955: total_loss: 0.188394, loss_sup: 0.134002, loss_mps: 0.021413, loss_cps: 0.032979
[12:27:04.833] iteration 1956: total_loss: 0.699675, loss_sup: 0.611983, loss_mps: 0.030297, loss_cps: 0.057394
[12:27:04.979] iteration 1957: total_loss: 0.239671, loss_sup: 0.183447, loss_mps: 0.020525, loss_cps: 0.035699
[12:27:05.124] iteration 1958: total_loss: 0.388693, loss_sup: 0.318632, loss_mps: 0.024833, loss_cps: 0.045229
[12:27:05.270] iteration 1959: total_loss: 0.171419, loss_sup: 0.116617, loss_mps: 0.020530, loss_cps: 0.034272
[12:27:05.416] iteration 1960: total_loss: 0.436418, loss_sup: 0.384423, loss_mps: 0.019837, loss_cps: 0.032157
[12:27:05.565] iteration 1961: total_loss: 0.232014, loss_sup: 0.174745, loss_mps: 0.021513, loss_cps: 0.035757
[12:27:05.711] iteration 1962: total_loss: 0.739880, loss_sup: 0.658148, loss_mps: 0.028013, loss_cps: 0.053718
[12:27:05.856] iteration 1963: total_loss: 0.329180, loss_sup: 0.255741, loss_mps: 0.025743, loss_cps: 0.047697
[12:27:06.002] iteration 1964: total_loss: 0.222180, loss_sup: 0.168457, loss_mps: 0.020475, loss_cps: 0.033248
[12:27:06.148] iteration 1965: total_loss: 0.585650, loss_sup: 0.502437, loss_mps: 0.028438, loss_cps: 0.054775
[12:27:06.294] iteration 1966: total_loss: 0.220938, loss_sup: 0.162560, loss_mps: 0.021255, loss_cps: 0.037123
[12:27:06.439] iteration 1967: total_loss: 0.432850, loss_sup: 0.378333, loss_mps: 0.020638, loss_cps: 0.033879
[12:27:06.585] iteration 1968: total_loss: 0.533707, loss_sup: 0.451149, loss_mps: 0.028781, loss_cps: 0.053777
[12:27:06.733] iteration 1969: total_loss: 0.413735, loss_sup: 0.339434, loss_mps: 0.026336, loss_cps: 0.047965
[12:27:06.879] iteration 1970: total_loss: 0.661414, loss_sup: 0.590761, loss_mps: 0.026117, loss_cps: 0.044536
[12:27:07.027] iteration 1971: total_loss: 0.419563, loss_sup: 0.368025, loss_mps: 0.020466, loss_cps: 0.031072
[12:27:07.174] iteration 1972: total_loss: 0.395557, loss_sup: 0.331596, loss_mps: 0.023922, loss_cps: 0.040038
[12:27:07.320] iteration 1973: total_loss: 0.395169, loss_sup: 0.322962, loss_mps: 0.026370, loss_cps: 0.045837
[12:27:07.466] iteration 1974: total_loss: 0.456985, loss_sup: 0.400601, loss_mps: 0.022086, loss_cps: 0.034298
[12:27:07.613] iteration 1975: total_loss: 0.388729, loss_sup: 0.327826, loss_mps: 0.023586, loss_cps: 0.037317
[12:27:07.759] iteration 1976: total_loss: 0.551381, loss_sup: 0.478653, loss_mps: 0.026398, loss_cps: 0.046331
[12:27:07.908] iteration 1977: total_loss: 0.378222, loss_sup: 0.312649, loss_mps: 0.024926, loss_cps: 0.040647
[12:27:08.054] iteration 1978: total_loss: 0.310478, loss_sup: 0.251402, loss_mps: 0.023802, loss_cps: 0.035273
[12:27:08.201] iteration 1979: total_loss: 0.468909, loss_sup: 0.397262, loss_mps: 0.027394, loss_cps: 0.044253
[12:27:08.347] iteration 1980: total_loss: 0.356686, loss_sup: 0.294048, loss_mps: 0.025176, loss_cps: 0.037463
[12:27:08.498] iteration 1981: total_loss: 0.362425, loss_sup: 0.298762, loss_mps: 0.024921, loss_cps: 0.038742
[12:27:08.646] iteration 1982: total_loss: 0.329492, loss_sup: 0.264697, loss_mps: 0.025215, loss_cps: 0.039580
[12:27:08.793] iteration 1983: total_loss: 0.504350, loss_sup: 0.429703, loss_mps: 0.028448, loss_cps: 0.046199
[12:27:08.940] iteration 1984: total_loss: 0.549749, loss_sup: 0.489797, loss_mps: 0.023634, loss_cps: 0.036318
[12:27:09.087] iteration 1985: total_loss: 0.371989, loss_sup: 0.313723, loss_mps: 0.023370, loss_cps: 0.034895
[12:27:09.233] iteration 1986: total_loss: 0.423636, loss_sup: 0.362991, loss_mps: 0.023442, loss_cps: 0.037203
[12:27:09.386] iteration 1987: total_loss: 0.368596, loss_sup: 0.300070, loss_mps: 0.025530, loss_cps: 0.042995
[12:27:09.533] iteration 1988: total_loss: 0.375824, loss_sup: 0.309931, loss_mps: 0.025143, loss_cps: 0.040750
[12:27:09.679] iteration 1989: total_loss: 0.346211, loss_sup: 0.260403, loss_mps: 0.031052, loss_cps: 0.054756
[12:27:09.829] iteration 1990: total_loss: 0.522886, loss_sup: 0.462893, loss_mps: 0.024289, loss_cps: 0.035705
[12:27:09.978] iteration 1991: total_loss: 0.308844, loss_sup: 0.256514, loss_mps: 0.021550, loss_cps: 0.030779
[12:27:10.126] iteration 1992: total_loss: 0.454834, loss_sup: 0.380767, loss_mps: 0.028155, loss_cps: 0.045913
[12:27:10.274] iteration 1993: total_loss: 0.269365, loss_sup: 0.202779, loss_mps: 0.025172, loss_cps: 0.041414
[12:27:10.420] iteration 1994: total_loss: 0.385792, loss_sup: 0.315063, loss_mps: 0.026423, loss_cps: 0.044305
[12:27:10.566] iteration 1995: total_loss: 0.199720, loss_sup: 0.149874, loss_mps: 0.019650, loss_cps: 0.030196
[12:27:10.712] iteration 1996: total_loss: 0.293063, loss_sup: 0.242625, loss_mps: 0.020238, loss_cps: 0.030201
[12:27:10.858] iteration 1997: total_loss: 0.317390, loss_sup: 0.250222, loss_mps: 0.025068, loss_cps: 0.042100
[12:27:11.006] iteration 1998: total_loss: 0.180140, loss_sup: 0.131543, loss_mps: 0.019370, loss_cps: 0.029228
[12:27:11.152] iteration 1999: total_loss: 0.235661, loss_sup: 0.189062, loss_mps: 0.018675, loss_cps: 0.027924
[12:27:11.299] iteration 2000: total_loss: 0.283578, loss_sup: 0.220316, loss_mps: 0.023868, loss_cps: 0.039394
[12:27:11.299] Evaluation Started ==>
[12:27:22.745] ==> valid iteration 2000: unet metrics: {'dc': 0.4490845546692763, 'jc': 0.33001237450424936, 'pre': 0.4677093834350636, 'hd': 7.905471225059307}, ynet metrics: {'dc': 0.4530113312304732, 'jc': 0.33779076278531867, 'pre': 0.5131871277362895, 'hd': 7.596329879847533}.
[12:27:22.747] Evaluation Finished!⏹️
[12:27:22.905] iteration 2001: total_loss: 0.439366, loss_sup: 0.369274, loss_mps: 0.025548, loss_cps: 0.044545
[12:27:23.052] iteration 2002: total_loss: 0.375688, loss_sup: 0.324281, loss_mps: 0.019786, loss_cps: 0.031621
[12:27:23.197] iteration 2003: total_loss: 0.190955, loss_sup: 0.141655, loss_mps: 0.019376, loss_cps: 0.029924
[12:27:23.342] iteration 2004: total_loss: 0.257887, loss_sup: 0.196573, loss_mps: 0.022470, loss_cps: 0.038845
[12:27:23.488] iteration 2005: total_loss: 0.384886, loss_sup: 0.332728, loss_mps: 0.019653, loss_cps: 0.032505
[12:27:23.634] iteration 2006: total_loss: 0.453206, loss_sup: 0.389502, loss_mps: 0.022845, loss_cps: 0.040859
[12:27:23.781] iteration 2007: total_loss: 0.163315, loss_sup: 0.110459, loss_mps: 0.019168, loss_cps: 0.033689
[12:27:23.927] iteration 2008: total_loss: 0.452655, loss_sup: 0.400572, loss_mps: 0.019659, loss_cps: 0.032424
[12:27:24.075] iteration 2009: total_loss: 0.565578, loss_sup: 0.519517, loss_mps: 0.018092, loss_cps: 0.027968
[12:27:24.221] iteration 2010: total_loss: 0.309964, loss_sup: 0.251900, loss_mps: 0.020960, loss_cps: 0.037104
[12:27:24.366] iteration 2011: total_loss: 0.302898, loss_sup: 0.247385, loss_mps: 0.019922, loss_cps: 0.035590
[12:27:24.514] iteration 2012: total_loss: 0.495006, loss_sup: 0.445562, loss_mps: 0.018773, loss_cps: 0.030671
[12:27:24.662] iteration 2013: total_loss: 0.385362, loss_sup: 0.328641, loss_mps: 0.021175, loss_cps: 0.035546
[12:27:24.807] iteration 2014: total_loss: 0.364324, loss_sup: 0.319495, loss_mps: 0.018040, loss_cps: 0.026790
[12:27:24.954] iteration 2015: total_loss: 0.248863, loss_sup: 0.203961, loss_mps: 0.017465, loss_cps: 0.027438
[12:27:25.102] iteration 2016: total_loss: 0.218447, loss_sup: 0.165730, loss_mps: 0.019989, loss_cps: 0.032729
[12:27:25.248] iteration 2017: total_loss: 0.410243, loss_sup: 0.363773, loss_mps: 0.018788, loss_cps: 0.027682
[12:27:25.396] iteration 2018: total_loss: 0.311505, loss_sup: 0.255769, loss_mps: 0.020937, loss_cps: 0.034800
[12:27:25.544] iteration 2019: total_loss: 0.265030, loss_sup: 0.205212, loss_mps: 0.021843, loss_cps: 0.037974
[12:27:25.691] iteration 2020: total_loss: 0.448076, loss_sup: 0.390293, loss_mps: 0.021603, loss_cps: 0.036180
[12:27:25.837] iteration 2021: total_loss: 0.332701, loss_sup: 0.275165, loss_mps: 0.020747, loss_cps: 0.036789
[12:27:25.982] iteration 2022: total_loss: 0.372135, loss_sup: 0.312587, loss_mps: 0.022278, loss_cps: 0.037269
[12:27:26.130] iteration 2023: total_loss: 0.292199, loss_sup: 0.240655, loss_mps: 0.020534, loss_cps: 0.031009
[12:27:26.276] iteration 2024: total_loss: 0.251607, loss_sup: 0.193206, loss_mps: 0.021911, loss_cps: 0.036490
[12:27:26.421] iteration 2025: total_loss: 0.320434, loss_sup: 0.256246, loss_mps: 0.023591, loss_cps: 0.040596
[12:27:26.566] iteration 2026: total_loss: 0.487798, loss_sup: 0.428961, loss_mps: 0.021783, loss_cps: 0.037054
[12:27:26.713] iteration 2027: total_loss: 0.246617, loss_sup: 0.179798, loss_mps: 0.024395, loss_cps: 0.042424
[12:27:26.861] iteration 2028: total_loss: 0.365344, loss_sup: 0.317526, loss_mps: 0.019092, loss_cps: 0.028726
[12:27:27.007] iteration 2029: total_loss: 0.338837, loss_sup: 0.277720, loss_mps: 0.022273, loss_cps: 0.038844
[12:27:27.153] iteration 2030: total_loss: 0.293996, loss_sup: 0.236482, loss_mps: 0.021227, loss_cps: 0.036288
[12:27:27.301] iteration 2031: total_loss: 0.304107, loss_sup: 0.239481, loss_mps: 0.023615, loss_cps: 0.041011
[12:27:27.447] iteration 2032: total_loss: 0.293800, loss_sup: 0.225001, loss_mps: 0.024640, loss_cps: 0.044160
[12:27:27.593] iteration 2033: total_loss: 0.263208, loss_sup: 0.199624, loss_mps: 0.023445, loss_cps: 0.040139
[12:27:27.738] iteration 2034: total_loss: 0.338528, loss_sup: 0.277740, loss_mps: 0.023123, loss_cps: 0.037665
[12:27:27.884] iteration 2035: total_loss: 0.545789, loss_sup: 0.471887, loss_mps: 0.025622, loss_cps: 0.048280
[12:27:28.031] iteration 2036: total_loss: 0.309075, loss_sup: 0.251029, loss_mps: 0.021092, loss_cps: 0.036954
[12:27:28.177] iteration 2037: total_loss: 0.309456, loss_sup: 0.255992, loss_mps: 0.020141, loss_cps: 0.033323
[12:27:28.323] iteration 2038: total_loss: 0.311257, loss_sup: 0.243190, loss_mps: 0.024088, loss_cps: 0.043978
[12:27:28.472] iteration 2039: total_loss: 0.373847, loss_sup: 0.305277, loss_mps: 0.024633, loss_cps: 0.043938
[12:27:28.618] iteration 2040: total_loss: 0.407889, loss_sup: 0.341916, loss_mps: 0.023589, loss_cps: 0.042385
[12:27:28.763] iteration 2041: total_loss: 0.367648, loss_sup: 0.305506, loss_mps: 0.022466, loss_cps: 0.039676
[12:27:28.911] iteration 2042: total_loss: 0.462499, loss_sup: 0.388896, loss_mps: 0.025520, loss_cps: 0.048084
[12:27:29.056] iteration 2043: total_loss: 0.268861, loss_sup: 0.192259, loss_mps: 0.026384, loss_cps: 0.050218
[12:27:29.203] iteration 2044: total_loss: 0.229986, loss_sup: 0.153009, loss_mps: 0.027030, loss_cps: 0.049947
[12:27:29.348] iteration 2045: total_loss: 0.578681, loss_sup: 0.496848, loss_mps: 0.028157, loss_cps: 0.053675
[12:27:29.494] iteration 2046: total_loss: 0.401973, loss_sup: 0.341101, loss_mps: 0.021745, loss_cps: 0.039126
[12:27:29.641] iteration 2047: total_loss: 0.317386, loss_sup: 0.252648, loss_mps: 0.023020, loss_cps: 0.041718
[12:27:29.787] iteration 2048: total_loss: 0.740915, loss_sup: 0.673169, loss_mps: 0.024570, loss_cps: 0.043177
[12:27:29.932] iteration 2049: total_loss: 0.372159, loss_sup: 0.293910, loss_mps: 0.027798, loss_cps: 0.050452
[12:27:30.077] iteration 2050: total_loss: 0.564526, loss_sup: 0.496787, loss_mps: 0.024685, loss_cps: 0.043054
[12:27:30.223] iteration 2051: total_loss: 0.617406, loss_sup: 0.549133, loss_mps: 0.024574, loss_cps: 0.043700
[12:27:30.368] iteration 2052: total_loss: 0.281431, loss_sup: 0.221570, loss_mps: 0.022604, loss_cps: 0.037257
[12:27:30.514] iteration 2053: total_loss: 0.281401, loss_sup: 0.226126, loss_mps: 0.021242, loss_cps: 0.034033
[12:27:30.659] iteration 2054: total_loss: 0.284818, loss_sup: 0.221870, loss_mps: 0.023234, loss_cps: 0.039714
[12:27:30.805] iteration 2055: total_loss: 0.201097, loss_sup: 0.150418, loss_mps: 0.020057, loss_cps: 0.030621
[12:27:30.954] iteration 2056: total_loss: 0.561864, loss_sup: 0.484770, loss_mps: 0.028105, loss_cps: 0.048989
[12:27:31.099] iteration 2057: total_loss: 0.432699, loss_sup: 0.365460, loss_mps: 0.025969, loss_cps: 0.041270
[12:27:31.245] iteration 2058: total_loss: 0.356253, loss_sup: 0.283493, loss_mps: 0.026289, loss_cps: 0.046472
[12:27:31.390] iteration 2059: total_loss: 0.407643, loss_sup: 0.340732, loss_mps: 0.025491, loss_cps: 0.041420
[12:27:31.536] iteration 2060: total_loss: 0.470920, loss_sup: 0.395023, loss_mps: 0.027813, loss_cps: 0.048085
[12:27:31.682] iteration 2061: total_loss: 0.242150, loss_sup: 0.168403, loss_mps: 0.027529, loss_cps: 0.046218
[12:27:31.831] iteration 2062: total_loss: 0.338836, loss_sup: 0.276927, loss_mps: 0.023728, loss_cps: 0.038180
[12:27:31.978] iteration 2063: total_loss: 0.529728, loss_sup: 0.459194, loss_mps: 0.026762, loss_cps: 0.043772
[12:27:32.124] iteration 2064: total_loss: 0.521144, loss_sup: 0.446600, loss_mps: 0.027251, loss_cps: 0.047293
[12:27:32.270] iteration 2065: total_loss: 0.260594, loss_sup: 0.203607, loss_mps: 0.023589, loss_cps: 0.033397
[12:27:32.416] iteration 2066: total_loss: 0.835038, loss_sup: 0.764022, loss_mps: 0.026904, loss_cps: 0.044112
[12:27:32.562] iteration 2067: total_loss: 0.652211, loss_sup: 0.585856, loss_mps: 0.025643, loss_cps: 0.040712
[12:27:32.707] iteration 2068: total_loss: 0.447568, loss_sup: 0.387410, loss_mps: 0.024121, loss_cps: 0.036038
[12:27:32.853] iteration 2069: total_loss: 0.439653, loss_sup: 0.369208, loss_mps: 0.026656, loss_cps: 0.043789
[12:27:33.001] iteration 2070: total_loss: 0.322270, loss_sup: 0.255434, loss_mps: 0.027094, loss_cps: 0.039742
[12:27:33.147] iteration 2071: total_loss: 0.375353, loss_sup: 0.297835, loss_mps: 0.029002, loss_cps: 0.048516
[12:27:33.293] iteration 2072: total_loss: 0.496234, loss_sup: 0.424032, loss_mps: 0.028485, loss_cps: 0.043717
[12:27:33.439] iteration 2073: total_loss: 0.372175, loss_sup: 0.302044, loss_mps: 0.026937, loss_cps: 0.043194
[12:27:33.587] iteration 2074: total_loss: 0.282698, loss_sup: 0.219894, loss_mps: 0.025253, loss_cps: 0.037551
[12:27:33.735] iteration 2075: total_loss: 0.428146, loss_sup: 0.362499, loss_mps: 0.025420, loss_cps: 0.040227
[12:27:33.881] iteration 2076: total_loss: 0.363318, loss_sup: 0.290724, loss_mps: 0.027850, loss_cps: 0.044744
[12:27:34.027] iteration 2077: total_loss: 0.329252, loss_sup: 0.262003, loss_mps: 0.025729, loss_cps: 0.041519
[12:27:34.172] iteration 2078: total_loss: 0.215770, loss_sup: 0.162028, loss_mps: 0.022046, loss_cps: 0.031696
[12:27:34.317] iteration 2079: total_loss: 0.373701, loss_sup: 0.297664, loss_mps: 0.028717, loss_cps: 0.047321
[12:27:34.463] iteration 2080: total_loss: 0.446501, loss_sup: 0.375743, loss_mps: 0.025648, loss_cps: 0.045111
[12:27:34.609] iteration 2081: total_loss: 0.409070, loss_sup: 0.342954, loss_mps: 0.024609, loss_cps: 0.041507
[12:27:34.756] iteration 2082: total_loss: 0.420282, loss_sup: 0.358698, loss_mps: 0.023155, loss_cps: 0.038429
[12:27:34.901] iteration 2083: total_loss: 0.355155, loss_sup: 0.295570, loss_mps: 0.022858, loss_cps: 0.036727
[12:27:35.047] iteration 2084: total_loss: 0.338722, loss_sup: 0.265930, loss_mps: 0.027506, loss_cps: 0.045286
[12:27:35.192] iteration 2085: total_loss: 0.286356, loss_sup: 0.228490, loss_mps: 0.022580, loss_cps: 0.035286
[12:27:35.338] iteration 2086: total_loss: 0.453319, loss_sup: 0.373025, loss_mps: 0.028574, loss_cps: 0.051720
[12:27:35.484] iteration 2087: total_loss: 0.431355, loss_sup: 0.382319, loss_mps: 0.019251, loss_cps: 0.029784
[12:27:35.629] iteration 2088: total_loss: 0.311960, loss_sup: 0.236971, loss_mps: 0.027168, loss_cps: 0.047821
[12:27:35.775] iteration 2089: total_loss: 0.248062, loss_sup: 0.185865, loss_mps: 0.022210, loss_cps: 0.039987
[12:27:35.836] iteration 2090: total_loss: 0.167176, loss_sup: 0.125746, loss_mps: 0.018279, loss_cps: 0.023151
[12:27:37.114] iteration 2091: total_loss: 0.454364, loss_sup: 0.392509, loss_mps: 0.022788, loss_cps: 0.039067
[12:27:37.264] iteration 2092: total_loss: 0.348631, loss_sup: 0.301968, loss_mps: 0.018904, loss_cps: 0.027760
[12:27:37.411] iteration 2093: total_loss: 0.383305, loss_sup: 0.321512, loss_mps: 0.022919, loss_cps: 0.038874
[12:27:37.558] iteration 2094: total_loss: 0.293588, loss_sup: 0.239635, loss_mps: 0.020869, loss_cps: 0.033084
[12:27:37.705] iteration 2095: total_loss: 0.362850, loss_sup: 0.313170, loss_mps: 0.019355, loss_cps: 0.030325
[12:27:37.851] iteration 2096: total_loss: 0.188703, loss_sup: 0.128838, loss_mps: 0.022126, loss_cps: 0.037740
[12:27:37.997] iteration 2097: total_loss: 0.595208, loss_sup: 0.517795, loss_mps: 0.027004, loss_cps: 0.050408
[12:27:38.144] iteration 2098: total_loss: 0.542864, loss_sup: 0.472607, loss_mps: 0.024853, loss_cps: 0.045403
[12:27:38.293] iteration 2099: total_loss: 0.323066, loss_sup: 0.267107, loss_mps: 0.021289, loss_cps: 0.034670
[12:27:38.439] iteration 2100: total_loss: 0.179421, loss_sup: 0.121635, loss_mps: 0.022186, loss_cps: 0.035600
[12:27:38.439] Evaluation Started ==>
[12:27:49.867] ==> valid iteration 2100: unet metrics: {'dc': 0.47081963870189225, 'jc': 0.3512201155983009, 'pre': 0.5229955474755654, 'hd': 7.326000612429072}, ynet metrics: {'dc': 0.4349092128806403, 'jc': 0.31783340505532404, 'pre': 0.5028123631116065, 'hd': 7.753601758516702}.
[12:27:49.993] ==> New best valid dice for unet: 0.470820, at iteration 2100
[12:27:49.994] Evaluation Finished!⏹️
[12:27:50.146] iteration 2101: total_loss: 0.310410, loss_sup: 0.260875, loss_mps: 0.019682, loss_cps: 0.029853
[12:27:50.292] iteration 2102: total_loss: 0.303161, loss_sup: 0.233103, loss_mps: 0.025016, loss_cps: 0.045043
[12:27:50.439] iteration 2103: total_loss: 0.350598, loss_sup: 0.291883, loss_mps: 0.021765, loss_cps: 0.036950
[12:27:50.584] iteration 2104: total_loss: 0.374692, loss_sup: 0.304657, loss_mps: 0.025054, loss_cps: 0.044982
[12:27:50.731] iteration 2105: total_loss: 0.429231, loss_sup: 0.357215, loss_mps: 0.026121, loss_cps: 0.045895
[12:27:50.876] iteration 2106: total_loss: 0.254440, loss_sup: 0.203777, loss_mps: 0.019834, loss_cps: 0.030829
[12:27:51.026] iteration 2107: total_loss: 0.357537, loss_sup: 0.289142, loss_mps: 0.025263, loss_cps: 0.043132
[12:27:51.171] iteration 2108: total_loss: 0.420599, loss_sup: 0.346913, loss_mps: 0.026313, loss_cps: 0.047373
[12:27:51.316] iteration 2109: total_loss: 0.623991, loss_sup: 0.558447, loss_mps: 0.024274, loss_cps: 0.041269
[12:27:51.461] iteration 2110: total_loss: 0.433536, loss_sup: 0.354608, loss_mps: 0.027762, loss_cps: 0.051166
[12:27:51.605] iteration 2111: total_loss: 0.207087, loss_sup: 0.142396, loss_mps: 0.023587, loss_cps: 0.041105
[12:27:51.750] iteration 2112: total_loss: 0.439832, loss_sup: 0.385632, loss_mps: 0.021298, loss_cps: 0.032902
[12:27:51.895] iteration 2113: total_loss: 0.518901, loss_sup: 0.455788, loss_mps: 0.023707, loss_cps: 0.039406
[12:27:52.040] iteration 2114: total_loss: 0.435281, loss_sup: 0.362247, loss_mps: 0.025715, loss_cps: 0.047319
[12:27:52.188] iteration 2115: total_loss: 0.402929, loss_sup: 0.334050, loss_mps: 0.024955, loss_cps: 0.043923
[12:27:52.337] iteration 2116: total_loss: 0.228663, loss_sup: 0.160845, loss_mps: 0.025286, loss_cps: 0.042532
[12:27:52.482] iteration 2117: total_loss: 0.413120, loss_sup: 0.340510, loss_mps: 0.025868, loss_cps: 0.046741
[12:27:52.628] iteration 2118: total_loss: 0.373983, loss_sup: 0.299823, loss_mps: 0.027162, loss_cps: 0.046998
[12:27:52.773] iteration 2119: total_loss: 0.324368, loss_sup: 0.247511, loss_mps: 0.027563, loss_cps: 0.049294
[12:27:52.918] iteration 2120: total_loss: 0.356591, loss_sup: 0.283420, loss_mps: 0.027602, loss_cps: 0.045570
[12:27:53.064] iteration 2121: total_loss: 0.335898, loss_sup: 0.268618, loss_mps: 0.024653, loss_cps: 0.042627
[12:27:53.210] iteration 2122: total_loss: 0.456343, loss_sup: 0.383475, loss_mps: 0.027027, loss_cps: 0.045841
[12:27:53.355] iteration 2123: total_loss: 0.439346, loss_sup: 0.357200, loss_mps: 0.029529, loss_cps: 0.052617
[12:27:53.501] iteration 2124: total_loss: 0.377318, loss_sup: 0.308554, loss_mps: 0.025335, loss_cps: 0.043429
[12:27:53.646] iteration 2125: total_loss: 0.330558, loss_sup: 0.274216, loss_mps: 0.021961, loss_cps: 0.034381
[12:27:53.792] iteration 2126: total_loss: 0.638211, loss_sup: 0.563371, loss_mps: 0.027376, loss_cps: 0.047464
[12:27:53.937] iteration 2127: total_loss: 0.391857, loss_sup: 0.323594, loss_mps: 0.025669, loss_cps: 0.042594
[12:27:54.083] iteration 2128: total_loss: 0.313159, loss_sup: 0.256310, loss_mps: 0.022165, loss_cps: 0.034685
[12:27:54.228] iteration 2129: total_loss: 0.455454, loss_sup: 0.380339, loss_mps: 0.028099, loss_cps: 0.047015
[12:27:54.375] iteration 2130: total_loss: 0.367609, loss_sup: 0.296282, loss_mps: 0.026749, loss_cps: 0.044578
[12:27:54.520] iteration 2131: total_loss: 0.175396, loss_sup: 0.100162, loss_mps: 0.027059, loss_cps: 0.048174
[12:27:54.666] iteration 2132: total_loss: 0.379160, loss_sup: 0.307343, loss_mps: 0.027244, loss_cps: 0.044574
[12:27:54.816] iteration 2133: total_loss: 0.232170, loss_sup: 0.170103, loss_mps: 0.023469, loss_cps: 0.038598
[12:27:54.961] iteration 2134: total_loss: 0.222286, loss_sup: 0.157431, loss_mps: 0.025222, loss_cps: 0.039633
[12:27:55.107] iteration 2135: total_loss: 0.331601, loss_sup: 0.260348, loss_mps: 0.026165, loss_cps: 0.045088
[12:27:55.253] iteration 2136: total_loss: 0.362895, loss_sup: 0.290074, loss_mps: 0.026481, loss_cps: 0.046339
[12:27:55.399] iteration 2137: total_loss: 0.501764, loss_sup: 0.435917, loss_mps: 0.024819, loss_cps: 0.041028
[12:27:55.545] iteration 2138: total_loss: 0.375962, loss_sup: 0.316400, loss_mps: 0.022804, loss_cps: 0.036758
[12:27:55.691] iteration 2139: total_loss: 0.180506, loss_sup: 0.128153, loss_mps: 0.020869, loss_cps: 0.031483
[12:27:55.837] iteration 2140: total_loss: 0.748786, loss_sup: 0.683745, loss_mps: 0.024389, loss_cps: 0.040652
[12:27:55.983] iteration 2141: total_loss: 0.350965, loss_sup: 0.301053, loss_mps: 0.019520, loss_cps: 0.030392
[12:27:56.129] iteration 2142: total_loss: 0.305048, loss_sup: 0.240082, loss_mps: 0.023892, loss_cps: 0.041074
[12:27:56.277] iteration 2143: total_loss: 0.340286, loss_sup: 0.280113, loss_mps: 0.023067, loss_cps: 0.037106
[12:27:56.426] iteration 2144: total_loss: 0.375358, loss_sup: 0.301421, loss_mps: 0.025936, loss_cps: 0.048001
[12:27:56.571] iteration 2145: total_loss: 0.376638, loss_sup: 0.323172, loss_mps: 0.020826, loss_cps: 0.032640
[12:27:56.718] iteration 2146: total_loss: 0.370067, loss_sup: 0.304206, loss_mps: 0.024184, loss_cps: 0.041677
[12:27:56.866] iteration 2147: total_loss: 0.188014, loss_sup: 0.120606, loss_mps: 0.024693, loss_cps: 0.042715
[12:27:57.014] iteration 2148: total_loss: 0.457935, loss_sup: 0.406446, loss_mps: 0.019923, loss_cps: 0.031565
[12:27:57.162] iteration 2149: total_loss: 0.290220, loss_sup: 0.228514, loss_mps: 0.023683, loss_cps: 0.038022
[12:27:57.308] iteration 2150: total_loss: 0.318429, loss_sup: 0.264777, loss_mps: 0.021164, loss_cps: 0.032489
[12:27:57.457] iteration 2151: total_loss: 0.294755, loss_sup: 0.235782, loss_mps: 0.022369, loss_cps: 0.036603
[12:27:57.603] iteration 2152: total_loss: 0.215938, loss_sup: 0.162841, loss_mps: 0.020504, loss_cps: 0.032592
[12:27:57.748] iteration 2153: total_loss: 0.326840, loss_sup: 0.271323, loss_mps: 0.020768, loss_cps: 0.034749
[12:27:57.894] iteration 2154: total_loss: 0.311648, loss_sup: 0.252736, loss_mps: 0.021836, loss_cps: 0.037076
[12:27:58.040] iteration 2155: total_loss: 0.578245, loss_sup: 0.528742, loss_mps: 0.019631, loss_cps: 0.029872
[12:27:58.190] iteration 2156: total_loss: 0.365732, loss_sup: 0.307009, loss_mps: 0.022719, loss_cps: 0.036004
[12:27:58.338] iteration 2157: total_loss: 0.272723, loss_sup: 0.213082, loss_mps: 0.022366, loss_cps: 0.037275
[12:27:58.483] iteration 2158: total_loss: 0.377380, loss_sup: 0.299591, loss_mps: 0.027014, loss_cps: 0.050776
[12:27:58.629] iteration 2159: total_loss: 0.319100, loss_sup: 0.263503, loss_mps: 0.021387, loss_cps: 0.034209
[12:27:58.776] iteration 2160: total_loss: 0.306315, loss_sup: 0.250243, loss_mps: 0.021736, loss_cps: 0.034336
[12:27:58.921] iteration 2161: total_loss: 0.431351, loss_sup: 0.364681, loss_mps: 0.024845, loss_cps: 0.041826
[12:27:59.067] iteration 2162: total_loss: 0.311218, loss_sup: 0.260591, loss_mps: 0.020037, loss_cps: 0.030590
[12:27:59.213] iteration 2163: total_loss: 0.300443, loss_sup: 0.255202, loss_mps: 0.018428, loss_cps: 0.026813
[12:27:59.360] iteration 2164: total_loss: 0.266347, loss_sup: 0.210938, loss_mps: 0.021461, loss_cps: 0.033948
[12:27:59.506] iteration 2165: total_loss: 0.306500, loss_sup: 0.248517, loss_mps: 0.022225, loss_cps: 0.035757
[12:27:59.652] iteration 2166: total_loss: 0.153254, loss_sup: 0.073481, loss_mps: 0.028038, loss_cps: 0.051734
[12:27:59.798] iteration 2167: total_loss: 0.438935, loss_sup: 0.378673, loss_mps: 0.023376, loss_cps: 0.036885
[12:27:59.944] iteration 2168: total_loss: 0.613612, loss_sup: 0.556508, loss_mps: 0.021013, loss_cps: 0.036091
[12:28:00.090] iteration 2169: total_loss: 0.381736, loss_sup: 0.316375, loss_mps: 0.023717, loss_cps: 0.041644
[12:28:00.236] iteration 2170: total_loss: 0.270894, loss_sup: 0.214396, loss_mps: 0.021819, loss_cps: 0.034679
[12:28:00.382] iteration 2171: total_loss: 0.483789, loss_sup: 0.424824, loss_mps: 0.022501, loss_cps: 0.036464
[12:28:00.528] iteration 2172: total_loss: 0.357610, loss_sup: 0.279539, loss_mps: 0.028733, loss_cps: 0.049339
[12:28:00.677] iteration 2173: total_loss: 0.254983, loss_sup: 0.185636, loss_mps: 0.025593, loss_cps: 0.043754
[12:28:00.824] iteration 2174: total_loss: 0.349680, loss_sup: 0.291572, loss_mps: 0.022637, loss_cps: 0.035471
[12:28:00.970] iteration 2175: total_loss: 0.375920, loss_sup: 0.306376, loss_mps: 0.025708, loss_cps: 0.043836
[12:28:01.116] iteration 2176: total_loss: 0.277364, loss_sup: 0.219689, loss_mps: 0.022718, loss_cps: 0.034957
[12:28:01.263] iteration 2177: total_loss: 0.332748, loss_sup: 0.263634, loss_mps: 0.025934, loss_cps: 0.043180
[12:28:01.409] iteration 2178: total_loss: 0.319398, loss_sup: 0.246882, loss_mps: 0.026258, loss_cps: 0.046258
[12:28:01.555] iteration 2179: total_loss: 0.287838, loss_sup: 0.222779, loss_mps: 0.024440, loss_cps: 0.040619
[12:28:01.701] iteration 2180: total_loss: 0.347819, loss_sup: 0.280394, loss_mps: 0.024826, loss_cps: 0.042599
[12:28:01.847] iteration 2181: total_loss: 0.353988, loss_sup: 0.287850, loss_mps: 0.024294, loss_cps: 0.041844
[12:28:01.993] iteration 2182: total_loss: 0.549810, loss_sup: 0.468251, loss_mps: 0.028329, loss_cps: 0.053230
[12:28:02.139] iteration 2183: total_loss: 0.358144, loss_sup: 0.291545, loss_mps: 0.024612, loss_cps: 0.041986
[12:28:02.288] iteration 2184: total_loss: 0.363778, loss_sup: 0.296430, loss_mps: 0.024445, loss_cps: 0.042902
[12:28:02.436] iteration 2185: total_loss: 0.420952, loss_sup: 0.368245, loss_mps: 0.020784, loss_cps: 0.031923
[12:28:02.583] iteration 2186: total_loss: 0.505195, loss_sup: 0.441445, loss_mps: 0.023880, loss_cps: 0.039869
[12:28:02.729] iteration 2187: total_loss: 0.349653, loss_sup: 0.295791, loss_mps: 0.020429, loss_cps: 0.033433
[12:28:02.875] iteration 2188: total_loss: 0.164767, loss_sup: 0.119913, loss_mps: 0.018161, loss_cps: 0.026693
[12:28:03.021] iteration 2189: total_loss: 0.294595, loss_sup: 0.238855, loss_mps: 0.021712, loss_cps: 0.034027
[12:28:03.170] iteration 2190: total_loss: 0.381132, loss_sup: 0.317661, loss_mps: 0.022926, loss_cps: 0.040545
[12:28:03.317] iteration 2191: total_loss: 0.220045, loss_sup: 0.169580, loss_mps: 0.019781, loss_cps: 0.030684
[12:28:03.464] iteration 2192: total_loss: 0.400209, loss_sup: 0.346413, loss_mps: 0.020279, loss_cps: 0.033516
[12:28:03.610] iteration 2193: total_loss: 0.171215, loss_sup: 0.116931, loss_mps: 0.020767, loss_cps: 0.033517
[12:28:03.756] iteration 2194: total_loss: 0.287372, loss_sup: 0.236881, loss_mps: 0.019647, loss_cps: 0.030845
[12:28:03.902] iteration 2195: total_loss: 0.288835, loss_sup: 0.230591, loss_mps: 0.021561, loss_cps: 0.036684
[12:28:04.048] iteration 2196: total_loss: 0.301306, loss_sup: 0.250181, loss_mps: 0.020061, loss_cps: 0.031064
[12:28:04.194] iteration 2197: total_loss: 0.245392, loss_sup: 0.174114, loss_mps: 0.025267, loss_cps: 0.046012
[12:28:04.341] iteration 2198: total_loss: 0.365645, loss_sup: 0.298856, loss_mps: 0.024286, loss_cps: 0.042503
[12:28:04.486] iteration 2199: total_loss: 0.781249, loss_sup: 0.693321, loss_mps: 0.030397, loss_cps: 0.057530
[12:28:04.635] iteration 2200: total_loss: 0.398910, loss_sup: 0.338078, loss_mps: 0.022634, loss_cps: 0.038198
[12:28:04.635] Evaluation Started ==>
[12:28:16.018] ==> valid iteration 2200: unet metrics: {'dc': 0.4419332355805017, 'jc': 0.3286575134630962, 'pre': 0.5573021215798656, 'hd': 7.017595994179566}, ynet metrics: {'dc': 0.4375560867991241, 'jc': 0.3258317799356161, 'pre': 0.5363029813276126, 'hd': 7.389985389879007}.
[12:28:16.020] Evaluation Finished!⏹️
[12:28:16.174] iteration 2201: total_loss: 0.283935, loss_sup: 0.214437, loss_mps: 0.025208, loss_cps: 0.044291
[12:28:16.324] iteration 2202: total_loss: 0.573240, loss_sup: 0.495294, loss_mps: 0.027410, loss_cps: 0.050536
[12:28:16.469] iteration 2203: total_loss: 0.229392, loss_sup: 0.171615, loss_mps: 0.021643, loss_cps: 0.036134
[12:28:16.616] iteration 2204: total_loss: 0.249859, loss_sup: 0.191063, loss_mps: 0.022382, loss_cps: 0.036415
[12:28:16.761] iteration 2205: total_loss: 0.266907, loss_sup: 0.202077, loss_mps: 0.024099, loss_cps: 0.040732
[12:28:16.906] iteration 2206: total_loss: 0.431659, loss_sup: 0.351687, loss_mps: 0.028499, loss_cps: 0.051473
[12:28:17.051] iteration 2207: total_loss: 0.577748, loss_sup: 0.513965, loss_mps: 0.024212, loss_cps: 0.039571
[12:28:17.197] iteration 2208: total_loss: 0.437337, loss_sup: 0.376109, loss_mps: 0.023845, loss_cps: 0.037383
[12:28:17.346] iteration 2209: total_loss: 0.290910, loss_sup: 0.234462, loss_mps: 0.023045, loss_cps: 0.033404
[12:28:17.493] iteration 2210: total_loss: 0.372460, loss_sup: 0.309062, loss_mps: 0.024491, loss_cps: 0.038908
[12:28:17.639] iteration 2211: total_loss: 0.284663, loss_sup: 0.227547, loss_mps: 0.021588, loss_cps: 0.035527
[12:28:17.784] iteration 2212: total_loss: 0.762667, loss_sup: 0.704616, loss_mps: 0.021652, loss_cps: 0.036399
[12:28:17.929] iteration 2213: total_loss: 0.335116, loss_sup: 0.277533, loss_mps: 0.023028, loss_cps: 0.034554
[12:28:18.077] iteration 2214: total_loss: 0.442369, loss_sup: 0.368996, loss_mps: 0.026862, loss_cps: 0.046511
[12:28:18.223] iteration 2215: total_loss: 0.365652, loss_sup: 0.298473, loss_mps: 0.025655, loss_cps: 0.041524
[12:28:18.368] iteration 2216: total_loss: 0.378396, loss_sup: 0.322127, loss_mps: 0.021821, loss_cps: 0.034448
[12:28:18.513] iteration 2217: total_loss: 0.302120, loss_sup: 0.234434, loss_mps: 0.025364, loss_cps: 0.042321
[12:28:18.659] iteration 2218: total_loss: 0.331309, loss_sup: 0.262146, loss_mps: 0.024945, loss_cps: 0.044217
[12:28:18.805] iteration 2219: total_loss: 0.372194, loss_sup: 0.298003, loss_mps: 0.027224, loss_cps: 0.046967
[12:28:18.954] iteration 2220: total_loss: 0.235302, loss_sup: 0.170154, loss_mps: 0.024135, loss_cps: 0.041013
[12:28:19.100] iteration 2221: total_loss: 0.579084, loss_sup: 0.513611, loss_mps: 0.025044, loss_cps: 0.040430
[12:28:19.246] iteration 2222: total_loss: 0.356617, loss_sup: 0.292709, loss_mps: 0.023709, loss_cps: 0.040199
[12:28:19.391] iteration 2223: total_loss: 0.536604, loss_sup: 0.464814, loss_mps: 0.026843, loss_cps: 0.044946
[12:28:19.537] iteration 2224: total_loss: 0.339564, loss_sup: 0.271983, loss_mps: 0.025040, loss_cps: 0.042541
[12:28:19.683] iteration 2225: total_loss: 0.280397, loss_sup: 0.209564, loss_mps: 0.025744, loss_cps: 0.045089
[12:28:19.829] iteration 2226: total_loss: 0.194134, loss_sup: 0.138818, loss_mps: 0.021343, loss_cps: 0.033974
[12:28:19.975] iteration 2227: total_loss: 0.303434, loss_sup: 0.224297, loss_mps: 0.028263, loss_cps: 0.050874
[12:28:20.120] iteration 2228: total_loss: 0.497831, loss_sup: 0.419958, loss_mps: 0.029209, loss_cps: 0.048664
[12:28:20.265] iteration 2229: total_loss: 0.449144, loss_sup: 0.400409, loss_mps: 0.019560, loss_cps: 0.029176
[12:28:20.412] iteration 2230: total_loss: 0.333375, loss_sup: 0.260241, loss_mps: 0.027367, loss_cps: 0.045767
[12:28:20.558] iteration 2231: total_loss: 0.401255, loss_sup: 0.336172, loss_mps: 0.024872, loss_cps: 0.040211
[12:28:20.703] iteration 2232: total_loss: 0.420466, loss_sup: 0.365038, loss_mps: 0.021615, loss_cps: 0.033813
[12:28:20.849] iteration 2233: total_loss: 0.354298, loss_sup: 0.292028, loss_mps: 0.023868, loss_cps: 0.038403
[12:28:20.994] iteration 2234: total_loss: 0.245760, loss_sup: 0.183220, loss_mps: 0.023878, loss_cps: 0.038663
[12:28:21.139] iteration 2235: total_loss: 0.267534, loss_sup: 0.207117, loss_mps: 0.023649, loss_cps: 0.036769
[12:28:21.284] iteration 2236: total_loss: 0.541063, loss_sup: 0.462523, loss_mps: 0.028745, loss_cps: 0.049795
[12:28:21.433] iteration 2237: total_loss: 0.361346, loss_sup: 0.288892, loss_mps: 0.027231, loss_cps: 0.045224
[12:28:21.578] iteration 2238: total_loss: 0.234493, loss_sup: 0.184261, loss_mps: 0.020722, loss_cps: 0.029510
[12:28:21.723] iteration 2239: total_loss: 0.256685, loss_sup: 0.196079, loss_mps: 0.022367, loss_cps: 0.038239
[12:28:21.869] iteration 2240: total_loss: 0.360175, loss_sup: 0.285721, loss_mps: 0.026953, loss_cps: 0.047501
[12:28:22.014] iteration 2241: total_loss: 0.274679, loss_sup: 0.220265, loss_mps: 0.021890, loss_cps: 0.032524
[12:28:22.159] iteration 2242: total_loss: 0.300858, loss_sup: 0.244696, loss_mps: 0.022120, loss_cps: 0.034041
[12:28:22.306] iteration 2243: total_loss: 0.259473, loss_sup: 0.192690, loss_mps: 0.024898, loss_cps: 0.041885
[12:28:22.451] iteration 2244: total_loss: 0.207660, loss_sup: 0.138392, loss_mps: 0.026113, loss_cps: 0.043156
[12:28:22.598] iteration 2245: total_loss: 0.340150, loss_sup: 0.268744, loss_mps: 0.026191, loss_cps: 0.045215
[12:28:22.743] iteration 2246: total_loss: 0.467330, loss_sup: 0.378534, loss_mps: 0.030806, loss_cps: 0.057991
[12:28:22.893] iteration 2247: total_loss: 0.287639, loss_sup: 0.219237, loss_mps: 0.024312, loss_cps: 0.044090
[12:28:23.038] iteration 2248: total_loss: 0.418504, loss_sup: 0.331536, loss_mps: 0.029300, loss_cps: 0.057668
[12:28:23.184] iteration 2249: total_loss: 0.707999, loss_sup: 0.608132, loss_mps: 0.032784, loss_cps: 0.067083
[12:28:23.329] iteration 2250: total_loss: 0.439717, loss_sup: 0.349063, loss_mps: 0.031423, loss_cps: 0.059232
[12:28:23.475] iteration 2251: total_loss: 0.254400, loss_sup: 0.164045, loss_mps: 0.030767, loss_cps: 0.059589
[12:28:23.620] iteration 2252: total_loss: 0.811589, loss_sup: 0.712820, loss_mps: 0.031944, loss_cps: 0.066826
[12:28:23.766] iteration 2253: total_loss: 0.338697, loss_sup: 0.260793, loss_mps: 0.027160, loss_cps: 0.050744
[12:28:23.912] iteration 2254: total_loss: 0.354941, loss_sup: 0.285607, loss_mps: 0.025106, loss_cps: 0.044228
[12:28:24.058] iteration 2255: total_loss: 0.279161, loss_sup: 0.205126, loss_mps: 0.026836, loss_cps: 0.047199
[12:28:24.204] iteration 2256: total_loss: 0.260978, loss_sup: 0.185164, loss_mps: 0.027938, loss_cps: 0.047877
[12:28:24.350] iteration 2257: total_loss: 0.321552, loss_sup: 0.236389, loss_mps: 0.030500, loss_cps: 0.054663
[12:28:24.495] iteration 2258: total_loss: 0.319378, loss_sup: 0.238937, loss_mps: 0.029459, loss_cps: 0.050982
[12:28:24.641] iteration 2259: total_loss: 0.325683, loss_sup: 0.246367, loss_mps: 0.029295, loss_cps: 0.050021
[12:28:24.787] iteration 2260: total_loss: 0.480988, loss_sup: 0.408639, loss_mps: 0.027837, loss_cps: 0.044512
[12:28:24.933] iteration 2261: total_loss: 0.158308, loss_sup: 0.091848, loss_mps: 0.025489, loss_cps: 0.040970
[12:28:25.080] iteration 2262: total_loss: 0.429905, loss_sup: 0.355191, loss_mps: 0.028121, loss_cps: 0.046594
[12:28:25.227] iteration 2263: total_loss: 0.575107, loss_sup: 0.496414, loss_mps: 0.028569, loss_cps: 0.050123
[12:28:25.375] iteration 2264: total_loss: 0.348939, loss_sup: 0.277380, loss_mps: 0.027078, loss_cps: 0.044481
[12:28:25.521] iteration 2265: total_loss: 0.560016, loss_sup: 0.490886, loss_mps: 0.026354, loss_cps: 0.042776
[12:28:25.667] iteration 2266: total_loss: 0.458886, loss_sup: 0.385876, loss_mps: 0.027236, loss_cps: 0.045774
[12:28:25.813] iteration 2267: total_loss: 0.308549, loss_sup: 0.249446, loss_mps: 0.023825, loss_cps: 0.035278
[12:28:25.959] iteration 2268: total_loss: 0.478816, loss_sup: 0.412462, loss_mps: 0.025184, loss_cps: 0.041169
[12:28:26.106] iteration 2269: total_loss: 0.283992, loss_sup: 0.228835, loss_mps: 0.022298, loss_cps: 0.032859
[12:28:26.252] iteration 2270: total_loss: 0.292065, loss_sup: 0.241755, loss_mps: 0.020392, loss_cps: 0.029918
[12:28:26.398] iteration 2271: total_loss: 0.258258, loss_sup: 0.197272, loss_mps: 0.023936, loss_cps: 0.037050
[12:28:26.544] iteration 2272: total_loss: 0.431953, loss_sup: 0.372457, loss_mps: 0.022885, loss_cps: 0.036612
[12:28:26.689] iteration 2273: total_loss: 0.373044, loss_sup: 0.300546, loss_mps: 0.026993, loss_cps: 0.045505
[12:28:26.835] iteration 2274: total_loss: 0.230287, loss_sup: 0.166753, loss_mps: 0.024032, loss_cps: 0.039503
[12:28:26.981] iteration 2275: total_loss: 0.360687, loss_sup: 0.281301, loss_mps: 0.027674, loss_cps: 0.051713
[12:28:27.131] iteration 2276: total_loss: 0.305270, loss_sup: 0.240463, loss_mps: 0.024442, loss_cps: 0.040365
[12:28:27.277] iteration 2277: total_loss: 0.324795, loss_sup: 0.248249, loss_mps: 0.026922, loss_cps: 0.049624
[12:28:27.423] iteration 2278: total_loss: 0.406772, loss_sup: 0.324086, loss_mps: 0.029322, loss_cps: 0.053365
[12:28:27.569] iteration 2279: total_loss: 0.377157, loss_sup: 0.301197, loss_mps: 0.026979, loss_cps: 0.048982
[12:28:27.714] iteration 2280: total_loss: 0.212152, loss_sup: 0.154665, loss_mps: 0.021997, loss_cps: 0.035491
[12:28:27.860] iteration 2281: total_loss: 0.247134, loss_sup: 0.188146, loss_mps: 0.022214, loss_cps: 0.036774
[12:28:28.006] iteration 2282: total_loss: 0.187904, loss_sup: 0.131935, loss_mps: 0.020946, loss_cps: 0.035023
[12:28:28.154] iteration 2283: total_loss: 0.424345, loss_sup: 0.362216, loss_mps: 0.022940, loss_cps: 0.039188
[12:28:28.303] iteration 2284: total_loss: 0.441480, loss_sup: 0.361231, loss_mps: 0.027209, loss_cps: 0.053040
[12:28:28.449] iteration 2285: total_loss: 0.611136, loss_sup: 0.541745, loss_mps: 0.024593, loss_cps: 0.044798
[12:28:28.597] iteration 2286: total_loss: 0.510205, loss_sup: 0.421537, loss_mps: 0.030232, loss_cps: 0.058435
[12:28:28.743] iteration 2287: total_loss: 0.329320, loss_sup: 0.270151, loss_mps: 0.021597, loss_cps: 0.037572
[12:28:28.892] iteration 2288: total_loss: 0.273422, loss_sup: 0.205089, loss_mps: 0.023820, loss_cps: 0.044514
[12:28:29.041] iteration 2289: total_loss: 0.516513, loss_sup: 0.442368, loss_mps: 0.026107, loss_cps: 0.048037
[12:28:29.187] iteration 2290: total_loss: 0.620628, loss_sup: 0.554956, loss_mps: 0.024999, loss_cps: 0.040673
[12:28:29.333] iteration 2291: total_loss: 0.352980, loss_sup: 0.276005, loss_mps: 0.027091, loss_cps: 0.049884
[12:28:29.479] iteration 2292: total_loss: 0.243655, loss_sup: 0.171266, loss_mps: 0.026331, loss_cps: 0.046058
[12:28:29.625] iteration 2293: total_loss: 0.321931, loss_sup: 0.251666, loss_mps: 0.025813, loss_cps: 0.044452
[12:28:29.771] iteration 2294: total_loss: 0.278655, loss_sup: 0.214131, loss_mps: 0.023865, loss_cps: 0.040659
[12:28:29.918] iteration 2295: total_loss: 0.498949, loss_sup: 0.434037, loss_mps: 0.024524, loss_cps: 0.040388
[12:28:30.067] iteration 2296: total_loss: 0.302843, loss_sup: 0.246222, loss_mps: 0.022588, loss_cps: 0.034034
[12:28:30.213] iteration 2297: total_loss: 0.202120, loss_sup: 0.143534, loss_mps: 0.023392, loss_cps: 0.035194
[12:28:30.358] iteration 2298: total_loss: 0.377913, loss_sup: 0.313241, loss_mps: 0.025171, loss_cps: 0.039501
[12:28:30.504] iteration 2299: total_loss: 0.260716, loss_sup: 0.190005, loss_mps: 0.027114, loss_cps: 0.043597
[12:28:30.653] iteration 2300: total_loss: 0.627205, loss_sup: 0.562636, loss_mps: 0.024655, loss_cps: 0.039915
[12:28:30.653] Evaluation Started ==>
[12:28:41.989] ==> valid iteration 2300: unet metrics: {'dc': 0.45768971188837154, 'jc': 0.3400674658348923, 'pre': 0.5116229163790488, 'hd': 7.485067029003419}, ynet metrics: {'dc': 0.4199776002323951, 'jc': 0.31147237828628693, 'pre': 0.4751595899714866, 'hd': 7.5073434472153195}.
[12:28:41.990] Evaluation Finished!⏹️
[12:28:42.143] iteration 2301: total_loss: 0.295421, loss_sup: 0.238605, loss_mps: 0.022581, loss_cps: 0.034235
[12:28:42.292] iteration 2302: total_loss: 0.428418, loss_sup: 0.356862, loss_mps: 0.026550, loss_cps: 0.045006
[12:28:42.442] iteration 2303: total_loss: 0.192727, loss_sup: 0.139484, loss_mps: 0.021426, loss_cps: 0.031817
[12:28:42.587] iteration 2304: total_loss: 0.403417, loss_sup: 0.345035, loss_mps: 0.023211, loss_cps: 0.035170
[12:28:42.731] iteration 2305: total_loss: 0.439279, loss_sup: 0.356772, loss_mps: 0.029760, loss_cps: 0.052746
[12:28:42.878] iteration 2306: total_loss: 0.369550, loss_sup: 0.298456, loss_mps: 0.026379, loss_cps: 0.044715
[12:28:43.024] iteration 2307: total_loss: 0.330736, loss_sup: 0.272329, loss_mps: 0.022602, loss_cps: 0.035805
[12:28:43.173] iteration 2308: total_loss: 0.293004, loss_sup: 0.231778, loss_mps: 0.023568, loss_cps: 0.037658
[12:28:43.319] iteration 2309: total_loss: 0.280694, loss_sup: 0.224141, loss_mps: 0.022532, loss_cps: 0.034021
[12:28:43.465] iteration 2310: total_loss: 0.384909, loss_sup: 0.322284, loss_mps: 0.023829, loss_cps: 0.038796
[12:28:43.611] iteration 2311: total_loss: 0.415120, loss_sup: 0.345383, loss_mps: 0.025705, loss_cps: 0.044033
[12:28:43.757] iteration 2312: total_loss: 0.293698, loss_sup: 0.219562, loss_mps: 0.027383, loss_cps: 0.046752
[12:28:43.902] iteration 2313: total_loss: 0.464925, loss_sup: 0.397006, loss_mps: 0.025144, loss_cps: 0.042775
[12:28:44.050] iteration 2314: total_loss: 0.539398, loss_sup: 0.455065, loss_mps: 0.030361, loss_cps: 0.053971
[12:28:44.198] iteration 2315: total_loss: 0.253060, loss_sup: 0.174890, loss_mps: 0.028802, loss_cps: 0.049368
[12:28:44.343] iteration 2316: total_loss: 0.180804, loss_sup: 0.120360, loss_mps: 0.022846, loss_cps: 0.037597
[12:28:44.491] iteration 2317: total_loss: 0.194864, loss_sup: 0.129442, loss_mps: 0.024276, loss_cps: 0.041146
[12:28:44.638] iteration 2318: total_loss: 0.719154, loss_sup: 0.630282, loss_mps: 0.031203, loss_cps: 0.057669
[12:28:44.787] iteration 2319: total_loss: 0.364108, loss_sup: 0.299789, loss_mps: 0.024248, loss_cps: 0.040070
[12:28:44.933] iteration 2320: total_loss: 0.538843, loss_sup: 0.467463, loss_mps: 0.026357, loss_cps: 0.045023
[12:28:45.081] iteration 2321: total_loss: 0.205110, loss_sup: 0.133306, loss_mps: 0.026083, loss_cps: 0.045721
[12:28:45.227] iteration 2322: total_loss: 0.720792, loss_sup: 0.633086, loss_mps: 0.030737, loss_cps: 0.056969
[12:28:45.376] iteration 2323: total_loss: 0.296607, loss_sup: 0.238580, loss_mps: 0.022538, loss_cps: 0.035489
[12:28:45.524] iteration 2324: total_loss: 0.215615, loss_sup: 0.138763, loss_mps: 0.028182, loss_cps: 0.048670
[12:28:45.669] iteration 2325: total_loss: 0.273995, loss_sup: 0.217554, loss_mps: 0.021788, loss_cps: 0.034652
[12:28:45.815] iteration 2326: total_loss: 0.301548, loss_sup: 0.234410, loss_mps: 0.024801, loss_cps: 0.042337
[12:28:45.960] iteration 2327: total_loss: 0.219401, loss_sup: 0.164049, loss_mps: 0.021592, loss_cps: 0.033760
[12:28:46.105] iteration 2328: total_loss: 0.386523, loss_sup: 0.321379, loss_mps: 0.024587, loss_cps: 0.040557
[12:28:46.250] iteration 2329: total_loss: 0.296581, loss_sup: 0.232338, loss_mps: 0.024556, loss_cps: 0.039687
[12:28:46.395] iteration 2330: total_loss: 0.421860, loss_sup: 0.352077, loss_mps: 0.025795, loss_cps: 0.043988
[12:28:46.541] iteration 2331: total_loss: 0.290425, loss_sup: 0.222522, loss_mps: 0.025162, loss_cps: 0.042741
[12:28:46.687] iteration 2332: total_loss: 0.283224, loss_sup: 0.224098, loss_mps: 0.023569, loss_cps: 0.035557
[12:28:46.836] iteration 2333: total_loss: 0.401106, loss_sup: 0.334882, loss_mps: 0.024601, loss_cps: 0.041623
[12:28:46.982] iteration 2334: total_loss: 0.348480, loss_sup: 0.273348, loss_mps: 0.027017, loss_cps: 0.048116
[12:28:47.129] iteration 2335: total_loss: 0.384578, loss_sup: 0.324009, loss_mps: 0.022843, loss_cps: 0.037726
[12:28:47.275] iteration 2336: total_loss: 0.572499, loss_sup: 0.495910, loss_mps: 0.026667, loss_cps: 0.049922
[12:28:47.421] iteration 2337: total_loss: 0.232853, loss_sup: 0.172449, loss_mps: 0.022767, loss_cps: 0.037638
[12:28:47.566] iteration 2338: total_loss: 0.576308, loss_sup: 0.514101, loss_mps: 0.022758, loss_cps: 0.039448
[12:28:47.716] iteration 2339: total_loss: 0.572940, loss_sup: 0.510888, loss_mps: 0.023875, loss_cps: 0.038177
[12:28:47.863] iteration 2340: total_loss: 0.295188, loss_sup: 0.227258, loss_mps: 0.025469, loss_cps: 0.042461
[12:28:48.010] iteration 2341: total_loss: 0.273752, loss_sup: 0.216831, loss_mps: 0.022220, loss_cps: 0.034700
[12:28:48.155] iteration 2342: total_loss: 0.311867, loss_sup: 0.245839, loss_mps: 0.024563, loss_cps: 0.041465
[12:28:48.300] iteration 2343: total_loss: 0.221046, loss_sup: 0.157518, loss_mps: 0.023584, loss_cps: 0.039944
[12:28:48.446] iteration 2344: total_loss: 0.406894, loss_sup: 0.337731, loss_mps: 0.026392, loss_cps: 0.042771
[12:28:48.593] iteration 2345: total_loss: 0.291105, loss_sup: 0.216299, loss_mps: 0.026973, loss_cps: 0.047832
[12:28:48.739] iteration 2346: total_loss: 0.581214, loss_sup: 0.504560, loss_mps: 0.028181, loss_cps: 0.048473
[12:28:48.884] iteration 2347: total_loss: 0.263778, loss_sup: 0.209635, loss_mps: 0.020674, loss_cps: 0.033469
[12:28:49.034] iteration 2348: total_loss: 0.287039, loss_sup: 0.195112, loss_mps: 0.031397, loss_cps: 0.060530
[12:28:49.180] iteration 2349: total_loss: 0.351031, loss_sup: 0.278262, loss_mps: 0.026277, loss_cps: 0.046492
[12:28:49.326] iteration 2350: total_loss: 0.255633, loss_sup: 0.197142, loss_mps: 0.021136, loss_cps: 0.037354
[12:28:49.471] iteration 2351: total_loss: 0.302645, loss_sup: 0.242433, loss_mps: 0.022819, loss_cps: 0.037393
[12:28:49.617] iteration 2352: total_loss: 0.153003, loss_sup: 0.095749, loss_mps: 0.021948, loss_cps: 0.035306
[12:28:49.763] iteration 2353: total_loss: 0.420311, loss_sup: 0.342400, loss_mps: 0.027521, loss_cps: 0.050389
[12:28:49.910] iteration 2354: total_loss: 0.293442, loss_sup: 0.198916, loss_mps: 0.031540, loss_cps: 0.062987
[12:28:50.055] iteration 2355: total_loss: 0.302580, loss_sup: 0.236085, loss_mps: 0.023802, loss_cps: 0.042692
[12:28:50.202] iteration 2356: total_loss: 0.262106, loss_sup: 0.196042, loss_mps: 0.023815, loss_cps: 0.042249
[12:28:50.349] iteration 2357: total_loss: 0.379446, loss_sup: 0.313315, loss_mps: 0.023661, loss_cps: 0.042470
[12:28:50.497] iteration 2358: total_loss: 0.331829, loss_sup: 0.254617, loss_mps: 0.026900, loss_cps: 0.050312
[12:28:50.642] iteration 2359: total_loss: 0.247749, loss_sup: 0.179523, loss_mps: 0.024683, loss_cps: 0.043543
[12:28:50.788] iteration 2360: total_loss: 0.579157, loss_sup: 0.511850, loss_mps: 0.024788, loss_cps: 0.042519
[12:28:50.934] iteration 2361: total_loss: 0.304668, loss_sup: 0.243393, loss_mps: 0.021804, loss_cps: 0.039471
[12:28:51.080] iteration 2362: total_loss: 0.522143, loss_sup: 0.453860, loss_mps: 0.024215, loss_cps: 0.044068
[12:28:51.225] iteration 2363: total_loss: 0.429448, loss_sup: 0.361325, loss_mps: 0.024881, loss_cps: 0.043243
[12:28:51.377] iteration 2364: total_loss: 0.255429, loss_sup: 0.172366, loss_mps: 0.028929, loss_cps: 0.054134
[12:28:51.523] iteration 2365: total_loss: 0.287748, loss_sup: 0.208406, loss_mps: 0.028317, loss_cps: 0.051025
[12:28:51.669] iteration 2366: total_loss: 0.486557, loss_sup: 0.404956, loss_mps: 0.028923, loss_cps: 0.052678
[12:28:51.816] iteration 2367: total_loss: 0.348735, loss_sup: 0.260589, loss_mps: 0.030381, loss_cps: 0.057764
[12:28:51.965] iteration 2368: total_loss: 0.235345, loss_sup: 0.163052, loss_mps: 0.025827, loss_cps: 0.046466
[12:28:52.111] iteration 2369: total_loss: 0.346944, loss_sup: 0.245899, loss_mps: 0.034146, loss_cps: 0.066900
[12:28:52.257] iteration 2370: total_loss: 0.206997, loss_sup: 0.135705, loss_mps: 0.027094, loss_cps: 0.044198
[12:28:52.402] iteration 2371: total_loss: 0.378053, loss_sup: 0.306379, loss_mps: 0.027490, loss_cps: 0.044184
[12:28:52.548] iteration 2372: total_loss: 0.301070, loss_sup: 0.212433, loss_mps: 0.031014, loss_cps: 0.057623
[12:28:52.694] iteration 2373: total_loss: 0.294883, loss_sup: 0.228261, loss_mps: 0.025110, loss_cps: 0.041512
[12:28:52.840] iteration 2374: total_loss: 0.199148, loss_sup: 0.140703, loss_mps: 0.022834, loss_cps: 0.035611
[12:28:52.985] iteration 2375: total_loss: 0.383142, loss_sup: 0.317308, loss_mps: 0.024487, loss_cps: 0.041346
[12:28:53.130] iteration 2376: total_loss: 0.400318, loss_sup: 0.326072, loss_mps: 0.027304, loss_cps: 0.046942
[12:28:53.277] iteration 2377: total_loss: 0.386272, loss_sup: 0.311307, loss_mps: 0.026586, loss_cps: 0.048379
[12:28:53.423] iteration 2378: total_loss: 0.444260, loss_sup: 0.372672, loss_mps: 0.026132, loss_cps: 0.045456
[12:28:53.568] iteration 2379: total_loss: 0.266891, loss_sup: 0.190459, loss_mps: 0.026668, loss_cps: 0.049764
[12:28:53.716] iteration 2380: total_loss: 0.259637, loss_sup: 0.193788, loss_mps: 0.024983, loss_cps: 0.040866
[12:28:53.866] iteration 2381: total_loss: 0.686989, loss_sup: 0.620967, loss_mps: 0.025345, loss_cps: 0.040677
[12:28:54.012] iteration 2382: total_loss: 0.295850, loss_sup: 0.216650, loss_mps: 0.027553, loss_cps: 0.051646
[12:28:54.158] iteration 2383: total_loss: 0.363349, loss_sup: 0.274719, loss_mps: 0.030485, loss_cps: 0.058145
[12:28:54.304] iteration 2384: total_loss: 0.287051, loss_sup: 0.216648, loss_mps: 0.026563, loss_cps: 0.043840
[12:28:54.450] iteration 2385: total_loss: 0.403391, loss_sup: 0.335769, loss_mps: 0.025034, loss_cps: 0.042588
[12:28:54.598] iteration 2386: total_loss: 0.447392, loss_sup: 0.367307, loss_mps: 0.028674, loss_cps: 0.051411
[12:28:54.743] iteration 2387: total_loss: 0.212122, loss_sup: 0.157674, loss_mps: 0.021540, loss_cps: 0.032908
[12:28:54.889] iteration 2388: total_loss: 0.648315, loss_sup: 0.592241, loss_mps: 0.022227, loss_cps: 0.033848
[12:28:55.035] iteration 2389: total_loss: 0.211343, loss_sup: 0.149677, loss_mps: 0.023012, loss_cps: 0.038654
[12:28:55.181] iteration 2390: total_loss: 0.233928, loss_sup: 0.170603, loss_mps: 0.023535, loss_cps: 0.039790
[12:28:55.326] iteration 2391: total_loss: 0.283184, loss_sup: 0.223090, loss_mps: 0.023251, loss_cps: 0.036843
[12:28:55.472] iteration 2392: total_loss: 0.771841, loss_sup: 0.701349, loss_mps: 0.025892, loss_cps: 0.044600
[12:28:55.618] iteration 2393: total_loss: 0.204103, loss_sup: 0.157821, loss_mps: 0.018776, loss_cps: 0.027507
[12:28:55.764] iteration 2394: total_loss: 0.444874, loss_sup: 0.372726, loss_mps: 0.026130, loss_cps: 0.046018
[12:28:55.910] iteration 2395: total_loss: 0.250903, loss_sup: 0.193884, loss_mps: 0.022689, loss_cps: 0.034330
[12:28:56.056] iteration 2396: total_loss: 0.620492, loss_sup: 0.545245, loss_mps: 0.027246, loss_cps: 0.048001
[12:28:56.201] iteration 2397: total_loss: 0.200734, loss_sup: 0.140929, loss_mps: 0.022839, loss_cps: 0.036966
[12:28:56.346] iteration 2398: total_loss: 0.308217, loss_sup: 0.230563, loss_mps: 0.028220, loss_cps: 0.049433
[12:28:56.492] iteration 2399: total_loss: 0.382262, loss_sup: 0.328077, loss_mps: 0.021637, loss_cps: 0.032548
[12:28:56.638] iteration 2400: total_loss: 0.304479, loss_sup: 0.238330, loss_mps: 0.025477, loss_cps: 0.040673
[12:28:56.638] Evaluation Started ==>
[12:29:07.995] ==> valid iteration 2400: unet metrics: {'dc': 0.462375223629182, 'jc': 0.3537855059892807, 'pre': 0.5674096930850107, 'hd': 6.714815525280457}, ynet metrics: {'dc': 0.4216294148710779, 'jc': 0.3202130767396579, 'pre': 0.5250639848994696, 'hd': 7.09139673423214}.
[12:29:07.996] Evaluation Finished!⏹️
[12:29:08.147] iteration 2401: total_loss: 0.273787, loss_sup: 0.214504, loss_mps: 0.023424, loss_cps: 0.035859
[12:29:08.294] iteration 2402: total_loss: 0.262921, loss_sup: 0.196455, loss_mps: 0.025287, loss_cps: 0.041180
[12:29:08.440] iteration 2403: total_loss: 0.243018, loss_sup: 0.175965, loss_mps: 0.025073, loss_cps: 0.041979
[12:29:08.584] iteration 2404: total_loss: 0.312130, loss_sup: 0.234017, loss_mps: 0.027961, loss_cps: 0.050152
[12:29:08.730] iteration 2405: total_loss: 0.455456, loss_sup: 0.382246, loss_mps: 0.027324, loss_cps: 0.045886
[12:29:08.875] iteration 2406: total_loss: 0.371341, loss_sup: 0.312897, loss_mps: 0.023307, loss_cps: 0.035137
[12:29:09.020] iteration 2407: total_loss: 0.570624, loss_sup: 0.493676, loss_mps: 0.027682, loss_cps: 0.049266
[12:29:09.170] iteration 2408: total_loss: 0.339772, loss_sup: 0.264449, loss_mps: 0.027605, loss_cps: 0.047719
[12:29:09.315] iteration 2409: total_loss: 0.275177, loss_sup: 0.213274, loss_mps: 0.023962, loss_cps: 0.037941
[12:29:09.460] iteration 2410: total_loss: 0.279971, loss_sup: 0.226090, loss_mps: 0.021737, loss_cps: 0.032144
[12:29:09.606] iteration 2411: total_loss: 0.575514, loss_sup: 0.509860, loss_mps: 0.025205, loss_cps: 0.040449
[12:29:09.752] iteration 2412: total_loss: 0.315279, loss_sup: 0.250524, loss_mps: 0.024641, loss_cps: 0.040115
[12:29:09.897] iteration 2413: total_loss: 0.355463, loss_sup: 0.271843, loss_mps: 0.029388, loss_cps: 0.054232
[12:29:10.043] iteration 2414: total_loss: 0.349708, loss_sup: 0.287351, loss_mps: 0.023931, loss_cps: 0.038426
[12:29:10.189] iteration 2415: total_loss: 0.517291, loss_sup: 0.450667, loss_mps: 0.024919, loss_cps: 0.041705
[12:29:10.335] iteration 2416: total_loss: 0.413791, loss_sup: 0.346338, loss_mps: 0.025923, loss_cps: 0.041530
[12:29:10.481] iteration 2417: total_loss: 0.346718, loss_sup: 0.275883, loss_mps: 0.026199, loss_cps: 0.044636
[12:29:10.627] iteration 2418: total_loss: 0.465994, loss_sup: 0.410801, loss_mps: 0.021804, loss_cps: 0.033389
[12:29:10.773] iteration 2419: total_loss: 0.200810, loss_sup: 0.127843, loss_mps: 0.028003, loss_cps: 0.044964
[12:29:10.919] iteration 2420: total_loss: 0.185736, loss_sup: 0.123272, loss_mps: 0.024485, loss_cps: 0.037979
[12:29:11.064] iteration 2421: total_loss: 0.316558, loss_sup: 0.230069, loss_mps: 0.031366, loss_cps: 0.055124
[12:29:11.210] iteration 2422: total_loss: 0.353455, loss_sup: 0.286280, loss_mps: 0.027001, loss_cps: 0.040174
[12:29:11.357] iteration 2423: total_loss: 0.272557, loss_sup: 0.225387, loss_mps: 0.019758, loss_cps: 0.027411
[12:29:11.503] iteration 2424: total_loss: 0.536925, loss_sup: 0.460517, loss_mps: 0.028798, loss_cps: 0.047609
[12:29:11.647] iteration 2425: total_loss: 0.337815, loss_sup: 0.283978, loss_mps: 0.021957, loss_cps: 0.031880
[12:29:11.794] iteration 2426: total_loss: 0.214518, loss_sup: 0.149152, loss_mps: 0.024904, loss_cps: 0.040462
[12:29:11.939] iteration 2427: total_loss: 0.269112, loss_sup: 0.204402, loss_mps: 0.024402, loss_cps: 0.040309
[12:29:12.085] iteration 2428: total_loss: 0.515345, loss_sup: 0.456121, loss_mps: 0.023478, loss_cps: 0.035746
[12:29:12.231] iteration 2429: total_loss: 0.364972, loss_sup: 0.285012, loss_mps: 0.028904, loss_cps: 0.051056
[12:29:12.377] iteration 2430: total_loss: 0.271552, loss_sup: 0.194763, loss_mps: 0.027299, loss_cps: 0.049490
[12:29:12.523] iteration 2431: total_loss: 0.373377, loss_sup: 0.308299, loss_mps: 0.025542, loss_cps: 0.039536
[12:29:12.669] iteration 2432: total_loss: 0.240270, loss_sup: 0.188542, loss_mps: 0.020650, loss_cps: 0.031079
[12:29:12.814] iteration 2433: total_loss: 0.205252, loss_sup: 0.141836, loss_mps: 0.025229, loss_cps: 0.038187
[12:29:12.961] iteration 2434: total_loss: 0.377865, loss_sup: 0.322683, loss_mps: 0.020810, loss_cps: 0.034372
[12:29:13.107] iteration 2435: total_loss: 0.332464, loss_sup: 0.250116, loss_mps: 0.029048, loss_cps: 0.053301
[12:29:13.254] iteration 2436: total_loss: 0.635952, loss_sup: 0.551386, loss_mps: 0.030552, loss_cps: 0.054014
[12:29:13.400] iteration 2437: total_loss: 0.666750, loss_sup: 0.556715, loss_mps: 0.036305, loss_cps: 0.073731
[12:29:13.547] iteration 2438: total_loss: 0.258906, loss_sup: 0.176459, loss_mps: 0.029349, loss_cps: 0.053098
[12:29:13.693] iteration 2439: total_loss: 0.225294, loss_sup: 0.155221, loss_mps: 0.025759, loss_cps: 0.044313
[12:29:13.839] iteration 2440: total_loss: 0.356639, loss_sup: 0.264839, loss_mps: 0.031981, loss_cps: 0.059819
[12:29:13.984] iteration 2441: total_loss: 0.418252, loss_sup: 0.353380, loss_mps: 0.023672, loss_cps: 0.041200
[12:29:14.130] iteration 2442: total_loss: 0.394321, loss_sup: 0.327582, loss_mps: 0.024492, loss_cps: 0.042246
[12:29:14.277] iteration 2443: total_loss: 0.230374, loss_sup: 0.167402, loss_mps: 0.024435, loss_cps: 0.038537
[12:29:14.423] iteration 2444: total_loss: 0.519020, loss_sup: 0.452594, loss_mps: 0.025169, loss_cps: 0.041256
[12:29:14.569] iteration 2445: total_loss: 0.614825, loss_sup: 0.542499, loss_mps: 0.026865, loss_cps: 0.045461
[12:29:14.714] iteration 2446: total_loss: 0.214919, loss_sup: 0.153852, loss_mps: 0.023083, loss_cps: 0.037983
[12:29:14.860] iteration 2447: total_loss: 0.265069, loss_sup: 0.191681, loss_mps: 0.026122, loss_cps: 0.047266
[12:29:15.006] iteration 2448: total_loss: 0.424021, loss_sup: 0.345764, loss_mps: 0.028432, loss_cps: 0.049825
[12:29:15.152] iteration 2449: total_loss: 0.486868, loss_sup: 0.410116, loss_mps: 0.028420, loss_cps: 0.048333
[12:29:15.301] iteration 2450: total_loss: 0.322693, loss_sup: 0.238797, loss_mps: 0.030287, loss_cps: 0.053609
[12:29:15.447] iteration 2451: total_loss: 0.275363, loss_sup: 0.187224, loss_mps: 0.031288, loss_cps: 0.056851
[12:29:15.593] iteration 2452: total_loss: 0.450236, loss_sup: 0.377291, loss_mps: 0.027176, loss_cps: 0.045769
[12:29:15.739] iteration 2453: total_loss: 0.225968, loss_sup: 0.167346, loss_mps: 0.022776, loss_cps: 0.035846
[12:29:15.889] iteration 2454: total_loss: 0.292871, loss_sup: 0.225643, loss_mps: 0.026231, loss_cps: 0.040998
[12:29:16.035] iteration 2455: total_loss: 0.307789, loss_sup: 0.233376, loss_mps: 0.027584, loss_cps: 0.046830
[12:29:16.181] iteration 2456: total_loss: 0.254926, loss_sup: 0.182444, loss_mps: 0.027176, loss_cps: 0.045306
[12:29:16.328] iteration 2457: total_loss: 0.512216, loss_sup: 0.428232, loss_mps: 0.029995, loss_cps: 0.053989
[12:29:16.474] iteration 2458: total_loss: 0.294872, loss_sup: 0.230791, loss_mps: 0.024516, loss_cps: 0.039565
[12:29:16.620] iteration 2459: total_loss: 0.174035, loss_sup: 0.123850, loss_mps: 0.020101, loss_cps: 0.030085
[12:29:16.767] iteration 2460: total_loss: 0.382106, loss_sup: 0.308658, loss_mps: 0.026815, loss_cps: 0.046633
[12:29:16.913] iteration 2461: total_loss: 0.243005, loss_sup: 0.175714, loss_mps: 0.024739, loss_cps: 0.042552
[12:29:17.059] iteration 2462: total_loss: 0.363029, loss_sup: 0.292825, loss_mps: 0.025585, loss_cps: 0.044619
[12:29:17.205] iteration 2463: total_loss: 0.231050, loss_sup: 0.167591, loss_mps: 0.023412, loss_cps: 0.040046
[12:29:17.350] iteration 2464: total_loss: 0.132018, loss_sup: 0.070615, loss_mps: 0.022674, loss_cps: 0.038729
[12:29:17.496] iteration 2465: total_loss: 0.521556, loss_sup: 0.445090, loss_mps: 0.027048, loss_cps: 0.049418
[12:29:17.642] iteration 2466: total_loss: 0.638118, loss_sup: 0.575018, loss_mps: 0.023310, loss_cps: 0.039790
[12:29:17.789] iteration 2467: total_loss: 0.176627, loss_sup: 0.109339, loss_mps: 0.025276, loss_cps: 0.042013
[12:29:17.934] iteration 2468: total_loss: 0.380731, loss_sup: 0.321363, loss_mps: 0.021891, loss_cps: 0.037477
[12:29:18.081] iteration 2469: total_loss: 0.297073, loss_sup: 0.240453, loss_mps: 0.022357, loss_cps: 0.034263
[12:29:18.227] iteration 2470: total_loss: 0.311483, loss_sup: 0.252366, loss_mps: 0.021429, loss_cps: 0.037687
[12:29:18.372] iteration 2471: total_loss: 0.360985, loss_sup: 0.280543, loss_mps: 0.028284, loss_cps: 0.052158
[12:29:18.520] iteration 2472: total_loss: 0.207270, loss_sup: 0.147353, loss_mps: 0.022292, loss_cps: 0.037625
[12:29:18.666] iteration 2473: total_loss: 0.318040, loss_sup: 0.248177, loss_mps: 0.025021, loss_cps: 0.044842
[12:29:18.814] iteration 2474: total_loss: 0.354170, loss_sup: 0.299219, loss_mps: 0.021136, loss_cps: 0.033815
[12:29:18.960] iteration 2475: total_loss: 0.265392, loss_sup: 0.194478, loss_mps: 0.024903, loss_cps: 0.046011
[12:29:19.106] iteration 2476: total_loss: 0.631006, loss_sup: 0.556198, loss_mps: 0.026061, loss_cps: 0.048746
[12:29:19.252] iteration 2477: total_loss: 0.466473, loss_sup: 0.409624, loss_mps: 0.021201, loss_cps: 0.035647
[12:29:19.397] iteration 2478: total_loss: 0.366337, loss_sup: 0.284931, loss_mps: 0.027728, loss_cps: 0.053678
[12:29:19.543] iteration 2479: total_loss: 0.419442, loss_sup: 0.347408, loss_mps: 0.026002, loss_cps: 0.046031
[12:29:19.689] iteration 2480: total_loss: 0.494517, loss_sup: 0.428917, loss_mps: 0.023761, loss_cps: 0.041838
[12:29:19.835] iteration 2481: total_loss: 0.331063, loss_sup: 0.267341, loss_mps: 0.023871, loss_cps: 0.039850
[12:29:19.982] iteration 2482: total_loss: 0.425109, loss_sup: 0.331460, loss_mps: 0.032863, loss_cps: 0.060785
[12:29:20.129] iteration 2483: total_loss: 0.400016, loss_sup: 0.342777, loss_mps: 0.022611, loss_cps: 0.034629
[12:29:20.276] iteration 2484: total_loss: 0.347668, loss_sup: 0.271598, loss_mps: 0.027846, loss_cps: 0.048224
[12:29:20.421] iteration 2485: total_loss: 0.281756, loss_sup: 0.198063, loss_mps: 0.029485, loss_cps: 0.054208
[12:29:20.567] iteration 2486: total_loss: 0.442691, loss_sup: 0.371571, loss_mps: 0.026239, loss_cps: 0.044881
[12:29:20.713] iteration 2487: total_loss: 0.424200, loss_sup: 0.340230, loss_mps: 0.029611, loss_cps: 0.054360
[12:29:20.859] iteration 2488: total_loss: 0.382564, loss_sup: 0.310708, loss_mps: 0.026299, loss_cps: 0.045557
[12:29:21.005] iteration 2489: total_loss: 0.251888, loss_sup: 0.183309, loss_mps: 0.025491, loss_cps: 0.043089
[12:29:21.154] iteration 2490: total_loss: 0.248308, loss_sup: 0.191479, loss_mps: 0.022387, loss_cps: 0.034443
[12:29:21.300] iteration 2491: total_loss: 0.776420, loss_sup: 0.705967, loss_mps: 0.026410, loss_cps: 0.044044
[12:29:21.445] iteration 2492: total_loss: 0.359909, loss_sup: 0.308119, loss_mps: 0.020382, loss_cps: 0.031407
[12:29:21.593] iteration 2493: total_loss: 0.711269, loss_sup: 0.612468, loss_mps: 0.034732, loss_cps: 0.064069
[12:29:21.739] iteration 2494: total_loss: 0.407316, loss_sup: 0.332965, loss_mps: 0.028310, loss_cps: 0.046041
[12:29:21.885] iteration 2495: total_loss: 0.183884, loss_sup: 0.111136, loss_mps: 0.026846, loss_cps: 0.045902
[12:29:22.030] iteration 2496: total_loss: 0.592946, loss_sup: 0.529483, loss_mps: 0.023861, loss_cps: 0.039601
[12:29:22.178] iteration 2497: total_loss: 0.400941, loss_sup: 0.311652, loss_mps: 0.031877, loss_cps: 0.057412
[12:29:22.324] iteration 2498: total_loss: 0.327134, loss_sup: 0.244959, loss_mps: 0.029874, loss_cps: 0.052301
[12:29:22.470] iteration 2499: total_loss: 0.373667, loss_sup: 0.279408, loss_mps: 0.033805, loss_cps: 0.060453
[12:29:22.615] iteration 2500: total_loss: 0.246270, loss_sup: 0.182422, loss_mps: 0.025151, loss_cps: 0.038697
[12:29:22.615] Evaluation Started ==>
[12:29:33.957] ==> valid iteration 2500: unet metrics: {'dc': 0.5008550654512601, 'jc': 0.37347636288655445, 'pre': 0.5562207389076157, 'hd': 7.296370292827783}, ynet metrics: {'dc': 0.45241016697533293, 'jc': 0.3365763427601206, 'pre': 0.5592668057695748, 'hd': 7.238794515710832}.
[12:29:34.016] ==> New best valid dice for unet: 0.500855, at iteration 2500
[12:29:34.018] Evaluation Finished!⏹️
[12:29:34.172] iteration 2501: total_loss: 0.479180, loss_sup: 0.403862, loss_mps: 0.027950, loss_cps: 0.047369
[12:29:34.322] iteration 2502: total_loss: 0.499387, loss_sup: 0.430053, loss_mps: 0.025969, loss_cps: 0.043364
[12:29:34.468] iteration 2503: total_loss: 0.535004, loss_sup: 0.447292, loss_mps: 0.030981, loss_cps: 0.056731
[12:29:34.613] iteration 2504: total_loss: 0.265995, loss_sup: 0.214531, loss_mps: 0.021521, loss_cps: 0.029944
[12:29:34.757] iteration 2505: total_loss: 0.279644, loss_sup: 0.217380, loss_mps: 0.024371, loss_cps: 0.037893
[12:29:34.903] iteration 2506: total_loss: 0.179057, loss_sup: 0.115066, loss_mps: 0.025211, loss_cps: 0.038781
[12:29:35.048] iteration 2507: total_loss: 0.222443, loss_sup: 0.171710, loss_mps: 0.020682, loss_cps: 0.030052
[12:29:35.111] iteration 2508: total_loss: 0.483416, loss_sup: 0.435826, loss_mps: 0.021230, loss_cps: 0.026359
[12:29:36.309] iteration 2509: total_loss: 0.330314, loss_sup: 0.280221, loss_mps: 0.020261, loss_cps: 0.029832
[12:29:36.458] iteration 2510: total_loss: 0.359902, loss_sup: 0.304018, loss_mps: 0.022371, loss_cps: 0.033513
[12:29:36.604] iteration 2511: total_loss: 0.333609, loss_sup: 0.266326, loss_mps: 0.025444, loss_cps: 0.041840
[12:29:36.750] iteration 2512: total_loss: 0.403682, loss_sup: 0.341152, loss_mps: 0.025132, loss_cps: 0.037398
[12:29:36.897] iteration 2513: total_loss: 0.256558, loss_sup: 0.195019, loss_mps: 0.023877, loss_cps: 0.037662
[12:29:37.042] iteration 2514: total_loss: 0.313098, loss_sup: 0.253905, loss_mps: 0.023742, loss_cps: 0.035450
[12:29:37.188] iteration 2515: total_loss: 0.272914, loss_sup: 0.209703, loss_mps: 0.024028, loss_cps: 0.039183
[12:29:37.338] iteration 2516: total_loss: 0.323658, loss_sup: 0.257465, loss_mps: 0.025666, loss_cps: 0.040527
[12:29:37.484] iteration 2517: total_loss: 0.173938, loss_sup: 0.109012, loss_mps: 0.024593, loss_cps: 0.040334
[12:29:37.630] iteration 2518: total_loss: 0.224861, loss_sup: 0.154830, loss_mps: 0.026279, loss_cps: 0.043752
[12:29:37.776] iteration 2519: total_loss: 0.226021, loss_sup: 0.159897, loss_mps: 0.025198, loss_cps: 0.040925
[12:29:37.922] iteration 2520: total_loss: 0.313022, loss_sup: 0.238054, loss_mps: 0.027469, loss_cps: 0.047499
[12:29:38.067] iteration 2521: total_loss: 0.539288, loss_sup: 0.474687, loss_mps: 0.024061, loss_cps: 0.040540
[12:29:38.215] iteration 2522: total_loss: 0.298699, loss_sup: 0.242148, loss_mps: 0.022370, loss_cps: 0.034182
[12:29:38.363] iteration 2523: total_loss: 0.400036, loss_sup: 0.302363, loss_mps: 0.033634, loss_cps: 0.064039
[12:29:38.511] iteration 2524: total_loss: 0.268281, loss_sup: 0.205778, loss_mps: 0.024927, loss_cps: 0.037576
[12:29:38.659] iteration 2525: total_loss: 0.236253, loss_sup: 0.155840, loss_mps: 0.028262, loss_cps: 0.052151
[12:29:38.806] iteration 2526: total_loss: 0.434231, loss_sup: 0.344173, loss_mps: 0.031127, loss_cps: 0.058931
[12:29:38.953] iteration 2527: total_loss: 0.369048, loss_sup: 0.304106, loss_mps: 0.024864, loss_cps: 0.040078
[12:29:39.101] iteration 2528: total_loss: 0.290967, loss_sup: 0.221355, loss_mps: 0.025258, loss_cps: 0.044354
[12:29:39.248] iteration 2529: total_loss: 0.393845, loss_sup: 0.321516, loss_mps: 0.026456, loss_cps: 0.045873
[12:29:39.395] iteration 2530: total_loss: 0.319294, loss_sup: 0.263491, loss_mps: 0.021531, loss_cps: 0.034272
[12:29:39.542] iteration 2531: total_loss: 0.473188, loss_sup: 0.390756, loss_mps: 0.027735, loss_cps: 0.054697
[12:29:39.691] iteration 2532: total_loss: 0.214859, loss_sup: 0.159136, loss_mps: 0.021317, loss_cps: 0.034406
[12:29:39.837] iteration 2533: total_loss: 0.203353, loss_sup: 0.141664, loss_mps: 0.023528, loss_cps: 0.038161
[12:29:39.984] iteration 2534: total_loss: 0.317105, loss_sup: 0.233590, loss_mps: 0.029480, loss_cps: 0.054035
[12:29:40.130] iteration 2535: total_loss: 0.220282, loss_sup: 0.158001, loss_mps: 0.023861, loss_cps: 0.038419
[12:29:40.278] iteration 2536: total_loss: 0.285057, loss_sup: 0.217163, loss_mps: 0.024480, loss_cps: 0.043414
[12:29:40.424] iteration 2537: total_loss: 0.304483, loss_sup: 0.227124, loss_mps: 0.027372, loss_cps: 0.049987
[12:29:40.570] iteration 2538: total_loss: 0.311806, loss_sup: 0.247532, loss_mps: 0.023635, loss_cps: 0.040639
[12:29:40.717] iteration 2539: total_loss: 0.219538, loss_sup: 0.148518, loss_mps: 0.026074, loss_cps: 0.044946
[12:29:40.863] iteration 2540: total_loss: 0.367412, loss_sup: 0.288363, loss_mps: 0.028025, loss_cps: 0.051023
[12:29:41.010] iteration 2541: total_loss: 0.139514, loss_sup: 0.073819, loss_mps: 0.024123, loss_cps: 0.041572
[12:29:41.155] iteration 2542: total_loss: 0.274111, loss_sup: 0.208265, loss_mps: 0.024474, loss_cps: 0.041372
[12:29:41.305] iteration 2543: total_loss: 0.789532, loss_sup: 0.682521, loss_mps: 0.034687, loss_cps: 0.072324
[12:29:41.456] iteration 2544: total_loss: 0.387962, loss_sup: 0.313222, loss_mps: 0.025850, loss_cps: 0.048889
[12:29:41.606] iteration 2545: total_loss: 0.135373, loss_sup: 0.078625, loss_mps: 0.020944, loss_cps: 0.035803
[12:29:41.753] iteration 2546: total_loss: 0.828135, loss_sup: 0.733019, loss_mps: 0.032070, loss_cps: 0.063046
[12:29:41.900] iteration 2547: total_loss: 0.255084, loss_sup: 0.173671, loss_mps: 0.028263, loss_cps: 0.053149
[12:29:42.049] iteration 2548: total_loss: 0.203451, loss_sup: 0.124579, loss_mps: 0.028545, loss_cps: 0.050328
[12:29:42.195] iteration 2549: total_loss: 0.442495, loss_sup: 0.351505, loss_mps: 0.032686, loss_cps: 0.058304
[12:29:42.342] iteration 2550: total_loss: 0.368464, loss_sup: 0.274911, loss_mps: 0.032537, loss_cps: 0.061015
[12:29:42.488] iteration 2551: total_loss: 0.328690, loss_sup: 0.244717, loss_mps: 0.030650, loss_cps: 0.053323
[12:29:42.635] iteration 2552: total_loss: 0.528906, loss_sup: 0.441468, loss_mps: 0.031713, loss_cps: 0.055725
[12:29:42.783] iteration 2553: total_loss: 0.260311, loss_sup: 0.190054, loss_mps: 0.027017, loss_cps: 0.043241
[12:29:42.929] iteration 2554: total_loss: 0.302348, loss_sup: 0.227987, loss_mps: 0.028726, loss_cps: 0.045635
[12:29:43.075] iteration 2555: total_loss: 0.312232, loss_sup: 0.232682, loss_mps: 0.029468, loss_cps: 0.050082
[12:29:43.223] iteration 2556: total_loss: 0.288317, loss_sup: 0.206771, loss_mps: 0.028945, loss_cps: 0.052601
[12:29:43.372] iteration 2557: total_loss: 0.488382, loss_sup: 0.417278, loss_mps: 0.026761, loss_cps: 0.044343
[12:29:43.519] iteration 2558: total_loss: 0.477578, loss_sup: 0.390485, loss_mps: 0.031830, loss_cps: 0.055263
[12:29:43.667] iteration 2559: total_loss: 0.360603, loss_sup: 0.289472, loss_mps: 0.027463, loss_cps: 0.043668
[12:29:43.814] iteration 2560: total_loss: 0.323405, loss_sup: 0.250640, loss_mps: 0.028364, loss_cps: 0.044401
[12:29:43.963] iteration 2561: total_loss: 0.202502, loss_sup: 0.133353, loss_mps: 0.026548, loss_cps: 0.042601
[12:29:44.109] iteration 2562: total_loss: 0.374943, loss_sup: 0.296802, loss_mps: 0.029330, loss_cps: 0.048811
[12:29:44.256] iteration 2563: total_loss: 0.247188, loss_sup: 0.180633, loss_mps: 0.026138, loss_cps: 0.040417
[12:29:44.403] iteration 2564: total_loss: 0.476986, loss_sup: 0.407070, loss_mps: 0.026880, loss_cps: 0.043036
[12:29:44.549] iteration 2565: total_loss: 0.406164, loss_sup: 0.333440, loss_mps: 0.028186, loss_cps: 0.044539
[12:29:44.696] iteration 2566: total_loss: 0.459614, loss_sup: 0.387157, loss_mps: 0.027780, loss_cps: 0.044676
[12:29:44.843] iteration 2567: total_loss: 0.405140, loss_sup: 0.332481, loss_mps: 0.027853, loss_cps: 0.044805
[12:29:44.992] iteration 2568: total_loss: 0.455866, loss_sup: 0.374337, loss_mps: 0.029963, loss_cps: 0.051566
[12:29:45.139] iteration 2569: total_loss: 0.258603, loss_sup: 0.175550, loss_mps: 0.031260, loss_cps: 0.051793
[12:29:45.287] iteration 2570: total_loss: 0.163855, loss_sup: 0.115368, loss_mps: 0.019994, loss_cps: 0.028494
[12:29:45.433] iteration 2571: total_loss: 0.336861, loss_sup: 0.268536, loss_mps: 0.025580, loss_cps: 0.042746
[12:29:45.583] iteration 2572: total_loss: 0.336217, loss_sup: 0.265118, loss_mps: 0.027100, loss_cps: 0.044000
[12:29:45.731] iteration 2573: total_loss: 0.280797, loss_sup: 0.216673, loss_mps: 0.024901, loss_cps: 0.039223
[12:29:45.878] iteration 2574: total_loss: 0.377864, loss_sup: 0.303648, loss_mps: 0.027414, loss_cps: 0.046802
[12:29:46.025] iteration 2575: total_loss: 0.245152, loss_sup: 0.180610, loss_mps: 0.024217, loss_cps: 0.040325
[12:29:46.174] iteration 2576: total_loss: 0.177385, loss_sup: 0.110744, loss_mps: 0.024978, loss_cps: 0.041663
[12:29:46.321] iteration 2577: total_loss: 0.245962, loss_sup: 0.174411, loss_mps: 0.026791, loss_cps: 0.044760
[12:29:46.468] iteration 2578: total_loss: 0.272329, loss_sup: 0.214758, loss_mps: 0.021907, loss_cps: 0.035664
[12:29:46.614] iteration 2579: total_loss: 0.391469, loss_sup: 0.317917, loss_mps: 0.026550, loss_cps: 0.047002
[12:29:46.762] iteration 2580: total_loss: 0.472004, loss_sup: 0.395530, loss_mps: 0.027143, loss_cps: 0.049331
[12:29:46.910] iteration 2581: total_loss: 0.403214, loss_sup: 0.328574, loss_mps: 0.027411, loss_cps: 0.047229
[12:29:47.057] iteration 2582: total_loss: 0.289627, loss_sup: 0.235796, loss_mps: 0.020979, loss_cps: 0.032852
[12:29:47.206] iteration 2583: total_loss: 0.470748, loss_sup: 0.408601, loss_mps: 0.022613, loss_cps: 0.039534
[12:29:47.352] iteration 2584: total_loss: 0.285954, loss_sup: 0.221184, loss_mps: 0.023785, loss_cps: 0.040984
[12:29:47.498] iteration 2585: total_loss: 0.321627, loss_sup: 0.242621, loss_mps: 0.027654, loss_cps: 0.051351
[12:29:47.645] iteration 2586: total_loss: 0.305084, loss_sup: 0.233795, loss_mps: 0.025927, loss_cps: 0.045362
[12:29:47.794] iteration 2587: total_loss: 0.336146, loss_sup: 0.273982, loss_mps: 0.023515, loss_cps: 0.038649
[12:29:47.941] iteration 2588: total_loss: 0.331279, loss_sup: 0.260013, loss_mps: 0.025722, loss_cps: 0.045544
[12:29:48.088] iteration 2589: total_loss: 0.410332, loss_sup: 0.343678, loss_mps: 0.024589, loss_cps: 0.042065
[12:29:48.236] iteration 2590: total_loss: 0.269274, loss_sup: 0.194812, loss_mps: 0.026814, loss_cps: 0.047648
[12:29:48.383] iteration 2591: total_loss: 0.236861, loss_sup: 0.178558, loss_mps: 0.022492, loss_cps: 0.035811
[12:29:48.531] iteration 2592: total_loss: 0.741359, loss_sup: 0.647727, loss_mps: 0.032647, loss_cps: 0.060984
[12:29:48.678] iteration 2593: total_loss: 0.352629, loss_sup: 0.292749, loss_mps: 0.022933, loss_cps: 0.036947
[12:29:48.824] iteration 2594: total_loss: 0.808944, loss_sup: 0.715938, loss_mps: 0.032880, loss_cps: 0.060126
[12:29:48.972] iteration 2595: total_loss: 0.366992, loss_sup: 0.296953, loss_mps: 0.026216, loss_cps: 0.043823
[12:29:49.121] iteration 2596: total_loss: 0.402143, loss_sup: 0.309400, loss_mps: 0.033308, loss_cps: 0.059435
[12:29:49.272] iteration 2597: total_loss: 0.224801, loss_sup: 0.155515, loss_mps: 0.025588, loss_cps: 0.043698
[12:29:49.419] iteration 2598: total_loss: 0.322124, loss_sup: 0.245725, loss_mps: 0.028538, loss_cps: 0.047861
[12:29:49.566] iteration 2599: total_loss: 0.433200, loss_sup: 0.350621, loss_mps: 0.030007, loss_cps: 0.052572
[12:29:49.714] iteration 2600: total_loss: 0.411944, loss_sup: 0.333565, loss_mps: 0.028645, loss_cps: 0.049734
[12:29:49.714] Evaluation Started ==>
[12:30:01.096] ==> valid iteration 2600: unet metrics: {'dc': 0.48949833535342546, 'jc': 0.3617314180834511, 'pre': 0.5460972468091552, 'hd': 7.5202961566174595}, ynet metrics: {'dc': 0.4583495650314346, 'jc': 0.33918853027602686, 'pre': 0.5941138383331315, 'hd': 7.057367326344736}.
[12:30:01.097] Evaluation Finished!⏹️
[12:30:01.249] iteration 2601: total_loss: 0.497772, loss_sup: 0.419391, loss_mps: 0.029837, loss_cps: 0.048543
[12:30:01.398] iteration 2602: total_loss: 0.328229, loss_sup: 0.247498, loss_mps: 0.029913, loss_cps: 0.050818
[12:30:01.546] iteration 2603: total_loss: 0.377386, loss_sup: 0.300079, loss_mps: 0.029645, loss_cps: 0.047662
[12:30:01.694] iteration 2604: total_loss: 0.394128, loss_sup: 0.297524, loss_mps: 0.036159, loss_cps: 0.060445
[12:30:01.840] iteration 2605: total_loss: 0.489477, loss_sup: 0.404786, loss_mps: 0.032518, loss_cps: 0.052174
[12:30:01.986] iteration 2606: total_loss: 0.319016, loss_sup: 0.236256, loss_mps: 0.030949, loss_cps: 0.051810
[12:30:02.133] iteration 2607: total_loss: 0.329176, loss_sup: 0.241403, loss_mps: 0.032724, loss_cps: 0.055049
[12:30:02.279] iteration 2608: total_loss: 0.294427, loss_sup: 0.218157, loss_mps: 0.029084, loss_cps: 0.047185
[12:30:02.425] iteration 2609: total_loss: 0.169504, loss_sup: 0.116391, loss_mps: 0.022362, loss_cps: 0.030751
[12:30:02.571] iteration 2610: total_loss: 0.294540, loss_sup: 0.212645, loss_mps: 0.030295, loss_cps: 0.051600
[12:30:02.717] iteration 2611: total_loss: 0.343105, loss_sup: 0.260682, loss_mps: 0.031199, loss_cps: 0.051224
[12:30:02.863] iteration 2612: total_loss: 0.179956, loss_sup: 0.131877, loss_mps: 0.019309, loss_cps: 0.028770
[12:30:03.008] iteration 2613: total_loss: 0.300588, loss_sup: 0.227421, loss_mps: 0.027627, loss_cps: 0.045540
[12:30:03.154] iteration 2614: total_loss: 0.376591, loss_sup: 0.319515, loss_mps: 0.021823, loss_cps: 0.035253
[12:30:03.301] iteration 2615: total_loss: 0.139736, loss_sup: 0.084422, loss_mps: 0.021523, loss_cps: 0.033792
[12:30:03.447] iteration 2616: total_loss: 0.155422, loss_sup: 0.107010, loss_mps: 0.018702, loss_cps: 0.029710
[12:30:03.594] iteration 2617: total_loss: 0.333190, loss_sup: 0.273687, loss_mps: 0.022249, loss_cps: 0.037253
[12:30:03.743] iteration 2618: total_loss: 0.169430, loss_sup: 0.107559, loss_mps: 0.022714, loss_cps: 0.039157
[12:30:03.889] iteration 2619: total_loss: 0.168921, loss_sup: 0.118073, loss_mps: 0.019375, loss_cps: 0.031473
[12:30:04.036] iteration 2620: total_loss: 0.142502, loss_sup: 0.088666, loss_mps: 0.020164, loss_cps: 0.033672
[12:30:04.183] iteration 2621: total_loss: 0.388359, loss_sup: 0.333010, loss_mps: 0.020472, loss_cps: 0.034877
[12:30:04.329] iteration 2622: total_loss: 0.222734, loss_sup: 0.164489, loss_mps: 0.020424, loss_cps: 0.037821
[12:30:04.476] iteration 2623: total_loss: 0.296406, loss_sup: 0.252083, loss_mps: 0.016516, loss_cps: 0.027807
[12:30:04.621] iteration 2624: total_loss: 0.237373, loss_sup: 0.194281, loss_mps: 0.016373, loss_cps: 0.026719
[12:30:04.766] iteration 2625: total_loss: 0.226804, loss_sup: 0.181788, loss_mps: 0.016415, loss_cps: 0.028601
[12:30:04.913] iteration 2626: total_loss: 0.265212, loss_sup: 0.201111, loss_mps: 0.021737, loss_cps: 0.042365
[12:30:05.059] iteration 2627: total_loss: 0.540895, loss_sup: 0.485901, loss_mps: 0.019897, loss_cps: 0.035096
[12:30:05.205] iteration 2628: total_loss: 0.210161, loss_sup: 0.138402, loss_mps: 0.024014, loss_cps: 0.047745
[12:30:05.351] iteration 2629: total_loss: 0.330469, loss_sup: 0.255427, loss_mps: 0.025725, loss_cps: 0.049317
[12:30:05.498] iteration 2630: total_loss: 0.294763, loss_sup: 0.238778, loss_mps: 0.019693, loss_cps: 0.036292
[12:30:05.647] iteration 2631: total_loss: 0.266265, loss_sup: 0.200300, loss_mps: 0.022134, loss_cps: 0.043831
[12:30:05.793] iteration 2632: total_loss: 0.283909, loss_sup: 0.216237, loss_mps: 0.022989, loss_cps: 0.044683
[12:30:05.939] iteration 2633: total_loss: 0.373515, loss_sup: 0.270924, loss_mps: 0.032143, loss_cps: 0.070448
[12:30:06.085] iteration 2634: total_loss: 0.144320, loss_sup: 0.092252, loss_mps: 0.018363, loss_cps: 0.033705
[12:30:06.231] iteration 2635: total_loss: 0.202373, loss_sup: 0.128989, loss_mps: 0.024540, loss_cps: 0.048843
[12:30:06.379] iteration 2636: total_loss: 0.372516, loss_sup: 0.295260, loss_mps: 0.026010, loss_cps: 0.051246
[12:30:06.525] iteration 2637: total_loss: 0.330031, loss_sup: 0.248844, loss_mps: 0.027229, loss_cps: 0.053958
[12:30:06.671] iteration 2638: total_loss: 0.218935, loss_sup: 0.162196, loss_mps: 0.019891, loss_cps: 0.036848
[12:30:06.817] iteration 2639: total_loss: 0.247824, loss_sup: 0.178908, loss_mps: 0.023460, loss_cps: 0.045456
[12:30:06.964] iteration 2640: total_loss: 0.194365, loss_sup: 0.109854, loss_mps: 0.027120, loss_cps: 0.057390
[12:30:07.111] iteration 2641: total_loss: 0.445643, loss_sup: 0.349920, loss_mps: 0.031709, loss_cps: 0.064014
[12:30:07.258] iteration 2642: total_loss: 0.364771, loss_sup: 0.268644, loss_mps: 0.032200, loss_cps: 0.063927
[12:30:07.405] iteration 2643: total_loss: 0.446794, loss_sup: 0.363034, loss_mps: 0.027891, loss_cps: 0.055869
[12:30:07.554] iteration 2644: total_loss: 0.300320, loss_sup: 0.226227, loss_mps: 0.025937, loss_cps: 0.048155
[12:30:07.699] iteration 2645: total_loss: 0.678985, loss_sup: 0.589698, loss_mps: 0.029424, loss_cps: 0.059863
[12:30:07.846] iteration 2646: total_loss: 0.222252, loss_sup: 0.164894, loss_mps: 0.020713, loss_cps: 0.036645
[12:30:07.993] iteration 2647: total_loss: 0.469931, loss_sup: 0.372228, loss_mps: 0.032388, loss_cps: 0.065315
[12:30:08.139] iteration 2648: total_loss: 0.395001, loss_sup: 0.330673, loss_mps: 0.023897, loss_cps: 0.040431
[12:30:08.285] iteration 2649: total_loss: 0.380616, loss_sup: 0.306451, loss_mps: 0.026486, loss_cps: 0.047679
[12:30:08.430] iteration 2650: total_loss: 0.365058, loss_sup: 0.264282, loss_mps: 0.033961, loss_cps: 0.066815
[12:30:08.576] iteration 2651: total_loss: 0.368563, loss_sup: 0.289167, loss_mps: 0.028176, loss_cps: 0.051220
[12:30:08.723] iteration 2652: total_loss: 0.634810, loss_sup: 0.547666, loss_mps: 0.030077, loss_cps: 0.057067
[12:30:08.870] iteration 2653: total_loss: 0.184988, loss_sup: 0.115417, loss_mps: 0.025709, loss_cps: 0.043863
[12:30:09.017] iteration 2654: total_loss: 0.325702, loss_sup: 0.260305, loss_mps: 0.024974, loss_cps: 0.040424
[12:30:09.166] iteration 2655: total_loss: 0.289338, loss_sup: 0.220564, loss_mps: 0.026289, loss_cps: 0.042485
[12:30:09.312] iteration 2656: total_loss: 0.239023, loss_sup: 0.175001, loss_mps: 0.024754, loss_cps: 0.039267
[12:30:09.459] iteration 2657: total_loss: 0.193552, loss_sup: 0.131507, loss_mps: 0.024152, loss_cps: 0.037893
[12:30:09.607] iteration 2658: total_loss: 0.258394, loss_sup: 0.196674, loss_mps: 0.023568, loss_cps: 0.038152
[12:30:09.753] iteration 2659: total_loss: 0.438210, loss_sup: 0.373341, loss_mps: 0.025394, loss_cps: 0.039476
[12:30:09.899] iteration 2660: total_loss: 0.352846, loss_sup: 0.292591, loss_mps: 0.023780, loss_cps: 0.036475
[12:30:10.045] iteration 2661: total_loss: 0.267072, loss_sup: 0.211983, loss_mps: 0.022334, loss_cps: 0.032755
[12:30:10.191] iteration 2662: total_loss: 0.182375, loss_sup: 0.123880, loss_mps: 0.022831, loss_cps: 0.035664
[12:30:10.339] iteration 2663: total_loss: 0.410012, loss_sup: 0.364923, loss_mps: 0.018014, loss_cps: 0.027075
[12:30:10.485] iteration 2664: total_loss: 0.328190, loss_sup: 0.261458, loss_mps: 0.024030, loss_cps: 0.042702
[12:30:10.631] iteration 2665: total_loss: 0.275536, loss_sup: 0.210683, loss_mps: 0.024090, loss_cps: 0.040764
[12:30:10.778] iteration 2666: total_loss: 0.433750, loss_sup: 0.367463, loss_mps: 0.024300, loss_cps: 0.041987
[12:30:10.925] iteration 2667: total_loss: 0.340731, loss_sup: 0.270591, loss_mps: 0.025044, loss_cps: 0.045096
[12:30:11.071] iteration 2668: total_loss: 0.258245, loss_sup: 0.199270, loss_mps: 0.022339, loss_cps: 0.036637
[12:30:11.218] iteration 2669: total_loss: 0.279721, loss_sup: 0.213793, loss_mps: 0.024591, loss_cps: 0.041336
[12:30:11.364] iteration 2670: total_loss: 0.301308, loss_sup: 0.230408, loss_mps: 0.026479, loss_cps: 0.044421
[12:30:11.510] iteration 2671: total_loss: 0.300007, loss_sup: 0.226151, loss_mps: 0.027463, loss_cps: 0.046393
[12:30:11.656] iteration 2672: total_loss: 0.222636, loss_sup: 0.142619, loss_mps: 0.027827, loss_cps: 0.052190
[12:30:11.802] iteration 2673: total_loss: 0.200482, loss_sup: 0.121838, loss_mps: 0.028032, loss_cps: 0.050611
[12:30:11.948] iteration 2674: total_loss: 0.597416, loss_sup: 0.506645, loss_mps: 0.031092, loss_cps: 0.059679
[12:30:12.094] iteration 2675: total_loss: 0.258227, loss_sup: 0.186702, loss_mps: 0.026045, loss_cps: 0.045480
[12:30:12.240] iteration 2676: total_loss: 0.206014, loss_sup: 0.130887, loss_mps: 0.026596, loss_cps: 0.048530
[12:30:12.388] iteration 2677: total_loss: 0.419971, loss_sup: 0.360038, loss_mps: 0.022493, loss_cps: 0.037440
[12:30:12.536] iteration 2678: total_loss: 0.233842, loss_sup: 0.156361, loss_mps: 0.027660, loss_cps: 0.049822
[12:30:12.682] iteration 2679: total_loss: 0.165281, loss_sup: 0.108484, loss_mps: 0.021903, loss_cps: 0.034894
[12:30:12.829] iteration 2680: total_loss: 0.362490, loss_sup: 0.297476, loss_mps: 0.024432, loss_cps: 0.040582
[12:30:12.975] iteration 2681: total_loss: 0.301494, loss_sup: 0.206030, loss_mps: 0.032502, loss_cps: 0.062963
[12:30:13.121] iteration 2682: total_loss: 0.390094, loss_sup: 0.307525, loss_mps: 0.029551, loss_cps: 0.053018
[12:30:13.269] iteration 2683: total_loss: 0.299815, loss_sup: 0.238814, loss_mps: 0.022694, loss_cps: 0.038308
[12:30:13.415] iteration 2684: total_loss: 0.364238, loss_sup: 0.296084, loss_mps: 0.025320, loss_cps: 0.042834
[12:30:13.563] iteration 2685: total_loss: 0.408285, loss_sup: 0.316449, loss_mps: 0.031364, loss_cps: 0.060472
[12:30:13.716] iteration 2686: total_loss: 0.324650, loss_sup: 0.264889, loss_mps: 0.022420, loss_cps: 0.037341
[12:30:13.862] iteration 2687: total_loss: 0.291000, loss_sup: 0.229425, loss_mps: 0.023545, loss_cps: 0.038030
[12:30:14.009] iteration 2688: total_loss: 0.333635, loss_sup: 0.259189, loss_mps: 0.026919, loss_cps: 0.047526
[12:30:14.155] iteration 2689: total_loss: 0.425901, loss_sup: 0.370325, loss_mps: 0.021076, loss_cps: 0.034499
[12:30:14.303] iteration 2690: total_loss: 0.466150, loss_sup: 0.383530, loss_mps: 0.028519, loss_cps: 0.054101
[12:30:14.451] iteration 2691: total_loss: 0.488222, loss_sup: 0.404585, loss_mps: 0.030897, loss_cps: 0.052740
[12:30:14.597] iteration 2692: total_loss: 0.477901, loss_sup: 0.417768, loss_mps: 0.023007, loss_cps: 0.037126
[12:30:14.744] iteration 2693: total_loss: 0.244037, loss_sup: 0.179672, loss_mps: 0.024528, loss_cps: 0.039837
[12:30:14.891] iteration 2694: total_loss: 0.413629, loss_sup: 0.344682, loss_mps: 0.027988, loss_cps: 0.040959
[12:30:15.037] iteration 2695: total_loss: 0.357636, loss_sup: 0.277156, loss_mps: 0.030339, loss_cps: 0.050141
[12:30:15.183] iteration 2696: total_loss: 0.283308, loss_sup: 0.210750, loss_mps: 0.028178, loss_cps: 0.044380
[12:30:15.329] iteration 2697: total_loss: 0.190738, loss_sup: 0.131879, loss_mps: 0.022822, loss_cps: 0.036037
[12:30:15.476] iteration 2698: total_loss: 0.267746, loss_sup: 0.202936, loss_mps: 0.026039, loss_cps: 0.038771
[12:30:15.623] iteration 2699: total_loss: 0.419251, loss_sup: 0.349223, loss_mps: 0.026930, loss_cps: 0.043098
[12:30:15.773] iteration 2700: total_loss: 0.398420, loss_sup: 0.342086, loss_mps: 0.022945, loss_cps: 0.033389
[12:30:15.773] Evaluation Started ==>
[12:30:27.192] ==> valid iteration 2700: unet metrics: {'dc': 0.5060968152673552, 'jc': 0.38358530701002774, 'pre': 0.5888851595435144, 'hd': 7.066246699054796}, ynet metrics: {'dc': 0.45071564617694004, 'jc': 0.3347566080335327, 'pre': 0.5631874261408552, 'hd': 7.380519248738847}.
[12:30:27.256] ==> New best valid dice for unet: 0.506097, at iteration 2700
[12:30:27.257] Evaluation Finished!⏹️
[12:30:27.412] iteration 2701: total_loss: 0.703944, loss_sup: 0.638046, loss_mps: 0.025497, loss_cps: 0.040400
[12:30:27.560] iteration 2702: total_loss: 0.267522, loss_sup: 0.189932, loss_mps: 0.029716, loss_cps: 0.047874
[12:30:27.706] iteration 2703: total_loss: 0.309018, loss_sup: 0.236436, loss_mps: 0.027239, loss_cps: 0.045342
[12:30:27.852] iteration 2704: total_loss: 0.257043, loss_sup: 0.193058, loss_mps: 0.025091, loss_cps: 0.038894
[12:30:27.997] iteration 2705: total_loss: 0.271921, loss_sup: 0.195183, loss_mps: 0.027847, loss_cps: 0.048892
[12:30:28.142] iteration 2706: total_loss: 0.294997, loss_sup: 0.230489, loss_mps: 0.025240, loss_cps: 0.039269
[12:30:28.288] iteration 2707: total_loss: 0.380467, loss_sup: 0.303264, loss_mps: 0.028882, loss_cps: 0.048321
[12:30:28.433] iteration 2708: total_loss: 0.201784, loss_sup: 0.135845, loss_mps: 0.024813, loss_cps: 0.041126
[12:30:28.580] iteration 2709: total_loss: 0.362575, loss_sup: 0.280354, loss_mps: 0.029293, loss_cps: 0.052928
[12:30:28.726] iteration 2710: total_loss: 0.155752, loss_sup: 0.102409, loss_mps: 0.020727, loss_cps: 0.032615
[12:30:28.872] iteration 2711: total_loss: 0.307840, loss_sup: 0.231195, loss_mps: 0.028110, loss_cps: 0.048534
[12:30:29.017] iteration 2712: total_loss: 0.232749, loss_sup: 0.141439, loss_mps: 0.032340, loss_cps: 0.058969
[12:30:29.162] iteration 2713: total_loss: 0.371497, loss_sup: 0.294525, loss_mps: 0.027182, loss_cps: 0.049790
[12:30:29.307] iteration 2714: total_loss: 0.148279, loss_sup: 0.085438, loss_mps: 0.024088, loss_cps: 0.038754
[12:30:29.452] iteration 2715: total_loss: 0.250323, loss_sup: 0.172735, loss_mps: 0.027101, loss_cps: 0.050487
[12:30:29.597] iteration 2716: total_loss: 0.380160, loss_sup: 0.315845, loss_mps: 0.023611, loss_cps: 0.040704
[12:30:29.742] iteration 2717: total_loss: 0.167075, loss_sup: 0.102821, loss_mps: 0.023080, loss_cps: 0.041173
[12:30:29.888] iteration 2718: total_loss: 0.133790, loss_sup: 0.076066, loss_mps: 0.021554, loss_cps: 0.036170
[12:30:30.033] iteration 2719: total_loss: 0.296159, loss_sup: 0.231111, loss_mps: 0.023670, loss_cps: 0.041378
[12:30:30.178] iteration 2720: total_loss: 0.306439, loss_sup: 0.243449, loss_mps: 0.022910, loss_cps: 0.040081
[12:30:30.327] iteration 2721: total_loss: 0.122741, loss_sup: 0.080724, loss_mps: 0.016221, loss_cps: 0.025796
[12:30:30.474] iteration 2722: total_loss: 0.339707, loss_sup: 0.276667, loss_mps: 0.022285, loss_cps: 0.040754
[12:30:30.620] iteration 2723: total_loss: 0.270486, loss_sup: 0.204051, loss_mps: 0.023457, loss_cps: 0.042978
[12:30:30.765] iteration 2724: total_loss: 0.188940, loss_sup: 0.138692, loss_mps: 0.018845, loss_cps: 0.031403
[12:30:30.911] iteration 2725: total_loss: 0.259790, loss_sup: 0.201426, loss_mps: 0.021518, loss_cps: 0.036846
[12:30:31.059] iteration 2726: total_loss: 0.296403, loss_sup: 0.229132, loss_mps: 0.024641, loss_cps: 0.042631
[12:30:31.207] iteration 2727: total_loss: 0.844074, loss_sup: 0.778232, loss_mps: 0.023123, loss_cps: 0.042719
[12:30:31.353] iteration 2728: total_loss: 0.303309, loss_sup: 0.245500, loss_mps: 0.020764, loss_cps: 0.037045
[12:30:31.502] iteration 2729: total_loss: 0.230034, loss_sup: 0.159193, loss_mps: 0.024718, loss_cps: 0.046123
[12:30:31.648] iteration 2730: total_loss: 0.271593, loss_sup: 0.199796, loss_mps: 0.025060, loss_cps: 0.046737
[12:30:31.797] iteration 2731: total_loss: 0.270647, loss_sup: 0.220445, loss_mps: 0.019325, loss_cps: 0.030876
[12:30:31.942] iteration 2732: total_loss: 0.333109, loss_sup: 0.268703, loss_mps: 0.022963, loss_cps: 0.041443
[12:30:32.088] iteration 2733: total_loss: 0.183163, loss_sup: 0.127700, loss_mps: 0.020022, loss_cps: 0.035441
[12:30:32.233] iteration 2734: total_loss: 0.120584, loss_sup: 0.076977, loss_mps: 0.017108, loss_cps: 0.026499
[12:30:32.379] iteration 2735: total_loss: 0.192633, loss_sup: 0.130492, loss_mps: 0.022428, loss_cps: 0.039713
[12:30:32.525] iteration 2736: total_loss: 0.263898, loss_sup: 0.188179, loss_mps: 0.025755, loss_cps: 0.049964
[12:30:32.670] iteration 2737: total_loss: 0.217734, loss_sup: 0.161585, loss_mps: 0.021183, loss_cps: 0.034966
[12:30:32.817] iteration 2738: total_loss: 0.351619, loss_sup: 0.286016, loss_mps: 0.023315, loss_cps: 0.042288
[12:30:32.962] iteration 2739: total_loss: 0.270063, loss_sup: 0.200625, loss_mps: 0.024306, loss_cps: 0.045132
[12:30:33.108] iteration 2740: total_loss: 0.343646, loss_sup: 0.278502, loss_mps: 0.023304, loss_cps: 0.041840
[12:30:33.254] iteration 2741: total_loss: 0.341405, loss_sup: 0.278725, loss_mps: 0.022645, loss_cps: 0.040035
[12:30:33.401] iteration 2742: total_loss: 0.644616, loss_sup: 0.597755, loss_mps: 0.017702, loss_cps: 0.029159
[12:30:33.553] iteration 2743: total_loss: 0.256178, loss_sup: 0.196107, loss_mps: 0.022240, loss_cps: 0.037832
[12:30:33.705] iteration 2744: total_loss: 0.162205, loss_sup: 0.096193, loss_mps: 0.023449, loss_cps: 0.042563
[12:30:33.851] iteration 2745: total_loss: 0.198037, loss_sup: 0.128075, loss_mps: 0.024605, loss_cps: 0.045358
[12:30:33.998] iteration 2746: total_loss: 0.374367, loss_sup: 0.314238, loss_mps: 0.021759, loss_cps: 0.038370
[12:30:34.151] iteration 2747: total_loss: 0.559172, loss_sup: 0.489354, loss_mps: 0.025820, loss_cps: 0.043998
[12:30:34.296] iteration 2748: total_loss: 0.184544, loss_sup: 0.119152, loss_mps: 0.024008, loss_cps: 0.041384
[12:30:34.444] iteration 2749: total_loss: 0.265619, loss_sup: 0.191611, loss_mps: 0.026368, loss_cps: 0.047641
[12:30:34.590] iteration 2750: total_loss: 0.359554, loss_sup: 0.284750, loss_mps: 0.026819, loss_cps: 0.047985
[12:30:34.737] iteration 2751: total_loss: 0.358001, loss_sup: 0.292699, loss_mps: 0.024544, loss_cps: 0.040757
[12:30:34.883] iteration 2752: total_loss: 0.335134, loss_sup: 0.264113, loss_mps: 0.026849, loss_cps: 0.044172
[12:30:35.030] iteration 2753: total_loss: 0.194311, loss_sup: 0.130403, loss_mps: 0.024168, loss_cps: 0.039740
[12:30:35.176] iteration 2754: total_loss: 0.382643, loss_sup: 0.318745, loss_mps: 0.024427, loss_cps: 0.039471
[12:30:35.321] iteration 2755: total_loss: 0.251640, loss_sup: 0.191877, loss_mps: 0.022515, loss_cps: 0.037248
[12:30:35.468] iteration 2756: total_loss: 0.256083, loss_sup: 0.159554, loss_mps: 0.032885, loss_cps: 0.063644
[12:30:35.614] iteration 2757: total_loss: 0.715528, loss_sup: 0.625040, loss_mps: 0.030796, loss_cps: 0.059692
[12:30:35.759] iteration 2758: total_loss: 0.220872, loss_sup: 0.167276, loss_mps: 0.020949, loss_cps: 0.032648
[12:30:35.905] iteration 2759: total_loss: 0.286765, loss_sup: 0.208755, loss_mps: 0.027949, loss_cps: 0.050061
[12:30:36.055] iteration 2760: total_loss: 0.604813, loss_sup: 0.522102, loss_mps: 0.029995, loss_cps: 0.052717
[12:30:36.200] iteration 2761: total_loss: 0.322639, loss_sup: 0.254658, loss_mps: 0.025852, loss_cps: 0.042129
[12:30:36.347] iteration 2762: total_loss: 0.348367, loss_sup: 0.266664, loss_mps: 0.029806, loss_cps: 0.051897
[12:30:36.492] iteration 2763: total_loss: 0.232351, loss_sup: 0.148939, loss_mps: 0.030420, loss_cps: 0.052992
[12:30:36.640] iteration 2764: total_loss: 0.319736, loss_sup: 0.245708, loss_mps: 0.027550, loss_cps: 0.046478
[12:30:36.786] iteration 2765: total_loss: 0.212727, loss_sup: 0.133832, loss_mps: 0.029121, loss_cps: 0.049774
[12:30:36.932] iteration 2766: total_loss: 0.331299, loss_sup: 0.259424, loss_mps: 0.027315, loss_cps: 0.044560
[12:30:37.080] iteration 2767: total_loss: 0.271575, loss_sup: 0.206110, loss_mps: 0.025520, loss_cps: 0.039946
[12:30:37.226] iteration 2768: total_loss: 0.257300, loss_sup: 0.195090, loss_mps: 0.024784, loss_cps: 0.037426
[12:30:37.372] iteration 2769: total_loss: 0.253559, loss_sup: 0.191771, loss_mps: 0.024549, loss_cps: 0.037239
[12:30:37.517] iteration 2770: total_loss: 0.314341, loss_sup: 0.237949, loss_mps: 0.028339, loss_cps: 0.048053
[12:30:37.663] iteration 2771: total_loss: 0.192086, loss_sup: 0.135915, loss_mps: 0.021606, loss_cps: 0.034565
[12:30:37.808] iteration 2772: total_loss: 0.383805, loss_sup: 0.311090, loss_mps: 0.027257, loss_cps: 0.045459
[12:30:37.955] iteration 2773: total_loss: 0.202098, loss_sup: 0.139796, loss_mps: 0.023065, loss_cps: 0.039237
[12:30:38.101] iteration 2774: total_loss: 0.251244, loss_sup: 0.192260, loss_mps: 0.022551, loss_cps: 0.036433
[12:30:38.247] iteration 2775: total_loss: 0.416290, loss_sup: 0.316058, loss_mps: 0.034395, loss_cps: 0.065837
[12:30:38.393] iteration 2776: total_loss: 0.511763, loss_sup: 0.432148, loss_mps: 0.028234, loss_cps: 0.051380
[12:30:38.539] iteration 2777: total_loss: 0.170696, loss_sup: 0.109991, loss_mps: 0.022548, loss_cps: 0.038157
[12:30:38.685] iteration 2778: total_loss: 0.453856, loss_sup: 0.388024, loss_mps: 0.024161, loss_cps: 0.041671
[12:30:38.830] iteration 2779: total_loss: 0.369466, loss_sup: 0.291957, loss_mps: 0.028522, loss_cps: 0.048987
[12:30:38.976] iteration 2780: total_loss: 0.260378, loss_sup: 0.203197, loss_mps: 0.022297, loss_cps: 0.034884
[12:30:39.124] iteration 2781: total_loss: 0.304484, loss_sup: 0.237184, loss_mps: 0.024008, loss_cps: 0.043292
[12:30:39.270] iteration 2782: total_loss: 0.383030, loss_sup: 0.308086, loss_mps: 0.026870, loss_cps: 0.048073
[12:30:39.416] iteration 2783: total_loss: 0.785808, loss_sup: 0.720877, loss_mps: 0.024794, loss_cps: 0.040137
[12:30:39.562] iteration 2784: total_loss: 0.310545, loss_sup: 0.235155, loss_mps: 0.026858, loss_cps: 0.048532
[12:30:39.708] iteration 2785: total_loss: 0.326043, loss_sup: 0.263970, loss_mps: 0.023517, loss_cps: 0.038556
[12:30:39.854] iteration 2786: total_loss: 0.190106, loss_sup: 0.141015, loss_mps: 0.018997, loss_cps: 0.030095
[12:30:40.000] iteration 2787: total_loss: 0.620165, loss_sup: 0.534030, loss_mps: 0.029893, loss_cps: 0.056242
[12:30:40.145] iteration 2788: total_loss: 0.286223, loss_sup: 0.206643, loss_mps: 0.028222, loss_cps: 0.051359
[12:30:40.291] iteration 2789: total_loss: 0.298618, loss_sup: 0.238455, loss_mps: 0.023077, loss_cps: 0.037086
[12:30:40.436] iteration 2790: total_loss: 0.389797, loss_sup: 0.328185, loss_mps: 0.023456, loss_cps: 0.038156
[12:30:40.584] iteration 2791: total_loss: 0.309945, loss_sup: 0.230328, loss_mps: 0.027666, loss_cps: 0.051951
[12:30:40.730] iteration 2792: total_loss: 0.199688, loss_sup: 0.141303, loss_mps: 0.023035, loss_cps: 0.035351
[12:30:40.876] iteration 2793: total_loss: 0.311660, loss_sup: 0.235805, loss_mps: 0.027982, loss_cps: 0.047873
[12:30:41.022] iteration 2794: total_loss: 0.352420, loss_sup: 0.259147, loss_mps: 0.033050, loss_cps: 0.060223
[12:30:41.167] iteration 2795: total_loss: 0.565064, loss_sup: 0.499866, loss_mps: 0.024916, loss_cps: 0.040283
[12:30:41.313] iteration 2796: total_loss: 0.426120, loss_sup: 0.356398, loss_mps: 0.027402, loss_cps: 0.042320
[12:30:41.458] iteration 2797: total_loss: 0.278110, loss_sup: 0.214549, loss_mps: 0.024594, loss_cps: 0.038967
[12:30:41.603] iteration 2798: total_loss: 0.342220, loss_sup: 0.282887, loss_mps: 0.023368, loss_cps: 0.035965
[12:30:41.749] iteration 2799: total_loss: 0.445700, loss_sup: 0.360415, loss_mps: 0.031420, loss_cps: 0.053865
[12:30:41.899] iteration 2800: total_loss: 0.447668, loss_sup: 0.363314, loss_mps: 0.030174, loss_cps: 0.054180
[12:30:41.899] Evaluation Started ==>
[12:30:53.226] ==> valid iteration 2800: unet metrics: {'dc': 0.513177314566523, 'jc': 0.3868176924364428, 'pre': 0.6259275354367287, 'hd': 6.690142344272338}, ynet metrics: {'dc': 0.4751190242534015, 'jc': 0.35644314021273155, 'pre': 0.5514470401173994, 'hd': 7.23957029089301}.
[12:30:53.290] ==> New best valid dice for unet: 0.513177, at iteration 2800
[12:30:53.456] ==> New best valid dice for ynet: 0.475119, at iteration 2800
[12:30:53.457] Evaluation Finished!⏹️
[12:30:53.609] iteration 2801: total_loss: 0.538830, loss_sup: 0.462472, loss_mps: 0.028192, loss_cps: 0.048165
[12:30:53.756] iteration 2802: total_loss: 0.424641, loss_sup: 0.347872, loss_mps: 0.028961, loss_cps: 0.047808
[12:30:53.902] iteration 2803: total_loss: 0.464395, loss_sup: 0.379848, loss_mps: 0.031792, loss_cps: 0.052755
[12:30:54.047] iteration 2804: total_loss: 0.295042, loss_sup: 0.239668, loss_mps: 0.023183, loss_cps: 0.032191
[12:30:54.193] iteration 2805: total_loss: 0.284656, loss_sup: 0.214104, loss_mps: 0.027453, loss_cps: 0.043099
[12:30:54.338] iteration 2806: total_loss: 0.368750, loss_sup: 0.298990, loss_mps: 0.028217, loss_cps: 0.041543
[12:30:54.483] iteration 2807: total_loss: 0.349049, loss_sup: 0.277150, loss_mps: 0.028578, loss_cps: 0.043321
[12:30:54.630] iteration 2808: total_loss: 0.325940, loss_sup: 0.251941, loss_mps: 0.029286, loss_cps: 0.044713
[12:30:54.775] iteration 2809: total_loss: 0.278077, loss_sup: 0.209839, loss_mps: 0.027737, loss_cps: 0.040502
[12:30:54.922] iteration 2810: total_loss: 0.419253, loss_sup: 0.345615, loss_mps: 0.027685, loss_cps: 0.045952
[12:30:55.067] iteration 2811: total_loss: 0.266880, loss_sup: 0.198897, loss_mps: 0.026742, loss_cps: 0.041240
[12:30:55.214] iteration 2812: total_loss: 0.240232, loss_sup: 0.179337, loss_mps: 0.024442, loss_cps: 0.036453
[12:30:55.360] iteration 2813: total_loss: 0.250664, loss_sup: 0.190294, loss_mps: 0.024977, loss_cps: 0.035393
[12:30:55.506] iteration 2814: total_loss: 0.450549, loss_sup: 0.364032, loss_mps: 0.031586, loss_cps: 0.054931
[12:30:55.659] iteration 2815: total_loss: 0.330030, loss_sup: 0.235450, loss_mps: 0.033605, loss_cps: 0.060975
[12:30:55.805] iteration 2816: total_loss: 0.157864, loss_sup: 0.091844, loss_mps: 0.025151, loss_cps: 0.040869
[12:30:55.952] iteration 2817: total_loss: 0.327438, loss_sup: 0.264896, loss_mps: 0.023824, loss_cps: 0.038717
[12:30:56.098] iteration 2818: total_loss: 0.208523, loss_sup: 0.129774, loss_mps: 0.027782, loss_cps: 0.050967
[12:30:56.244] iteration 2819: total_loss: 0.337783, loss_sup: 0.265566, loss_mps: 0.027833, loss_cps: 0.044385
[12:30:56.390] iteration 2820: total_loss: 0.184382, loss_sup: 0.120710, loss_mps: 0.024407, loss_cps: 0.039266
[12:30:56.535] iteration 2821: total_loss: 0.149576, loss_sup: 0.093515, loss_mps: 0.021796, loss_cps: 0.034264
[12:30:56.684] iteration 2822: total_loss: 0.285025, loss_sup: 0.232521, loss_mps: 0.020373, loss_cps: 0.032131
[12:30:56.832] iteration 2823: total_loss: 0.216688, loss_sup: 0.129665, loss_mps: 0.030367, loss_cps: 0.056655
[12:30:56.977] iteration 2824: total_loss: 0.298274, loss_sup: 0.239306, loss_mps: 0.022032, loss_cps: 0.036935
[12:30:57.123] iteration 2825: total_loss: 0.259835, loss_sup: 0.186626, loss_mps: 0.025850, loss_cps: 0.047358
[12:30:57.268] iteration 2826: total_loss: 0.496287, loss_sup: 0.435365, loss_mps: 0.022366, loss_cps: 0.038556
[12:30:57.416] iteration 2827: total_loss: 0.151470, loss_sup: 0.093910, loss_mps: 0.021537, loss_cps: 0.036024
[12:30:57.563] iteration 2828: total_loss: 0.375665, loss_sup: 0.310778, loss_mps: 0.023229, loss_cps: 0.041658
[12:30:57.709] iteration 2829: total_loss: 0.225539, loss_sup: 0.163951, loss_mps: 0.022578, loss_cps: 0.039010
[12:30:57.858] iteration 2830: total_loss: 0.625299, loss_sup: 0.543602, loss_mps: 0.028050, loss_cps: 0.053646
[12:30:58.003] iteration 2831: total_loss: 0.221247, loss_sup: 0.149844, loss_mps: 0.025578, loss_cps: 0.045824
[12:30:58.149] iteration 2832: total_loss: 0.332234, loss_sup: 0.273701, loss_mps: 0.021532, loss_cps: 0.037001
[12:30:58.294] iteration 2833: total_loss: 0.341190, loss_sup: 0.274694, loss_mps: 0.023915, loss_cps: 0.042581
[12:30:58.441] iteration 2834: total_loss: 0.391390, loss_sup: 0.330942, loss_mps: 0.022544, loss_cps: 0.037903
[12:30:58.586] iteration 2835: total_loss: 0.263744, loss_sup: 0.198729, loss_mps: 0.023895, loss_cps: 0.041120
[12:30:58.733] iteration 2836: total_loss: 0.333473, loss_sup: 0.256301, loss_mps: 0.027053, loss_cps: 0.050120
[12:30:58.878] iteration 2837: total_loss: 0.295219, loss_sup: 0.215713, loss_mps: 0.028709, loss_cps: 0.050796
[12:30:59.025] iteration 2838: total_loss: 0.438464, loss_sup: 0.353801, loss_mps: 0.029439, loss_cps: 0.055224
[12:30:59.170] iteration 2839: total_loss: 0.620066, loss_sup: 0.528581, loss_mps: 0.030582, loss_cps: 0.060903
[12:30:59.317] iteration 2840: total_loss: 0.243358, loss_sup: 0.182471, loss_mps: 0.023070, loss_cps: 0.037817
[12:30:59.464] iteration 2841: total_loss: 0.279778, loss_sup: 0.216411, loss_mps: 0.024913, loss_cps: 0.038454
[12:30:59.609] iteration 2842: total_loss: 0.196137, loss_sup: 0.130178, loss_mps: 0.024567, loss_cps: 0.041391
[12:30:59.754] iteration 2843: total_loss: 0.330293, loss_sup: 0.258978, loss_mps: 0.026281, loss_cps: 0.045034
[12:30:59.900] iteration 2844: total_loss: 0.360835, loss_sup: 0.283474, loss_mps: 0.027661, loss_cps: 0.049699
[12:31:00.045] iteration 2845: total_loss: 0.323308, loss_sup: 0.255756, loss_mps: 0.025046, loss_cps: 0.042507
[12:31:00.193] iteration 2846: total_loss: 0.320122, loss_sup: 0.256683, loss_mps: 0.024226, loss_cps: 0.039212
[12:31:00.338] iteration 2847: total_loss: 0.429026, loss_sup: 0.373355, loss_mps: 0.021561, loss_cps: 0.034109
[12:31:00.486] iteration 2848: total_loss: 0.340685, loss_sup: 0.276387, loss_mps: 0.024940, loss_cps: 0.039358
[12:31:00.632] iteration 2849: total_loss: 0.283114, loss_sup: 0.225784, loss_mps: 0.022778, loss_cps: 0.034552
[12:31:00.778] iteration 2850: total_loss: 0.224368, loss_sup: 0.163980, loss_mps: 0.023384, loss_cps: 0.037004
[12:31:00.923] iteration 2851: total_loss: 0.400753, loss_sup: 0.326979, loss_mps: 0.027888, loss_cps: 0.045887
[12:31:01.071] iteration 2852: total_loss: 0.328492, loss_sup: 0.270291, loss_mps: 0.024376, loss_cps: 0.033825
[12:31:01.217] iteration 2853: total_loss: 0.358433, loss_sup: 0.282567, loss_mps: 0.028018, loss_cps: 0.047849
[12:31:01.363] iteration 2854: total_loss: 0.357443, loss_sup: 0.290446, loss_mps: 0.025086, loss_cps: 0.041912
[12:31:01.509] iteration 2855: total_loss: 0.210908, loss_sup: 0.136336, loss_mps: 0.027489, loss_cps: 0.047083
[12:31:01.657] iteration 2856: total_loss: 0.504170, loss_sup: 0.419248, loss_mps: 0.030354, loss_cps: 0.054567
[12:31:01.803] iteration 2857: total_loss: 0.667833, loss_sup: 0.585669, loss_mps: 0.029451, loss_cps: 0.052712
[12:31:01.951] iteration 2858: total_loss: 0.363950, loss_sup: 0.261694, loss_mps: 0.035805, loss_cps: 0.066451
[12:31:02.097] iteration 2859: total_loss: 0.368890, loss_sup: 0.289584, loss_mps: 0.030047, loss_cps: 0.049259
[12:31:02.243] iteration 2860: total_loss: 0.270496, loss_sup: 0.203299, loss_mps: 0.025773, loss_cps: 0.041424
[12:31:02.389] iteration 2861: total_loss: 0.418041, loss_sup: 0.342657, loss_mps: 0.027547, loss_cps: 0.047837
[12:31:02.536] iteration 2862: total_loss: 0.353713, loss_sup: 0.278315, loss_mps: 0.027638, loss_cps: 0.047759
[12:31:02.682] iteration 2863: total_loss: 0.400158, loss_sup: 0.324878, loss_mps: 0.029700, loss_cps: 0.045580
[12:31:02.828] iteration 2864: total_loss: 0.353948, loss_sup: 0.279777, loss_mps: 0.029091, loss_cps: 0.045080
[12:31:02.974] iteration 2865: total_loss: 0.330638, loss_sup: 0.243176, loss_mps: 0.031753, loss_cps: 0.055709
[12:31:03.121] iteration 2866: total_loss: 0.249549, loss_sup: 0.183074, loss_mps: 0.025623, loss_cps: 0.040852
[12:31:03.272] iteration 2867: total_loss: 0.332160, loss_sup: 0.244115, loss_mps: 0.032653, loss_cps: 0.055392
[12:31:03.418] iteration 2868: total_loss: 0.336958, loss_sup: 0.262508, loss_mps: 0.028971, loss_cps: 0.045478
[12:31:03.565] iteration 2869: total_loss: 0.696878, loss_sup: 0.609903, loss_mps: 0.031671, loss_cps: 0.055304
[12:31:03.712] iteration 2870: total_loss: 0.310440, loss_sup: 0.238853, loss_mps: 0.027766, loss_cps: 0.043820
[12:31:03.858] iteration 2871: total_loss: 0.366114, loss_sup: 0.296234, loss_mps: 0.027034, loss_cps: 0.042846
[12:31:04.004] iteration 2872: total_loss: 0.368689, loss_sup: 0.299969, loss_mps: 0.027292, loss_cps: 0.041428
[12:31:04.151] iteration 2873: total_loss: 0.253736, loss_sup: 0.175955, loss_mps: 0.030025, loss_cps: 0.047755
[12:31:04.297] iteration 2874: total_loss: 0.471259, loss_sup: 0.367792, loss_mps: 0.036377, loss_cps: 0.067091
[12:31:04.444] iteration 2875: total_loss: 0.333347, loss_sup: 0.245978, loss_mps: 0.032274, loss_cps: 0.055095
[12:31:04.589] iteration 2876: total_loss: 0.244767, loss_sup: 0.171180, loss_mps: 0.029308, loss_cps: 0.044280
[12:31:04.736] iteration 2877: total_loss: 0.301331, loss_sup: 0.230521, loss_mps: 0.027621, loss_cps: 0.043189
[12:31:04.882] iteration 2878: total_loss: 0.493204, loss_sup: 0.398995, loss_mps: 0.034244, loss_cps: 0.059965
[12:31:05.027] iteration 2879: total_loss: 0.404655, loss_sup: 0.328212, loss_mps: 0.027955, loss_cps: 0.048487
[12:31:05.174] iteration 2880: total_loss: 0.327041, loss_sup: 0.271949, loss_mps: 0.022054, loss_cps: 0.033038
[12:31:05.322] iteration 2881: total_loss: 0.384796, loss_sup: 0.291357, loss_mps: 0.033670, loss_cps: 0.059770
[12:31:05.467] iteration 2882: total_loss: 0.446522, loss_sup: 0.369303, loss_mps: 0.028349, loss_cps: 0.048870
[12:31:05.613] iteration 2883: total_loss: 0.196529, loss_sup: 0.132805, loss_mps: 0.023969, loss_cps: 0.039755
[12:31:05.760] iteration 2884: total_loss: 0.396335, loss_sup: 0.326490, loss_mps: 0.025815, loss_cps: 0.044031
[12:31:05.906] iteration 2885: total_loss: 0.288895, loss_sup: 0.188594, loss_mps: 0.035075, loss_cps: 0.065226
[12:31:06.052] iteration 2886: total_loss: 0.250067, loss_sup: 0.189839, loss_mps: 0.022695, loss_cps: 0.037533
[12:31:06.198] iteration 2887: total_loss: 0.507939, loss_sup: 0.423220, loss_mps: 0.028767, loss_cps: 0.055953
[12:31:06.346] iteration 2888: total_loss: 0.260857, loss_sup: 0.198500, loss_mps: 0.022994, loss_cps: 0.039363
[12:31:06.495] iteration 2889: total_loss: 0.310138, loss_sup: 0.258268, loss_mps: 0.021112, loss_cps: 0.030758
[12:31:06.641] iteration 2890: total_loss: 0.317332, loss_sup: 0.244798, loss_mps: 0.027106, loss_cps: 0.045428
[12:31:06.787] iteration 2891: total_loss: 0.634461, loss_sup: 0.532832, loss_mps: 0.035513, loss_cps: 0.066116
[12:31:06.935] iteration 2892: total_loss: 0.249763, loss_sup: 0.169102, loss_mps: 0.028934, loss_cps: 0.051727
[12:31:07.082] iteration 2893: total_loss: 0.408620, loss_sup: 0.333394, loss_mps: 0.026415, loss_cps: 0.048811
[12:31:07.228] iteration 2894: total_loss: 0.380109, loss_sup: 0.323413, loss_mps: 0.022797, loss_cps: 0.033899
[12:31:07.374] iteration 2895: total_loss: 0.266812, loss_sup: 0.197751, loss_mps: 0.025570, loss_cps: 0.043492
[12:31:07.521] iteration 2896: total_loss: 0.382272, loss_sup: 0.324924, loss_mps: 0.021871, loss_cps: 0.035476
[12:31:07.666] iteration 2897: total_loss: 0.236400, loss_sup: 0.155599, loss_mps: 0.028786, loss_cps: 0.052015
[12:31:07.812] iteration 2898: total_loss: 0.391903, loss_sup: 0.293326, loss_mps: 0.034092, loss_cps: 0.064486
[12:31:07.958] iteration 2899: total_loss: 0.287294, loss_sup: 0.223650, loss_mps: 0.024189, loss_cps: 0.039455
[12:31:08.104] iteration 2900: total_loss: 0.287955, loss_sup: 0.218440, loss_mps: 0.025352, loss_cps: 0.044163
[12:31:08.104] Evaluation Started ==>
[12:31:19.420] ==> valid iteration 2900: unet metrics: {'dc': 0.48492268396739346, 'jc': 0.36878431130993977, 'pre': 0.5841776521912658, 'hd': 6.811343961292629}, ynet metrics: {'dc': 0.43481818036952247, 'jc': 0.32386039974687814, 'pre': 0.5816965473640786, 'hd': 6.877387919796202}.
[12:31:19.422] Evaluation Finished!⏹️
[12:31:19.572] iteration 2901: total_loss: 0.211717, loss_sup: 0.154913, loss_mps: 0.021597, loss_cps: 0.035208
[12:31:19.739] iteration 2902: total_loss: 0.523377, loss_sup: 0.454143, loss_mps: 0.026416, loss_cps: 0.042818
[12:31:19.885] iteration 2903: total_loss: 0.141363, loss_sup: 0.079501, loss_mps: 0.023072, loss_cps: 0.038790
[12:31:20.030] iteration 2904: total_loss: 0.282618, loss_sup: 0.214548, loss_mps: 0.024396, loss_cps: 0.043673
[12:31:20.175] iteration 2905: total_loss: 0.220090, loss_sup: 0.166265, loss_mps: 0.020335, loss_cps: 0.033489
[12:31:20.321] iteration 2906: total_loss: 0.628231, loss_sup: 0.514723, loss_mps: 0.037736, loss_cps: 0.075772
[12:31:20.466] iteration 2907: total_loss: 0.274023, loss_sup: 0.214997, loss_mps: 0.021274, loss_cps: 0.037752
[12:31:20.611] iteration 2908: total_loss: 0.424978, loss_sup: 0.371035, loss_mps: 0.021304, loss_cps: 0.032639
[12:31:20.757] iteration 2909: total_loss: 0.437959, loss_sup: 0.368630, loss_mps: 0.024947, loss_cps: 0.044382
[12:31:20.903] iteration 2910: total_loss: 0.353697, loss_sup: 0.285164, loss_mps: 0.025221, loss_cps: 0.043312
[12:31:21.049] iteration 2911: total_loss: 0.297408, loss_sup: 0.224757, loss_mps: 0.026433, loss_cps: 0.046218
[12:31:21.195] iteration 2912: total_loss: 0.241240, loss_sup: 0.170644, loss_mps: 0.026113, loss_cps: 0.044483
[12:31:21.340] iteration 2913: total_loss: 0.370211, loss_sup: 0.279469, loss_mps: 0.032163, loss_cps: 0.058579
[12:31:21.485] iteration 2914: total_loss: 0.457588, loss_sup: 0.387085, loss_mps: 0.026762, loss_cps: 0.043740
[12:31:21.632] iteration 2915: total_loss: 0.335950, loss_sup: 0.251848, loss_mps: 0.029760, loss_cps: 0.054342
[12:31:21.777] iteration 2916: total_loss: 0.202816, loss_sup: 0.120745, loss_mps: 0.029931, loss_cps: 0.052140
[12:31:21.923] iteration 2917: total_loss: 0.224951, loss_sup: 0.152581, loss_mps: 0.026718, loss_cps: 0.045652
[12:31:22.068] iteration 2918: total_loss: 0.576856, loss_sup: 0.468457, loss_mps: 0.036903, loss_cps: 0.071496
[12:31:22.213] iteration 2919: total_loss: 0.390855, loss_sup: 0.309418, loss_mps: 0.029928, loss_cps: 0.051509
[12:31:22.358] iteration 2920: total_loss: 0.217304, loss_sup: 0.161635, loss_mps: 0.022332, loss_cps: 0.033337
[12:31:22.503] iteration 2921: total_loss: 0.400725, loss_sup: 0.300677, loss_mps: 0.034015, loss_cps: 0.066034
[12:31:22.649] iteration 2922: total_loss: 0.308111, loss_sup: 0.238436, loss_mps: 0.026504, loss_cps: 0.043170
[12:31:22.794] iteration 2923: total_loss: 0.368460, loss_sup: 0.274305, loss_mps: 0.033001, loss_cps: 0.061154
[12:31:22.939] iteration 2924: total_loss: 0.280761, loss_sup: 0.193148, loss_mps: 0.031773, loss_cps: 0.055840
[12:31:23.085] iteration 2925: total_loss: 0.279712, loss_sup: 0.180475, loss_mps: 0.034634, loss_cps: 0.064603
[12:31:23.145] iteration 2926: total_loss: 0.275439, loss_sup: 0.211940, loss_mps: 0.022787, loss_cps: 0.040712
[12:31:24.381] iteration 2927: total_loss: 0.380116, loss_sup: 0.292404, loss_mps: 0.031507, loss_cps: 0.056206
[12:31:24.530] iteration 2928: total_loss: 0.170094, loss_sup: 0.089073, loss_mps: 0.029752, loss_cps: 0.051269
[12:31:24.676] iteration 2929: total_loss: 0.357700, loss_sup: 0.261575, loss_mps: 0.034219, loss_cps: 0.061907
[12:31:24.826] iteration 2930: total_loss: 0.600529, loss_sup: 0.502804, loss_mps: 0.033166, loss_cps: 0.064560
[12:31:24.973] iteration 2931: total_loss: 0.317968, loss_sup: 0.233431, loss_mps: 0.030531, loss_cps: 0.054006
[12:31:25.121] iteration 2932: total_loss: 0.300888, loss_sup: 0.210050, loss_mps: 0.031530, loss_cps: 0.059308
[12:31:25.268] iteration 2933: total_loss: 0.389687, loss_sup: 0.311307, loss_mps: 0.028776, loss_cps: 0.049604
[12:31:25.417] iteration 2934: total_loss: 0.154943, loss_sup: 0.079525, loss_mps: 0.027653, loss_cps: 0.047764
[12:31:25.563] iteration 2935: total_loss: 0.307966, loss_sup: 0.235479, loss_mps: 0.025864, loss_cps: 0.046623
[12:31:25.709] iteration 2936: total_loss: 0.286048, loss_sup: 0.214163, loss_mps: 0.026913, loss_cps: 0.044972
[12:31:25.854] iteration 2937: total_loss: 0.176870, loss_sup: 0.114035, loss_mps: 0.024359, loss_cps: 0.038476
[12:31:26.001] iteration 2938: total_loss: 0.279506, loss_sup: 0.205387, loss_mps: 0.026600, loss_cps: 0.047519
[12:31:26.151] iteration 2939: total_loss: 0.398904, loss_sup: 0.298842, loss_mps: 0.034087, loss_cps: 0.065975
[12:31:26.297] iteration 2940: total_loss: 0.311680, loss_sup: 0.249320, loss_mps: 0.023715, loss_cps: 0.038645
[12:31:26.445] iteration 2941: total_loss: 0.326138, loss_sup: 0.253110, loss_mps: 0.026528, loss_cps: 0.046500
[12:31:26.596] iteration 2942: total_loss: 0.346649, loss_sup: 0.244802, loss_mps: 0.033603, loss_cps: 0.068243
[12:31:26.742] iteration 2943: total_loss: 0.328901, loss_sup: 0.263698, loss_mps: 0.024510, loss_cps: 0.040694
[12:31:26.888] iteration 2944: total_loss: 0.180388, loss_sup: 0.118992, loss_mps: 0.024209, loss_cps: 0.037187
[12:31:27.034] iteration 2945: total_loss: 0.379480, loss_sup: 0.310271, loss_mps: 0.025832, loss_cps: 0.043378
[12:31:27.186] iteration 2946: total_loss: 0.485445, loss_sup: 0.407136, loss_mps: 0.028046, loss_cps: 0.050263
[12:31:27.332] iteration 2947: total_loss: 0.270434, loss_sup: 0.186552, loss_mps: 0.030007, loss_cps: 0.053876
[12:31:27.478] iteration 2948: total_loss: 0.284984, loss_sup: 0.219419, loss_mps: 0.024603, loss_cps: 0.040962
[12:31:27.624] iteration 2949: total_loss: 0.275604, loss_sup: 0.202050, loss_mps: 0.027046, loss_cps: 0.046508
[12:31:27.773] iteration 2950: total_loss: 0.273536, loss_sup: 0.202937, loss_mps: 0.025523, loss_cps: 0.045075
[12:31:27.920] iteration 2951: total_loss: 0.287507, loss_sup: 0.225556, loss_mps: 0.023846, loss_cps: 0.038105
[12:31:28.068] iteration 2952: total_loss: 0.279393, loss_sup: 0.211509, loss_mps: 0.025465, loss_cps: 0.042419
[12:31:28.220] iteration 2953: total_loss: 0.316296, loss_sup: 0.232310, loss_mps: 0.029790, loss_cps: 0.054196
[12:31:28.368] iteration 2954: total_loss: 0.382144, loss_sup: 0.298548, loss_mps: 0.030442, loss_cps: 0.053153
[12:31:28.514] iteration 2955: total_loss: 0.418587, loss_sup: 0.341630, loss_mps: 0.028078, loss_cps: 0.048879
[12:31:28.661] iteration 2956: total_loss: 0.304325, loss_sup: 0.233922, loss_mps: 0.027467, loss_cps: 0.042935
[12:31:28.808] iteration 2957: total_loss: 0.287062, loss_sup: 0.215300, loss_mps: 0.026839, loss_cps: 0.044923
[12:31:28.955] iteration 2958: total_loss: 0.216330, loss_sup: 0.143652, loss_mps: 0.027615, loss_cps: 0.045064
[12:31:29.102] iteration 2959: total_loss: 0.261414, loss_sup: 0.198947, loss_mps: 0.024126, loss_cps: 0.038341
[12:31:29.249] iteration 2960: total_loss: 0.238486, loss_sup: 0.166516, loss_mps: 0.027672, loss_cps: 0.044298
[12:31:29.396] iteration 2961: total_loss: 0.383121, loss_sup: 0.316979, loss_mps: 0.024754, loss_cps: 0.041388
[12:31:29.542] iteration 2962: total_loss: 0.273976, loss_sup: 0.177175, loss_mps: 0.033232, loss_cps: 0.063569
[12:31:29.689] iteration 2963: total_loss: 0.299435, loss_sup: 0.215399, loss_mps: 0.030182, loss_cps: 0.053854
[12:31:29.837] iteration 2964: total_loss: 0.233515, loss_sup: 0.150880, loss_mps: 0.030074, loss_cps: 0.052561
[12:31:29.983] iteration 2965: total_loss: 0.325460, loss_sup: 0.253400, loss_mps: 0.026845, loss_cps: 0.045215
[12:31:30.129] iteration 2966: total_loss: 0.237360, loss_sup: 0.168670, loss_mps: 0.026685, loss_cps: 0.042005
[12:31:30.276] iteration 2967: total_loss: 0.277294, loss_sup: 0.199034, loss_mps: 0.028315, loss_cps: 0.049945
[12:31:30.423] iteration 2968: total_loss: 0.621765, loss_sup: 0.542275, loss_mps: 0.029079, loss_cps: 0.050411
[12:31:30.569] iteration 2969: total_loss: 0.239164, loss_sup: 0.164190, loss_mps: 0.027673, loss_cps: 0.047302
[12:31:30.716] iteration 2970: total_loss: 0.194083, loss_sup: 0.135594, loss_mps: 0.022523, loss_cps: 0.035965
[12:31:30.862] iteration 2971: total_loss: 0.253211, loss_sup: 0.186803, loss_mps: 0.025232, loss_cps: 0.041175
[12:31:31.008] iteration 2972: total_loss: 0.372494, loss_sup: 0.301507, loss_mps: 0.027360, loss_cps: 0.043627
[12:31:31.154] iteration 2973: total_loss: 0.274128, loss_sup: 0.210846, loss_mps: 0.025230, loss_cps: 0.038052
[12:31:31.302] iteration 2974: total_loss: 0.183529, loss_sup: 0.127980, loss_mps: 0.022618, loss_cps: 0.032931
[12:31:31.448] iteration 2975: total_loss: 0.251958, loss_sup: 0.201464, loss_mps: 0.019842, loss_cps: 0.030651
[12:31:31.600] iteration 2976: total_loss: 0.271842, loss_sup: 0.184432, loss_mps: 0.029890, loss_cps: 0.057521
[12:31:31.750] iteration 2977: total_loss: 0.335598, loss_sup: 0.268721, loss_mps: 0.025486, loss_cps: 0.041391
[12:31:31.896] iteration 2978: total_loss: 0.299387, loss_sup: 0.223484, loss_mps: 0.026770, loss_cps: 0.049133
[12:31:32.043] iteration 2979: total_loss: 0.347544, loss_sup: 0.246151, loss_mps: 0.033956, loss_cps: 0.067438
[12:31:32.191] iteration 2980: total_loss: 0.434836, loss_sup: 0.340417, loss_mps: 0.032405, loss_cps: 0.062013
[12:31:32.340] iteration 2981: total_loss: 0.246176, loss_sup: 0.141152, loss_mps: 0.035292, loss_cps: 0.069733
[12:31:32.487] iteration 2982: total_loss: 0.278874, loss_sup: 0.182688, loss_mps: 0.033155, loss_cps: 0.063031
[12:31:32.633] iteration 2983: total_loss: 0.273842, loss_sup: 0.195749, loss_mps: 0.027139, loss_cps: 0.050953
[12:31:32.782] iteration 2984: total_loss: 0.362869, loss_sup: 0.264910, loss_mps: 0.032662, loss_cps: 0.065297
[12:31:32.928] iteration 2985: total_loss: 0.355011, loss_sup: 0.288867, loss_mps: 0.023840, loss_cps: 0.042303
[12:31:33.075] iteration 2986: total_loss: 0.334917, loss_sup: 0.255044, loss_mps: 0.028183, loss_cps: 0.051690
[12:31:33.221] iteration 2987: total_loss: 0.133650, loss_sup: 0.072722, loss_mps: 0.022363, loss_cps: 0.038565
[12:31:33.369] iteration 2988: total_loss: 0.201035, loss_sup: 0.141388, loss_mps: 0.022422, loss_cps: 0.037225
[12:31:33.515] iteration 2989: total_loss: 0.441471, loss_sup: 0.339004, loss_mps: 0.034618, loss_cps: 0.067850
[12:31:33.662] iteration 2990: total_loss: 0.348631, loss_sup: 0.276719, loss_mps: 0.025412, loss_cps: 0.046499
[12:31:33.808] iteration 2991: total_loss: 0.333029, loss_sup: 0.283640, loss_mps: 0.019162, loss_cps: 0.030227
[12:31:33.955] iteration 2992: total_loss: 0.224841, loss_sup: 0.134246, loss_mps: 0.032140, loss_cps: 0.058456
[12:31:34.109] iteration 2993: total_loss: 0.237582, loss_sup: 0.166312, loss_mps: 0.026856, loss_cps: 0.044414
[12:31:34.256] iteration 2994: total_loss: 0.173533, loss_sup: 0.103190, loss_mps: 0.026257, loss_cps: 0.044086
[12:31:34.404] iteration 2995: total_loss: 0.204635, loss_sup: 0.147485, loss_mps: 0.021955, loss_cps: 0.035194
[12:31:34.551] iteration 2996: total_loss: 0.429425, loss_sup: 0.360948, loss_mps: 0.024887, loss_cps: 0.043590
[12:31:34.697] iteration 2997: total_loss: 0.340356, loss_sup: 0.280319, loss_mps: 0.022428, loss_cps: 0.037609
[12:31:34.844] iteration 2998: total_loss: 0.231059, loss_sup: 0.162039, loss_mps: 0.024240, loss_cps: 0.044780
[12:31:34.991] iteration 2999: total_loss: 0.283598, loss_sup: 0.212974, loss_mps: 0.026391, loss_cps: 0.044233
[12:31:35.137] iteration 3000: total_loss: 0.269960, loss_sup: 0.187159, loss_mps: 0.028646, loss_cps: 0.054155
[12:31:35.137] Evaluation Started ==>
[12:31:46.464] ==> valid iteration 3000: unet metrics: {'dc': 0.5360723257110965, 'jc': 0.41314741537434213, 'pre': 0.5649610621407434, 'hd': 7.122491781538204}, ynet metrics: {'dc': 0.4783534191477425, 'jc': 0.36024089491897254, 'pre': 0.5810076460566012, 'hd': 7.074511908114369}.
[12:31:46.529] ==> New best valid dice for unet: 0.536072, at iteration 3000
[12:31:46.691] ==> New best valid dice for ynet: 0.478353, at iteration 3000
[12:31:46.693] Evaluation Finished!⏹️
[12:31:46.846] iteration 3001: total_loss: 0.174533, loss_sup: 0.098835, loss_mps: 0.027242, loss_cps: 0.048456
[12:31:46.993] iteration 3002: total_loss: 0.126430, loss_sup: 0.056285, loss_mps: 0.025163, loss_cps: 0.044981
[12:31:47.138] iteration 3003: total_loss: 0.376435, loss_sup: 0.294392, loss_mps: 0.029625, loss_cps: 0.052418
[12:31:47.283] iteration 3004: total_loss: 0.228725, loss_sup: 0.157877, loss_mps: 0.025139, loss_cps: 0.045710
[12:31:47.430] iteration 3005: total_loss: 0.371957, loss_sup: 0.298611, loss_mps: 0.025217, loss_cps: 0.048128
[12:31:47.575] iteration 3006: total_loss: 0.239884, loss_sup: 0.192560, loss_mps: 0.018071, loss_cps: 0.029253
[12:31:47.720] iteration 3007: total_loss: 0.237781, loss_sup: 0.172116, loss_mps: 0.023427, loss_cps: 0.042238
[12:31:47.866] iteration 3008: total_loss: 0.240518, loss_sup: 0.175940, loss_mps: 0.023205, loss_cps: 0.041374
[12:31:48.011] iteration 3009: total_loss: 0.293227, loss_sup: 0.234065, loss_mps: 0.021129, loss_cps: 0.038032
[12:31:48.156] iteration 3010: total_loss: 0.167881, loss_sup: 0.107305, loss_mps: 0.022152, loss_cps: 0.038424
[12:31:48.303] iteration 3011: total_loss: 0.184310, loss_sup: 0.133754, loss_mps: 0.019081, loss_cps: 0.031475
[12:31:48.449] iteration 3012: total_loss: 0.187694, loss_sup: 0.122065, loss_mps: 0.022605, loss_cps: 0.043024
[12:31:48.594] iteration 3013: total_loss: 0.476462, loss_sup: 0.400495, loss_mps: 0.027039, loss_cps: 0.048927
[12:31:48.741] iteration 3014: total_loss: 0.786927, loss_sup: 0.694783, loss_mps: 0.030861, loss_cps: 0.061284
[12:31:48.887] iteration 3015: total_loss: 0.258323, loss_sup: 0.199237, loss_mps: 0.020937, loss_cps: 0.038149
[12:31:49.033] iteration 3016: total_loss: 0.487788, loss_sup: 0.419195, loss_mps: 0.024556, loss_cps: 0.044036
[12:31:49.179] iteration 3017: total_loss: 0.411377, loss_sup: 0.358326, loss_mps: 0.020100, loss_cps: 0.032952
[12:31:49.324] iteration 3018: total_loss: 0.295806, loss_sup: 0.248000, loss_mps: 0.018052, loss_cps: 0.029754
[12:31:49.470] iteration 3019: total_loss: 0.254227, loss_sup: 0.203924, loss_mps: 0.018857, loss_cps: 0.031445
[12:31:49.621] iteration 3020: total_loss: 0.180955, loss_sup: 0.114357, loss_mps: 0.023442, loss_cps: 0.043156
[12:31:49.767] iteration 3021: total_loss: 0.420250, loss_sup: 0.335713, loss_mps: 0.029573, loss_cps: 0.054964
[12:31:49.914] iteration 3022: total_loss: 0.426653, loss_sup: 0.350129, loss_mps: 0.027286, loss_cps: 0.049238
[12:31:50.066] iteration 3023: total_loss: 0.209263, loss_sup: 0.136827, loss_mps: 0.026732, loss_cps: 0.045704
[12:31:50.211] iteration 3024: total_loss: 0.250362, loss_sup: 0.179963, loss_mps: 0.026424, loss_cps: 0.043975
[12:31:50.362] iteration 3025: total_loss: 0.199938, loss_sup: 0.125725, loss_mps: 0.027430, loss_cps: 0.046783
[12:31:50.507] iteration 3026: total_loss: 0.225243, loss_sup: 0.159828, loss_mps: 0.024039, loss_cps: 0.041377
[12:31:50.653] iteration 3027: total_loss: 0.517448, loss_sup: 0.412150, loss_mps: 0.036336, loss_cps: 0.068963
[12:31:50.798] iteration 3028: total_loss: 0.309966, loss_sup: 0.220377, loss_mps: 0.031956, loss_cps: 0.057633
[12:31:50.945] iteration 3029: total_loss: 0.217366, loss_sup: 0.137610, loss_mps: 0.029470, loss_cps: 0.050286
[12:31:51.092] iteration 3030: total_loss: 0.423431, loss_sup: 0.337283, loss_mps: 0.031212, loss_cps: 0.054937
[12:31:51.237] iteration 3031: total_loss: 0.229581, loss_sup: 0.173310, loss_mps: 0.022656, loss_cps: 0.033614
[12:31:51.383] iteration 3032: total_loss: 0.419011, loss_sup: 0.325194, loss_mps: 0.033443, loss_cps: 0.060374
[12:31:51.529] iteration 3033: total_loss: 0.248960, loss_sup: 0.170511, loss_mps: 0.029487, loss_cps: 0.048961
[12:31:51.677] iteration 3034: total_loss: 0.267899, loss_sup: 0.197378, loss_mps: 0.026078, loss_cps: 0.044442
[12:31:51.824] iteration 3035: total_loss: 0.288526, loss_sup: 0.219271, loss_mps: 0.025724, loss_cps: 0.043531
[12:31:51.973] iteration 3036: total_loss: 0.235691, loss_sup: 0.171441, loss_mps: 0.024370, loss_cps: 0.039881
[12:31:52.118] iteration 3037: total_loss: 0.410438, loss_sup: 0.311245, loss_mps: 0.034231, loss_cps: 0.064962
[12:31:52.265] iteration 3038: total_loss: 0.393888, loss_sup: 0.308582, loss_mps: 0.031184, loss_cps: 0.054121
[12:31:52.411] iteration 3039: total_loss: 0.370360, loss_sup: 0.272604, loss_mps: 0.033835, loss_cps: 0.063921
[12:31:52.557] iteration 3040: total_loss: 0.423285, loss_sup: 0.330802, loss_mps: 0.032047, loss_cps: 0.060436
[12:31:52.705] iteration 3041: total_loss: 0.407546, loss_sup: 0.308180, loss_mps: 0.035219, loss_cps: 0.064146
[12:31:52.852] iteration 3042: total_loss: 0.471545, loss_sup: 0.382257, loss_mps: 0.031913, loss_cps: 0.057375
[12:31:52.998] iteration 3043: total_loss: 0.225300, loss_sup: 0.138823, loss_mps: 0.030252, loss_cps: 0.056224
[12:31:53.144] iteration 3044: total_loss: 0.418704, loss_sup: 0.324079, loss_mps: 0.033352, loss_cps: 0.061273
[12:31:53.290] iteration 3045: total_loss: 0.380935, loss_sup: 0.296870, loss_mps: 0.030821, loss_cps: 0.053244
[12:31:53.437] iteration 3046: total_loss: 0.162792, loss_sup: 0.088053, loss_mps: 0.028400, loss_cps: 0.046339
[12:31:53.584] iteration 3047: total_loss: 0.274063, loss_sup: 0.198324, loss_mps: 0.029340, loss_cps: 0.046399
[12:31:53.734] iteration 3048: total_loss: 0.290441, loss_sup: 0.200772, loss_mps: 0.032001, loss_cps: 0.057668
[12:31:53.880] iteration 3049: total_loss: 0.434313, loss_sup: 0.346920, loss_mps: 0.032138, loss_cps: 0.055255
[12:31:54.025] iteration 3050: total_loss: 0.330710, loss_sup: 0.256092, loss_mps: 0.028477, loss_cps: 0.046142
[12:31:54.171] iteration 3051: total_loss: 0.389579, loss_sup: 0.301733, loss_mps: 0.031549, loss_cps: 0.056297
[12:31:54.317] iteration 3052: total_loss: 0.245132, loss_sup: 0.172751, loss_mps: 0.027481, loss_cps: 0.044900
[12:31:54.465] iteration 3053: total_loss: 0.245651, loss_sup: 0.182572, loss_mps: 0.023695, loss_cps: 0.039384
[12:31:54.611] iteration 3054: total_loss: 0.267437, loss_sup: 0.212480, loss_mps: 0.021509, loss_cps: 0.033448
[12:31:54.757] iteration 3055: total_loss: 0.248461, loss_sup: 0.185814, loss_mps: 0.024359, loss_cps: 0.038289
[12:31:54.904] iteration 3056: total_loss: 0.263687, loss_sup: 0.185124, loss_mps: 0.029509, loss_cps: 0.049053
[12:31:55.049] iteration 3057: total_loss: 0.172555, loss_sup: 0.106390, loss_mps: 0.025453, loss_cps: 0.040712
[12:31:55.195] iteration 3058: total_loss: 0.251606, loss_sup: 0.182257, loss_mps: 0.026179, loss_cps: 0.043170
[12:31:55.341] iteration 3059: total_loss: 0.360050, loss_sup: 0.291444, loss_mps: 0.026851, loss_cps: 0.041756
[12:31:55.487] iteration 3060: total_loss: 0.502304, loss_sup: 0.402260, loss_mps: 0.035019, loss_cps: 0.065024
[12:31:55.632] iteration 3061: total_loss: 0.385261, loss_sup: 0.313465, loss_mps: 0.026618, loss_cps: 0.045179
[12:31:55.778] iteration 3062: total_loss: 0.195266, loss_sup: 0.141500, loss_mps: 0.021487, loss_cps: 0.032279
[12:31:55.924] iteration 3063: total_loss: 0.402428, loss_sup: 0.339446, loss_mps: 0.024312, loss_cps: 0.038671
[12:31:56.069] iteration 3064: total_loss: 0.389035, loss_sup: 0.326813, loss_mps: 0.023854, loss_cps: 0.038369
[12:31:56.215] iteration 3065: total_loss: 0.153347, loss_sup: 0.094809, loss_mps: 0.022931, loss_cps: 0.035606
[12:31:56.361] iteration 3066: total_loss: 0.157363, loss_sup: 0.092651, loss_mps: 0.024970, loss_cps: 0.039742
[12:31:56.507] iteration 3067: total_loss: 0.407396, loss_sup: 0.336782, loss_mps: 0.026530, loss_cps: 0.044084
[12:31:56.652] iteration 3068: total_loss: 0.215338, loss_sup: 0.166406, loss_mps: 0.019568, loss_cps: 0.029364
[12:31:56.799] iteration 3069: total_loss: 0.253588, loss_sup: 0.177461, loss_mps: 0.028120, loss_cps: 0.048007
[12:31:56.945] iteration 3070: total_loss: 0.236468, loss_sup: 0.159389, loss_mps: 0.028961, loss_cps: 0.048118
[12:31:57.091] iteration 3071: total_loss: 0.465972, loss_sup: 0.380592, loss_mps: 0.030289, loss_cps: 0.055091
[12:31:57.237] iteration 3072: total_loss: 0.213344, loss_sup: 0.152183, loss_mps: 0.023073, loss_cps: 0.038088
[12:31:57.383] iteration 3073: total_loss: 0.182959, loss_sup: 0.119257, loss_mps: 0.023206, loss_cps: 0.040496
[12:31:57.528] iteration 3074: total_loss: 0.731243, loss_sup: 0.646446, loss_mps: 0.030486, loss_cps: 0.054311
[12:31:57.674] iteration 3075: total_loss: 0.416024, loss_sup: 0.307698, loss_mps: 0.036401, loss_cps: 0.071925
[12:31:57.819] iteration 3076: total_loss: 0.302541, loss_sup: 0.230460, loss_mps: 0.027127, loss_cps: 0.044954
[12:31:57.968] iteration 3077: total_loss: 0.283168, loss_sup: 0.223055, loss_mps: 0.022462, loss_cps: 0.037651
[12:31:58.113] iteration 3078: total_loss: 0.434757, loss_sup: 0.361904, loss_mps: 0.027966, loss_cps: 0.044887
[12:31:58.258] iteration 3079: total_loss: 0.247790, loss_sup: 0.194083, loss_mps: 0.021073, loss_cps: 0.032635
[12:31:58.404] iteration 3080: total_loss: 0.256748, loss_sup: 0.178422, loss_mps: 0.028099, loss_cps: 0.050227
[12:31:58.549] iteration 3081: total_loss: 0.189229, loss_sup: 0.134183, loss_mps: 0.021492, loss_cps: 0.033553
[12:31:58.695] iteration 3082: total_loss: 0.271923, loss_sup: 0.196466, loss_mps: 0.028014, loss_cps: 0.047443
[12:31:58.841] iteration 3083: total_loss: 0.122669, loss_sup: 0.061806, loss_mps: 0.023086, loss_cps: 0.037776
[12:31:58.986] iteration 3084: total_loss: 0.276775, loss_sup: 0.211351, loss_mps: 0.025170, loss_cps: 0.040253
[12:31:59.131] iteration 3085: total_loss: 0.233922, loss_sup: 0.164650, loss_mps: 0.025801, loss_cps: 0.043470
[12:31:59.277] iteration 3086: total_loss: 0.256152, loss_sup: 0.192180, loss_mps: 0.024609, loss_cps: 0.039363
[12:31:59.423] iteration 3087: total_loss: 0.288439, loss_sup: 0.206757, loss_mps: 0.028593, loss_cps: 0.053089
[12:31:59.568] iteration 3088: total_loss: 0.504383, loss_sup: 0.405071, loss_mps: 0.033476, loss_cps: 0.065836
[12:31:59.713] iteration 3089: total_loss: 0.318609, loss_sup: 0.235467, loss_mps: 0.030010, loss_cps: 0.053132
[12:31:59.859] iteration 3090: total_loss: 0.150991, loss_sup: 0.096228, loss_mps: 0.021310, loss_cps: 0.033453
[12:32:00.004] iteration 3091: total_loss: 0.184755, loss_sup: 0.123514, loss_mps: 0.023894, loss_cps: 0.037346
[12:32:00.156] iteration 3092: total_loss: 0.169741, loss_sup: 0.116834, loss_mps: 0.020978, loss_cps: 0.031929
[12:32:00.302] iteration 3093: total_loss: 0.162355, loss_sup: 0.114814, loss_mps: 0.019052, loss_cps: 0.028489
[12:32:00.449] iteration 3094: total_loss: 0.322985, loss_sup: 0.243515, loss_mps: 0.027730, loss_cps: 0.051740
[12:32:00.595] iteration 3095: total_loss: 0.669838, loss_sup: 0.559191, loss_mps: 0.037050, loss_cps: 0.073597
[12:32:00.742] iteration 3096: total_loss: 0.476501, loss_sup: 0.395503, loss_mps: 0.028207, loss_cps: 0.052790
[12:32:00.888] iteration 3097: total_loss: 0.285023, loss_sup: 0.222118, loss_mps: 0.023299, loss_cps: 0.039606
[12:32:01.038] iteration 3098: total_loss: 0.168488, loss_sup: 0.118054, loss_mps: 0.019686, loss_cps: 0.030748
[12:32:01.185] iteration 3099: total_loss: 0.174540, loss_sup: 0.111447, loss_mps: 0.023268, loss_cps: 0.039824
[12:32:01.331] iteration 3100: total_loss: 0.287946, loss_sup: 0.212705, loss_mps: 0.027161, loss_cps: 0.048080
[12:32:01.331] Evaluation Started ==>
[12:32:12.639] ==> valid iteration 3100: unet metrics: {'dc': 0.521440674756108, 'jc': 0.3975326951808629, 'pre': 0.6095361388741822, 'hd': 6.841712757255738}, ynet metrics: {'dc': 0.45446613552743587, 'jc': 0.33981101555505544, 'pre': 0.6279990432917264, 'hd': 6.774450038268785}.
[12:32:12.641] Evaluation Finished!⏹️
[12:32:12.791] iteration 3101: total_loss: 0.379477, loss_sup: 0.321708, loss_mps: 0.021516, loss_cps: 0.036253
[12:32:12.943] iteration 3102: total_loss: 0.248572, loss_sup: 0.175819, loss_mps: 0.026697, loss_cps: 0.046056
[12:32:13.089] iteration 3103: total_loss: 0.205968, loss_sup: 0.145359, loss_mps: 0.022794, loss_cps: 0.037815
[12:32:13.236] iteration 3104: total_loss: 0.331859, loss_sup: 0.267632, loss_mps: 0.023678, loss_cps: 0.040549
[12:32:13.381] iteration 3105: total_loss: 0.216751, loss_sup: 0.152040, loss_mps: 0.023956, loss_cps: 0.040756
[12:32:13.526] iteration 3106: total_loss: 0.257941, loss_sup: 0.168549, loss_mps: 0.030579, loss_cps: 0.058812
[12:32:13.672] iteration 3107: total_loss: 0.611194, loss_sup: 0.532509, loss_mps: 0.028240, loss_cps: 0.050445
[12:32:13.817] iteration 3108: total_loss: 0.424678, loss_sup: 0.328111, loss_mps: 0.032516, loss_cps: 0.064051
[12:32:13.963] iteration 3109: total_loss: 0.281708, loss_sup: 0.226746, loss_mps: 0.020702, loss_cps: 0.034260
[12:32:14.108] iteration 3110: total_loss: 0.288606, loss_sup: 0.221310, loss_mps: 0.024596, loss_cps: 0.042700
[12:32:14.254] iteration 3111: total_loss: 0.391799, loss_sup: 0.308247, loss_mps: 0.029496, loss_cps: 0.054056
[12:32:14.401] iteration 3112: total_loss: 0.198238, loss_sup: 0.141035, loss_mps: 0.022576, loss_cps: 0.034628
[12:32:14.547] iteration 3113: total_loss: 0.568801, loss_sup: 0.477859, loss_mps: 0.031889, loss_cps: 0.059052
[12:32:14.693] iteration 3114: total_loss: 0.230278, loss_sup: 0.157469, loss_mps: 0.026475, loss_cps: 0.046334
[12:32:14.839] iteration 3115: total_loss: 0.218387, loss_sup: 0.143859, loss_mps: 0.027619, loss_cps: 0.046909
[12:32:14.987] iteration 3116: total_loss: 0.411559, loss_sup: 0.346866, loss_mps: 0.024100, loss_cps: 0.040593
[12:32:15.132] iteration 3117: total_loss: 0.208734, loss_sup: 0.150026, loss_mps: 0.022565, loss_cps: 0.036143
[12:32:15.278] iteration 3118: total_loss: 0.211561, loss_sup: 0.127548, loss_mps: 0.030399, loss_cps: 0.053615
[12:32:15.424] iteration 3119: total_loss: 0.415061, loss_sup: 0.319224, loss_mps: 0.033998, loss_cps: 0.061839
[12:32:15.573] iteration 3120: total_loss: 0.226410, loss_sup: 0.146423, loss_mps: 0.028970, loss_cps: 0.051017
[12:32:15.720] iteration 3121: total_loss: 0.472461, loss_sup: 0.375118, loss_mps: 0.034050, loss_cps: 0.063293
[12:32:15.866] iteration 3122: total_loss: 0.276159, loss_sup: 0.203975, loss_mps: 0.025739, loss_cps: 0.046445
[12:32:16.015] iteration 3123: total_loss: 0.325452, loss_sup: 0.244309, loss_mps: 0.030303, loss_cps: 0.050840
[12:32:16.162] iteration 3124: total_loss: 0.142214, loss_sup: 0.080250, loss_mps: 0.022788, loss_cps: 0.039177
[12:32:16.308] iteration 3125: total_loss: 0.206962, loss_sup: 0.117139, loss_mps: 0.031744, loss_cps: 0.058079
[12:32:16.454] iteration 3126: total_loss: 0.283458, loss_sup: 0.221600, loss_mps: 0.023554, loss_cps: 0.038304
[12:32:16.599] iteration 3127: total_loss: 0.213086, loss_sup: 0.144649, loss_mps: 0.025197, loss_cps: 0.043240
[12:32:16.745] iteration 3128: total_loss: 0.179662, loss_sup: 0.097655, loss_mps: 0.029468, loss_cps: 0.052539
[12:32:16.891] iteration 3129: total_loss: 0.395368, loss_sup: 0.317132, loss_mps: 0.027726, loss_cps: 0.050510
[12:32:17.037] iteration 3130: total_loss: 0.272834, loss_sup: 0.203919, loss_mps: 0.025750, loss_cps: 0.043166
[12:32:17.182] iteration 3131: total_loss: 0.400474, loss_sup: 0.345250, loss_mps: 0.021652, loss_cps: 0.033572
[12:32:17.330] iteration 3132: total_loss: 0.400271, loss_sup: 0.338162, loss_mps: 0.023334, loss_cps: 0.038776
[12:32:17.483] iteration 3133: total_loss: 0.182110, loss_sup: 0.106908, loss_mps: 0.027417, loss_cps: 0.047785
[12:32:17.629] iteration 3134: total_loss: 0.586041, loss_sup: 0.475837, loss_mps: 0.037409, loss_cps: 0.072794
[12:32:17.775] iteration 3135: total_loss: 0.390426, loss_sup: 0.319466, loss_mps: 0.025116, loss_cps: 0.045844
[12:32:17.922] iteration 3136: total_loss: 0.317622, loss_sup: 0.235143, loss_mps: 0.028491, loss_cps: 0.053988
[12:32:18.068] iteration 3137: total_loss: 0.203978, loss_sup: 0.148176, loss_mps: 0.021123, loss_cps: 0.034679
[12:32:18.215] iteration 3138: total_loss: 0.332854, loss_sup: 0.250833, loss_mps: 0.028829, loss_cps: 0.053192
[12:32:18.361] iteration 3139: total_loss: 0.301012, loss_sup: 0.214958, loss_mps: 0.030935, loss_cps: 0.055118
[12:32:18.508] iteration 3140: total_loss: 0.156738, loss_sup: 0.081221, loss_mps: 0.026758, loss_cps: 0.048760
[12:32:18.657] iteration 3141: total_loss: 0.254833, loss_sup: 0.170228, loss_mps: 0.029765, loss_cps: 0.054840
[12:32:18.803] iteration 3142: total_loss: 0.329124, loss_sup: 0.262007, loss_mps: 0.024920, loss_cps: 0.042197
[12:32:18.949] iteration 3143: total_loss: 0.153812, loss_sup: 0.106731, loss_mps: 0.019282, loss_cps: 0.027800
[12:32:19.096] iteration 3144: total_loss: 0.370562, loss_sup: 0.284017, loss_mps: 0.030501, loss_cps: 0.056044
[12:32:19.242] iteration 3145: total_loss: 0.233803, loss_sup: 0.163944, loss_mps: 0.024905, loss_cps: 0.044954
[12:32:19.388] iteration 3146: total_loss: 0.244680, loss_sup: 0.155434, loss_mps: 0.031161, loss_cps: 0.058085
[12:32:19.536] iteration 3147: total_loss: 0.185986, loss_sup: 0.106990, loss_mps: 0.028200, loss_cps: 0.050797
[12:32:19.683] iteration 3148: total_loss: 0.189018, loss_sup: 0.125092, loss_mps: 0.023054, loss_cps: 0.040872
[12:32:19.833] iteration 3149: total_loss: 0.387613, loss_sup: 0.309468, loss_mps: 0.029142, loss_cps: 0.049003
[12:32:19.979] iteration 3150: total_loss: 0.220662, loss_sup: 0.154425, loss_mps: 0.024636, loss_cps: 0.041601
[12:32:20.124] iteration 3151: total_loss: 0.221456, loss_sup: 0.172181, loss_mps: 0.019167, loss_cps: 0.030109
[12:32:20.270] iteration 3152: total_loss: 0.266754, loss_sup: 0.199943, loss_mps: 0.024670, loss_cps: 0.042140
[12:32:20.416] iteration 3153: total_loss: 0.352122, loss_sup: 0.277342, loss_mps: 0.026502, loss_cps: 0.048278
[12:32:20.563] iteration 3154: total_loss: 0.170353, loss_sup: 0.114174, loss_mps: 0.021565, loss_cps: 0.034614
[12:32:20.708] iteration 3155: total_loss: 0.636502, loss_sup: 0.519694, loss_mps: 0.038505, loss_cps: 0.078303
[12:32:20.854] iteration 3156: total_loss: 0.323168, loss_sup: 0.252711, loss_mps: 0.025657, loss_cps: 0.044799
[12:32:21.000] iteration 3157: total_loss: 0.272302, loss_sup: 0.191420, loss_mps: 0.029105, loss_cps: 0.051777
[12:32:21.146] iteration 3158: total_loss: 0.311481, loss_sup: 0.248663, loss_mps: 0.023421, loss_cps: 0.039397
[12:32:21.292] iteration 3159: total_loss: 0.283103, loss_sup: 0.196781, loss_mps: 0.030345, loss_cps: 0.055977
[12:32:21.437] iteration 3160: total_loss: 0.507488, loss_sup: 0.396475, loss_mps: 0.036496, loss_cps: 0.074517
[12:32:21.583] iteration 3161: total_loss: 0.390862, loss_sup: 0.309930, loss_mps: 0.028477, loss_cps: 0.052455
[12:32:21.729] iteration 3162: total_loss: 0.300704, loss_sup: 0.224099, loss_mps: 0.028595, loss_cps: 0.048011
[12:32:21.875] iteration 3163: total_loss: 0.345634, loss_sup: 0.241721, loss_mps: 0.037002, loss_cps: 0.066912
[12:32:22.024] iteration 3164: total_loss: 0.283240, loss_sup: 0.207118, loss_mps: 0.027266, loss_cps: 0.048856
[12:32:22.171] iteration 3165: total_loss: 0.267813, loss_sup: 0.169089, loss_mps: 0.034610, loss_cps: 0.064114
[12:32:22.317] iteration 3166: total_loss: 0.229756, loss_sup: 0.149445, loss_mps: 0.029140, loss_cps: 0.051171
[12:32:22.463] iteration 3167: total_loss: 0.465233, loss_sup: 0.387358, loss_mps: 0.029678, loss_cps: 0.048196
[12:32:22.608] iteration 3168: total_loss: 0.482729, loss_sup: 0.399497, loss_mps: 0.029952, loss_cps: 0.053281
[12:32:22.754] iteration 3169: total_loss: 0.205478, loss_sup: 0.126111, loss_mps: 0.030266, loss_cps: 0.049101
[12:32:22.899] iteration 3170: total_loss: 0.243334, loss_sup: 0.181831, loss_mps: 0.023529, loss_cps: 0.037975
[12:32:23.045] iteration 3171: total_loss: 0.340100, loss_sup: 0.275256, loss_mps: 0.024633, loss_cps: 0.040212
[12:32:23.193] iteration 3172: total_loss: 0.209345, loss_sup: 0.149024, loss_mps: 0.023407, loss_cps: 0.036914
[12:32:23.339] iteration 3173: total_loss: 0.398331, loss_sup: 0.322400, loss_mps: 0.028312, loss_cps: 0.047618
[12:32:23.485] iteration 3174: total_loss: 0.340770, loss_sup: 0.261912, loss_mps: 0.029351, loss_cps: 0.049507
[12:32:23.631] iteration 3175: total_loss: 0.297562, loss_sup: 0.219818, loss_mps: 0.027844, loss_cps: 0.049900
[12:32:23.778] iteration 3176: total_loss: 0.420305, loss_sup: 0.347832, loss_mps: 0.026407, loss_cps: 0.046067
[12:32:23.924] iteration 3177: total_loss: 0.292792, loss_sup: 0.217329, loss_mps: 0.027136, loss_cps: 0.048327
[12:32:24.070] iteration 3178: total_loss: 0.401776, loss_sup: 0.331154, loss_mps: 0.025369, loss_cps: 0.045253
[12:32:24.215] iteration 3179: total_loss: 0.330066, loss_sup: 0.236936, loss_mps: 0.032313, loss_cps: 0.060818
[12:32:24.364] iteration 3180: total_loss: 0.337146, loss_sup: 0.268912, loss_mps: 0.026052, loss_cps: 0.042182
[12:32:24.509] iteration 3181: total_loss: 0.278656, loss_sup: 0.195957, loss_mps: 0.030030, loss_cps: 0.052669
[12:32:24.655] iteration 3182: total_loss: 0.126301, loss_sup: 0.066452, loss_mps: 0.022738, loss_cps: 0.037112
[12:32:24.801] iteration 3183: total_loss: 0.373127, loss_sup: 0.306140, loss_mps: 0.024533, loss_cps: 0.042455
[12:32:24.947] iteration 3184: total_loss: 0.234413, loss_sup: 0.166039, loss_mps: 0.025523, loss_cps: 0.042852
[12:32:25.093] iteration 3185: total_loss: 0.750894, loss_sup: 0.646290, loss_mps: 0.035536, loss_cps: 0.069068
[12:32:25.239] iteration 3186: total_loss: 0.168933, loss_sup: 0.087845, loss_mps: 0.030373, loss_cps: 0.050714
[12:32:25.385] iteration 3187: total_loss: 0.202871, loss_sup: 0.131424, loss_mps: 0.026791, loss_cps: 0.044656
[12:32:25.531] iteration 3188: total_loss: 0.353588, loss_sup: 0.259342, loss_mps: 0.033429, loss_cps: 0.060818
[12:32:25.676] iteration 3189: total_loss: 0.320764, loss_sup: 0.209886, loss_mps: 0.037557, loss_cps: 0.073321
[12:32:25.822] iteration 3190: total_loss: 0.310571, loss_sup: 0.243179, loss_mps: 0.025367, loss_cps: 0.042025
[12:32:25.967] iteration 3191: total_loss: 0.355557, loss_sup: 0.287301, loss_mps: 0.025834, loss_cps: 0.042422
[12:32:26.113] iteration 3192: total_loss: 0.368797, loss_sup: 0.309835, loss_mps: 0.022477, loss_cps: 0.036485
[12:32:26.259] iteration 3193: total_loss: 0.404583, loss_sup: 0.331016, loss_mps: 0.026750, loss_cps: 0.046817
[12:32:26.404] iteration 3194: total_loss: 0.304726, loss_sup: 0.254584, loss_mps: 0.019926, loss_cps: 0.030217
[12:32:26.550] iteration 3195: total_loss: 0.150313, loss_sup: 0.082379, loss_mps: 0.025021, loss_cps: 0.042912
[12:32:26.695] iteration 3196: total_loss: 0.372461, loss_sup: 0.309799, loss_mps: 0.024582, loss_cps: 0.038080
[12:32:26.842] iteration 3197: total_loss: 0.360295, loss_sup: 0.297171, loss_mps: 0.023121, loss_cps: 0.040004
[12:32:26.989] iteration 3198: total_loss: 0.144885, loss_sup: 0.083267, loss_mps: 0.024206, loss_cps: 0.037413
[12:32:27.136] iteration 3199: total_loss: 0.315563, loss_sup: 0.246338, loss_mps: 0.027472, loss_cps: 0.041753
[12:32:27.282] iteration 3200: total_loss: 0.325623, loss_sup: 0.262296, loss_mps: 0.024666, loss_cps: 0.038661
[12:32:27.282] Evaluation Started ==>
[12:32:38.557] ==> valid iteration 3200: unet metrics: {'dc': 0.5421468226321526, 'jc': 0.4184171490736094, 'pre': 0.6261530135478365, 'hd': 6.722482311954015}, ynet metrics: {'dc': 0.47454884861809266, 'jc': 0.3608934268227692, 'pre': 0.5902976528177682, 'hd': 6.941704002560329}.
[12:32:38.624] ==> New best valid dice for unet: 0.542147, at iteration 3200
[12:32:38.626] Evaluation Finished!⏹️
[12:32:38.779] iteration 3201: total_loss: 0.301607, loss_sup: 0.236934, loss_mps: 0.024464, loss_cps: 0.040210
[12:32:38.927] iteration 3202: total_loss: 0.192991, loss_sup: 0.137115, loss_mps: 0.021665, loss_cps: 0.034212
[12:32:39.072] iteration 3203: total_loss: 0.204034, loss_sup: 0.155896, loss_mps: 0.019871, loss_cps: 0.028267
[12:32:39.216] iteration 3204: total_loss: 0.176974, loss_sup: 0.130794, loss_mps: 0.019165, loss_cps: 0.027015
[12:32:39.363] iteration 3205: total_loss: 0.275701, loss_sup: 0.216694, loss_mps: 0.022980, loss_cps: 0.036027
[12:32:39.509] iteration 3206: total_loss: 0.438017, loss_sup: 0.360721, loss_mps: 0.027696, loss_cps: 0.049600
[12:32:39.654] iteration 3207: total_loss: 0.177183, loss_sup: 0.111527, loss_mps: 0.024748, loss_cps: 0.040909
[12:32:39.798] iteration 3208: total_loss: 0.242515, loss_sup: 0.170196, loss_mps: 0.026404, loss_cps: 0.045915
[12:32:39.944] iteration 3209: total_loss: 0.154391, loss_sup: 0.096757, loss_mps: 0.022635, loss_cps: 0.034999
[12:32:40.089] iteration 3210: total_loss: 0.463620, loss_sup: 0.372942, loss_mps: 0.031732, loss_cps: 0.058946
[12:32:40.234] iteration 3211: total_loss: 0.249658, loss_sup: 0.183569, loss_mps: 0.024646, loss_cps: 0.041442
[12:32:40.379] iteration 3212: total_loss: 0.209904, loss_sup: 0.153145, loss_mps: 0.021111, loss_cps: 0.035648
[12:32:40.526] iteration 3213: total_loss: 0.239590, loss_sup: 0.168517, loss_mps: 0.025635, loss_cps: 0.045437
[12:32:40.672] iteration 3214: total_loss: 0.448915, loss_sup: 0.363387, loss_mps: 0.031174, loss_cps: 0.054354
[12:32:40.817] iteration 3215: total_loss: 0.221149, loss_sup: 0.148198, loss_mps: 0.025727, loss_cps: 0.047225
[12:32:40.963] iteration 3216: total_loss: 0.255705, loss_sup: 0.172083, loss_mps: 0.030025, loss_cps: 0.053597
[12:32:41.108] iteration 3217: total_loss: 0.225092, loss_sup: 0.141153, loss_mps: 0.029657, loss_cps: 0.054283
[12:32:41.254] iteration 3218: total_loss: 0.285997, loss_sup: 0.180688, loss_mps: 0.035076, loss_cps: 0.070232
[12:32:41.401] iteration 3219: total_loss: 0.423491, loss_sup: 0.336516, loss_mps: 0.030113, loss_cps: 0.056862
[12:32:41.547] iteration 3220: total_loss: 0.275829, loss_sup: 0.207684, loss_mps: 0.024802, loss_cps: 0.043343
[12:32:41.694] iteration 3221: total_loss: 0.272424, loss_sup: 0.216850, loss_mps: 0.020998, loss_cps: 0.034576
[12:32:41.841] iteration 3222: total_loss: 0.231048, loss_sup: 0.157296, loss_mps: 0.027366, loss_cps: 0.046386
[12:32:41.986] iteration 3223: total_loss: 0.225692, loss_sup: 0.160500, loss_mps: 0.024688, loss_cps: 0.040503
[12:32:42.132] iteration 3224: total_loss: 0.358602, loss_sup: 0.296940, loss_mps: 0.023136, loss_cps: 0.038526
[12:32:42.277] iteration 3225: total_loss: 0.299584, loss_sup: 0.231846, loss_mps: 0.024823, loss_cps: 0.042915
[12:32:42.423] iteration 3226: total_loss: 0.581597, loss_sup: 0.497424, loss_mps: 0.029877, loss_cps: 0.054296
[12:32:42.568] iteration 3227: total_loss: 0.262332, loss_sup: 0.196029, loss_mps: 0.024004, loss_cps: 0.042299
[12:32:42.714] iteration 3228: total_loss: 0.340491, loss_sup: 0.278720, loss_mps: 0.023252, loss_cps: 0.038520
[12:32:42.862] iteration 3229: total_loss: 0.309653, loss_sup: 0.247789, loss_mps: 0.023779, loss_cps: 0.038085
[12:32:43.007] iteration 3230: total_loss: 0.334355, loss_sup: 0.243520, loss_mps: 0.031329, loss_cps: 0.059506
[12:32:43.153] iteration 3231: total_loss: 0.294613, loss_sup: 0.193070, loss_mps: 0.035348, loss_cps: 0.066196
[12:32:43.298] iteration 3232: total_loss: 0.431568, loss_sup: 0.335356, loss_mps: 0.032614, loss_cps: 0.063598
[12:32:43.443] iteration 3233: total_loss: 0.206420, loss_sup: 0.133602, loss_mps: 0.026536, loss_cps: 0.046282
[12:32:43.589] iteration 3234: total_loss: 0.567410, loss_sup: 0.466262, loss_mps: 0.034753, loss_cps: 0.066395
[12:32:43.736] iteration 3235: total_loss: 0.187435, loss_sup: 0.133424, loss_mps: 0.020860, loss_cps: 0.033151
[12:32:43.882] iteration 3236: total_loss: 0.183976, loss_sup: 0.123639, loss_mps: 0.022071, loss_cps: 0.038265
[12:32:44.027] iteration 3237: total_loss: 0.284267, loss_sup: 0.203271, loss_mps: 0.028907, loss_cps: 0.052089
[12:32:44.173] iteration 3238: total_loss: 0.282052, loss_sup: 0.199634, loss_mps: 0.030288, loss_cps: 0.052129
[12:32:44.318] iteration 3239: total_loss: 0.199578, loss_sup: 0.118434, loss_mps: 0.029127, loss_cps: 0.052017
[12:32:44.463] iteration 3240: total_loss: 0.329436, loss_sup: 0.231947, loss_mps: 0.033224, loss_cps: 0.064264
[12:32:44.609] iteration 3241: total_loss: 0.323497, loss_sup: 0.256997, loss_mps: 0.024360, loss_cps: 0.042139
[12:32:44.754] iteration 3242: total_loss: 0.317078, loss_sup: 0.244891, loss_mps: 0.026660, loss_cps: 0.045527
[12:32:44.899] iteration 3243: total_loss: 0.389505, loss_sup: 0.311966, loss_mps: 0.028968, loss_cps: 0.048571
[12:32:45.045] iteration 3244: total_loss: 0.386688, loss_sup: 0.306937, loss_mps: 0.028755, loss_cps: 0.050996
[12:32:45.191] iteration 3245: total_loss: 0.364044, loss_sup: 0.282689, loss_mps: 0.029495, loss_cps: 0.051860
[12:32:45.336] iteration 3246: total_loss: 0.327009, loss_sup: 0.253826, loss_mps: 0.026925, loss_cps: 0.046258
[12:32:45.481] iteration 3247: total_loss: 0.247768, loss_sup: 0.167962, loss_mps: 0.030441, loss_cps: 0.049365
[12:32:45.627] iteration 3248: total_loss: 0.172298, loss_sup: 0.120550, loss_mps: 0.021316, loss_cps: 0.030432
[12:32:45.772] iteration 3249: total_loss: 0.345681, loss_sup: 0.274406, loss_mps: 0.027705, loss_cps: 0.043570
[12:32:45.918] iteration 3250: total_loss: 0.272291, loss_sup: 0.181396, loss_mps: 0.032874, loss_cps: 0.058021
[12:32:46.064] iteration 3251: total_loss: 0.165679, loss_sup: 0.099803, loss_mps: 0.025475, loss_cps: 0.040401
[12:32:46.209] iteration 3252: total_loss: 0.205894, loss_sup: 0.130826, loss_mps: 0.028605, loss_cps: 0.046463
[12:32:46.355] iteration 3253: total_loss: 0.364451, loss_sup: 0.293087, loss_mps: 0.026613, loss_cps: 0.044750
[12:32:46.500] iteration 3254: total_loss: 0.182648, loss_sup: 0.121401, loss_mps: 0.023877, loss_cps: 0.037370
[12:32:46.646] iteration 3255: total_loss: 0.194806, loss_sup: 0.108576, loss_mps: 0.030793, loss_cps: 0.055438
[12:32:46.792] iteration 3256: total_loss: 0.200058, loss_sup: 0.126933, loss_mps: 0.027272, loss_cps: 0.045852
[12:32:46.939] iteration 3257: total_loss: 0.298900, loss_sup: 0.225611, loss_mps: 0.028477, loss_cps: 0.044812
[12:32:47.090] iteration 3258: total_loss: 0.279681, loss_sup: 0.210552, loss_mps: 0.025657, loss_cps: 0.043471
[12:32:47.235] iteration 3259: total_loss: 0.194552, loss_sup: 0.129709, loss_mps: 0.025619, loss_cps: 0.039224
[12:32:47.382] iteration 3260: total_loss: 0.404368, loss_sup: 0.341234, loss_mps: 0.024363, loss_cps: 0.038771
[12:32:47.527] iteration 3261: total_loss: 0.385688, loss_sup: 0.303227, loss_mps: 0.029072, loss_cps: 0.053389
[12:32:47.673] iteration 3262: total_loss: 0.313545, loss_sup: 0.243265, loss_mps: 0.025874, loss_cps: 0.044405
[12:32:47.819] iteration 3263: total_loss: 0.183800, loss_sup: 0.117716, loss_mps: 0.024220, loss_cps: 0.041864
[12:32:47.965] iteration 3264: total_loss: 0.407163, loss_sup: 0.346210, loss_mps: 0.022291, loss_cps: 0.038662
[12:32:48.111] iteration 3265: total_loss: 0.141045, loss_sup: 0.063046, loss_mps: 0.027737, loss_cps: 0.050263
[12:32:48.256] iteration 3266: total_loss: 0.326143, loss_sup: 0.256834, loss_mps: 0.025117, loss_cps: 0.044191
[12:32:48.403] iteration 3267: total_loss: 0.458471, loss_sup: 0.390087, loss_mps: 0.024406, loss_cps: 0.043978
[12:32:48.549] iteration 3268: total_loss: 0.224493, loss_sup: 0.161791, loss_mps: 0.024435, loss_cps: 0.038268
[12:32:48.695] iteration 3269: total_loss: 0.374406, loss_sup: 0.315723, loss_mps: 0.022046, loss_cps: 0.036637
[12:32:48.840] iteration 3270: total_loss: 0.338319, loss_sup: 0.247461, loss_mps: 0.030575, loss_cps: 0.060283
[12:32:48.986] iteration 3271: total_loss: 0.460240, loss_sup: 0.363051, loss_mps: 0.032909, loss_cps: 0.064279
[12:32:49.136] iteration 3272: total_loss: 0.109578, loss_sup: 0.049015, loss_mps: 0.022595, loss_cps: 0.037968
[12:32:49.282] iteration 3273: total_loss: 0.383911, loss_sup: 0.305410, loss_mps: 0.027571, loss_cps: 0.050930
[12:32:49.428] iteration 3274: total_loss: 0.406374, loss_sup: 0.310145, loss_mps: 0.033093, loss_cps: 0.063136
[12:32:49.575] iteration 3275: total_loss: 0.446429, loss_sup: 0.367159, loss_mps: 0.029246, loss_cps: 0.050024
[12:32:49.721] iteration 3276: total_loss: 0.578632, loss_sup: 0.496938, loss_mps: 0.029322, loss_cps: 0.052372
[12:32:49.868] iteration 3277: total_loss: 0.332974, loss_sup: 0.242122, loss_mps: 0.031554, loss_cps: 0.059298
[12:32:50.014] iteration 3278: total_loss: 0.358300, loss_sup: 0.287564, loss_mps: 0.025833, loss_cps: 0.044903
[12:32:50.162] iteration 3279: total_loss: 0.244616, loss_sup: 0.168109, loss_mps: 0.028417, loss_cps: 0.048090
[12:32:50.308] iteration 3280: total_loss: 0.392415, loss_sup: 0.304744, loss_mps: 0.032078, loss_cps: 0.055593
[12:32:50.453] iteration 3281: total_loss: 0.350950, loss_sup: 0.250922, loss_mps: 0.034977, loss_cps: 0.065051
[12:32:50.599] iteration 3282: total_loss: 0.283450, loss_sup: 0.202416, loss_mps: 0.030788, loss_cps: 0.050246
[12:32:50.745] iteration 3283: total_loss: 0.291460, loss_sup: 0.221591, loss_mps: 0.027788, loss_cps: 0.042081
[12:32:50.891] iteration 3284: total_loss: 0.260050, loss_sup: 0.161577, loss_mps: 0.034713, loss_cps: 0.063761
[12:32:51.038] iteration 3285: total_loss: 0.533602, loss_sup: 0.428857, loss_mps: 0.037596, loss_cps: 0.067148
[12:32:51.186] iteration 3286: total_loss: 0.315186, loss_sup: 0.228082, loss_mps: 0.032279, loss_cps: 0.054825
[12:32:51.331] iteration 3287: total_loss: 0.289410, loss_sup: 0.207376, loss_mps: 0.031432, loss_cps: 0.050601
[12:32:51.477] iteration 3288: total_loss: 0.327206, loss_sup: 0.256308, loss_mps: 0.027604, loss_cps: 0.043294
[12:32:51.623] iteration 3289: total_loss: 0.375737, loss_sup: 0.295244, loss_mps: 0.031509, loss_cps: 0.048983
[12:32:51.769] iteration 3290: total_loss: 0.281387, loss_sup: 0.193537, loss_mps: 0.033322, loss_cps: 0.054527
[12:32:51.914] iteration 3291: total_loss: 0.255816, loss_sup: 0.191202, loss_mps: 0.025559, loss_cps: 0.039055
[12:32:52.061] iteration 3292: total_loss: 0.224152, loss_sup: 0.162854, loss_mps: 0.025051, loss_cps: 0.036247
[12:32:52.210] iteration 3293: total_loss: 0.368039, loss_sup: 0.293522, loss_mps: 0.029365, loss_cps: 0.045152
[12:32:52.356] iteration 3294: total_loss: 0.270449, loss_sup: 0.193293, loss_mps: 0.028754, loss_cps: 0.048402
[12:32:52.502] iteration 3295: total_loss: 0.333966, loss_sup: 0.257742, loss_mps: 0.028440, loss_cps: 0.047784
[12:32:52.649] iteration 3296: total_loss: 0.338994, loss_sup: 0.290523, loss_mps: 0.020363, loss_cps: 0.028108
[12:32:52.796] iteration 3297: total_loss: 0.523246, loss_sup: 0.439977, loss_mps: 0.031533, loss_cps: 0.051736
[12:32:52.942] iteration 3298: total_loss: 0.175778, loss_sup: 0.114883, loss_mps: 0.024069, loss_cps: 0.036826
[12:32:53.088] iteration 3299: total_loss: 0.258646, loss_sup: 0.183345, loss_mps: 0.028304, loss_cps: 0.046996
[12:32:53.235] iteration 3300: total_loss: 0.152257, loss_sup: 0.086987, loss_mps: 0.024827, loss_cps: 0.040443
[12:32:53.235] Evaluation Started ==>
[12:33:04.575] ==> valid iteration 3300: unet metrics: {'dc': 0.5873862210971514, 'jc': 0.45627565219737004, 'pre': 0.6496011521848816, 'hd': 6.457842056189166}, ynet metrics: {'dc': 0.5027132332170847, 'jc': 0.37510396070416985, 'pre': 0.5979108525046014, 'hd': 6.931118639858435}.
[12:33:04.629] ==> New best valid dice for unet: 0.587386, at iteration 3300
[12:33:04.792] ==> New best valid dice for ynet: 0.502713, at iteration 3300
[12:33:04.794] Evaluation Finished!⏹️
[12:33:04.945] iteration 3301: total_loss: 0.255419, loss_sup: 0.181062, loss_mps: 0.027144, loss_cps: 0.047213
[12:33:05.093] iteration 3302: total_loss: 0.234843, loss_sup: 0.164950, loss_mps: 0.025240, loss_cps: 0.044654
[12:33:05.239] iteration 3303: total_loss: 0.366311, loss_sup: 0.292018, loss_mps: 0.027513, loss_cps: 0.046779
[12:33:05.385] iteration 3304: total_loss: 0.429246, loss_sup: 0.335616, loss_mps: 0.032615, loss_cps: 0.061015
[12:33:05.532] iteration 3305: total_loss: 0.251250, loss_sup: 0.154738, loss_mps: 0.033310, loss_cps: 0.063202
[12:33:05.680] iteration 3306: total_loss: 0.378354, loss_sup: 0.267470, loss_mps: 0.037112, loss_cps: 0.073772
[12:33:05.827] iteration 3307: total_loss: 0.426064, loss_sup: 0.325044, loss_mps: 0.033699, loss_cps: 0.067321
[12:33:05.974] iteration 3308: total_loss: 0.336368, loss_sup: 0.266098, loss_mps: 0.025698, loss_cps: 0.044572
[12:33:06.119] iteration 3309: total_loss: 0.179870, loss_sup: 0.078978, loss_mps: 0.033960, loss_cps: 0.066932
[12:33:06.266] iteration 3310: total_loss: 0.186377, loss_sup: 0.116276, loss_mps: 0.026315, loss_cps: 0.043787
[12:33:06.413] iteration 3311: total_loss: 0.275322, loss_sup: 0.195027, loss_mps: 0.028225, loss_cps: 0.052070
[12:33:06.559] iteration 3312: total_loss: 0.588290, loss_sup: 0.483646, loss_mps: 0.035722, loss_cps: 0.068923
[12:33:06.705] iteration 3313: total_loss: 0.149236, loss_sup: 0.071118, loss_mps: 0.028622, loss_cps: 0.049497
[12:33:06.851] iteration 3314: total_loss: 0.211806, loss_sup: 0.153120, loss_mps: 0.023255, loss_cps: 0.035431
[12:33:06.998] iteration 3315: total_loss: 0.222198, loss_sup: 0.139005, loss_mps: 0.030197, loss_cps: 0.052996
[12:33:07.144] iteration 3316: total_loss: 0.233364, loss_sup: 0.162042, loss_mps: 0.026424, loss_cps: 0.044899
[12:33:07.290] iteration 3317: total_loss: 0.359736, loss_sup: 0.295858, loss_mps: 0.024245, loss_cps: 0.039632
[12:33:07.437] iteration 3318: total_loss: 0.285415, loss_sup: 0.207550, loss_mps: 0.027923, loss_cps: 0.049942
[12:33:07.586] iteration 3319: total_loss: 0.252958, loss_sup: 0.176631, loss_mps: 0.027305, loss_cps: 0.049022
[12:33:07.734] iteration 3320: total_loss: 0.106363, loss_sup: 0.062816, loss_mps: 0.017242, loss_cps: 0.026305
[12:33:07.880] iteration 3321: total_loss: 0.235538, loss_sup: 0.178975, loss_mps: 0.020956, loss_cps: 0.035607
[12:33:08.027] iteration 3322: total_loss: 0.198005, loss_sup: 0.136294, loss_mps: 0.024238, loss_cps: 0.037473
[12:33:08.174] iteration 3323: total_loss: 0.126500, loss_sup: 0.067838, loss_mps: 0.022472, loss_cps: 0.036190
[12:33:08.321] iteration 3324: total_loss: 0.251412, loss_sup: 0.189271, loss_mps: 0.024118, loss_cps: 0.038023
[12:33:08.467] iteration 3325: total_loss: 0.355346, loss_sup: 0.292830, loss_mps: 0.023217, loss_cps: 0.039299
[12:33:08.613] iteration 3326: total_loss: 0.208136, loss_sup: 0.141640, loss_mps: 0.024603, loss_cps: 0.041893
[12:33:08.760] iteration 3327: total_loss: 0.290163, loss_sup: 0.208579, loss_mps: 0.028097, loss_cps: 0.053486
[12:33:08.906] iteration 3328: total_loss: 0.311870, loss_sup: 0.246239, loss_mps: 0.024275, loss_cps: 0.041356
[12:33:09.053] iteration 3329: total_loss: 0.303524, loss_sup: 0.229264, loss_mps: 0.027255, loss_cps: 0.047005
[12:33:09.199] iteration 3330: total_loss: 0.364499, loss_sup: 0.289377, loss_mps: 0.026657, loss_cps: 0.048465
[12:33:09.345] iteration 3331: total_loss: 0.162508, loss_sup: 0.094127, loss_mps: 0.025138, loss_cps: 0.043243
[12:33:09.491] iteration 3332: total_loss: 0.252307, loss_sup: 0.170374, loss_mps: 0.029058, loss_cps: 0.052874
[12:33:09.639] iteration 3333: total_loss: 0.524754, loss_sup: 0.459588, loss_mps: 0.023563, loss_cps: 0.041603
[12:33:09.786] iteration 3334: total_loss: 0.254173, loss_sup: 0.191311, loss_mps: 0.023374, loss_cps: 0.039488
[12:33:09.932] iteration 3335: total_loss: 0.198013, loss_sup: 0.142775, loss_mps: 0.021169, loss_cps: 0.034069
[12:33:10.078] iteration 3336: total_loss: 0.331162, loss_sup: 0.237468, loss_mps: 0.032174, loss_cps: 0.061520
[12:33:10.224] iteration 3337: total_loss: 0.499171, loss_sup: 0.430702, loss_mps: 0.026108, loss_cps: 0.042360
[12:33:10.370] iteration 3338: total_loss: 0.217297, loss_sup: 0.120892, loss_mps: 0.032953, loss_cps: 0.063452
[12:33:10.515] iteration 3339: total_loss: 0.261331, loss_sup: 0.183097, loss_mps: 0.027683, loss_cps: 0.050551
[12:33:10.662] iteration 3340: total_loss: 0.271458, loss_sup: 0.179087, loss_mps: 0.032040, loss_cps: 0.060331
[12:33:10.808] iteration 3341: total_loss: 0.377172, loss_sup: 0.260127, loss_mps: 0.039837, loss_cps: 0.077209
[12:33:10.954] iteration 3342: total_loss: 0.190274, loss_sup: 0.106769, loss_mps: 0.030799, loss_cps: 0.052706
[12:33:11.100] iteration 3343: total_loss: 0.224734, loss_sup: 0.132447, loss_mps: 0.032747, loss_cps: 0.059539
[12:33:11.163] iteration 3344: total_loss: 0.206715, loss_sup: 0.155831, loss_mps: 0.021014, loss_cps: 0.029871
[12:33:12.368] iteration 3345: total_loss: 0.362594, loss_sup: 0.291560, loss_mps: 0.025783, loss_cps: 0.045251
[12:33:12.516] iteration 3346: total_loss: 0.407283, loss_sup: 0.314876, loss_mps: 0.031437, loss_cps: 0.060969
[12:33:12.663] iteration 3347: total_loss: 0.124973, loss_sup: 0.064750, loss_mps: 0.022251, loss_cps: 0.037971
[12:33:12.808] iteration 3348: total_loss: 0.275376, loss_sup: 0.206809, loss_mps: 0.023649, loss_cps: 0.044919
[12:33:12.954] iteration 3349: total_loss: 0.227854, loss_sup: 0.162008, loss_mps: 0.023966, loss_cps: 0.041880
[12:33:13.100] iteration 3350: total_loss: 0.342165, loss_sup: 0.265998, loss_mps: 0.025860, loss_cps: 0.050307
[12:33:13.247] iteration 3351: total_loss: 0.203932, loss_sup: 0.124914, loss_mps: 0.027118, loss_cps: 0.051900
[12:33:13.393] iteration 3352: total_loss: 0.155706, loss_sup: 0.081921, loss_mps: 0.025978, loss_cps: 0.047807
[12:33:13.539] iteration 3353: total_loss: 0.147402, loss_sup: 0.097156, loss_mps: 0.018921, loss_cps: 0.031324
[12:33:13.685] iteration 3354: total_loss: 0.301278, loss_sup: 0.238093, loss_mps: 0.022214, loss_cps: 0.040971
[12:33:13.831] iteration 3355: total_loss: 0.247998, loss_sup: 0.163542, loss_mps: 0.029253, loss_cps: 0.055203
[12:33:13.977] iteration 3356: total_loss: 0.281740, loss_sup: 0.176358, loss_mps: 0.035415, loss_cps: 0.069967
[12:33:14.122] iteration 3357: total_loss: 0.232141, loss_sup: 0.164769, loss_mps: 0.024243, loss_cps: 0.043129
[12:33:14.271] iteration 3358: total_loss: 0.226741, loss_sup: 0.154277, loss_mps: 0.025804, loss_cps: 0.046660
[12:33:14.417] iteration 3359: total_loss: 0.320302, loss_sup: 0.258067, loss_mps: 0.022243, loss_cps: 0.039991
[12:33:14.562] iteration 3360: total_loss: 0.513949, loss_sup: 0.442739, loss_mps: 0.025361, loss_cps: 0.045850
[12:33:14.709] iteration 3361: total_loss: 0.305044, loss_sup: 0.211581, loss_mps: 0.031677, loss_cps: 0.061786
[12:33:14.854] iteration 3362: total_loss: 0.207832, loss_sup: 0.139700, loss_mps: 0.024030, loss_cps: 0.044102
[12:33:15.000] iteration 3363: total_loss: 0.399773, loss_sup: 0.319759, loss_mps: 0.027624, loss_cps: 0.052390
[12:33:15.146] iteration 3364: total_loss: 0.367966, loss_sup: 0.294416, loss_mps: 0.025894, loss_cps: 0.047655
[12:33:15.291] iteration 3365: total_loss: 0.234934, loss_sup: 0.160256, loss_mps: 0.026824, loss_cps: 0.047854
[12:33:15.437] iteration 3366: total_loss: 0.205059, loss_sup: 0.136916, loss_mps: 0.024737, loss_cps: 0.043406
[12:33:15.584] iteration 3367: total_loss: 0.193446, loss_sup: 0.132421, loss_mps: 0.023180, loss_cps: 0.037845
[12:33:15.731] iteration 3368: total_loss: 0.619976, loss_sup: 0.499239, loss_mps: 0.039433, loss_cps: 0.081304
[12:33:15.882] iteration 3369: total_loss: 0.392227, loss_sup: 0.318379, loss_mps: 0.025843, loss_cps: 0.048004
[12:33:16.029] iteration 3370: total_loss: 0.407217, loss_sup: 0.302871, loss_mps: 0.035005, loss_cps: 0.069341
[12:33:16.175] iteration 3371: total_loss: 0.329214, loss_sup: 0.231619, loss_mps: 0.033186, loss_cps: 0.064409
[12:33:16.321] iteration 3372: total_loss: 0.311776, loss_sup: 0.246107, loss_mps: 0.023537, loss_cps: 0.042131
[12:33:16.467] iteration 3373: total_loss: 0.443738, loss_sup: 0.368030, loss_mps: 0.027138, loss_cps: 0.048569
[12:33:16.613] iteration 3374: total_loss: 0.370074, loss_sup: 0.293462, loss_mps: 0.027752, loss_cps: 0.048860
[12:33:16.759] iteration 3375: total_loss: 0.354470, loss_sup: 0.268012, loss_mps: 0.030147, loss_cps: 0.056311
[12:33:16.905] iteration 3376: total_loss: 0.433353, loss_sup: 0.361810, loss_mps: 0.026588, loss_cps: 0.044955
[12:33:17.052] iteration 3377: total_loss: 0.262688, loss_sup: 0.164945, loss_mps: 0.034918, loss_cps: 0.062824
[12:33:17.198] iteration 3378: total_loss: 0.109955, loss_sup: 0.039445, loss_mps: 0.027015, loss_cps: 0.043495
[12:33:17.347] iteration 3379: total_loss: 0.469912, loss_sup: 0.376139, loss_mps: 0.033844, loss_cps: 0.059928
[12:33:17.493] iteration 3380: total_loss: 0.233941, loss_sup: 0.143947, loss_mps: 0.033708, loss_cps: 0.056285
[12:33:17.639] iteration 3381: total_loss: 0.337597, loss_sup: 0.241208, loss_mps: 0.034150, loss_cps: 0.062240
[12:33:17.785] iteration 3382: total_loss: 0.337806, loss_sup: 0.254824, loss_mps: 0.031213, loss_cps: 0.051769
[12:33:17.932] iteration 3383: total_loss: 0.371140, loss_sup: 0.275630, loss_mps: 0.034572, loss_cps: 0.060938
[12:33:18.077] iteration 3384: total_loss: 0.313818, loss_sup: 0.217981, loss_mps: 0.034567, loss_cps: 0.061270
[12:33:18.223] iteration 3385: total_loss: 0.621090, loss_sup: 0.540212, loss_mps: 0.030163, loss_cps: 0.050716
[12:33:18.371] iteration 3386: total_loss: 0.590209, loss_sup: 0.468822, loss_mps: 0.042430, loss_cps: 0.078958
[12:33:18.516] iteration 3387: total_loss: 0.316685, loss_sup: 0.229133, loss_mps: 0.032747, loss_cps: 0.054805
[12:33:18.662] iteration 3388: total_loss: 0.269213, loss_sup: 0.191222, loss_mps: 0.030573, loss_cps: 0.047418
[12:33:18.809] iteration 3389: total_loss: 0.183424, loss_sup: 0.095460, loss_mps: 0.032323, loss_cps: 0.055641
[12:33:18.955] iteration 3390: total_loss: 0.251601, loss_sup: 0.158375, loss_mps: 0.034517, loss_cps: 0.058709
[12:33:19.102] iteration 3391: total_loss: 0.237716, loss_sup: 0.139437, loss_mps: 0.036945, loss_cps: 0.061335
[12:33:19.249] iteration 3392: total_loss: 0.211472, loss_sup: 0.139471, loss_mps: 0.027457, loss_cps: 0.044544
[12:33:19.398] iteration 3393: total_loss: 0.279711, loss_sup: 0.199908, loss_mps: 0.029243, loss_cps: 0.050560
[12:33:19.545] iteration 3394: total_loss: 0.331931, loss_sup: 0.225921, loss_mps: 0.037798, loss_cps: 0.068211
[12:33:19.691] iteration 3395: total_loss: 0.207586, loss_sup: 0.126963, loss_mps: 0.030984, loss_cps: 0.049640
[12:33:19.838] iteration 3396: total_loss: 0.444007, loss_sup: 0.346525, loss_mps: 0.035430, loss_cps: 0.062053
[12:33:19.986] iteration 3397: total_loss: 0.475149, loss_sup: 0.375360, loss_mps: 0.036443, loss_cps: 0.063346
[12:33:20.134] iteration 3398: total_loss: 0.180631, loss_sup: 0.100361, loss_mps: 0.029384, loss_cps: 0.050886
[12:33:20.280] iteration 3399: total_loss: 0.239585, loss_sup: 0.150693, loss_mps: 0.031784, loss_cps: 0.057108
[12:33:20.425] iteration 3400: total_loss: 0.173785, loss_sup: 0.111020, loss_mps: 0.023972, loss_cps: 0.038793
[12:33:20.426] Evaluation Started ==>
[12:33:31.744] ==> valid iteration 3400: unet metrics: {'dc': 0.5586611286240903, 'jc': 0.43145773341102456, 'pre': 0.6107509083800928, 'hd': 6.829195170524484}, ynet metrics: {'dc': 0.4945262989608362, 'jc': 0.37113398599874486, 'pre': 0.4826890093238994, 'hd': 7.894054457151972}.
[12:33:31.746] Evaluation Finished!⏹️
[12:33:31.894] iteration 3401: total_loss: 0.360141, loss_sup: 0.258680, loss_mps: 0.034091, loss_cps: 0.067369
[12:33:32.041] iteration 3402: total_loss: 0.214458, loss_sup: 0.148825, loss_mps: 0.025336, loss_cps: 0.040297
[12:33:32.187] iteration 3403: total_loss: 0.299936, loss_sup: 0.222661, loss_mps: 0.028068, loss_cps: 0.049207
[12:33:32.332] iteration 3404: total_loss: 0.111220, loss_sup: 0.055804, loss_mps: 0.021051, loss_cps: 0.034365
[12:33:32.477] iteration 3405: total_loss: 0.577416, loss_sup: 0.503168, loss_mps: 0.026027, loss_cps: 0.048221
[12:33:32.622] iteration 3406: total_loss: 0.252129, loss_sup: 0.173254, loss_mps: 0.028959, loss_cps: 0.049916
[12:33:32.767] iteration 3407: total_loss: 0.457032, loss_sup: 0.358904, loss_mps: 0.034244, loss_cps: 0.063883
[12:33:32.915] iteration 3408: total_loss: 0.361177, loss_sup: 0.299763, loss_mps: 0.023237, loss_cps: 0.038177
[12:33:33.060] iteration 3409: total_loss: 0.380763, loss_sup: 0.322984, loss_mps: 0.022158, loss_cps: 0.035621
[12:33:33.205] iteration 3410: total_loss: 0.230315, loss_sup: 0.173144, loss_mps: 0.021488, loss_cps: 0.035683
[12:33:33.350] iteration 3411: total_loss: 0.247993, loss_sup: 0.192066, loss_mps: 0.022297, loss_cps: 0.033630
[12:33:33.495] iteration 3412: total_loss: 0.125967, loss_sup: 0.055808, loss_mps: 0.026953, loss_cps: 0.043206
[12:33:33.640] iteration 3413: total_loss: 0.233456, loss_sup: 0.157866, loss_mps: 0.027524, loss_cps: 0.048066
[12:33:33.785] iteration 3414: total_loss: 0.252699, loss_sup: 0.184180, loss_mps: 0.025871, loss_cps: 0.042648
[12:33:33.931] iteration 3415: total_loss: 0.242732, loss_sup: 0.177703, loss_mps: 0.024384, loss_cps: 0.040644
[12:33:34.076] iteration 3416: total_loss: 0.349109, loss_sup: 0.262762, loss_mps: 0.029871, loss_cps: 0.056476
[12:33:34.221] iteration 3417: total_loss: 0.420450, loss_sup: 0.343277, loss_mps: 0.027869, loss_cps: 0.049305
[12:33:34.367] iteration 3418: total_loss: 0.222122, loss_sup: 0.160042, loss_mps: 0.022510, loss_cps: 0.039571
[12:33:34.515] iteration 3419: total_loss: 0.345647, loss_sup: 0.276962, loss_mps: 0.024642, loss_cps: 0.044043
[12:33:34.660] iteration 3420: total_loss: 0.313415, loss_sup: 0.215291, loss_mps: 0.033168, loss_cps: 0.064956
[12:33:34.805] iteration 3421: total_loss: 0.338574, loss_sup: 0.272494, loss_mps: 0.023555, loss_cps: 0.042525
[12:33:34.950] iteration 3422: total_loss: 0.197604, loss_sup: 0.126319, loss_mps: 0.025226, loss_cps: 0.046058
[12:33:35.095] iteration 3423: total_loss: 0.158062, loss_sup: 0.094667, loss_mps: 0.023285, loss_cps: 0.040110
[12:33:35.240] iteration 3424: total_loss: 0.375955, loss_sup: 0.279648, loss_mps: 0.032559, loss_cps: 0.063747
[12:33:35.385] iteration 3425: total_loss: 0.230859, loss_sup: 0.145139, loss_mps: 0.030206, loss_cps: 0.055515
[12:33:35.531] iteration 3426: total_loss: 0.158337, loss_sup: 0.085674, loss_mps: 0.026906, loss_cps: 0.045757
[12:33:35.676] iteration 3427: total_loss: 0.193908, loss_sup: 0.128714, loss_mps: 0.024510, loss_cps: 0.040683
[12:33:35.821] iteration 3428: total_loss: 0.254250, loss_sup: 0.185695, loss_mps: 0.024508, loss_cps: 0.044047
[12:33:35.966] iteration 3429: total_loss: 0.362933, loss_sup: 0.268833, loss_mps: 0.032236, loss_cps: 0.061864
[12:33:36.112] iteration 3430: total_loss: 0.255601, loss_sup: 0.191696, loss_mps: 0.024109, loss_cps: 0.039797
[12:33:36.257] iteration 3431: total_loss: 0.306161, loss_sup: 0.224357, loss_mps: 0.029475, loss_cps: 0.052329
[12:33:36.402] iteration 3432: total_loss: 0.260721, loss_sup: 0.175924, loss_mps: 0.029315, loss_cps: 0.055482
[12:33:36.549] iteration 3433: total_loss: 0.342694, loss_sup: 0.257307, loss_mps: 0.030470, loss_cps: 0.054917
[12:33:36.694] iteration 3434: total_loss: 0.207168, loss_sup: 0.153332, loss_mps: 0.020364, loss_cps: 0.033473
[12:33:36.839] iteration 3435: total_loss: 0.367446, loss_sup: 0.285505, loss_mps: 0.029537, loss_cps: 0.052405
[12:33:36.986] iteration 3436: total_loss: 0.429004, loss_sup: 0.341686, loss_mps: 0.031202, loss_cps: 0.056115
[12:33:37.131] iteration 3437: total_loss: 0.214625, loss_sup: 0.120622, loss_mps: 0.032236, loss_cps: 0.061766
[12:33:37.276] iteration 3438: total_loss: 0.303905, loss_sup: 0.235401, loss_mps: 0.025420, loss_cps: 0.043083
[12:33:37.422] iteration 3439: total_loss: 0.212710, loss_sup: 0.130657, loss_mps: 0.029432, loss_cps: 0.052621
[12:33:37.567] iteration 3440: total_loss: 0.294391, loss_sup: 0.228203, loss_mps: 0.024746, loss_cps: 0.041442
[12:33:37.713] iteration 3441: total_loss: 0.239036, loss_sup: 0.170197, loss_mps: 0.025571, loss_cps: 0.043267
[12:33:37.858] iteration 3442: total_loss: 0.356509, loss_sup: 0.258684, loss_mps: 0.033714, loss_cps: 0.064111
[12:33:38.006] iteration 3443: total_loss: 0.148463, loss_sup: 0.095434, loss_mps: 0.020690, loss_cps: 0.032339
[12:33:38.152] iteration 3444: total_loss: 0.314004, loss_sup: 0.211335, loss_mps: 0.034827, loss_cps: 0.067843
[12:33:38.298] iteration 3445: total_loss: 0.236416, loss_sup: 0.159569, loss_mps: 0.029305, loss_cps: 0.047541
[12:33:38.443] iteration 3446: total_loss: 0.156938, loss_sup: 0.093745, loss_mps: 0.023596, loss_cps: 0.039596
[12:33:38.589] iteration 3447: total_loss: 0.347095, loss_sup: 0.279899, loss_mps: 0.024312, loss_cps: 0.042884
[12:33:38.735] iteration 3448: total_loss: 0.349862, loss_sup: 0.279651, loss_mps: 0.025336, loss_cps: 0.044875
[12:33:38.880] iteration 3449: total_loss: 0.407493, loss_sup: 0.330249, loss_mps: 0.028044, loss_cps: 0.049200
[12:33:39.026] iteration 3450: total_loss: 0.222841, loss_sup: 0.133138, loss_mps: 0.031341, loss_cps: 0.058362
[12:33:39.172] iteration 3451: total_loss: 0.128343, loss_sup: 0.070073, loss_mps: 0.021923, loss_cps: 0.036347
[12:33:39.318] iteration 3452: total_loss: 0.408093, loss_sup: 0.314406, loss_mps: 0.032828, loss_cps: 0.060860
[12:33:39.464] iteration 3453: total_loss: 0.210277, loss_sup: 0.134491, loss_mps: 0.027399, loss_cps: 0.048387
[12:33:39.611] iteration 3454: total_loss: 0.152289, loss_sup: 0.100333, loss_mps: 0.020215, loss_cps: 0.031741
[12:33:39.756] iteration 3455: total_loss: 0.283575, loss_sup: 0.212807, loss_mps: 0.025829, loss_cps: 0.044939
[12:33:39.902] iteration 3456: total_loss: 0.296807, loss_sup: 0.230054, loss_mps: 0.023675, loss_cps: 0.043079
[12:33:40.049] iteration 3457: total_loss: 0.328404, loss_sup: 0.229137, loss_mps: 0.034176, loss_cps: 0.065092
[12:33:40.194] iteration 3458: total_loss: 0.150053, loss_sup: 0.080615, loss_mps: 0.024588, loss_cps: 0.044850
[12:33:40.339] iteration 3459: total_loss: 0.377212, loss_sup: 0.275955, loss_mps: 0.034185, loss_cps: 0.067072
[12:33:40.485] iteration 3460: total_loss: 0.377774, loss_sup: 0.298483, loss_mps: 0.028366, loss_cps: 0.050925
[12:33:40.630] iteration 3461: total_loss: 0.460175, loss_sup: 0.343166, loss_mps: 0.038462, loss_cps: 0.078546
[12:33:40.775] iteration 3462: total_loss: 0.161725, loss_sup: 0.083285, loss_mps: 0.027664, loss_cps: 0.050776
[12:33:40.920] iteration 3463: total_loss: 0.402353, loss_sup: 0.333697, loss_mps: 0.025218, loss_cps: 0.043437
[12:33:41.066] iteration 3464: total_loss: 0.299345, loss_sup: 0.238435, loss_mps: 0.023045, loss_cps: 0.037865
[12:33:41.212] iteration 3465: total_loss: 0.193795, loss_sup: 0.130603, loss_mps: 0.023270, loss_cps: 0.039922
[12:33:41.357] iteration 3466: total_loss: 0.169752, loss_sup: 0.086795, loss_mps: 0.029026, loss_cps: 0.053930
[12:33:41.502] iteration 3467: total_loss: 0.204818, loss_sup: 0.113751, loss_mps: 0.031089, loss_cps: 0.059978
[12:33:41.648] iteration 3468: total_loss: 0.181228, loss_sup: 0.121040, loss_mps: 0.022697, loss_cps: 0.037492
[12:33:41.794] iteration 3469: total_loss: 0.312026, loss_sup: 0.231153, loss_mps: 0.029919, loss_cps: 0.050954
[12:33:41.939] iteration 3470: total_loss: 0.325178, loss_sup: 0.230444, loss_mps: 0.033019, loss_cps: 0.061716
[12:33:42.084] iteration 3471: total_loss: 0.233025, loss_sup: 0.163358, loss_mps: 0.026127, loss_cps: 0.043540
[12:33:42.230] iteration 3472: total_loss: 0.370258, loss_sup: 0.293160, loss_mps: 0.027587, loss_cps: 0.049512
[12:33:42.375] iteration 3473: total_loss: 0.192491, loss_sup: 0.128992, loss_mps: 0.023826, loss_cps: 0.039673
[12:33:42.522] iteration 3474: total_loss: 0.237659, loss_sup: 0.173348, loss_mps: 0.023633, loss_cps: 0.040678
[12:33:42.667] iteration 3475: total_loss: 0.169346, loss_sup: 0.125070, loss_mps: 0.017481, loss_cps: 0.026795
[12:33:42.814] iteration 3476: total_loss: 0.151299, loss_sup: 0.084428, loss_mps: 0.023976, loss_cps: 0.042894
[12:33:42.963] iteration 3477: total_loss: 0.231104, loss_sup: 0.185747, loss_mps: 0.018559, loss_cps: 0.026797
[12:33:43.109] iteration 3478: total_loss: 0.207892, loss_sup: 0.124049, loss_mps: 0.029552, loss_cps: 0.054291
[12:33:43.255] iteration 3479: total_loss: 0.227974, loss_sup: 0.168492, loss_mps: 0.021875, loss_cps: 0.037607
[12:33:43.401] iteration 3480: total_loss: 0.174697, loss_sup: 0.096988, loss_mps: 0.029005, loss_cps: 0.048704
[12:33:43.547] iteration 3481: total_loss: 0.329162, loss_sup: 0.260613, loss_mps: 0.025020, loss_cps: 0.043529
[12:33:43.692] iteration 3482: total_loss: 0.258026, loss_sup: 0.201170, loss_mps: 0.020617, loss_cps: 0.036238
[12:33:43.837] iteration 3483: total_loss: 0.390068, loss_sup: 0.330753, loss_mps: 0.022752, loss_cps: 0.036563
[12:33:43.983] iteration 3484: total_loss: 0.249349, loss_sup: 0.206946, loss_mps: 0.017308, loss_cps: 0.025095
[12:33:44.130] iteration 3485: total_loss: 0.338313, loss_sup: 0.279658, loss_mps: 0.022257, loss_cps: 0.036398
[12:33:44.276] iteration 3486: total_loss: 0.292868, loss_sup: 0.237012, loss_mps: 0.021240, loss_cps: 0.034616
[12:33:44.422] iteration 3487: total_loss: 0.405849, loss_sup: 0.325715, loss_mps: 0.027865, loss_cps: 0.052268
[12:33:44.569] iteration 3488: total_loss: 0.315158, loss_sup: 0.225376, loss_mps: 0.030945, loss_cps: 0.058837
[12:33:44.714] iteration 3489: total_loss: 0.374807, loss_sup: 0.300203, loss_mps: 0.025959, loss_cps: 0.048646
[12:33:44.859] iteration 3490: total_loss: 0.147703, loss_sup: 0.078576, loss_mps: 0.025446, loss_cps: 0.043681
[12:33:45.005] iteration 3491: total_loss: 0.323230, loss_sup: 0.214474, loss_mps: 0.036585, loss_cps: 0.072171
[12:33:45.150] iteration 3492: total_loss: 0.205499, loss_sup: 0.148772, loss_mps: 0.021585, loss_cps: 0.035143
[12:33:45.295] iteration 3493: total_loss: 0.279745, loss_sup: 0.215440, loss_mps: 0.024834, loss_cps: 0.039471
[12:33:45.441] iteration 3494: total_loss: 0.224108, loss_sup: 0.130114, loss_mps: 0.032837, loss_cps: 0.061157
[12:33:45.588] iteration 3495: total_loss: 0.328246, loss_sup: 0.237398, loss_mps: 0.031974, loss_cps: 0.058874
[12:33:45.734] iteration 3496: total_loss: 0.213449, loss_sup: 0.116636, loss_mps: 0.033223, loss_cps: 0.063590
[12:33:45.880] iteration 3497: total_loss: 0.389750, loss_sup: 0.309953, loss_mps: 0.028363, loss_cps: 0.051434
[12:33:46.026] iteration 3498: total_loss: 0.410323, loss_sup: 0.328113, loss_mps: 0.029919, loss_cps: 0.052290
[12:33:46.171] iteration 3499: total_loss: 0.217449, loss_sup: 0.147998, loss_mps: 0.026593, loss_cps: 0.042858
[12:33:46.317] iteration 3500: total_loss: 0.176887, loss_sup: 0.104174, loss_mps: 0.027874, loss_cps: 0.044839
[12:33:46.317] Evaluation Started ==>
[12:33:57.693] ==> valid iteration 3500: unet metrics: {'dc': 0.5857464865719231, 'jc': 0.4554188014607395, 'pre': 0.6219606572492256, 'hd': 6.9164929389852094}, ynet metrics: {'dc': 0.5112640187311277, 'jc': 0.38551306466698115, 'pre': 0.6354039671295774, 'hd': 7.059583432778831}.
[12:33:57.868] ==> New best valid dice for ynet: 0.511264, at iteration 3500
[12:33:57.870] Evaluation Finished!⏹️
[12:33:58.022] iteration 3501: total_loss: 0.146271, loss_sup: 0.081898, loss_mps: 0.023941, loss_cps: 0.040432
[12:33:58.169] iteration 3502: total_loss: 0.165344, loss_sup: 0.093634, loss_mps: 0.026970, loss_cps: 0.044739
[12:33:58.314] iteration 3503: total_loss: 0.277391, loss_sup: 0.195725, loss_mps: 0.028419, loss_cps: 0.053246
[12:33:58.459] iteration 3504: total_loss: 0.140992, loss_sup: 0.061232, loss_mps: 0.028711, loss_cps: 0.051048
[12:33:58.604] iteration 3505: total_loss: 0.221625, loss_sup: 0.159214, loss_mps: 0.023288, loss_cps: 0.039123
[12:33:58.749] iteration 3506: total_loss: 0.392627, loss_sup: 0.323840, loss_mps: 0.024954, loss_cps: 0.043833
[12:33:58.895] iteration 3507: total_loss: 0.566256, loss_sup: 0.481948, loss_mps: 0.030002, loss_cps: 0.054305
[12:33:59.043] iteration 3508: total_loss: 0.305794, loss_sup: 0.228107, loss_mps: 0.028705, loss_cps: 0.048982
[12:33:59.187] iteration 3509: total_loss: 0.115343, loss_sup: 0.067177, loss_mps: 0.019354, loss_cps: 0.028813
[12:33:59.331] iteration 3510: total_loss: 0.105297, loss_sup: 0.042825, loss_mps: 0.023642, loss_cps: 0.038830
[12:33:59.479] iteration 3511: total_loss: 0.311576, loss_sup: 0.208992, loss_mps: 0.035690, loss_cps: 0.066894
[12:33:59.625] iteration 3512: total_loss: 0.199499, loss_sup: 0.128366, loss_mps: 0.026084, loss_cps: 0.045048
[12:33:59.774] iteration 3513: total_loss: 0.323655, loss_sup: 0.250596, loss_mps: 0.027230, loss_cps: 0.045829
[12:33:59.920] iteration 3514: total_loss: 0.297901, loss_sup: 0.200042, loss_mps: 0.034109, loss_cps: 0.063751
[12:34:00.065] iteration 3515: total_loss: 0.532427, loss_sup: 0.446353, loss_mps: 0.030635, loss_cps: 0.055439
[12:34:00.211] iteration 3516: total_loss: 0.399619, loss_sup: 0.343669, loss_mps: 0.021865, loss_cps: 0.034085
[12:34:00.357] iteration 3517: total_loss: 0.389375, loss_sup: 0.288148, loss_mps: 0.036208, loss_cps: 0.065019
[12:34:00.502] iteration 3518: total_loss: 0.225233, loss_sup: 0.144779, loss_mps: 0.030297, loss_cps: 0.050157
[12:34:00.650] iteration 3519: total_loss: 0.178826, loss_sup: 0.106672, loss_mps: 0.026720, loss_cps: 0.045433
[12:34:00.795] iteration 3520: total_loss: 0.324270, loss_sup: 0.254249, loss_mps: 0.025458, loss_cps: 0.044562
[12:34:00.942] iteration 3521: total_loss: 0.191526, loss_sup: 0.124310, loss_mps: 0.025038, loss_cps: 0.042177
[12:34:01.090] iteration 3522: total_loss: 0.335303, loss_sup: 0.253024, loss_mps: 0.029520, loss_cps: 0.052760
[12:34:01.236] iteration 3523: total_loss: 0.398733, loss_sup: 0.279922, loss_mps: 0.040276, loss_cps: 0.078535
[12:34:01.382] iteration 3524: total_loss: 0.405992, loss_sup: 0.306501, loss_mps: 0.035416, loss_cps: 0.064075
[12:34:01.527] iteration 3525: total_loss: 0.348492, loss_sup: 0.253051, loss_mps: 0.033018, loss_cps: 0.062423
[12:34:01.673] iteration 3526: total_loss: 0.128781, loss_sup: 0.075592, loss_mps: 0.020358, loss_cps: 0.032831
[12:34:01.819] iteration 3527: total_loss: 0.233086, loss_sup: 0.156154, loss_mps: 0.027496, loss_cps: 0.049436
[12:34:01.965] iteration 3528: total_loss: 0.354343, loss_sup: 0.260841, loss_mps: 0.032806, loss_cps: 0.060696
[12:34:02.111] iteration 3529: total_loss: 0.143625, loss_sup: 0.061939, loss_mps: 0.029621, loss_cps: 0.052065
[12:34:02.256] iteration 3530: total_loss: 0.254263, loss_sup: 0.190859, loss_mps: 0.024439, loss_cps: 0.038965
[12:34:02.402] iteration 3531: total_loss: 0.284691, loss_sup: 0.188065, loss_mps: 0.033105, loss_cps: 0.063521
[12:34:02.548] iteration 3532: total_loss: 0.350314, loss_sup: 0.279947, loss_mps: 0.026031, loss_cps: 0.044336
[12:34:02.694] iteration 3533: total_loss: 0.318982, loss_sup: 0.238882, loss_mps: 0.029388, loss_cps: 0.050712
[12:34:02.840] iteration 3534: total_loss: 0.419586, loss_sup: 0.342038, loss_mps: 0.028697, loss_cps: 0.048851
[12:34:02.985] iteration 3535: total_loss: 0.543602, loss_sup: 0.470134, loss_mps: 0.026709, loss_cps: 0.046758
[12:34:03.130] iteration 3536: total_loss: 0.241176, loss_sup: 0.172534, loss_mps: 0.026687, loss_cps: 0.041956
[12:34:03.276] iteration 3537: total_loss: 0.239026, loss_sup: 0.173681, loss_mps: 0.024877, loss_cps: 0.040467
[12:34:03.422] iteration 3538: total_loss: 0.298219, loss_sup: 0.224146, loss_mps: 0.028339, loss_cps: 0.045734
[12:34:03.568] iteration 3539: total_loss: 0.247363, loss_sup: 0.145988, loss_mps: 0.035517, loss_cps: 0.065858
[12:34:03.713] iteration 3540: total_loss: 0.568782, loss_sup: 0.466648, loss_mps: 0.036024, loss_cps: 0.066110
[12:34:03.859] iteration 3541: total_loss: 0.387269, loss_sup: 0.306979, loss_mps: 0.029715, loss_cps: 0.050575
[12:34:04.005] iteration 3542: total_loss: 0.197566, loss_sup: 0.137603, loss_mps: 0.023883, loss_cps: 0.036081
[12:34:04.150] iteration 3543: total_loss: 0.401788, loss_sup: 0.310706, loss_mps: 0.032026, loss_cps: 0.059055
[12:34:04.296] iteration 3544: total_loss: 0.514328, loss_sup: 0.429917, loss_mps: 0.031394, loss_cps: 0.053017
[12:34:04.442] iteration 3545: total_loss: 0.264815, loss_sup: 0.159506, loss_mps: 0.038183, loss_cps: 0.067126
[12:34:04.588] iteration 3546: total_loss: 0.237473, loss_sup: 0.154780, loss_mps: 0.030336, loss_cps: 0.052357
[12:34:04.733] iteration 3547: total_loss: 0.648936, loss_sup: 0.527726, loss_mps: 0.041840, loss_cps: 0.079371
[12:34:04.880] iteration 3548: total_loss: 0.388142, loss_sup: 0.297052, loss_mps: 0.034052, loss_cps: 0.057038
[12:34:05.026] iteration 3549: total_loss: 0.296775, loss_sup: 0.201523, loss_mps: 0.035035, loss_cps: 0.060216
[12:34:05.171] iteration 3550: total_loss: 0.344409, loss_sup: 0.273884, loss_mps: 0.027198, loss_cps: 0.043327
[12:34:05.317] iteration 3551: total_loss: 0.214662, loss_sup: 0.133979, loss_mps: 0.032670, loss_cps: 0.048013
[12:34:05.463] iteration 3552: total_loss: 0.589638, loss_sup: 0.481234, loss_mps: 0.039755, loss_cps: 0.068649
[12:34:05.609] iteration 3553: total_loss: 0.202329, loss_sup: 0.129060, loss_mps: 0.028442, loss_cps: 0.044828
[12:34:05.755] iteration 3554: total_loss: 0.212502, loss_sup: 0.133126, loss_mps: 0.030620, loss_cps: 0.048755
[12:34:05.901] iteration 3555: total_loss: 0.174149, loss_sup: 0.104268, loss_mps: 0.027484, loss_cps: 0.042397
[12:34:06.047] iteration 3556: total_loss: 0.467410, loss_sup: 0.370049, loss_mps: 0.037110, loss_cps: 0.060251
[12:34:06.193] iteration 3557: total_loss: 0.283023, loss_sup: 0.189458, loss_mps: 0.035507, loss_cps: 0.058058
[12:34:06.338] iteration 3558: total_loss: 0.378740, loss_sup: 0.270942, loss_mps: 0.039247, loss_cps: 0.068551
[12:34:06.484] iteration 3559: total_loss: 0.148884, loss_sup: 0.061666, loss_mps: 0.032535, loss_cps: 0.054683
[12:34:06.629] iteration 3560: total_loss: 0.191772, loss_sup: 0.127341, loss_mps: 0.025726, loss_cps: 0.038705
[12:34:06.775] iteration 3561: total_loss: 0.341049, loss_sup: 0.259750, loss_mps: 0.031765, loss_cps: 0.049533
[12:34:06.920] iteration 3562: total_loss: 0.275862, loss_sup: 0.191010, loss_mps: 0.032493, loss_cps: 0.052359
[12:34:07.066] iteration 3563: total_loss: 0.296256, loss_sup: 0.223072, loss_mps: 0.028823, loss_cps: 0.044362
[12:34:07.211] iteration 3564: total_loss: 0.224981, loss_sup: 0.146254, loss_mps: 0.030387, loss_cps: 0.048340
[12:34:07.357] iteration 3565: total_loss: 0.209673, loss_sup: 0.123189, loss_mps: 0.031279, loss_cps: 0.055205
[12:34:07.503] iteration 3566: total_loss: 0.193756, loss_sup: 0.113542, loss_mps: 0.029909, loss_cps: 0.050304
[12:34:07.649] iteration 3567: total_loss: 0.222306, loss_sup: 0.152114, loss_mps: 0.026995, loss_cps: 0.043198
[12:34:07.795] iteration 3568: total_loss: 0.356934, loss_sup: 0.255423, loss_mps: 0.034630, loss_cps: 0.066880
[12:34:07.941] iteration 3569: total_loss: 0.171107, loss_sup: 0.119440, loss_mps: 0.020873, loss_cps: 0.030794
[12:34:08.087] iteration 3570: total_loss: 0.205330, loss_sup: 0.150823, loss_mps: 0.021558, loss_cps: 0.032949
[12:34:08.233] iteration 3571: total_loss: 0.357281, loss_sup: 0.279553, loss_mps: 0.027876, loss_cps: 0.049852
[12:34:08.379] iteration 3572: total_loss: 0.213932, loss_sup: 0.151004, loss_mps: 0.023324, loss_cps: 0.039604
[12:34:08.524] iteration 3573: total_loss: 0.192181, loss_sup: 0.132263, loss_mps: 0.023515, loss_cps: 0.036403
[12:34:08.670] iteration 3574: total_loss: 0.344326, loss_sup: 0.286425, loss_mps: 0.022873, loss_cps: 0.035027
[12:34:08.816] iteration 3575: total_loss: 0.293883, loss_sup: 0.219558, loss_mps: 0.027257, loss_cps: 0.047068
[12:34:08.961] iteration 3576: total_loss: 0.319768, loss_sup: 0.239474, loss_mps: 0.028290, loss_cps: 0.052005
[12:34:09.108] iteration 3577: total_loss: 0.216864, loss_sup: 0.152807, loss_mps: 0.023258, loss_cps: 0.040800
[12:34:09.254] iteration 3578: total_loss: 0.130564, loss_sup: 0.084891, loss_mps: 0.017497, loss_cps: 0.028175
[12:34:09.401] iteration 3579: total_loss: 0.302968, loss_sup: 0.235534, loss_mps: 0.024961, loss_cps: 0.042474
[12:34:09.547] iteration 3580: total_loss: 0.292147, loss_sup: 0.205486, loss_mps: 0.030310, loss_cps: 0.056351
[12:34:09.693] iteration 3581: total_loss: 0.178017, loss_sup: 0.119538, loss_mps: 0.022510, loss_cps: 0.035968
[12:34:09.839] iteration 3582: total_loss: 0.276934, loss_sup: 0.199019, loss_mps: 0.027669, loss_cps: 0.050246
[12:34:09.985] iteration 3583: total_loss: 0.137420, loss_sup: 0.085741, loss_mps: 0.019811, loss_cps: 0.031867
[12:34:10.131] iteration 3584: total_loss: 0.316773, loss_sup: 0.238803, loss_mps: 0.027609, loss_cps: 0.050361
[12:34:10.276] iteration 3585: total_loss: 0.172729, loss_sup: 0.118892, loss_mps: 0.019836, loss_cps: 0.034000
[12:34:10.423] iteration 3586: total_loss: 0.269450, loss_sup: 0.187267, loss_mps: 0.028880, loss_cps: 0.053303
[12:34:10.569] iteration 3587: total_loss: 0.206187, loss_sup: 0.128544, loss_mps: 0.027380, loss_cps: 0.050262
[12:34:10.717] iteration 3588: total_loss: 0.229336, loss_sup: 0.167527, loss_mps: 0.023236, loss_cps: 0.038573
[12:34:10.862] iteration 3589: total_loss: 0.428397, loss_sup: 0.326825, loss_mps: 0.034772, loss_cps: 0.066800
[12:34:11.011] iteration 3590: total_loss: 0.219817, loss_sup: 0.150169, loss_mps: 0.025045, loss_cps: 0.044603
[12:34:11.161] iteration 3591: total_loss: 0.314014, loss_sup: 0.241075, loss_mps: 0.025810, loss_cps: 0.047129
[12:34:11.306] iteration 3592: total_loss: 0.341354, loss_sup: 0.251381, loss_mps: 0.031068, loss_cps: 0.058904
[12:34:11.453] iteration 3593: total_loss: 0.229833, loss_sup: 0.147707, loss_mps: 0.028818, loss_cps: 0.053308
[12:34:11.599] iteration 3594: total_loss: 0.432630, loss_sup: 0.330375, loss_mps: 0.033981, loss_cps: 0.068274
[12:34:11.745] iteration 3595: total_loss: 0.172209, loss_sup: 0.086259, loss_mps: 0.029344, loss_cps: 0.056606
[12:34:11.891] iteration 3596: total_loss: 0.333558, loss_sup: 0.225531, loss_mps: 0.035175, loss_cps: 0.072853
[12:34:12.036] iteration 3597: total_loss: 0.257113, loss_sup: 0.154239, loss_mps: 0.035134, loss_cps: 0.067740
[12:34:12.184] iteration 3598: total_loss: 0.191621, loss_sup: 0.113217, loss_mps: 0.027886, loss_cps: 0.050518
[12:34:12.332] iteration 3599: total_loss: 0.132905, loss_sup: 0.081099, loss_mps: 0.019715, loss_cps: 0.032091
[12:34:12.478] iteration 3600: total_loss: 0.394191, loss_sup: 0.311116, loss_mps: 0.029884, loss_cps: 0.053190
[12:34:12.478] Evaluation Started ==>
[12:34:23.857] ==> valid iteration 3600: unet metrics: {'dc': 0.533482543155167, 'jc': 0.403101232078972, 'pre': 0.5970754994899321, 'hd': 7.146882508142132}, ynet metrics: {'dc': 0.49555961399783727, 'jc': 0.3728065086706764, 'pre': 0.5719272913460305, 'hd': 7.128823561634072}.
[12:34:23.859] Evaluation Finished!⏹️
[12:34:24.011] iteration 3601: total_loss: 0.403120, loss_sup: 0.331037, loss_mps: 0.026561, loss_cps: 0.045522
[12:34:24.159] iteration 3602: total_loss: 0.338810, loss_sup: 0.266636, loss_mps: 0.025535, loss_cps: 0.046638
[12:34:24.313] iteration 3603: total_loss: 0.266151, loss_sup: 0.189392, loss_mps: 0.027812, loss_cps: 0.048947
[12:34:24.458] iteration 3604: total_loss: 0.392310, loss_sup: 0.274195, loss_mps: 0.037750, loss_cps: 0.080364
[12:34:24.603] iteration 3605: total_loss: 0.385643, loss_sup: 0.305425, loss_mps: 0.028455, loss_cps: 0.051763
[12:34:24.747] iteration 3606: total_loss: 0.300689, loss_sup: 0.224302, loss_mps: 0.027229, loss_cps: 0.049158
[12:34:24.892] iteration 3607: total_loss: 0.188020, loss_sup: 0.134580, loss_mps: 0.020836, loss_cps: 0.032604
[12:34:25.037] iteration 3608: total_loss: 0.284411, loss_sup: 0.221486, loss_mps: 0.022917, loss_cps: 0.040009
[12:34:25.181] iteration 3609: total_loss: 0.164422, loss_sup: 0.084359, loss_mps: 0.028143, loss_cps: 0.051920
[12:34:25.330] iteration 3610: total_loss: 0.339319, loss_sup: 0.239626, loss_mps: 0.035212, loss_cps: 0.064481
[12:34:25.475] iteration 3611: total_loss: 0.184699, loss_sup: 0.098686, loss_mps: 0.030059, loss_cps: 0.055953
[12:34:25.624] iteration 3612: total_loss: 0.214814, loss_sup: 0.146344, loss_mps: 0.025218, loss_cps: 0.043252
[12:34:25.770] iteration 3613: total_loss: 0.139891, loss_sup: 0.087424, loss_mps: 0.020699, loss_cps: 0.031768
[12:34:25.915] iteration 3614: total_loss: 0.136074, loss_sup: 0.071360, loss_mps: 0.024280, loss_cps: 0.040434
[12:34:26.061] iteration 3615: total_loss: 0.347807, loss_sup: 0.272393, loss_mps: 0.027776, loss_cps: 0.047638
[12:34:26.208] iteration 3616: total_loss: 0.282143, loss_sup: 0.178637, loss_mps: 0.035574, loss_cps: 0.067932
[12:34:26.355] iteration 3617: total_loss: 0.195393, loss_sup: 0.119262, loss_mps: 0.027951, loss_cps: 0.048180
[12:34:26.501] iteration 3618: total_loss: 0.297150, loss_sup: 0.226291, loss_mps: 0.026560, loss_cps: 0.044299
[12:34:26.646] iteration 3619: total_loss: 0.244521, loss_sup: 0.160620, loss_mps: 0.030658, loss_cps: 0.053243
[12:34:26.792] iteration 3620: total_loss: 0.682701, loss_sup: 0.597151, loss_mps: 0.030709, loss_cps: 0.054841
[12:34:26.937] iteration 3621: total_loss: 0.139333, loss_sup: 0.074384, loss_mps: 0.024903, loss_cps: 0.040046
[12:34:27.083] iteration 3622: total_loss: 0.188593, loss_sup: 0.095866, loss_mps: 0.032675, loss_cps: 0.060051
[12:34:27.229] iteration 3623: total_loss: 0.300717, loss_sup: 0.208755, loss_mps: 0.031527, loss_cps: 0.060435
[12:34:27.376] iteration 3624: total_loss: 0.310603, loss_sup: 0.233250, loss_mps: 0.027162, loss_cps: 0.050191
[12:34:27.521] iteration 3625: total_loss: 0.380824, loss_sup: 0.277008, loss_mps: 0.035229, loss_cps: 0.068587
[12:34:27.667] iteration 3626: total_loss: 0.233084, loss_sup: 0.162433, loss_mps: 0.026094, loss_cps: 0.044557
[12:34:27.813] iteration 3627: total_loss: 0.320348, loss_sup: 0.243469, loss_mps: 0.027512, loss_cps: 0.049367
[12:34:27.958] iteration 3628: total_loss: 0.147549, loss_sup: 0.073444, loss_mps: 0.026810, loss_cps: 0.047296
[12:34:28.103] iteration 3629: total_loss: 0.429321, loss_sup: 0.356461, loss_mps: 0.026241, loss_cps: 0.046620
[12:34:28.249] iteration 3630: total_loss: 0.441091, loss_sup: 0.370139, loss_mps: 0.025839, loss_cps: 0.045113
[12:34:28.395] iteration 3631: total_loss: 0.283680, loss_sup: 0.216706, loss_mps: 0.025180, loss_cps: 0.041794
[12:34:28.541] iteration 3632: total_loss: 0.309081, loss_sup: 0.221036, loss_mps: 0.031523, loss_cps: 0.056522
[12:34:28.687] iteration 3633: total_loss: 0.237566, loss_sup: 0.143700, loss_mps: 0.032213, loss_cps: 0.061653
[12:34:28.836] iteration 3634: total_loss: 0.119324, loss_sup: 0.065195, loss_mps: 0.021225, loss_cps: 0.032903
[12:34:28.985] iteration 3635: total_loss: 0.340076, loss_sup: 0.260685, loss_mps: 0.028643, loss_cps: 0.050748
[12:34:29.131] iteration 3636: total_loss: 0.482409, loss_sup: 0.379156, loss_mps: 0.035368, loss_cps: 0.067885
[12:34:29.278] iteration 3637: total_loss: 0.584125, loss_sup: 0.476702, loss_mps: 0.036708, loss_cps: 0.070715
[12:34:29.425] iteration 3638: total_loss: 0.293569, loss_sup: 0.200904, loss_mps: 0.032756, loss_cps: 0.059909
[12:34:29.570] iteration 3639: total_loss: 0.205971, loss_sup: 0.101649, loss_mps: 0.036457, loss_cps: 0.067864
[12:34:29.716] iteration 3640: total_loss: 0.369997, loss_sup: 0.258323, loss_mps: 0.037403, loss_cps: 0.074271
[12:34:29.862] iteration 3641: total_loss: 0.349239, loss_sup: 0.264567, loss_mps: 0.030187, loss_cps: 0.054485
[12:34:30.008] iteration 3642: total_loss: 0.382448, loss_sup: 0.309693, loss_mps: 0.026084, loss_cps: 0.046671
[12:34:30.156] iteration 3643: total_loss: 0.303503, loss_sup: 0.225066, loss_mps: 0.027865, loss_cps: 0.050571
[12:34:30.303] iteration 3644: total_loss: 0.246977, loss_sup: 0.167847, loss_mps: 0.028873, loss_cps: 0.050257
[12:34:30.448] iteration 3645: total_loss: 0.395878, loss_sup: 0.297174, loss_mps: 0.034748, loss_cps: 0.063956
[12:34:30.595] iteration 3646: total_loss: 0.232672, loss_sup: 0.139366, loss_mps: 0.033670, loss_cps: 0.059636
[12:34:30.740] iteration 3647: total_loss: 0.407364, loss_sup: 0.324678, loss_mps: 0.029891, loss_cps: 0.052796
[12:34:30.888] iteration 3648: total_loss: 0.270911, loss_sup: 0.183874, loss_mps: 0.031159, loss_cps: 0.055878
[12:34:31.035] iteration 3649: total_loss: 0.324327, loss_sup: 0.232701, loss_mps: 0.032542, loss_cps: 0.059084
[12:34:31.181] iteration 3650: total_loss: 0.376202, loss_sup: 0.293122, loss_mps: 0.029543, loss_cps: 0.053537
[12:34:31.327] iteration 3651: total_loss: 0.359751, loss_sup: 0.268839, loss_mps: 0.031865, loss_cps: 0.059047
[12:34:31.473] iteration 3652: total_loss: 0.272217, loss_sup: 0.184591, loss_mps: 0.030482, loss_cps: 0.057144
[12:34:31.620] iteration 3653: total_loss: 0.188965, loss_sup: 0.110052, loss_mps: 0.028584, loss_cps: 0.050329
[12:34:31.766] iteration 3654: total_loss: 0.277482, loss_sup: 0.202750, loss_mps: 0.027639, loss_cps: 0.047093
[12:34:31.914] iteration 3655: total_loss: 0.157569, loss_sup: 0.098054, loss_mps: 0.022768, loss_cps: 0.036747
[12:34:32.065] iteration 3656: total_loss: 0.209334, loss_sup: 0.137828, loss_mps: 0.026806, loss_cps: 0.044700
[12:34:32.211] iteration 3657: total_loss: 0.194251, loss_sup: 0.125539, loss_mps: 0.025650, loss_cps: 0.043062
[12:34:32.358] iteration 3658: total_loss: 0.262116, loss_sup: 0.185841, loss_mps: 0.028621, loss_cps: 0.047654
[12:34:32.505] iteration 3659: total_loss: 0.174981, loss_sup: 0.107321, loss_mps: 0.026123, loss_cps: 0.041537
[12:34:32.653] iteration 3660: total_loss: 0.176839, loss_sup: 0.099281, loss_mps: 0.028140, loss_cps: 0.049418
[12:34:32.798] iteration 3661: total_loss: 0.196834, loss_sup: 0.113811, loss_mps: 0.029177, loss_cps: 0.053845
[12:34:32.945] iteration 3662: total_loss: 0.208914, loss_sup: 0.135360, loss_mps: 0.027247, loss_cps: 0.046307
[12:34:33.093] iteration 3663: total_loss: 0.197368, loss_sup: 0.123615, loss_mps: 0.026646, loss_cps: 0.047107
[12:34:33.238] iteration 3664: total_loss: 0.182754, loss_sup: 0.111152, loss_mps: 0.026232, loss_cps: 0.045370
[12:34:33.384] iteration 3665: total_loss: 0.266667, loss_sup: 0.174190, loss_mps: 0.032575, loss_cps: 0.059901
[12:34:33.535] iteration 3666: total_loss: 0.208876, loss_sup: 0.129606, loss_mps: 0.027776, loss_cps: 0.051495
[12:34:33.682] iteration 3667: total_loss: 0.346059, loss_sup: 0.268790, loss_mps: 0.028599, loss_cps: 0.048670
[12:34:33.827] iteration 3668: total_loss: 0.603300, loss_sup: 0.528726, loss_mps: 0.026821, loss_cps: 0.047752
[12:34:33.975] iteration 3669: total_loss: 0.425302, loss_sup: 0.336969, loss_mps: 0.030849, loss_cps: 0.057483
[12:34:34.120] iteration 3670: total_loss: 0.355356, loss_sup: 0.263183, loss_mps: 0.031132, loss_cps: 0.061041
[12:34:34.266] iteration 3671: total_loss: 0.211133, loss_sup: 0.133679, loss_mps: 0.027624, loss_cps: 0.049830
[12:34:34.412] iteration 3672: total_loss: 0.256885, loss_sup: 0.192512, loss_mps: 0.024648, loss_cps: 0.039725
[12:34:34.557] iteration 3673: total_loss: 0.341320, loss_sup: 0.278831, loss_mps: 0.023650, loss_cps: 0.038839
[12:34:34.709] iteration 3674: total_loss: 0.145664, loss_sup: 0.079700, loss_mps: 0.024727, loss_cps: 0.041237
[12:34:34.854] iteration 3675: total_loss: 0.215368, loss_sup: 0.142261, loss_mps: 0.025986, loss_cps: 0.047121
[12:34:35.000] iteration 3676: total_loss: 0.192271, loss_sup: 0.108119, loss_mps: 0.029522, loss_cps: 0.054630
[12:34:35.148] iteration 3677: total_loss: 0.309360, loss_sup: 0.250383, loss_mps: 0.021551, loss_cps: 0.037427
[12:34:35.293] iteration 3678: total_loss: 0.157291, loss_sup: 0.061872, loss_mps: 0.032939, loss_cps: 0.062480
[12:34:35.440] iteration 3679: total_loss: 0.185944, loss_sup: 0.103133, loss_mps: 0.028549, loss_cps: 0.054263
[12:34:35.586] iteration 3680: total_loss: 0.183663, loss_sup: 0.112308, loss_mps: 0.026279, loss_cps: 0.045076
[12:34:35.731] iteration 3681: total_loss: 0.285033, loss_sup: 0.202782, loss_mps: 0.028803, loss_cps: 0.053447
[12:34:35.877] iteration 3682: total_loss: 0.249432, loss_sup: 0.151454, loss_mps: 0.033407, loss_cps: 0.064571
[12:34:36.023] iteration 3683: total_loss: 0.142243, loss_sup: 0.090589, loss_mps: 0.019805, loss_cps: 0.031849
[12:34:36.169] iteration 3684: total_loss: 0.191478, loss_sup: 0.132444, loss_mps: 0.022963, loss_cps: 0.036071
[12:34:36.317] iteration 3685: total_loss: 0.250629, loss_sup: 0.164294, loss_mps: 0.031102, loss_cps: 0.055233
[12:34:36.463] iteration 3686: total_loss: 0.321175, loss_sup: 0.266343, loss_mps: 0.020102, loss_cps: 0.034731
[12:34:36.609] iteration 3687: total_loss: 0.278417, loss_sup: 0.186238, loss_mps: 0.030897, loss_cps: 0.061282
[12:34:36.757] iteration 3688: total_loss: 0.143026, loss_sup: 0.083189, loss_mps: 0.022034, loss_cps: 0.037802
[12:34:36.902] iteration 3689: total_loss: 0.163592, loss_sup: 0.084501, loss_mps: 0.028124, loss_cps: 0.050967
[12:34:37.052] iteration 3690: total_loss: 0.136968, loss_sup: 0.075010, loss_mps: 0.022885, loss_cps: 0.039073
[12:34:37.198] iteration 3691: total_loss: 0.256834, loss_sup: 0.153622, loss_mps: 0.035644, loss_cps: 0.067568
[12:34:37.344] iteration 3692: total_loss: 0.340592, loss_sup: 0.248923, loss_mps: 0.031179, loss_cps: 0.060491
[12:34:37.490] iteration 3693: total_loss: 0.210370, loss_sup: 0.127976, loss_mps: 0.029290, loss_cps: 0.053104
[12:34:37.638] iteration 3694: total_loss: 0.209881, loss_sup: 0.148853, loss_mps: 0.022140, loss_cps: 0.038888
[12:34:37.787] iteration 3695: total_loss: 0.275942, loss_sup: 0.198816, loss_mps: 0.027185, loss_cps: 0.049941
[12:34:37.937] iteration 3696: total_loss: 0.182190, loss_sup: 0.113702, loss_mps: 0.025234, loss_cps: 0.043254
[12:34:38.083] iteration 3697: total_loss: 0.107616, loss_sup: 0.047110, loss_mps: 0.022275, loss_cps: 0.038231
[12:34:38.229] iteration 3698: total_loss: 0.284187, loss_sup: 0.205792, loss_mps: 0.028259, loss_cps: 0.050137
[12:34:38.374] iteration 3699: total_loss: 0.305260, loss_sup: 0.216600, loss_mps: 0.030298, loss_cps: 0.058362
[12:34:38.520] iteration 3700: total_loss: 0.227300, loss_sup: 0.154537, loss_mps: 0.025676, loss_cps: 0.047088
[12:34:38.520] Evaluation Started ==>
[12:34:49.894] ==> valid iteration 3700: unet metrics: {'dc': 0.5826318650141441, 'jc': 0.455103825650436, 'pre': 0.6460669477581802, 'hd': 6.506127616093834}, ynet metrics: {'dc': 0.5111700097608887, 'jc': 0.3861122655229114, 'pre': 0.6006492371055773, 'hd': 6.95269259892433}.
[12:34:49.895] Evaluation Finished!⏹️
[12:34:50.048] iteration 3701: total_loss: 0.326675, loss_sup: 0.257398, loss_mps: 0.025394, loss_cps: 0.043883
[12:34:50.199] iteration 3702: total_loss: 0.239687, loss_sup: 0.161645, loss_mps: 0.028578, loss_cps: 0.049464
[12:34:50.344] iteration 3703: total_loss: 0.188994, loss_sup: 0.122493, loss_mps: 0.024421, loss_cps: 0.042079
[12:34:50.489] iteration 3704: total_loss: 0.145012, loss_sup: 0.091568, loss_mps: 0.020659, loss_cps: 0.032784
[12:34:50.637] iteration 3705: total_loss: 0.147428, loss_sup: 0.087022, loss_mps: 0.022216, loss_cps: 0.038191
[12:34:50.782] iteration 3706: total_loss: 0.201400, loss_sup: 0.138516, loss_mps: 0.022428, loss_cps: 0.040455
[12:34:50.932] iteration 3707: total_loss: 0.234841, loss_sup: 0.167586, loss_mps: 0.023641, loss_cps: 0.043614
[12:34:51.078] iteration 3708: total_loss: 0.636221, loss_sup: 0.551172, loss_mps: 0.029724, loss_cps: 0.055325
[12:34:51.224] iteration 3709: total_loss: 0.259787, loss_sup: 0.185203, loss_mps: 0.026979, loss_cps: 0.047605
[12:34:51.369] iteration 3710: total_loss: 0.150163, loss_sup: 0.081045, loss_mps: 0.024485, loss_cps: 0.044633
[12:34:51.516] iteration 3711: total_loss: 0.116441, loss_sup: 0.067128, loss_mps: 0.018342, loss_cps: 0.030970
[12:34:51.662] iteration 3712: total_loss: 0.251979, loss_sup: 0.167484, loss_mps: 0.028146, loss_cps: 0.056350
[12:34:51.809] iteration 3713: total_loss: 0.252797, loss_sup: 0.196821, loss_mps: 0.020294, loss_cps: 0.035682
[12:34:51.954] iteration 3714: total_loss: 0.150999, loss_sup: 0.083406, loss_mps: 0.022987, loss_cps: 0.044606
[12:34:52.101] iteration 3715: total_loss: 0.363399, loss_sup: 0.283258, loss_mps: 0.026842, loss_cps: 0.053299
[12:34:52.250] iteration 3716: total_loss: 0.438223, loss_sup: 0.371054, loss_mps: 0.023364, loss_cps: 0.043805
[12:34:52.400] iteration 3717: total_loss: 0.290184, loss_sup: 0.226459, loss_mps: 0.022841, loss_cps: 0.040884
[12:34:52.545] iteration 3718: total_loss: 0.220495, loss_sup: 0.135200, loss_mps: 0.028922, loss_cps: 0.056373
[12:34:52.691] iteration 3719: total_loss: 0.355797, loss_sup: 0.296727, loss_mps: 0.021493, loss_cps: 0.037577
[12:34:52.838] iteration 3720: total_loss: 0.444341, loss_sup: 0.354289, loss_mps: 0.030562, loss_cps: 0.059490
[12:34:52.984] iteration 3721: total_loss: 0.414893, loss_sup: 0.341334, loss_mps: 0.026203, loss_cps: 0.047356
[12:34:53.132] iteration 3722: total_loss: 0.230327, loss_sup: 0.137121, loss_mps: 0.030361, loss_cps: 0.062845
[12:34:53.278] iteration 3723: total_loss: 0.619784, loss_sup: 0.526549, loss_mps: 0.031658, loss_cps: 0.061577
[12:34:53.426] iteration 3724: total_loss: 0.234418, loss_sup: 0.159429, loss_mps: 0.026661, loss_cps: 0.048329
[12:34:53.573] iteration 3725: total_loss: 0.347463, loss_sup: 0.278844, loss_mps: 0.026038, loss_cps: 0.042581
[12:34:53.719] iteration 3726: total_loss: 0.490072, loss_sup: 0.386764, loss_mps: 0.035346, loss_cps: 0.067963
[12:34:53.866] iteration 3727: total_loss: 0.229858, loss_sup: 0.148674, loss_mps: 0.029878, loss_cps: 0.051306
[12:34:54.012] iteration 3728: total_loss: 0.549566, loss_sup: 0.468464, loss_mps: 0.029589, loss_cps: 0.051513
[12:34:54.159] iteration 3729: total_loss: 0.417835, loss_sup: 0.335332, loss_mps: 0.030456, loss_cps: 0.052047
[12:34:54.305] iteration 3730: total_loss: 0.249930, loss_sup: 0.178749, loss_mps: 0.028269, loss_cps: 0.042911
[12:34:54.451] iteration 3731: total_loss: 0.291054, loss_sup: 0.219260, loss_mps: 0.028195, loss_cps: 0.043600
[12:34:54.596] iteration 3732: total_loss: 0.173395, loss_sup: 0.096803, loss_mps: 0.029213, loss_cps: 0.047379
[12:34:54.743] iteration 3733: total_loss: 0.183859, loss_sup: 0.109637, loss_mps: 0.028855, loss_cps: 0.045367
[12:34:54.890] iteration 3734: total_loss: 0.296369, loss_sup: 0.201589, loss_mps: 0.035790, loss_cps: 0.058990
[12:34:55.036] iteration 3735: total_loss: 0.451114, loss_sup: 0.363088, loss_mps: 0.032266, loss_cps: 0.055760
[12:34:55.183] iteration 3736: total_loss: 0.294292, loss_sup: 0.195866, loss_mps: 0.036455, loss_cps: 0.061970
[12:34:55.330] iteration 3737: total_loss: 0.328480, loss_sup: 0.255594, loss_mps: 0.028656, loss_cps: 0.044231
[12:34:55.475] iteration 3738: total_loss: 0.311436, loss_sup: 0.200339, loss_mps: 0.039536, loss_cps: 0.071560
[12:34:55.621] iteration 3739: total_loss: 0.234030, loss_sup: 0.138348, loss_mps: 0.034972, loss_cps: 0.060711
[12:34:55.767] iteration 3740: total_loss: 0.248320, loss_sup: 0.178838, loss_mps: 0.026209, loss_cps: 0.043274
[12:34:55.913] iteration 3741: total_loss: 0.288300, loss_sup: 0.205673, loss_mps: 0.030845, loss_cps: 0.051783
[12:34:56.059] iteration 3742: total_loss: 0.402462, loss_sup: 0.322179, loss_mps: 0.030742, loss_cps: 0.049542
[12:34:56.209] iteration 3743: total_loss: 0.582551, loss_sup: 0.480777, loss_mps: 0.036999, loss_cps: 0.064775
[12:34:56.355] iteration 3744: total_loss: 0.345077, loss_sup: 0.278310, loss_mps: 0.026203, loss_cps: 0.040564
[12:34:56.501] iteration 3745: total_loss: 0.175488, loss_sup: 0.103444, loss_mps: 0.026659, loss_cps: 0.045385
[12:34:56.647] iteration 3746: total_loss: 0.345959, loss_sup: 0.268284, loss_mps: 0.028628, loss_cps: 0.049046
[12:34:56.793] iteration 3747: total_loss: 0.279379, loss_sup: 0.193229, loss_mps: 0.030649, loss_cps: 0.055501
[12:34:56.939] iteration 3748: total_loss: 0.239922, loss_sup: 0.186221, loss_mps: 0.021202, loss_cps: 0.032500
[12:34:57.085] iteration 3749: total_loss: 0.134236, loss_sup: 0.065065, loss_mps: 0.025269, loss_cps: 0.043902
[12:34:57.231] iteration 3750: total_loss: 0.184518, loss_sup: 0.115375, loss_mps: 0.025852, loss_cps: 0.043290
[12:34:57.377] iteration 3751: total_loss: 0.241427, loss_sup: 0.134112, loss_mps: 0.037090, loss_cps: 0.070225
[12:34:57.522] iteration 3752: total_loss: 0.099830, loss_sup: 0.041639, loss_mps: 0.021810, loss_cps: 0.036381
[12:34:57.668] iteration 3753: total_loss: 0.186198, loss_sup: 0.116651, loss_mps: 0.024479, loss_cps: 0.045067
[12:34:57.814] iteration 3754: total_loss: 0.256771, loss_sup: 0.156478, loss_mps: 0.033748, loss_cps: 0.066546
[12:34:57.960] iteration 3755: total_loss: 0.180385, loss_sup: 0.109629, loss_mps: 0.026424, loss_cps: 0.044332
[12:34:58.106] iteration 3756: total_loss: 0.336660, loss_sup: 0.262605, loss_mps: 0.025860, loss_cps: 0.048194
[12:34:58.252] iteration 3757: total_loss: 0.293323, loss_sup: 0.229940, loss_mps: 0.022787, loss_cps: 0.040596
[12:34:58.398] iteration 3758: total_loss: 0.284244, loss_sup: 0.221872, loss_mps: 0.022019, loss_cps: 0.040353
[12:34:58.544] iteration 3759: total_loss: 0.188596, loss_sup: 0.103053, loss_mps: 0.028810, loss_cps: 0.056733
[12:34:58.691] iteration 3760: total_loss: 0.273248, loss_sup: 0.171129, loss_mps: 0.034191, loss_cps: 0.067928
[12:34:58.837] iteration 3761: total_loss: 0.323013, loss_sup: 0.242801, loss_mps: 0.027807, loss_cps: 0.052405
[12:34:58.899] iteration 3762: total_loss: 0.166463, loss_sup: 0.038956, loss_mps: 0.040937, loss_cps: 0.086569
[12:35:00.099] iteration 3763: total_loss: 0.204047, loss_sup: 0.130761, loss_mps: 0.025751, loss_cps: 0.047535
[12:35:00.249] iteration 3764: total_loss: 0.444762, loss_sup: 0.364291, loss_mps: 0.028113, loss_cps: 0.052358
[12:35:00.397] iteration 3765: total_loss: 0.272142, loss_sup: 0.195330, loss_mps: 0.027228, loss_cps: 0.049584
[12:35:00.544] iteration 3766: total_loss: 0.360058, loss_sup: 0.289624, loss_mps: 0.025391, loss_cps: 0.045043
[12:35:00.690] iteration 3767: total_loss: 0.236894, loss_sup: 0.152059, loss_mps: 0.029237, loss_cps: 0.055599
[12:35:00.837] iteration 3768: total_loss: 0.266440, loss_sup: 0.157996, loss_mps: 0.035826, loss_cps: 0.072618
[12:35:00.984] iteration 3769: total_loss: 0.523985, loss_sup: 0.395003, loss_mps: 0.040356, loss_cps: 0.088626
[12:35:01.130] iteration 3770: total_loss: 0.165088, loss_sup: 0.068189, loss_mps: 0.033497, loss_cps: 0.063402
[12:35:01.277] iteration 3771: total_loss: 0.268102, loss_sup: 0.172812, loss_mps: 0.032461, loss_cps: 0.062830
[12:35:01.425] iteration 3772: total_loss: 0.212757, loss_sup: 0.145887, loss_mps: 0.024278, loss_cps: 0.042591
[12:35:01.572] iteration 3773: total_loss: 0.229870, loss_sup: 0.148317, loss_mps: 0.028476, loss_cps: 0.053077
[12:35:01.718] iteration 3774: total_loss: 0.265943, loss_sup: 0.169431, loss_mps: 0.033220, loss_cps: 0.063292
[12:35:01.865] iteration 3775: total_loss: 0.160695, loss_sup: 0.088547, loss_mps: 0.026100, loss_cps: 0.046048
[12:35:02.011] iteration 3776: total_loss: 0.200098, loss_sup: 0.126868, loss_mps: 0.026557, loss_cps: 0.046674
[12:35:02.161] iteration 3777: total_loss: 0.335398, loss_sup: 0.255384, loss_mps: 0.028979, loss_cps: 0.051035
[12:35:02.307] iteration 3778: total_loss: 0.320780, loss_sup: 0.235113, loss_mps: 0.030125, loss_cps: 0.055542
[12:35:02.455] iteration 3779: total_loss: 0.321712, loss_sup: 0.193583, loss_mps: 0.041542, loss_cps: 0.086587
[12:35:02.603] iteration 3780: total_loss: 0.215344, loss_sup: 0.141049, loss_mps: 0.027501, loss_cps: 0.046793
[12:35:02.749] iteration 3781: total_loss: 0.200427, loss_sup: 0.112100, loss_mps: 0.029733, loss_cps: 0.058593
[12:35:02.895] iteration 3782: total_loss: 0.237627, loss_sup: 0.156923, loss_mps: 0.027593, loss_cps: 0.053111
[12:35:03.041] iteration 3783: total_loss: 0.211997, loss_sup: 0.131810, loss_mps: 0.028139, loss_cps: 0.052048
[12:35:03.188] iteration 3784: total_loss: 0.264209, loss_sup: 0.178398, loss_mps: 0.029682, loss_cps: 0.056129
[12:35:03.334] iteration 3785: total_loss: 0.408916, loss_sup: 0.340071, loss_mps: 0.024236, loss_cps: 0.044610
[12:35:03.482] iteration 3786: total_loss: 0.303548, loss_sup: 0.214698, loss_mps: 0.031436, loss_cps: 0.057414
[12:35:03.629] iteration 3787: total_loss: 0.338158, loss_sup: 0.248887, loss_mps: 0.030955, loss_cps: 0.058315
[12:35:03.775] iteration 3788: total_loss: 0.417977, loss_sup: 0.334777, loss_mps: 0.030450, loss_cps: 0.052750
[12:35:03.922] iteration 3789: total_loss: 0.175018, loss_sup: 0.102610, loss_mps: 0.026622, loss_cps: 0.045786
[12:35:04.069] iteration 3790: total_loss: 0.251408, loss_sup: 0.168672, loss_mps: 0.029659, loss_cps: 0.053076
[12:35:04.216] iteration 3791: total_loss: 0.303948, loss_sup: 0.224409, loss_mps: 0.029101, loss_cps: 0.050439
[12:35:04.363] iteration 3792: total_loss: 0.253265, loss_sup: 0.178225, loss_mps: 0.027787, loss_cps: 0.047253
[12:35:04.509] iteration 3793: total_loss: 0.192930, loss_sup: 0.130660, loss_mps: 0.023122, loss_cps: 0.039148
[12:35:04.658] iteration 3794: total_loss: 0.262870, loss_sup: 0.180466, loss_mps: 0.029351, loss_cps: 0.053053
[12:35:04.804] iteration 3795: total_loss: 0.153120, loss_sup: 0.081522, loss_mps: 0.025726, loss_cps: 0.045872
[12:35:04.950] iteration 3796: total_loss: 0.361093, loss_sup: 0.250265, loss_mps: 0.037548, loss_cps: 0.073280
[12:35:05.097] iteration 3797: total_loss: 0.406112, loss_sup: 0.321905, loss_mps: 0.029397, loss_cps: 0.054810
[12:35:05.244] iteration 3798: total_loss: 0.378463, loss_sup: 0.306889, loss_mps: 0.026390, loss_cps: 0.045184
[12:35:05.395] iteration 3799: total_loss: 0.150699, loss_sup: 0.092346, loss_mps: 0.021640, loss_cps: 0.036713
[12:35:05.541] iteration 3800: total_loss: 0.212477, loss_sup: 0.131564, loss_mps: 0.028690, loss_cps: 0.052223
[12:35:05.541] Evaluation Started ==>
[12:35:16.880] ==> valid iteration 3800: unet metrics: {'dc': 0.5660226958414699, 'jc': 0.4334905163516312, 'pre': 0.6094509083886736, 'hd': 6.957166487963209}, ynet metrics: {'dc': 0.5089239145948559, 'jc': 0.3782438737898728, 'pre': 0.6133542026086175, 'hd': 6.8801490891984916}.
[12:35:16.882] Evaluation Finished!⏹️
[12:35:17.036] iteration 3801: total_loss: 0.439550, loss_sup: 0.343838, loss_mps: 0.033114, loss_cps: 0.062598
[12:35:17.187] iteration 3802: total_loss: 0.161028, loss_sup: 0.104933, loss_mps: 0.020974, loss_cps: 0.035121
[12:35:17.334] iteration 3803: total_loss: 0.300356, loss_sup: 0.209537, loss_mps: 0.031121, loss_cps: 0.059698
[12:35:17.479] iteration 3804: total_loss: 0.324609, loss_sup: 0.224379, loss_mps: 0.035630, loss_cps: 0.064599
[12:35:17.624] iteration 3805: total_loss: 0.240712, loss_sup: 0.169231, loss_mps: 0.025606, loss_cps: 0.045874
[12:35:17.769] iteration 3806: total_loss: 0.440828, loss_sup: 0.375368, loss_mps: 0.025229, loss_cps: 0.040231
[12:35:17.914] iteration 3807: total_loss: 0.244943, loss_sup: 0.179181, loss_mps: 0.024413, loss_cps: 0.041349
[12:35:18.060] iteration 3808: total_loss: 0.219748, loss_sup: 0.149644, loss_mps: 0.025415, loss_cps: 0.044689
[12:35:18.205] iteration 3809: total_loss: 0.284303, loss_sup: 0.189430, loss_mps: 0.033105, loss_cps: 0.061768
[12:35:18.349] iteration 3810: total_loss: 0.380463, loss_sup: 0.301597, loss_mps: 0.029018, loss_cps: 0.049847
[12:35:18.495] iteration 3811: total_loss: 0.398039, loss_sup: 0.312290, loss_mps: 0.031123, loss_cps: 0.054627
[12:35:18.640] iteration 3812: total_loss: 0.632553, loss_sup: 0.564985, loss_mps: 0.024851, loss_cps: 0.042717
[12:35:18.786] iteration 3813: total_loss: 0.701051, loss_sup: 0.605257, loss_mps: 0.033540, loss_cps: 0.062254
[12:35:18.931] iteration 3814: total_loss: 0.315555, loss_sup: 0.204621, loss_mps: 0.038012, loss_cps: 0.072922
[12:35:19.077] iteration 3815: total_loss: 0.697702, loss_sup: 0.569408, loss_mps: 0.044537, loss_cps: 0.083757
[12:35:19.222] iteration 3816: total_loss: 0.275163, loss_sup: 0.210630, loss_mps: 0.026476, loss_cps: 0.038057
[12:35:19.367] iteration 3817: total_loss: 0.173421, loss_sup: 0.105628, loss_mps: 0.026519, loss_cps: 0.041274
[12:35:19.513] iteration 3818: total_loss: 0.351969, loss_sup: 0.279013, loss_mps: 0.027882, loss_cps: 0.045074
[12:35:19.658] iteration 3819: total_loss: 0.215855, loss_sup: 0.133341, loss_mps: 0.030827, loss_cps: 0.051686
[12:35:19.806] iteration 3820: total_loss: 0.209413, loss_sup: 0.114152, loss_mps: 0.034717, loss_cps: 0.060544
[12:35:19.951] iteration 3821: total_loss: 0.261794, loss_sup: 0.149656, loss_mps: 0.040640, loss_cps: 0.071498
[12:35:20.099] iteration 3822: total_loss: 0.146273, loss_sup: 0.085749, loss_mps: 0.024709, loss_cps: 0.035815
[12:35:20.245] iteration 3823: total_loss: 0.318118, loss_sup: 0.221575, loss_mps: 0.034981, loss_cps: 0.061562
[12:35:20.390] iteration 3824: total_loss: 0.414135, loss_sup: 0.315779, loss_mps: 0.036334, loss_cps: 0.062021
[12:35:20.536] iteration 3825: total_loss: 0.281953, loss_sup: 0.172683, loss_mps: 0.039373, loss_cps: 0.069897
[12:35:20.682] iteration 3826: total_loss: 0.410718, loss_sup: 0.332069, loss_mps: 0.031138, loss_cps: 0.047510
[12:35:20.829] iteration 3827: total_loss: 0.311138, loss_sup: 0.224656, loss_mps: 0.032982, loss_cps: 0.053500
[12:35:20.975] iteration 3828: total_loss: 0.394845, loss_sup: 0.285700, loss_mps: 0.038286, loss_cps: 0.070859
[12:35:21.120] iteration 3829: total_loss: 0.310425, loss_sup: 0.228441, loss_mps: 0.031009, loss_cps: 0.050975
[12:35:21.266] iteration 3830: total_loss: 0.268895, loss_sup: 0.162379, loss_mps: 0.038021, loss_cps: 0.068496
[12:35:21.412] iteration 3831: total_loss: 0.322227, loss_sup: 0.216525, loss_mps: 0.037968, loss_cps: 0.067735
[12:35:21.558] iteration 3832: total_loss: 0.201077, loss_sup: 0.100582, loss_mps: 0.037515, loss_cps: 0.062979
[12:35:21.703] iteration 3833: total_loss: 0.317906, loss_sup: 0.205305, loss_mps: 0.039642, loss_cps: 0.072959
[12:35:21.854] iteration 3834: total_loss: 0.275282, loss_sup: 0.181346, loss_mps: 0.034304, loss_cps: 0.059633
[12:35:22.001] iteration 3835: total_loss: 0.235735, loss_sup: 0.141256, loss_mps: 0.033279, loss_cps: 0.061199
[12:35:22.146] iteration 3836: total_loss: 0.445289, loss_sup: 0.367348, loss_mps: 0.028834, loss_cps: 0.049107
[12:35:22.292] iteration 3837: total_loss: 0.173283, loss_sup: 0.095295, loss_mps: 0.029129, loss_cps: 0.048859
[12:35:22.438] iteration 3838: total_loss: 0.251150, loss_sup: 0.176810, loss_mps: 0.028779, loss_cps: 0.045560
[12:35:22.585] iteration 3839: total_loss: 0.344624, loss_sup: 0.252939, loss_mps: 0.032257, loss_cps: 0.059428
[12:35:22.731] iteration 3840: total_loss: 0.328338, loss_sup: 0.221727, loss_mps: 0.037627, loss_cps: 0.068984
[12:35:22.878] iteration 3841: total_loss: 0.262449, loss_sup: 0.185044, loss_mps: 0.027306, loss_cps: 0.050098
[12:35:23.024] iteration 3842: total_loss: 0.220652, loss_sup: 0.139929, loss_mps: 0.029262, loss_cps: 0.051461
[12:35:23.170] iteration 3843: total_loss: 0.189558, loss_sup: 0.097124, loss_mps: 0.032757, loss_cps: 0.059677
[12:35:23.316] iteration 3844: total_loss: 0.507331, loss_sup: 0.440420, loss_mps: 0.025118, loss_cps: 0.041793
[12:35:23.462] iteration 3845: total_loss: 0.251668, loss_sup: 0.183091, loss_mps: 0.025162, loss_cps: 0.043414
[12:35:23.608] iteration 3846: total_loss: 0.430025, loss_sup: 0.342337, loss_mps: 0.031881, loss_cps: 0.055807
[12:35:23.754] iteration 3847: total_loss: 0.151221, loss_sup: 0.091431, loss_mps: 0.022818, loss_cps: 0.036972
[12:35:23.902] iteration 3848: total_loss: 0.462479, loss_sup: 0.391440, loss_mps: 0.026230, loss_cps: 0.044810
[12:35:24.048] iteration 3849: total_loss: 0.130569, loss_sup: 0.070846, loss_mps: 0.022051, loss_cps: 0.037673
[12:35:24.194] iteration 3850: total_loss: 0.221603, loss_sup: 0.135922, loss_mps: 0.029564, loss_cps: 0.056117
[12:35:24.340] iteration 3851: total_loss: 0.312861, loss_sup: 0.247102, loss_mps: 0.025112, loss_cps: 0.040647
[12:35:24.486] iteration 3852: total_loss: 0.380026, loss_sup: 0.298467, loss_mps: 0.029266, loss_cps: 0.052293
[12:35:24.633] iteration 3853: total_loss: 0.383736, loss_sup: 0.280498, loss_mps: 0.035981, loss_cps: 0.067258
[12:35:24.779] iteration 3854: total_loss: 0.173086, loss_sup: 0.105620, loss_mps: 0.025364, loss_cps: 0.042102
[12:35:24.927] iteration 3855: total_loss: 0.202623, loss_sup: 0.137670, loss_mps: 0.025109, loss_cps: 0.039844
[12:35:25.073] iteration 3856: total_loss: 0.219750, loss_sup: 0.150198, loss_mps: 0.025877, loss_cps: 0.043675
[12:35:25.219] iteration 3857: total_loss: 0.209664, loss_sup: 0.145049, loss_mps: 0.024293, loss_cps: 0.040321
[12:35:25.365] iteration 3858: total_loss: 0.227220, loss_sup: 0.139811, loss_mps: 0.030008, loss_cps: 0.057401
[12:35:25.514] iteration 3859: total_loss: 0.250997, loss_sup: 0.150261, loss_mps: 0.034658, loss_cps: 0.066079
[12:35:25.660] iteration 3860: total_loss: 0.229741, loss_sup: 0.142703, loss_mps: 0.031483, loss_cps: 0.055555
[12:35:25.806] iteration 3861: total_loss: 0.289809, loss_sup: 0.204883, loss_mps: 0.029917, loss_cps: 0.055009
[12:35:25.954] iteration 3862: total_loss: 0.243923, loss_sup: 0.170916, loss_mps: 0.027805, loss_cps: 0.045201
[12:35:26.100] iteration 3863: total_loss: 0.213483, loss_sup: 0.122827, loss_mps: 0.031846, loss_cps: 0.058810
[12:35:26.247] iteration 3864: total_loss: 0.148904, loss_sup: 0.085738, loss_mps: 0.023155, loss_cps: 0.040011
[12:35:26.395] iteration 3865: total_loss: 0.206017, loss_sup: 0.106122, loss_mps: 0.033438, loss_cps: 0.066457
[12:35:26.542] iteration 3866: total_loss: 0.117590, loss_sup: 0.046635, loss_mps: 0.025780, loss_cps: 0.045176
[12:35:26.691] iteration 3867: total_loss: 0.285806, loss_sup: 0.196001, loss_mps: 0.031086, loss_cps: 0.058718
[12:35:26.837] iteration 3868: total_loss: 0.085393, loss_sup: 0.040061, loss_mps: 0.017448, loss_cps: 0.027884
[12:35:26.984] iteration 3869: total_loss: 0.193944, loss_sup: 0.118786, loss_mps: 0.026117, loss_cps: 0.049041
[12:35:27.131] iteration 3870: total_loss: 0.250237, loss_sup: 0.174579, loss_mps: 0.027802, loss_cps: 0.047856
[12:35:27.277] iteration 3871: total_loss: 0.179234, loss_sup: 0.118186, loss_mps: 0.022358, loss_cps: 0.038690
[12:35:27.423] iteration 3872: total_loss: 0.374467, loss_sup: 0.285131, loss_mps: 0.030255, loss_cps: 0.059081
[12:35:27.569] iteration 3873: total_loss: 0.256786, loss_sup: 0.155501, loss_mps: 0.033876, loss_cps: 0.067409
[12:35:27.717] iteration 3874: total_loss: 0.415237, loss_sup: 0.326170, loss_mps: 0.031362, loss_cps: 0.057705
[12:35:27.863] iteration 3875: total_loss: 0.096867, loss_sup: 0.038363, loss_mps: 0.021747, loss_cps: 0.036756
[12:35:28.011] iteration 3876: total_loss: 0.327484, loss_sup: 0.263430, loss_mps: 0.023808, loss_cps: 0.040246
[12:35:28.157] iteration 3877: total_loss: 0.198268, loss_sup: 0.140954, loss_mps: 0.021082, loss_cps: 0.036232
[12:35:28.303] iteration 3878: total_loss: 0.255067, loss_sup: 0.192086, loss_mps: 0.022470, loss_cps: 0.040511
[12:35:28.452] iteration 3879: total_loss: 0.452315, loss_sup: 0.362309, loss_mps: 0.031293, loss_cps: 0.058713
[12:35:28.600] iteration 3880: total_loss: 0.167845, loss_sup: 0.090510, loss_mps: 0.027123, loss_cps: 0.050211
[12:35:28.746] iteration 3881: total_loss: 0.266002, loss_sup: 0.178615, loss_mps: 0.029107, loss_cps: 0.058280
[12:35:28.891] iteration 3882: total_loss: 0.264080, loss_sup: 0.182561, loss_mps: 0.028223, loss_cps: 0.053297
[12:35:29.038] iteration 3883: total_loss: 0.218802, loss_sup: 0.138848, loss_mps: 0.028485, loss_cps: 0.051468
[12:35:29.188] iteration 3884: total_loss: 0.198855, loss_sup: 0.121055, loss_mps: 0.028373, loss_cps: 0.049428
[12:35:29.334] iteration 3885: total_loss: 0.407799, loss_sup: 0.267405, loss_mps: 0.045458, loss_cps: 0.094936
[12:35:29.479] iteration 3886: total_loss: 0.336556, loss_sup: 0.255235, loss_mps: 0.029567, loss_cps: 0.051754
[12:35:29.625] iteration 3887: total_loss: 0.298818, loss_sup: 0.237831, loss_mps: 0.023493, loss_cps: 0.037494
[12:35:29.771] iteration 3888: total_loss: 0.166442, loss_sup: 0.096768, loss_mps: 0.025676, loss_cps: 0.043998
[12:35:29.917] iteration 3889: total_loss: 0.307640, loss_sup: 0.226092, loss_mps: 0.029550, loss_cps: 0.051998
[12:35:30.063] iteration 3890: total_loss: 0.257538, loss_sup: 0.172553, loss_mps: 0.029767, loss_cps: 0.055218
[12:35:30.209] iteration 3891: total_loss: 0.148709, loss_sup: 0.070351, loss_mps: 0.028530, loss_cps: 0.049827
[12:35:30.359] iteration 3892: total_loss: 0.194936, loss_sup: 0.112051, loss_mps: 0.030099, loss_cps: 0.052786
[12:35:30.512] iteration 3893: total_loss: 0.358245, loss_sup: 0.245936, loss_mps: 0.037902, loss_cps: 0.074407
[12:35:30.659] iteration 3894: total_loss: 0.181187, loss_sup: 0.099047, loss_mps: 0.029463, loss_cps: 0.052677
[12:35:30.808] iteration 3895: total_loss: 0.240057, loss_sup: 0.169786, loss_mps: 0.026421, loss_cps: 0.043849
[12:35:30.955] iteration 3896: total_loss: 0.312454, loss_sup: 0.226426, loss_mps: 0.031431, loss_cps: 0.054597
[12:35:31.103] iteration 3897: total_loss: 0.189316, loss_sup: 0.116862, loss_mps: 0.027330, loss_cps: 0.045124
[12:35:31.251] iteration 3898: total_loss: 0.232424, loss_sup: 0.165994, loss_mps: 0.024425, loss_cps: 0.042006
[12:35:31.398] iteration 3899: total_loss: 0.202027, loss_sup: 0.111119, loss_mps: 0.032390, loss_cps: 0.058518
[12:35:31.546] iteration 3900: total_loss: 0.195131, loss_sup: 0.121267, loss_mps: 0.026876, loss_cps: 0.046987
[12:35:31.546] Evaluation Started ==>
[12:35:42.884] ==> valid iteration 3900: unet metrics: {'dc': 0.5861828785280839, 'jc': 0.45691065695783156, 'pre': 0.6207411995366329, 'hd': 6.947375993785038}, ynet metrics: {'dc': 0.5336199758232474, 'jc': 0.40445948789274266, 'pre': 0.5853900510815786, 'hd': 7.2649496601358186}.
[12:35:43.041] ==> New best valid dice for ynet: 0.533620, at iteration 3900
[12:35:43.043] Evaluation Finished!⏹️
[12:35:43.196] iteration 3901: total_loss: 0.200063, loss_sup: 0.116646, loss_mps: 0.029312, loss_cps: 0.054106
[12:35:43.344] iteration 3902: total_loss: 0.313682, loss_sup: 0.227886, loss_mps: 0.029521, loss_cps: 0.056275
[12:35:43.489] iteration 3903: total_loss: 0.200077, loss_sup: 0.113694, loss_mps: 0.030591, loss_cps: 0.055791
[12:35:43.635] iteration 3904: total_loss: 0.193063, loss_sup: 0.137372, loss_mps: 0.021113, loss_cps: 0.034577
[12:35:43.779] iteration 3905: total_loss: 0.407486, loss_sup: 0.323904, loss_mps: 0.028981, loss_cps: 0.054601
[12:35:43.924] iteration 3906: total_loss: 0.362790, loss_sup: 0.270970, loss_mps: 0.032959, loss_cps: 0.058861
[12:35:44.069] iteration 3907: total_loss: 0.206384, loss_sup: 0.126110, loss_mps: 0.028394, loss_cps: 0.051879
[12:35:44.214] iteration 3908: total_loss: 0.318277, loss_sup: 0.224255, loss_mps: 0.032926, loss_cps: 0.061096
[12:35:44.359] iteration 3909: total_loss: 0.257123, loss_sup: 0.177745, loss_mps: 0.029147, loss_cps: 0.050231
[12:35:44.503] iteration 3910: total_loss: 0.186826, loss_sup: 0.113746, loss_mps: 0.026355, loss_cps: 0.046725
[12:35:44.649] iteration 3911: total_loss: 0.127113, loss_sup: 0.053732, loss_mps: 0.026447, loss_cps: 0.046934
[12:35:44.795] iteration 3912: total_loss: 0.260546, loss_sup: 0.180981, loss_mps: 0.028504, loss_cps: 0.051061
[12:35:44.940] iteration 3913: total_loss: 0.141710, loss_sup: 0.072151, loss_mps: 0.025627, loss_cps: 0.043932
[12:35:45.086] iteration 3914: total_loss: 0.280202, loss_sup: 0.216473, loss_mps: 0.023589, loss_cps: 0.040140
[12:35:45.231] iteration 3915: total_loss: 0.210630, loss_sup: 0.126904, loss_mps: 0.029755, loss_cps: 0.053972
[12:35:45.376] iteration 3916: total_loss: 0.420335, loss_sup: 0.316611, loss_mps: 0.034680, loss_cps: 0.069045
[12:35:45.521] iteration 3917: total_loss: 0.189012, loss_sup: 0.109636, loss_mps: 0.027883, loss_cps: 0.051493
[12:35:45.669] iteration 3918: total_loss: 0.380385, loss_sup: 0.303806, loss_mps: 0.027881, loss_cps: 0.048698
[12:35:45.815] iteration 3919: total_loss: 0.152525, loss_sup: 0.098226, loss_mps: 0.021185, loss_cps: 0.033114
[12:35:45.961] iteration 3920: total_loss: 0.238712, loss_sup: 0.167061, loss_mps: 0.026740, loss_cps: 0.044911
[12:35:46.109] iteration 3921: total_loss: 0.251453, loss_sup: 0.146274, loss_mps: 0.035839, loss_cps: 0.069340
[12:35:46.254] iteration 3922: total_loss: 0.340595, loss_sup: 0.274647, loss_mps: 0.023961, loss_cps: 0.041987
[12:35:46.399] iteration 3923: total_loss: 0.155784, loss_sup: 0.088883, loss_mps: 0.024815, loss_cps: 0.042086
[12:35:46.545] iteration 3924: total_loss: 0.351329, loss_sup: 0.257506, loss_mps: 0.032383, loss_cps: 0.061440
[12:35:46.690] iteration 3925: total_loss: 0.165907, loss_sup: 0.053033, loss_mps: 0.036637, loss_cps: 0.076237
[12:35:46.835] iteration 3926: total_loss: 0.252957, loss_sup: 0.134240, loss_mps: 0.040382, loss_cps: 0.078335
[12:35:46.981] iteration 3927: total_loss: 0.375435, loss_sup: 0.300468, loss_mps: 0.026529, loss_cps: 0.048438
[12:35:47.126] iteration 3928: total_loss: 0.199224, loss_sup: 0.122744, loss_mps: 0.026833, loss_cps: 0.049648
[12:35:47.272] iteration 3929: total_loss: 0.293609, loss_sup: 0.200582, loss_mps: 0.032445, loss_cps: 0.060582
[12:35:47.417] iteration 3930: total_loss: 0.663178, loss_sup: 0.577120, loss_mps: 0.029768, loss_cps: 0.056291
[12:35:47.563] iteration 3931: total_loss: 0.222586, loss_sup: 0.148383, loss_mps: 0.025906, loss_cps: 0.048297
[12:35:47.710] iteration 3932: total_loss: 0.298902, loss_sup: 0.226564, loss_mps: 0.025839, loss_cps: 0.046499
[12:35:47.856] iteration 3933: total_loss: 0.538093, loss_sup: 0.446702, loss_mps: 0.031620, loss_cps: 0.059770
[12:35:48.001] iteration 3934: total_loss: 0.398177, loss_sup: 0.305228, loss_mps: 0.032307, loss_cps: 0.060643
[12:35:48.147] iteration 3935: total_loss: 0.192721, loss_sup: 0.114214, loss_mps: 0.028816, loss_cps: 0.049690
[12:35:48.293] iteration 3936: total_loss: 0.130259, loss_sup: 0.080648, loss_mps: 0.019700, loss_cps: 0.029911
[12:35:48.439] iteration 3937: total_loss: 0.206622, loss_sup: 0.146209, loss_mps: 0.023831, loss_cps: 0.036583
[12:35:48.584] iteration 3938: total_loss: 0.219334, loss_sup: 0.139932, loss_mps: 0.028071, loss_cps: 0.051332
[12:35:48.730] iteration 3939: total_loss: 0.178677, loss_sup: 0.108108, loss_mps: 0.026670, loss_cps: 0.043899
[12:35:48.876] iteration 3940: total_loss: 0.220436, loss_sup: 0.145302, loss_mps: 0.027757, loss_cps: 0.047377
[12:35:49.024] iteration 3941: total_loss: 0.289191, loss_sup: 0.175354, loss_mps: 0.037002, loss_cps: 0.076835
[12:35:49.170] iteration 3942: total_loss: 0.226066, loss_sup: 0.128871, loss_mps: 0.033341, loss_cps: 0.063854
[12:35:49.316] iteration 3943: total_loss: 0.188437, loss_sup: 0.108641, loss_mps: 0.028557, loss_cps: 0.051238
[12:35:49.464] iteration 3944: total_loss: 0.141923, loss_sup: 0.045252, loss_mps: 0.033378, loss_cps: 0.063293
[12:35:49.611] iteration 3945: total_loss: 0.268432, loss_sup: 0.171953, loss_mps: 0.034056, loss_cps: 0.062423
[12:35:49.757] iteration 3946: total_loss: 0.217670, loss_sup: 0.133143, loss_mps: 0.030747, loss_cps: 0.053781
[12:35:49.903] iteration 3947: total_loss: 0.216672, loss_sup: 0.109865, loss_mps: 0.035722, loss_cps: 0.071085
[12:35:50.050] iteration 3948: total_loss: 0.187534, loss_sup: 0.106594, loss_mps: 0.028849, loss_cps: 0.052092
[12:35:50.196] iteration 3949: total_loss: 0.250178, loss_sup: 0.201659, loss_mps: 0.019352, loss_cps: 0.029167
[12:35:50.342] iteration 3950: total_loss: 0.264828, loss_sup: 0.205629, loss_mps: 0.021222, loss_cps: 0.037977
[12:35:50.488] iteration 3951: total_loss: 0.282685, loss_sup: 0.187076, loss_mps: 0.032083, loss_cps: 0.063526
[12:35:50.633] iteration 3952: total_loss: 0.128850, loss_sup: 0.068647, loss_mps: 0.022500, loss_cps: 0.037704
[12:35:50.779] iteration 3953: total_loss: 0.150954, loss_sup: 0.088217, loss_mps: 0.022735, loss_cps: 0.040002
[12:35:50.924] iteration 3954: total_loss: 0.173160, loss_sup: 0.092137, loss_mps: 0.027821, loss_cps: 0.053201
[12:35:51.069] iteration 3955: total_loss: 0.515407, loss_sup: 0.383072, loss_mps: 0.042524, loss_cps: 0.089811
[12:35:51.215] iteration 3956: total_loss: 0.279109, loss_sup: 0.165709, loss_mps: 0.038462, loss_cps: 0.074938
[12:35:51.360] iteration 3957: total_loss: 0.195039, loss_sup: 0.101829, loss_mps: 0.030844, loss_cps: 0.062366
[12:35:51.506] iteration 3958: total_loss: 0.233654, loss_sup: 0.165923, loss_mps: 0.024257, loss_cps: 0.043474
[12:35:51.651] iteration 3959: total_loss: 0.403624, loss_sup: 0.312573, loss_mps: 0.030869, loss_cps: 0.060182
[12:35:51.796] iteration 3960: total_loss: 0.414380, loss_sup: 0.305600, loss_mps: 0.036487, loss_cps: 0.072293
[12:35:51.942] iteration 3961: total_loss: 0.186722, loss_sup: 0.122386, loss_mps: 0.023870, loss_cps: 0.040466
[12:35:52.088] iteration 3962: total_loss: 0.329686, loss_sup: 0.227905, loss_mps: 0.036613, loss_cps: 0.065167
[12:35:52.233] iteration 3963: total_loss: 0.175983, loss_sup: 0.091767, loss_mps: 0.030407, loss_cps: 0.053809
[12:35:52.381] iteration 3964: total_loss: 0.181867, loss_sup: 0.092713, loss_mps: 0.030990, loss_cps: 0.058164
[12:35:52.526] iteration 3965: total_loss: 0.284934, loss_sup: 0.210693, loss_mps: 0.028284, loss_cps: 0.045957
[12:35:52.671] iteration 3966: total_loss: 0.386471, loss_sup: 0.294701, loss_mps: 0.032904, loss_cps: 0.058867
[12:35:52.816] iteration 3967: total_loss: 0.264328, loss_sup: 0.172387, loss_mps: 0.034041, loss_cps: 0.057900
[12:35:52.962] iteration 3968: total_loss: 0.335951, loss_sup: 0.257027, loss_mps: 0.029086, loss_cps: 0.049838
[12:35:53.107] iteration 3969: total_loss: 0.319704, loss_sup: 0.241000, loss_mps: 0.029288, loss_cps: 0.049416
[12:35:53.253] iteration 3970: total_loss: 0.165286, loss_sup: 0.086293, loss_mps: 0.029840, loss_cps: 0.049153
[12:35:53.398] iteration 3971: total_loss: 0.211763, loss_sup: 0.109643, loss_mps: 0.036866, loss_cps: 0.065254
[12:35:53.544] iteration 3972: total_loss: 0.178994, loss_sup: 0.105970, loss_mps: 0.027418, loss_cps: 0.045606
[12:35:53.690] iteration 3973: total_loss: 0.280881, loss_sup: 0.192155, loss_mps: 0.032416, loss_cps: 0.056309
[12:35:53.835] iteration 3974: total_loss: 0.285858, loss_sup: 0.185729, loss_mps: 0.035555, loss_cps: 0.064574
[12:35:53.981] iteration 3975: total_loss: 0.275782, loss_sup: 0.168879, loss_mps: 0.037526, loss_cps: 0.069377
[12:35:54.126] iteration 3976: total_loss: 0.227840, loss_sup: 0.148943, loss_mps: 0.029314, loss_cps: 0.049583
[12:35:54.272] iteration 3977: total_loss: 0.399465, loss_sup: 0.318675, loss_mps: 0.030194, loss_cps: 0.050596
[12:35:54.418] iteration 3978: total_loss: 0.185740, loss_sup: 0.080718, loss_mps: 0.036961, loss_cps: 0.068061
[12:35:54.563] iteration 3979: total_loss: 0.207489, loss_sup: 0.154707, loss_mps: 0.021193, loss_cps: 0.031588
[12:35:54.708] iteration 3980: total_loss: 0.144985, loss_sup: 0.067863, loss_mps: 0.029206, loss_cps: 0.047916
[12:35:54.854] iteration 3981: total_loss: 0.106837, loss_sup: 0.039581, loss_mps: 0.025306, loss_cps: 0.041950
[12:35:54.999] iteration 3982: total_loss: 0.291907, loss_sup: 0.222148, loss_mps: 0.026496, loss_cps: 0.043263
[12:35:55.145] iteration 3983: total_loss: 0.118601, loss_sup: 0.072534, loss_mps: 0.017906, loss_cps: 0.028161
[12:35:55.290] iteration 3984: total_loss: 0.314288, loss_sup: 0.217559, loss_mps: 0.032902, loss_cps: 0.063827
[12:35:55.435] iteration 3985: total_loss: 0.136323, loss_sup: 0.062180, loss_mps: 0.027014, loss_cps: 0.047129
[12:35:55.581] iteration 3986: total_loss: 0.210743, loss_sup: 0.132561, loss_mps: 0.027354, loss_cps: 0.050828
[12:35:55.726] iteration 3987: total_loss: 0.411061, loss_sup: 0.322948, loss_mps: 0.030769, loss_cps: 0.057344
[12:35:55.871] iteration 3988: total_loss: 0.282270, loss_sup: 0.218126, loss_mps: 0.023730, loss_cps: 0.040414
[12:35:56.017] iteration 3989: total_loss: 0.264924, loss_sup: 0.163008, loss_mps: 0.033961, loss_cps: 0.067954
[12:35:56.162] iteration 3990: total_loss: 0.164697, loss_sup: 0.109951, loss_mps: 0.020921, loss_cps: 0.033826
[12:35:56.308] iteration 3991: total_loss: 0.179862, loss_sup: 0.117196, loss_mps: 0.023415, loss_cps: 0.039251
[12:35:56.453] iteration 3992: total_loss: 0.176164, loss_sup: 0.104672, loss_mps: 0.025993, loss_cps: 0.045498
[12:35:56.599] iteration 3993: total_loss: 0.298505, loss_sup: 0.234852, loss_mps: 0.023463, loss_cps: 0.040191
[12:35:56.745] iteration 3994: total_loss: 0.096267, loss_sup: 0.038821, loss_mps: 0.021389, loss_cps: 0.036057
[12:35:56.890] iteration 3995: total_loss: 0.370041, loss_sup: 0.302570, loss_mps: 0.023510, loss_cps: 0.043961
[12:35:57.037] iteration 3996: total_loss: 0.515711, loss_sup: 0.444207, loss_mps: 0.024795, loss_cps: 0.046709
[12:35:57.183] iteration 3997: total_loss: 0.246628, loss_sup: 0.202553, loss_mps: 0.016859, loss_cps: 0.027216
[12:35:57.328] iteration 3998: total_loss: 0.404383, loss_sup: 0.330758, loss_mps: 0.025310, loss_cps: 0.048316
[12:35:57.474] iteration 3999: total_loss: 0.117804, loss_sup: 0.064371, loss_mps: 0.019607, loss_cps: 0.033826
[12:35:57.620] iteration 4000: total_loss: 0.340638, loss_sup: 0.265210, loss_mps: 0.026478, loss_cps: 0.048949
[12:35:57.620] Evaluation Started ==>
[12:36:08.997] ==> valid iteration 4000: unet metrics: {'dc': 0.5674180030112195, 'jc': 0.443771946887224, 'pre': 0.6810313791548394, 'hd': 6.297119074772563}, ynet metrics: {'dc': 0.4954255520191164, 'jc': 0.37675429617943723, 'pre': 0.6092841383443494, 'hd': 6.870181235473811}.
[12:36:08.999] Evaluation Finished!⏹️
[12:36:09.148] iteration 4001: total_loss: 0.293757, loss_sup: 0.201095, loss_mps: 0.032060, loss_cps: 0.060602
[12:36:09.297] iteration 4002: total_loss: 0.337103, loss_sup: 0.249685, loss_mps: 0.030029, loss_cps: 0.057389
[12:36:09.443] iteration 4003: total_loss: 0.167634, loss_sup: 0.101553, loss_mps: 0.023239, loss_cps: 0.042842
[12:36:09.588] iteration 4004: total_loss: 0.242336, loss_sup: 0.164388, loss_mps: 0.028458, loss_cps: 0.049490
[12:36:09.733] iteration 4005: total_loss: 0.190459, loss_sup: 0.122363, loss_mps: 0.024123, loss_cps: 0.043974
[12:36:09.878] iteration 4006: total_loss: 0.276468, loss_sup: 0.144866, loss_mps: 0.042150, loss_cps: 0.089453
[12:36:10.027] iteration 4007: total_loss: 0.317147, loss_sup: 0.210752, loss_mps: 0.035575, loss_cps: 0.070819
[12:36:10.172] iteration 4008: total_loss: 0.169880, loss_sup: 0.075447, loss_mps: 0.031923, loss_cps: 0.062511
[12:36:10.317] iteration 4009: total_loss: 0.305035, loss_sup: 0.186266, loss_mps: 0.039782, loss_cps: 0.078987
[12:36:10.463] iteration 4010: total_loss: 0.408761, loss_sup: 0.347064, loss_mps: 0.023060, loss_cps: 0.038638
[12:36:10.608] iteration 4011: total_loss: 0.210945, loss_sup: 0.139637, loss_mps: 0.025848, loss_cps: 0.045460
[12:36:10.755] iteration 4012: total_loss: 0.321136, loss_sup: 0.238271, loss_mps: 0.029488, loss_cps: 0.053377
[12:36:10.901] iteration 4013: total_loss: 0.145165, loss_sup: 0.075343, loss_mps: 0.025258, loss_cps: 0.044564
[12:36:11.046] iteration 4014: total_loss: 0.286509, loss_sup: 0.176333, loss_mps: 0.037940, loss_cps: 0.072236
[12:36:11.191] iteration 4015: total_loss: 0.139249, loss_sup: 0.076349, loss_mps: 0.024027, loss_cps: 0.038874
[12:36:11.337] iteration 4016: total_loss: 0.175094, loss_sup: 0.103265, loss_mps: 0.025899, loss_cps: 0.045930
[12:36:11.482] iteration 4017: total_loss: 0.198864, loss_sup: 0.108467, loss_mps: 0.032320, loss_cps: 0.058078
[12:36:11.627] iteration 4018: total_loss: 0.542270, loss_sup: 0.443482, loss_mps: 0.033685, loss_cps: 0.065103
[12:36:11.772] iteration 4019: total_loss: 0.256339, loss_sup: 0.180794, loss_mps: 0.027363, loss_cps: 0.048181
[12:36:11.918] iteration 4020: total_loss: 0.237553, loss_sup: 0.148922, loss_mps: 0.032218, loss_cps: 0.056413
[12:36:12.063] iteration 4021: total_loss: 0.347770, loss_sup: 0.235045, loss_mps: 0.038066, loss_cps: 0.074659
[12:36:12.208] iteration 4022: total_loss: 0.151958, loss_sup: 0.049512, loss_mps: 0.035035, loss_cps: 0.067412
[12:36:12.353] iteration 4023: total_loss: 0.284470, loss_sup: 0.201135, loss_mps: 0.029330, loss_cps: 0.054004
[12:36:12.498] iteration 4024: total_loss: 0.275554, loss_sup: 0.182600, loss_mps: 0.032437, loss_cps: 0.060517
[12:36:12.643] iteration 4025: total_loss: 0.198174, loss_sup: 0.107641, loss_mps: 0.030802, loss_cps: 0.059731
[12:36:12.788] iteration 4026: total_loss: 0.187823, loss_sup: 0.084503, loss_mps: 0.035715, loss_cps: 0.067605
[12:36:12.933] iteration 4027: total_loss: 0.177452, loss_sup: 0.090427, loss_mps: 0.031085, loss_cps: 0.055940
[12:36:13.079] iteration 4028: total_loss: 0.281231, loss_sup: 0.192056, loss_mps: 0.030944, loss_cps: 0.058231
[12:36:13.224] iteration 4029: total_loss: 0.201038, loss_sup: 0.131529, loss_mps: 0.025535, loss_cps: 0.043973
[12:36:13.369] iteration 4030: total_loss: 0.290070, loss_sup: 0.179385, loss_mps: 0.038585, loss_cps: 0.072101
[12:36:13.514] iteration 4031: total_loss: 0.286724, loss_sup: 0.161454, loss_mps: 0.041129, loss_cps: 0.084141
[12:36:13.662] iteration 4032: total_loss: 0.258486, loss_sup: 0.178370, loss_mps: 0.027479, loss_cps: 0.052637
[12:36:13.808] iteration 4033: total_loss: 0.159317, loss_sup: 0.078757, loss_mps: 0.028786, loss_cps: 0.051774
[12:36:13.953] iteration 4034: total_loss: 0.238014, loss_sup: 0.148168, loss_mps: 0.030974, loss_cps: 0.058872
[12:36:14.099] iteration 4035: total_loss: 0.172015, loss_sup: 0.092803, loss_mps: 0.028431, loss_cps: 0.050780
[12:36:14.245] iteration 4036: total_loss: 0.219518, loss_sup: 0.150446, loss_mps: 0.025323, loss_cps: 0.043749
[12:36:14.389] iteration 4037: total_loss: 0.220445, loss_sup: 0.109037, loss_mps: 0.036991, loss_cps: 0.074417
[12:36:14.536] iteration 4038: total_loss: 0.107389, loss_sup: 0.042958, loss_mps: 0.023258, loss_cps: 0.041173
[12:36:14.681] iteration 4039: total_loss: 0.218714, loss_sup: 0.133023, loss_mps: 0.029725, loss_cps: 0.055966
[12:36:14.826] iteration 4040: total_loss: 0.490045, loss_sup: 0.376954, loss_mps: 0.038107, loss_cps: 0.074984
[12:36:14.972] iteration 4041: total_loss: 0.257797, loss_sup: 0.136513, loss_mps: 0.039674, loss_cps: 0.081610
[12:36:15.117] iteration 4042: total_loss: 0.233458, loss_sup: 0.161880, loss_mps: 0.026960, loss_cps: 0.044618
[12:36:15.264] iteration 4043: total_loss: 0.162209, loss_sup: 0.102156, loss_mps: 0.022438, loss_cps: 0.037615
[12:36:15.416] iteration 4044: total_loss: 0.407953, loss_sup: 0.321539, loss_mps: 0.030743, loss_cps: 0.055670
[12:36:15.565] iteration 4045: total_loss: 0.184809, loss_sup: 0.102156, loss_mps: 0.029577, loss_cps: 0.053076
[12:36:15.711] iteration 4046: total_loss: 0.476072, loss_sup: 0.354471, loss_mps: 0.038667, loss_cps: 0.082934
[12:36:15.856] iteration 4047: total_loss: 0.389113, loss_sup: 0.281209, loss_mps: 0.036309, loss_cps: 0.071595
[12:36:16.002] iteration 4048: total_loss: 0.225825, loss_sup: 0.140048, loss_mps: 0.030318, loss_cps: 0.055459
[12:36:16.148] iteration 4049: total_loss: 0.257560, loss_sup: 0.166715, loss_mps: 0.032035, loss_cps: 0.058809
[12:36:16.298] iteration 4050: total_loss: 0.342450, loss_sup: 0.252315, loss_mps: 0.031130, loss_cps: 0.059006
[12:36:16.444] iteration 4051: total_loss: 0.222878, loss_sup: 0.141732, loss_mps: 0.028110, loss_cps: 0.053036
[12:36:16.589] iteration 4052: total_loss: 0.160915, loss_sup: 0.062622, loss_mps: 0.033151, loss_cps: 0.065141
[12:36:16.735] iteration 4053: total_loss: 0.199665, loss_sup: 0.126912, loss_mps: 0.026865, loss_cps: 0.045888
[12:36:16.880] iteration 4054: total_loss: 0.477548, loss_sup: 0.374695, loss_mps: 0.034484, loss_cps: 0.068369
[12:36:17.026] iteration 4055: total_loss: 0.184087, loss_sup: 0.111802, loss_mps: 0.026630, loss_cps: 0.045655
[12:36:17.177] iteration 4056: total_loss: 0.299038, loss_sup: 0.202040, loss_mps: 0.033908, loss_cps: 0.063090
[12:36:17.323] iteration 4057: total_loss: 0.441590, loss_sup: 0.325961, loss_mps: 0.038240, loss_cps: 0.077389
[12:36:17.471] iteration 4058: total_loss: 0.283407, loss_sup: 0.200677, loss_mps: 0.029645, loss_cps: 0.053085
[12:36:17.618] iteration 4059: total_loss: 0.328547, loss_sup: 0.219181, loss_mps: 0.036511, loss_cps: 0.072856
[12:36:17.766] iteration 4060: total_loss: 0.320626, loss_sup: 0.242170, loss_mps: 0.028813, loss_cps: 0.049643
[12:36:17.915] iteration 4061: total_loss: 0.176323, loss_sup: 0.106683, loss_mps: 0.026183, loss_cps: 0.043457
[12:36:18.062] iteration 4062: total_loss: 0.269676, loss_sup: 0.195722, loss_mps: 0.027475, loss_cps: 0.046479
[12:36:18.208] iteration 4063: total_loss: 0.207239, loss_sup: 0.119397, loss_mps: 0.032054, loss_cps: 0.055788
[12:36:18.354] iteration 4064: total_loss: 0.329288, loss_sup: 0.225818, loss_mps: 0.036120, loss_cps: 0.067350
[12:36:18.501] iteration 4065: total_loss: 0.332001, loss_sup: 0.255136, loss_mps: 0.029550, loss_cps: 0.047315
[12:36:18.646] iteration 4066: total_loss: 0.360789, loss_sup: 0.274977, loss_mps: 0.030913, loss_cps: 0.054899
[12:36:18.795] iteration 4067: total_loss: 0.246995, loss_sup: 0.148476, loss_mps: 0.035228, loss_cps: 0.063292
[12:36:18.941] iteration 4068: total_loss: 0.332866, loss_sup: 0.238791, loss_mps: 0.033519, loss_cps: 0.060556
[12:36:19.089] iteration 4069: total_loss: 0.191802, loss_sup: 0.120307, loss_mps: 0.027799, loss_cps: 0.043696
[12:36:19.237] iteration 4070: total_loss: 0.174112, loss_sup: 0.124564, loss_mps: 0.020575, loss_cps: 0.028973
[12:36:19.383] iteration 4071: total_loss: 0.248833, loss_sup: 0.134956, loss_mps: 0.040376, loss_cps: 0.073501
[12:36:19.529] iteration 4072: total_loss: 0.208777, loss_sup: 0.133746, loss_mps: 0.029259, loss_cps: 0.045772
[12:36:19.675] iteration 4073: total_loss: 0.166094, loss_sup: 0.104299, loss_mps: 0.024393, loss_cps: 0.037402
[12:36:19.823] iteration 4074: total_loss: 0.471983, loss_sup: 0.379915, loss_mps: 0.034322, loss_cps: 0.057745
[12:36:19.969] iteration 4075: total_loss: 0.418265, loss_sup: 0.309242, loss_mps: 0.037774, loss_cps: 0.071248
[12:36:20.115] iteration 4076: total_loss: 0.208661, loss_sup: 0.094435, loss_mps: 0.039708, loss_cps: 0.074518
[12:36:20.261] iteration 4077: total_loss: 0.479031, loss_sup: 0.385247, loss_mps: 0.032675, loss_cps: 0.061109
[12:36:20.407] iteration 4078: total_loss: 0.254209, loss_sup: 0.155915, loss_mps: 0.035041, loss_cps: 0.063253
[12:36:20.553] iteration 4079: total_loss: 0.435887, loss_sup: 0.335537, loss_mps: 0.034836, loss_cps: 0.065514
[12:36:20.698] iteration 4080: total_loss: 0.312566, loss_sup: 0.230666, loss_mps: 0.030325, loss_cps: 0.051574
[12:36:20.844] iteration 4081: total_loss: 0.227552, loss_sup: 0.160691, loss_mps: 0.025572, loss_cps: 0.041289
[12:36:20.989] iteration 4082: total_loss: 0.244237, loss_sup: 0.164846, loss_mps: 0.028930, loss_cps: 0.050461
[12:36:21.141] iteration 4083: total_loss: 0.151792, loss_sup: 0.068942, loss_mps: 0.029579, loss_cps: 0.053271
[12:36:21.287] iteration 4084: total_loss: 0.145705, loss_sup: 0.092994, loss_mps: 0.020466, loss_cps: 0.032245
[12:36:21.433] iteration 4085: total_loss: 0.162879, loss_sup: 0.067765, loss_mps: 0.033851, loss_cps: 0.061263
[12:36:21.580] iteration 4086: total_loss: 0.342855, loss_sup: 0.242907, loss_mps: 0.035051, loss_cps: 0.064897
[12:36:21.726] iteration 4087: total_loss: 0.316858, loss_sup: 0.197262, loss_mps: 0.039611, loss_cps: 0.079985
[12:36:21.873] iteration 4088: total_loss: 0.268005, loss_sup: 0.158316, loss_mps: 0.036514, loss_cps: 0.073175
[12:36:22.021] iteration 4089: total_loss: 0.143710, loss_sup: 0.063248, loss_mps: 0.028784, loss_cps: 0.051677
[12:36:22.166] iteration 4090: total_loss: 0.482998, loss_sup: 0.398375, loss_mps: 0.029401, loss_cps: 0.055222
[12:36:22.313] iteration 4091: total_loss: 0.278219, loss_sup: 0.190873, loss_mps: 0.030888, loss_cps: 0.056458
[12:36:22.459] iteration 4092: total_loss: 0.184650, loss_sup: 0.111823, loss_mps: 0.026897, loss_cps: 0.045931
[12:36:22.604] iteration 4093: total_loss: 0.184390, loss_sup: 0.109581, loss_mps: 0.027050, loss_cps: 0.047759
[12:36:22.750] iteration 4094: total_loss: 0.169773, loss_sup: 0.103122, loss_mps: 0.025138, loss_cps: 0.041513
[12:36:22.898] iteration 4095: total_loss: 0.123246, loss_sup: 0.042840, loss_mps: 0.029858, loss_cps: 0.050548
[12:36:23.043] iteration 4096: total_loss: 0.409971, loss_sup: 0.333374, loss_mps: 0.027316, loss_cps: 0.049281
[12:36:23.190] iteration 4097: total_loss: 0.086240, loss_sup: 0.026340, loss_mps: 0.022230, loss_cps: 0.037670
[12:36:23.335] iteration 4098: total_loss: 0.384141, loss_sup: 0.287789, loss_mps: 0.033643, loss_cps: 0.062709
[12:36:23.481] iteration 4099: total_loss: 0.201377, loss_sup: 0.133491, loss_mps: 0.024534, loss_cps: 0.043353
[12:36:23.627] iteration 4100: total_loss: 0.165119, loss_sup: 0.077460, loss_mps: 0.030842, loss_cps: 0.056817
[12:36:23.627] Evaluation Started ==>
[12:36:34.966] ==> valid iteration 4100: unet metrics: {'dc': 0.5275426705360383, 'jc': 0.4037911628511101, 'pre': 0.580421090164794, 'hd': 7.100458505370568}, ynet metrics: {'dc': 0.4474379388697721, 'jc': 0.33620167704722875, 'pre': 0.5951700403100693, 'hd': 6.73067620501215}.
[12:36:34.968] Evaluation Finished!⏹️
[12:36:35.124] iteration 4101: total_loss: 0.179198, loss_sup: 0.103802, loss_mps: 0.027410, loss_cps: 0.047986
[12:36:35.270] iteration 4102: total_loss: 0.320145, loss_sup: 0.237453, loss_mps: 0.030061, loss_cps: 0.052630
[12:36:35.417] iteration 4103: total_loss: 0.314869, loss_sup: 0.220093, loss_mps: 0.032840, loss_cps: 0.061936
[12:36:35.563] iteration 4104: total_loss: 0.336077, loss_sup: 0.236804, loss_mps: 0.033667, loss_cps: 0.065606
[12:36:35.710] iteration 4105: total_loss: 0.294920, loss_sup: 0.188621, loss_mps: 0.037005, loss_cps: 0.069294
[12:36:35.856] iteration 4106: total_loss: 0.213131, loss_sup: 0.106507, loss_mps: 0.037114, loss_cps: 0.069509
[12:36:36.002] iteration 4107: total_loss: 0.253633, loss_sup: 0.188189, loss_mps: 0.024382, loss_cps: 0.041061
[12:36:36.148] iteration 4108: total_loss: 0.283953, loss_sup: 0.189374, loss_mps: 0.034389, loss_cps: 0.060190
[12:36:36.297] iteration 4109: total_loss: 0.313425, loss_sup: 0.215778, loss_mps: 0.033638, loss_cps: 0.064008
[12:36:36.442] iteration 4110: total_loss: 0.210142, loss_sup: 0.125122, loss_mps: 0.030139, loss_cps: 0.054881
[12:36:36.588] iteration 4111: total_loss: 0.312261, loss_sup: 0.252687, loss_mps: 0.022590, loss_cps: 0.036984
[12:36:36.733] iteration 4112: total_loss: 0.122826, loss_sup: 0.045312, loss_mps: 0.028252, loss_cps: 0.049261
[12:36:36.882] iteration 4113: total_loss: 0.104706, loss_sup: 0.042702, loss_mps: 0.024720, loss_cps: 0.037284
[12:36:37.028] iteration 4114: total_loss: 0.421230, loss_sup: 0.329864, loss_mps: 0.031593, loss_cps: 0.059774
[12:36:37.174] iteration 4115: total_loss: 0.292718, loss_sup: 0.216496, loss_mps: 0.028031, loss_cps: 0.048191
[12:36:37.319] iteration 4116: total_loss: 0.198734, loss_sup: 0.119075, loss_mps: 0.028402, loss_cps: 0.051257
[12:36:37.465] iteration 4117: total_loss: 0.202738, loss_sup: 0.122581, loss_mps: 0.028889, loss_cps: 0.051268
[12:36:37.611] iteration 4118: total_loss: 0.496373, loss_sup: 0.393895, loss_mps: 0.035461, loss_cps: 0.067017
[12:36:37.757] iteration 4119: total_loss: 0.305394, loss_sup: 0.233691, loss_mps: 0.025808, loss_cps: 0.045894
[12:36:37.903] iteration 4120: total_loss: 0.158690, loss_sup: 0.076830, loss_mps: 0.029279, loss_cps: 0.052582
[12:36:38.048] iteration 4121: total_loss: 0.197890, loss_sup: 0.116975, loss_mps: 0.029677, loss_cps: 0.051239
[12:36:38.194] iteration 4122: total_loss: 0.271810, loss_sup: 0.176531, loss_mps: 0.033455, loss_cps: 0.061824
[12:36:38.340] iteration 4123: total_loss: 0.215308, loss_sup: 0.151870, loss_mps: 0.024111, loss_cps: 0.039327
[12:36:38.488] iteration 4124: total_loss: 0.180631, loss_sup: 0.117907, loss_mps: 0.024311, loss_cps: 0.038413
[12:36:38.634] iteration 4125: total_loss: 0.242363, loss_sup: 0.166851, loss_mps: 0.028458, loss_cps: 0.047055
[12:36:38.783] iteration 4126: total_loss: 0.254470, loss_sup: 0.162465, loss_mps: 0.031691, loss_cps: 0.060313
[12:36:38.928] iteration 4127: total_loss: 0.115105, loss_sup: 0.059899, loss_mps: 0.022194, loss_cps: 0.033011
[12:36:39.074] iteration 4128: total_loss: 0.220260, loss_sup: 0.165305, loss_mps: 0.021385, loss_cps: 0.033570
[12:36:39.220] iteration 4129: total_loss: 0.306247, loss_sup: 0.227134, loss_mps: 0.029333, loss_cps: 0.049781
[12:36:39.366] iteration 4130: total_loss: 0.136380, loss_sup: 0.077892, loss_mps: 0.022676, loss_cps: 0.035812
[12:36:39.512] iteration 4131: total_loss: 0.451559, loss_sup: 0.344737, loss_mps: 0.035426, loss_cps: 0.071396
[12:36:39.658] iteration 4132: total_loss: 0.205846, loss_sup: 0.129022, loss_mps: 0.028272, loss_cps: 0.048552
[12:36:39.804] iteration 4133: total_loss: 0.140022, loss_sup: 0.055826, loss_mps: 0.030350, loss_cps: 0.053845
[12:36:39.950] iteration 4134: total_loss: 0.348401, loss_sup: 0.248951, loss_mps: 0.034510, loss_cps: 0.064940
[12:36:40.095] iteration 4135: total_loss: 0.517879, loss_sup: 0.421229, loss_mps: 0.033705, loss_cps: 0.062944
[12:36:40.241] iteration 4136: total_loss: 0.330388, loss_sup: 0.229918, loss_mps: 0.035803, loss_cps: 0.064667
[12:36:40.388] iteration 4137: total_loss: 0.211674, loss_sup: 0.148263, loss_mps: 0.024485, loss_cps: 0.038926
[12:36:40.535] iteration 4138: total_loss: 0.268984, loss_sup: 0.204323, loss_mps: 0.024923, loss_cps: 0.039738
[12:36:40.680] iteration 4139: total_loss: 0.150397, loss_sup: 0.072286, loss_mps: 0.029675, loss_cps: 0.048436
[12:36:40.828] iteration 4140: total_loss: 0.237262, loss_sup: 0.171400, loss_mps: 0.024950, loss_cps: 0.040912
[12:36:40.974] iteration 4141: total_loss: 0.244243, loss_sup: 0.172841, loss_mps: 0.026400, loss_cps: 0.045002
[12:36:41.120] iteration 4142: total_loss: 0.235738, loss_sup: 0.130560, loss_mps: 0.035610, loss_cps: 0.069568
[12:36:41.266] iteration 4143: total_loss: 0.582810, loss_sup: 0.481592, loss_mps: 0.035453, loss_cps: 0.065765
[12:36:41.412] iteration 4144: total_loss: 0.282306, loss_sup: 0.222002, loss_mps: 0.023865, loss_cps: 0.036439
[12:36:41.558] iteration 4145: total_loss: 0.197205, loss_sup: 0.122680, loss_mps: 0.028442, loss_cps: 0.046083
[12:36:41.705] iteration 4146: total_loss: 0.181562, loss_sup: 0.090153, loss_mps: 0.032811, loss_cps: 0.058598
[12:36:41.850] iteration 4147: total_loss: 0.334076, loss_sup: 0.237598, loss_mps: 0.034393, loss_cps: 0.062085
[12:36:41.996] iteration 4148: total_loss: 0.181341, loss_sup: 0.108796, loss_mps: 0.027914, loss_cps: 0.044631
[12:36:42.142] iteration 4149: total_loss: 0.405882, loss_sup: 0.297588, loss_mps: 0.038551, loss_cps: 0.069742
[12:36:42.289] iteration 4150: total_loss: 0.138270, loss_sup: 0.055908, loss_mps: 0.030320, loss_cps: 0.052042
[12:36:42.436] iteration 4151: total_loss: 0.341601, loss_sup: 0.274198, loss_mps: 0.025579, loss_cps: 0.041824
[12:36:42.584] iteration 4152: total_loss: 0.335043, loss_sup: 0.234530, loss_mps: 0.035342, loss_cps: 0.065171
[12:36:42.732] iteration 4153: total_loss: 0.321078, loss_sup: 0.233518, loss_mps: 0.031924, loss_cps: 0.055635
[12:36:42.877] iteration 4154: total_loss: 0.239497, loss_sup: 0.160576, loss_mps: 0.028914, loss_cps: 0.050007
[12:36:43.025] iteration 4155: total_loss: 0.208906, loss_sup: 0.128494, loss_mps: 0.030269, loss_cps: 0.050143
[12:36:43.171] iteration 4156: total_loss: 0.382930, loss_sup: 0.298524, loss_mps: 0.030441, loss_cps: 0.053966
[12:36:43.319] iteration 4157: total_loss: 0.244880, loss_sup: 0.182866, loss_mps: 0.024632, loss_cps: 0.037382
[12:36:43.465] iteration 4158: total_loss: 0.165039, loss_sup: 0.080586, loss_mps: 0.030853, loss_cps: 0.053600
[12:36:43.615] iteration 4159: total_loss: 0.254662, loss_sup: 0.181427, loss_mps: 0.028637, loss_cps: 0.044598
[12:36:43.761] iteration 4160: total_loss: 0.299963, loss_sup: 0.239184, loss_mps: 0.023233, loss_cps: 0.037546
[12:36:43.908] iteration 4161: total_loss: 0.256931, loss_sup: 0.185222, loss_mps: 0.026589, loss_cps: 0.045120
[12:36:44.054] iteration 4162: total_loss: 0.149636, loss_sup: 0.082314, loss_mps: 0.024865, loss_cps: 0.042457
[12:36:44.202] iteration 4163: total_loss: 0.320277, loss_sup: 0.228793, loss_mps: 0.033038, loss_cps: 0.058445
[12:36:44.348] iteration 4164: total_loss: 0.279166, loss_sup: 0.195360, loss_mps: 0.030604, loss_cps: 0.053201
[12:36:44.494] iteration 4165: total_loss: 0.267440, loss_sup: 0.187140, loss_mps: 0.029570, loss_cps: 0.050730
[12:36:44.641] iteration 4166: total_loss: 0.195331, loss_sup: 0.122549, loss_mps: 0.025546, loss_cps: 0.047236
[12:36:44.787] iteration 4167: total_loss: 0.319202, loss_sup: 0.239811, loss_mps: 0.028495, loss_cps: 0.050896
[12:36:44.933] iteration 4168: total_loss: 0.225083, loss_sup: 0.151266, loss_mps: 0.027544, loss_cps: 0.046273
[12:36:45.078] iteration 4169: total_loss: 0.371164, loss_sup: 0.262188, loss_mps: 0.038907, loss_cps: 0.070069
[12:36:45.224] iteration 4170: total_loss: 0.216965, loss_sup: 0.110840, loss_mps: 0.035426, loss_cps: 0.070699
[12:36:45.371] iteration 4171: total_loss: 0.370946, loss_sup: 0.301777, loss_mps: 0.025315, loss_cps: 0.043854
[12:36:45.516] iteration 4172: total_loss: 0.212821, loss_sup: 0.139204, loss_mps: 0.026592, loss_cps: 0.047025
[12:36:45.662] iteration 4173: total_loss: 0.249201, loss_sup: 0.160810, loss_mps: 0.031857, loss_cps: 0.056534
[12:36:45.807] iteration 4174: total_loss: 0.229919, loss_sup: 0.177656, loss_mps: 0.020009, loss_cps: 0.032255
[12:36:45.953] iteration 4175: total_loss: 0.223649, loss_sup: 0.142276, loss_mps: 0.029061, loss_cps: 0.052312
[12:36:46.099] iteration 4176: total_loss: 0.360475, loss_sup: 0.254697, loss_mps: 0.036999, loss_cps: 0.068779
[12:36:46.244] iteration 4177: total_loss: 0.271529, loss_sup: 0.176639, loss_mps: 0.033966, loss_cps: 0.060924
[12:36:46.390] iteration 4178: total_loss: 0.290361, loss_sup: 0.189349, loss_mps: 0.035082, loss_cps: 0.065930
[12:36:46.536] iteration 4179: total_loss: 0.186315, loss_sup: 0.123092, loss_mps: 0.023805, loss_cps: 0.039418
[12:36:46.597] iteration 4180: total_loss: 0.075931, loss_sup: 0.024599, loss_mps: 0.019938, loss_cps: 0.031394
[12:36:47.819] iteration 4181: total_loss: 0.310949, loss_sup: 0.220626, loss_mps: 0.033032, loss_cps: 0.057291
[12:36:47.968] iteration 4182: total_loss: 0.238391, loss_sup: 0.143959, loss_mps: 0.034399, loss_cps: 0.060033
[12:36:48.115] iteration 4183: total_loss: 0.262186, loss_sup: 0.197445, loss_mps: 0.023704, loss_cps: 0.041037
[12:36:48.262] iteration 4184: total_loss: 0.370547, loss_sup: 0.293611, loss_mps: 0.027758, loss_cps: 0.049178
[12:36:48.409] iteration 4185: total_loss: 0.368398, loss_sup: 0.284191, loss_mps: 0.030910, loss_cps: 0.053297
[12:36:48.559] iteration 4186: total_loss: 0.186394, loss_sup: 0.109075, loss_mps: 0.028600, loss_cps: 0.048719
[12:36:48.709] iteration 4187: total_loss: 0.169849, loss_sup: 0.106740, loss_mps: 0.023930, loss_cps: 0.039178
[12:36:48.862] iteration 4188: total_loss: 0.264075, loss_sup: 0.193305, loss_mps: 0.026636, loss_cps: 0.044135
[12:36:49.010] iteration 4189: total_loss: 0.270662, loss_sup: 0.184138, loss_mps: 0.031539, loss_cps: 0.054985
[12:36:49.156] iteration 4190: total_loss: 0.306296, loss_sup: 0.231173, loss_mps: 0.026947, loss_cps: 0.048176
[12:36:49.303] iteration 4191: total_loss: 0.202371, loss_sup: 0.110738, loss_mps: 0.032340, loss_cps: 0.059294
[12:36:49.453] iteration 4192: total_loss: 0.156304, loss_sup: 0.092983, loss_mps: 0.023335, loss_cps: 0.039986
[12:36:49.600] iteration 4193: total_loss: 0.333809, loss_sup: 0.247149, loss_mps: 0.030919, loss_cps: 0.055741
[12:36:49.746] iteration 4194: total_loss: 0.399902, loss_sup: 0.318539, loss_mps: 0.030340, loss_cps: 0.051023
[12:36:49.894] iteration 4195: total_loss: 0.322807, loss_sup: 0.229138, loss_mps: 0.032421, loss_cps: 0.061248
[12:36:50.044] iteration 4196: total_loss: 0.458002, loss_sup: 0.342250, loss_mps: 0.038905, loss_cps: 0.076847
[12:36:50.191] iteration 4197: total_loss: 0.263773, loss_sup: 0.170851, loss_mps: 0.032449, loss_cps: 0.060472
[12:36:50.337] iteration 4198: total_loss: 0.158297, loss_sup: 0.088531, loss_mps: 0.026714, loss_cps: 0.043052
[12:36:50.484] iteration 4199: total_loss: 0.362199, loss_sup: 0.263719, loss_mps: 0.034792, loss_cps: 0.063687
[12:36:50.631] iteration 4200: total_loss: 0.207521, loss_sup: 0.136620, loss_mps: 0.027604, loss_cps: 0.043298
[12:36:50.631] Evaluation Started ==>
[12:37:02.034] ==> valid iteration 4200: unet metrics: {'dc': 0.5771316713478807, 'jc': 0.4483604624212921, 'pre': 0.6209541052448216, 'hd': 6.978976018434287}, ynet metrics: {'dc': 0.46859410373772004, 'jc': 0.35767187055355587, 'pre': 0.5731083350804658, 'hd': 7.022193380507515}.
[12:37:02.036] Evaluation Finished!⏹️
[12:37:02.187] iteration 4201: total_loss: 0.174444, loss_sup: 0.121793, loss_mps: 0.020997, loss_cps: 0.031654
[12:37:02.335] iteration 4202: total_loss: 0.284432, loss_sup: 0.191012, loss_mps: 0.033940, loss_cps: 0.059480
[12:37:02.481] iteration 4203: total_loss: 0.204248, loss_sup: 0.124453, loss_mps: 0.029359, loss_cps: 0.050437
[12:37:02.627] iteration 4204: total_loss: 0.218606, loss_sup: 0.156281, loss_mps: 0.024213, loss_cps: 0.038113
[12:37:02.773] iteration 4205: total_loss: 0.312630, loss_sup: 0.220855, loss_mps: 0.032883, loss_cps: 0.058892
[12:37:02.918] iteration 4206: total_loss: 0.195823, loss_sup: 0.115285, loss_mps: 0.029958, loss_cps: 0.050580
[12:37:03.064] iteration 4207: total_loss: 0.176554, loss_sup: 0.109253, loss_mps: 0.026120, loss_cps: 0.041181
[12:37:03.209] iteration 4208: total_loss: 0.174148, loss_sup: 0.088672, loss_mps: 0.030314, loss_cps: 0.055162
[12:37:03.354] iteration 4209: total_loss: 0.228935, loss_sup: 0.153019, loss_mps: 0.028748, loss_cps: 0.047168
[12:37:03.500] iteration 4210: total_loss: 0.346092, loss_sup: 0.284634, loss_mps: 0.023755, loss_cps: 0.037702
[12:37:03.645] iteration 4211: total_loss: 0.256044, loss_sup: 0.177843, loss_mps: 0.029214, loss_cps: 0.048987
[12:37:03.794] iteration 4212: total_loss: 0.323889, loss_sup: 0.240209, loss_mps: 0.030867, loss_cps: 0.052814
[12:37:03.939] iteration 4213: total_loss: 0.098477, loss_sup: 0.045486, loss_mps: 0.021020, loss_cps: 0.031972
[12:37:04.084] iteration 4214: total_loss: 0.140698, loss_sup: 0.085641, loss_mps: 0.022554, loss_cps: 0.032502
[12:37:04.230] iteration 4215: total_loss: 0.279027, loss_sup: 0.198785, loss_mps: 0.029302, loss_cps: 0.050940
[12:37:04.375] iteration 4216: total_loss: 0.209536, loss_sup: 0.131840, loss_mps: 0.028674, loss_cps: 0.049022
[12:37:04.521] iteration 4217: total_loss: 0.133247, loss_sup: 0.053372, loss_mps: 0.028227, loss_cps: 0.051648
[12:37:04.666] iteration 4218: total_loss: 0.310469, loss_sup: 0.222970, loss_mps: 0.030133, loss_cps: 0.057367
[12:37:04.811] iteration 4219: total_loss: 0.225290, loss_sup: 0.147909, loss_mps: 0.027356, loss_cps: 0.050024
[12:37:04.961] iteration 4220: total_loss: 0.252758, loss_sup: 0.151903, loss_mps: 0.034414, loss_cps: 0.066442
[12:37:05.106] iteration 4221: total_loss: 0.295879, loss_sup: 0.224410, loss_mps: 0.025924, loss_cps: 0.045545
[12:37:05.251] iteration 4222: total_loss: 0.151430, loss_sup: 0.071021, loss_mps: 0.027834, loss_cps: 0.052575
[12:37:05.397] iteration 4223: total_loss: 0.232222, loss_sup: 0.143360, loss_mps: 0.031394, loss_cps: 0.057468
[12:37:05.542] iteration 4224: total_loss: 0.146325, loss_sup: 0.069169, loss_mps: 0.028107, loss_cps: 0.049049
[12:37:05.688] iteration 4225: total_loss: 0.189223, loss_sup: 0.093616, loss_mps: 0.032204, loss_cps: 0.063403
[12:37:05.833] iteration 4226: total_loss: 0.182983, loss_sup: 0.130331, loss_mps: 0.020686, loss_cps: 0.031965
[12:37:05.979] iteration 4227: total_loss: 0.310837, loss_sup: 0.215136, loss_mps: 0.033630, loss_cps: 0.062071
[12:37:06.125] iteration 4228: total_loss: 0.329050, loss_sup: 0.258239, loss_mps: 0.026411, loss_cps: 0.044401
[12:37:06.271] iteration 4229: total_loss: 0.208546, loss_sup: 0.146393, loss_mps: 0.022578, loss_cps: 0.039575
[12:37:06.418] iteration 4230: total_loss: 0.197736, loss_sup: 0.132005, loss_mps: 0.024680, loss_cps: 0.041052
[12:37:06.567] iteration 4231: total_loss: 0.283679, loss_sup: 0.208179, loss_mps: 0.027582, loss_cps: 0.047918
[12:37:06.713] iteration 4232: total_loss: 0.341658, loss_sup: 0.261724, loss_mps: 0.028512, loss_cps: 0.051422
[12:37:06.859] iteration 4233: total_loss: 0.177838, loss_sup: 0.113496, loss_mps: 0.024016, loss_cps: 0.040326
[12:37:07.005] iteration 4234: total_loss: 0.138403, loss_sup: 0.064335, loss_mps: 0.026429, loss_cps: 0.047639
[12:37:07.151] iteration 4235: total_loss: 0.123598, loss_sup: 0.066995, loss_mps: 0.022550, loss_cps: 0.034054
[12:37:07.297] iteration 4236: total_loss: 0.167969, loss_sup: 0.100321, loss_mps: 0.025040, loss_cps: 0.042608
[12:37:07.446] iteration 4237: total_loss: 0.382526, loss_sup: 0.318516, loss_mps: 0.024415, loss_cps: 0.039596
[12:37:07.592] iteration 4238: total_loss: 0.097823, loss_sup: 0.045215, loss_mps: 0.020272, loss_cps: 0.032335
[12:37:07.737] iteration 4239: total_loss: 0.296630, loss_sup: 0.190109, loss_mps: 0.035358, loss_cps: 0.071163
[12:37:07.884] iteration 4240: total_loss: 0.222198, loss_sup: 0.160975, loss_mps: 0.023036, loss_cps: 0.038187
[12:37:08.030] iteration 4241: total_loss: 0.126839, loss_sup: 0.047125, loss_mps: 0.028294, loss_cps: 0.051420
[12:37:08.176] iteration 4242: total_loss: 0.244138, loss_sup: 0.166978, loss_mps: 0.027438, loss_cps: 0.049722
[12:37:08.324] iteration 4243: total_loss: 0.136364, loss_sup: 0.072688, loss_mps: 0.024372, loss_cps: 0.039305
[12:37:08.470] iteration 4244: total_loss: 0.219655, loss_sup: 0.161873, loss_mps: 0.022510, loss_cps: 0.035271
[12:37:08.615] iteration 4245: total_loss: 0.188250, loss_sup: 0.128602, loss_mps: 0.022480, loss_cps: 0.037168
[12:37:08.761] iteration 4246: total_loss: 0.504066, loss_sup: 0.400367, loss_mps: 0.034478, loss_cps: 0.069222
[12:37:08.907] iteration 4247: total_loss: 0.328459, loss_sup: 0.252840, loss_mps: 0.026934, loss_cps: 0.048685
[12:37:09.052] iteration 4248: total_loss: 0.176735, loss_sup: 0.118985, loss_mps: 0.022171, loss_cps: 0.035578
[12:37:09.198] iteration 4249: total_loss: 0.373636, loss_sup: 0.274709, loss_mps: 0.033526, loss_cps: 0.065402
[12:37:09.344] iteration 4250: total_loss: 0.194748, loss_sup: 0.107658, loss_mps: 0.031023, loss_cps: 0.056066
[12:37:09.492] iteration 4251: total_loss: 0.115833, loss_sup: 0.041499, loss_mps: 0.026866, loss_cps: 0.047468
[12:37:09.641] iteration 4252: total_loss: 0.268127, loss_sup: 0.204182, loss_mps: 0.023706, loss_cps: 0.040240
[12:37:09.786] iteration 4253: total_loss: 0.204482, loss_sup: 0.120932, loss_mps: 0.029073, loss_cps: 0.054477
[12:37:09.933] iteration 4254: total_loss: 0.154673, loss_sup: 0.087892, loss_mps: 0.024444, loss_cps: 0.042337
[12:37:10.082] iteration 4255: total_loss: 0.193201, loss_sup: 0.106843, loss_mps: 0.030657, loss_cps: 0.055701
[12:37:10.232] iteration 4256: total_loss: 0.230632, loss_sup: 0.134842, loss_mps: 0.033677, loss_cps: 0.062112
[12:37:10.377] iteration 4257: total_loss: 0.238608, loss_sup: 0.157949, loss_mps: 0.028688, loss_cps: 0.051971
[12:37:10.523] iteration 4258: total_loss: 0.343837, loss_sup: 0.258125, loss_mps: 0.030433, loss_cps: 0.055279
[12:37:10.674] iteration 4259: total_loss: 0.213208, loss_sup: 0.123657, loss_mps: 0.030355, loss_cps: 0.059196
[12:37:10.819] iteration 4260: total_loss: 0.235848, loss_sup: 0.164515, loss_mps: 0.025231, loss_cps: 0.046102
[12:37:10.965] iteration 4261: total_loss: 0.139225, loss_sup: 0.084731, loss_mps: 0.020321, loss_cps: 0.034173
[12:37:11.111] iteration 4262: total_loss: 0.272681, loss_sup: 0.187036, loss_mps: 0.029494, loss_cps: 0.056151
[12:37:11.257] iteration 4263: total_loss: 0.171080, loss_sup: 0.092585, loss_mps: 0.027443, loss_cps: 0.051052
[12:37:11.404] iteration 4264: total_loss: 0.491042, loss_sup: 0.399021, loss_mps: 0.030967, loss_cps: 0.061054
[12:37:11.549] iteration 4265: total_loss: 0.095725, loss_sup: 0.048254, loss_mps: 0.018953, loss_cps: 0.028518
[12:37:11.697] iteration 4266: total_loss: 0.533634, loss_sup: 0.448684, loss_mps: 0.029567, loss_cps: 0.055384
[12:37:11.843] iteration 4267: total_loss: 0.194707, loss_sup: 0.116349, loss_mps: 0.028114, loss_cps: 0.050244
[12:37:11.989] iteration 4268: total_loss: 0.274598, loss_sup: 0.181707, loss_mps: 0.033882, loss_cps: 0.059010
[12:37:12.135] iteration 4269: total_loss: 0.163273, loss_sup: 0.092969, loss_mps: 0.024828, loss_cps: 0.045476
[12:37:12.281] iteration 4270: total_loss: 0.466882, loss_sup: 0.376911, loss_mps: 0.031602, loss_cps: 0.058369
[12:37:12.426] iteration 4271: total_loss: 0.390970, loss_sup: 0.290563, loss_mps: 0.033465, loss_cps: 0.066941
[12:37:12.572] iteration 4272: total_loss: 0.148145, loss_sup: 0.064423, loss_mps: 0.030383, loss_cps: 0.053339
[12:37:12.718] iteration 4273: total_loss: 0.245504, loss_sup: 0.125861, loss_mps: 0.040729, loss_cps: 0.078915
[12:37:12.867] iteration 4274: total_loss: 0.338244, loss_sup: 0.235046, loss_mps: 0.035767, loss_cps: 0.067431
[12:37:13.013] iteration 4275: total_loss: 0.231279, loss_sup: 0.145059, loss_mps: 0.031103, loss_cps: 0.055117
[12:37:13.160] iteration 4276: total_loss: 0.344847, loss_sup: 0.257322, loss_mps: 0.032279, loss_cps: 0.055247
[12:37:13.307] iteration 4277: total_loss: 0.269459, loss_sup: 0.160817, loss_mps: 0.037026, loss_cps: 0.071616
[12:37:13.453] iteration 4278: total_loss: 0.296672, loss_sup: 0.151176, loss_mps: 0.046386, loss_cps: 0.099110
[12:37:13.600] iteration 4279: total_loss: 0.210293, loss_sup: 0.104918, loss_mps: 0.037373, loss_cps: 0.068002
[12:37:13.748] iteration 4280: total_loss: 0.397188, loss_sup: 0.268461, loss_mps: 0.043063, loss_cps: 0.085664
[12:37:13.893] iteration 4281: total_loss: 0.222679, loss_sup: 0.150112, loss_mps: 0.026059, loss_cps: 0.046508
[12:37:14.040] iteration 4282: total_loss: 0.199929, loss_sup: 0.107332, loss_mps: 0.032869, loss_cps: 0.059728
[12:37:14.188] iteration 4283: total_loss: 0.163108, loss_sup: 0.087752, loss_mps: 0.028461, loss_cps: 0.046895
[12:37:14.334] iteration 4284: total_loss: 0.227056, loss_sup: 0.151707, loss_mps: 0.028123, loss_cps: 0.047225
[12:37:14.481] iteration 4285: total_loss: 0.159602, loss_sup: 0.087770, loss_mps: 0.026293, loss_cps: 0.045539
[12:37:14.628] iteration 4286: total_loss: 0.202381, loss_sup: 0.134519, loss_mps: 0.026501, loss_cps: 0.041362
[12:37:14.779] iteration 4287: total_loss: 0.110648, loss_sup: 0.035618, loss_mps: 0.026810, loss_cps: 0.048221
[12:37:14.927] iteration 4288: total_loss: 0.166798, loss_sup: 0.100915, loss_mps: 0.024320, loss_cps: 0.041563
[12:37:15.073] iteration 4289: total_loss: 0.214865, loss_sup: 0.137336, loss_mps: 0.027910, loss_cps: 0.049618
[12:37:15.220] iteration 4290: total_loss: 0.296687, loss_sup: 0.209283, loss_mps: 0.030843, loss_cps: 0.056561
[12:37:15.366] iteration 4291: total_loss: 0.227802, loss_sup: 0.151117, loss_mps: 0.027954, loss_cps: 0.048731
[12:37:15.512] iteration 4292: total_loss: 0.271976, loss_sup: 0.203891, loss_mps: 0.024385, loss_cps: 0.043700
[12:37:15.658] iteration 4293: total_loss: 0.121818, loss_sup: 0.056973, loss_mps: 0.023464, loss_cps: 0.041381
[12:37:15.804] iteration 4294: total_loss: 0.257891, loss_sup: 0.182499, loss_mps: 0.025947, loss_cps: 0.049444
[12:37:15.950] iteration 4295: total_loss: 0.277996, loss_sup: 0.196911, loss_mps: 0.028681, loss_cps: 0.052404
[12:37:16.097] iteration 4296: total_loss: 0.250525, loss_sup: 0.188652, loss_mps: 0.022637, loss_cps: 0.039237
[12:37:16.244] iteration 4297: total_loss: 0.290177, loss_sup: 0.223590, loss_mps: 0.023498, loss_cps: 0.043089
[12:37:16.390] iteration 4298: total_loss: 0.221216, loss_sup: 0.152680, loss_mps: 0.024180, loss_cps: 0.044356
[12:37:16.539] iteration 4299: total_loss: 0.357256, loss_sup: 0.273706, loss_mps: 0.028478, loss_cps: 0.055071
[12:37:16.687] iteration 4300: total_loss: 0.281582, loss_sup: 0.149200, loss_mps: 0.042007, loss_cps: 0.090375
[12:37:16.687] Evaluation Started ==>
[12:37:28.020] ==> valid iteration 4300: unet metrics: {'dc': 0.5723610257628625, 'jc': 0.4479801737964104, 'pre': 0.6400088434566547, 'hd': 6.589174933763006}, ynet metrics: {'dc': 0.5195430774725229, 'jc': 0.4005844067990156, 'pre': 0.6056503160598202, 'hd': 6.812753357180384}.
[12:37:28.022] Evaluation Finished!⏹️
[12:37:28.174] iteration 4301: total_loss: 0.153196, loss_sup: 0.093280, loss_mps: 0.023100, loss_cps: 0.036816
[12:37:28.321] iteration 4302: total_loss: 0.256183, loss_sup: 0.194287, loss_mps: 0.022821, loss_cps: 0.039075
[12:37:28.466] iteration 4303: total_loss: 0.275579, loss_sup: 0.198610, loss_mps: 0.027719, loss_cps: 0.049249
[12:37:28.612] iteration 4304: total_loss: 0.096522, loss_sup: 0.042291, loss_mps: 0.020406, loss_cps: 0.033825
[12:37:28.757] iteration 4305: total_loss: 0.284061, loss_sup: 0.150356, loss_mps: 0.043085, loss_cps: 0.090621
[12:37:28.904] iteration 4306: total_loss: 0.227264, loss_sup: 0.126957, loss_mps: 0.034266, loss_cps: 0.066041
[12:37:29.050] iteration 4307: total_loss: 0.174644, loss_sup: 0.079377, loss_mps: 0.032978, loss_cps: 0.062288
[12:37:29.196] iteration 4308: total_loss: 0.156804, loss_sup: 0.083315, loss_mps: 0.026378, loss_cps: 0.047110
[12:37:29.342] iteration 4309: total_loss: 0.159122, loss_sup: 0.108025, loss_mps: 0.019000, loss_cps: 0.032097
[12:37:29.488] iteration 4310: total_loss: 0.310008, loss_sup: 0.195929, loss_mps: 0.037447, loss_cps: 0.076631
[12:37:29.634] iteration 4311: total_loss: 0.132777, loss_sup: 0.062080, loss_mps: 0.026590, loss_cps: 0.044107
[12:37:29.780] iteration 4312: total_loss: 0.196804, loss_sup: 0.102632, loss_mps: 0.032152, loss_cps: 0.062021
[12:37:29.928] iteration 4313: total_loss: 0.189869, loss_sup: 0.085280, loss_mps: 0.035120, loss_cps: 0.069470
[12:37:30.078] iteration 4314: total_loss: 0.320103, loss_sup: 0.215864, loss_mps: 0.035856, loss_cps: 0.068383
[12:37:30.226] iteration 4315: total_loss: 0.198512, loss_sup: 0.132453, loss_mps: 0.024007, loss_cps: 0.042052
[12:37:30.372] iteration 4316: total_loss: 0.131634, loss_sup: 0.073940, loss_mps: 0.021688, loss_cps: 0.036006
[12:37:30.518] iteration 4317: total_loss: 0.171272, loss_sup: 0.090472, loss_mps: 0.027834, loss_cps: 0.052966
[12:37:30.665] iteration 4318: total_loss: 0.168577, loss_sup: 0.100120, loss_mps: 0.024601, loss_cps: 0.043855
[12:37:30.815] iteration 4319: total_loss: 0.363015, loss_sup: 0.275959, loss_mps: 0.030583, loss_cps: 0.056473
[12:37:30.962] iteration 4320: total_loss: 0.302642, loss_sup: 0.204079, loss_mps: 0.033281, loss_cps: 0.065282
[12:37:31.109] iteration 4321: total_loss: 0.116389, loss_sup: 0.061926, loss_mps: 0.020943, loss_cps: 0.033520
[12:37:31.255] iteration 4322: total_loss: 0.216832, loss_sup: 0.135124, loss_mps: 0.029584, loss_cps: 0.052124
[12:37:31.403] iteration 4323: total_loss: 0.310458, loss_sup: 0.234674, loss_mps: 0.026810, loss_cps: 0.048973
[12:37:31.549] iteration 4324: total_loss: 0.189537, loss_sup: 0.104671, loss_mps: 0.028912, loss_cps: 0.055954
[12:37:31.695] iteration 4325: total_loss: 0.140190, loss_sup: 0.038579, loss_mps: 0.034480, loss_cps: 0.067131
[12:37:31.841] iteration 4326: total_loss: 0.143643, loss_sup: 0.079572, loss_mps: 0.023767, loss_cps: 0.040303
[12:37:31.988] iteration 4327: total_loss: 0.193642, loss_sup: 0.111180, loss_mps: 0.028162, loss_cps: 0.054301
[12:37:32.135] iteration 4328: total_loss: 0.280432, loss_sup: 0.183239, loss_mps: 0.032828, loss_cps: 0.064365
[12:37:32.281] iteration 4329: total_loss: 0.163138, loss_sup: 0.087742, loss_mps: 0.026208, loss_cps: 0.049188
[12:37:32.427] iteration 4330: total_loss: 0.209426, loss_sup: 0.132425, loss_mps: 0.026088, loss_cps: 0.050912
[12:37:32.573] iteration 4331: total_loss: 0.164297, loss_sup: 0.099749, loss_mps: 0.023828, loss_cps: 0.040720
[12:37:32.720] iteration 4332: total_loss: 0.357276, loss_sup: 0.277242, loss_mps: 0.026689, loss_cps: 0.053344
[12:37:32.866] iteration 4333: total_loss: 0.216822, loss_sup: 0.138245, loss_mps: 0.027567, loss_cps: 0.051011
[12:37:33.019] iteration 4334: total_loss: 0.177304, loss_sup: 0.087590, loss_mps: 0.030462, loss_cps: 0.059252
[12:37:33.164] iteration 4335: total_loss: 0.212874, loss_sup: 0.127947, loss_mps: 0.029080, loss_cps: 0.055847
[12:37:33.311] iteration 4336: total_loss: 0.194087, loss_sup: 0.109003, loss_mps: 0.028987, loss_cps: 0.056098
[12:37:33.457] iteration 4337: total_loss: 0.137895, loss_sup: 0.073945, loss_mps: 0.022496, loss_cps: 0.041454
[12:37:33.603] iteration 4338: total_loss: 0.247873, loss_sup: 0.173887, loss_mps: 0.027741, loss_cps: 0.046246
[12:37:33.749] iteration 4339: total_loss: 0.259773, loss_sup: 0.172090, loss_mps: 0.030088, loss_cps: 0.057595
[12:37:33.897] iteration 4340: total_loss: 0.315650, loss_sup: 0.249400, loss_mps: 0.024403, loss_cps: 0.041847
[12:37:34.043] iteration 4341: total_loss: 0.241506, loss_sup: 0.133238, loss_mps: 0.036151, loss_cps: 0.072117
[12:37:34.191] iteration 4342: total_loss: 0.256606, loss_sup: 0.183191, loss_mps: 0.026272, loss_cps: 0.047142
[12:37:34.336] iteration 4343: total_loss: 0.243222, loss_sup: 0.155298, loss_mps: 0.030139, loss_cps: 0.057785
[12:37:34.482] iteration 4344: total_loss: 0.342459, loss_sup: 0.264908, loss_mps: 0.027172, loss_cps: 0.050380
[12:37:34.628] iteration 4345: total_loss: 0.372467, loss_sup: 0.269152, loss_mps: 0.035473, loss_cps: 0.067842
[12:37:34.774] iteration 4346: total_loss: 0.199619, loss_sup: 0.077530, loss_mps: 0.041703, loss_cps: 0.080386
[12:37:34.920] iteration 4347: total_loss: 0.351833, loss_sup: 0.244081, loss_mps: 0.035648, loss_cps: 0.072104
[12:37:35.066] iteration 4348: total_loss: 0.295751, loss_sup: 0.175380, loss_mps: 0.040960, loss_cps: 0.079411
[12:37:35.211] iteration 4349: total_loss: 0.195129, loss_sup: 0.125665, loss_mps: 0.026027, loss_cps: 0.043437
[12:37:35.357] iteration 4350: total_loss: 0.155175, loss_sup: 0.064544, loss_mps: 0.031576, loss_cps: 0.059055
[12:37:35.502] iteration 4351: total_loss: 0.194745, loss_sup: 0.125835, loss_mps: 0.025258, loss_cps: 0.043652
[12:37:35.649] iteration 4352: total_loss: 0.207422, loss_sup: 0.125202, loss_mps: 0.029771, loss_cps: 0.052449
[12:37:35.795] iteration 4353: total_loss: 0.176922, loss_sup: 0.115459, loss_mps: 0.023530, loss_cps: 0.037933
[12:37:35.940] iteration 4354: total_loss: 0.279552, loss_sup: 0.199770, loss_mps: 0.028631, loss_cps: 0.051150
[12:37:36.086] iteration 4355: total_loss: 0.156534, loss_sup: 0.089355, loss_mps: 0.024030, loss_cps: 0.043149
[12:37:36.232] iteration 4356: total_loss: 0.306341, loss_sup: 0.215918, loss_mps: 0.032895, loss_cps: 0.057528
[12:37:36.378] iteration 4357: total_loss: 0.171983, loss_sup: 0.120315, loss_mps: 0.020458, loss_cps: 0.031210
[12:37:36.523] iteration 4358: total_loss: 0.170082, loss_sup: 0.104371, loss_mps: 0.024648, loss_cps: 0.041064
[12:37:36.671] iteration 4359: total_loss: 0.315722, loss_sup: 0.235330, loss_mps: 0.027770, loss_cps: 0.052623
[12:37:36.817] iteration 4360: total_loss: 0.286973, loss_sup: 0.183629, loss_mps: 0.034226, loss_cps: 0.069117
[12:37:36.962] iteration 4361: total_loss: 0.142687, loss_sup: 0.053020, loss_mps: 0.029934, loss_cps: 0.059733
[12:37:37.109] iteration 4362: total_loss: 0.207181, loss_sup: 0.137779, loss_mps: 0.024757, loss_cps: 0.044644
[12:37:37.255] iteration 4363: total_loss: 0.219045, loss_sup: 0.149993, loss_mps: 0.024644, loss_cps: 0.044409
[12:37:37.405] iteration 4364: total_loss: 0.443853, loss_sup: 0.317047, loss_mps: 0.042002, loss_cps: 0.084804
[12:37:37.554] iteration 4365: total_loss: 0.329141, loss_sup: 0.266019, loss_mps: 0.023762, loss_cps: 0.039360
[12:37:37.702] iteration 4366: total_loss: 0.304787, loss_sup: 0.221492, loss_mps: 0.030129, loss_cps: 0.053165
[12:37:37.848] iteration 4367: total_loss: 0.332893, loss_sup: 0.261128, loss_mps: 0.025881, loss_cps: 0.045884
[12:37:37.995] iteration 4368: total_loss: 0.283696, loss_sup: 0.214455, loss_mps: 0.025554, loss_cps: 0.043687
[12:37:38.142] iteration 4369: total_loss: 0.138060, loss_sup: 0.080517, loss_mps: 0.021163, loss_cps: 0.036380
[12:37:38.288] iteration 4370: total_loss: 0.236662, loss_sup: 0.177673, loss_mps: 0.021926, loss_cps: 0.037063
[12:37:38.435] iteration 4371: total_loss: 0.195031, loss_sup: 0.107707, loss_mps: 0.031168, loss_cps: 0.056155
[12:37:38.586] iteration 4372: total_loss: 0.176804, loss_sup: 0.110510, loss_mps: 0.024574, loss_cps: 0.041721
[12:37:38.732] iteration 4373: total_loss: 0.119328, loss_sup: 0.064376, loss_mps: 0.020912, loss_cps: 0.034039
[12:37:38.878] iteration 4374: total_loss: 0.163904, loss_sup: 0.089220, loss_mps: 0.027422, loss_cps: 0.047261
[12:37:39.025] iteration 4375: total_loss: 0.153521, loss_sup: 0.048532, loss_mps: 0.035935, loss_cps: 0.069054
[12:37:39.171] iteration 4376: total_loss: 0.129406, loss_sup: 0.052311, loss_mps: 0.027116, loss_cps: 0.049980
[12:37:39.319] iteration 4377: total_loss: 0.226565, loss_sup: 0.133892, loss_mps: 0.032782, loss_cps: 0.059892
[12:37:39.468] iteration 4378: total_loss: 0.297944, loss_sup: 0.185532, loss_mps: 0.036672, loss_cps: 0.075740
[12:37:39.615] iteration 4379: total_loss: 0.369518, loss_sup: 0.248886, loss_mps: 0.039697, loss_cps: 0.080935
[12:37:39.764] iteration 4380: total_loss: 0.184698, loss_sup: 0.105019, loss_mps: 0.027447, loss_cps: 0.052231
[12:37:39.910] iteration 4381: total_loss: 0.325861, loss_sup: 0.223994, loss_mps: 0.034022, loss_cps: 0.067844
[12:37:40.057] iteration 4382: total_loss: 0.201882, loss_sup: 0.125280, loss_mps: 0.027947, loss_cps: 0.048655
[12:37:40.204] iteration 4383: total_loss: 0.142264, loss_sup: 0.073759, loss_mps: 0.025345, loss_cps: 0.043160
[12:37:40.350] iteration 4384: total_loss: 0.287268, loss_sup: 0.173887, loss_mps: 0.038356, loss_cps: 0.075025
[12:37:40.496] iteration 4385: total_loss: 0.213174, loss_sup: 0.116740, loss_mps: 0.032284, loss_cps: 0.064150
[12:37:40.642] iteration 4386: total_loss: 0.225991, loss_sup: 0.163607, loss_mps: 0.024848, loss_cps: 0.037537
[12:37:40.789] iteration 4387: total_loss: 0.238491, loss_sup: 0.159301, loss_mps: 0.027936, loss_cps: 0.051254
[12:37:40.935] iteration 4388: total_loss: 0.132659, loss_sup: 0.058863, loss_mps: 0.026094, loss_cps: 0.047702
[12:37:41.081] iteration 4389: total_loss: 0.381716, loss_sup: 0.273931, loss_mps: 0.035302, loss_cps: 0.072483
[12:37:41.229] iteration 4390: total_loss: 0.125700, loss_sup: 0.060176, loss_mps: 0.024159, loss_cps: 0.041365
[12:37:41.376] iteration 4391: total_loss: 0.314059, loss_sup: 0.185539, loss_mps: 0.042023, loss_cps: 0.086497
[12:37:41.522] iteration 4392: total_loss: 0.227007, loss_sup: 0.148809, loss_mps: 0.028060, loss_cps: 0.050138
[12:37:41.669] iteration 4393: total_loss: 0.189030, loss_sup: 0.098376, loss_mps: 0.031164, loss_cps: 0.059489
[12:37:41.815] iteration 4394: total_loss: 0.112199, loss_sup: 0.032865, loss_mps: 0.029440, loss_cps: 0.049894
[12:37:41.961] iteration 4395: total_loss: 0.198982, loss_sup: 0.134836, loss_mps: 0.023411, loss_cps: 0.040735
[12:37:42.108] iteration 4396: total_loss: 0.427289, loss_sup: 0.364667, loss_mps: 0.024478, loss_cps: 0.038144
[12:37:42.256] iteration 4397: total_loss: 0.404324, loss_sup: 0.324503, loss_mps: 0.028886, loss_cps: 0.050934
[12:37:42.402] iteration 4398: total_loss: 0.263080, loss_sup: 0.187581, loss_mps: 0.027297, loss_cps: 0.048202
[12:37:42.548] iteration 4399: total_loss: 0.160255, loss_sup: 0.089103, loss_mps: 0.026100, loss_cps: 0.045053
[12:37:42.696] iteration 4400: total_loss: 0.205561, loss_sup: 0.123569, loss_mps: 0.029167, loss_cps: 0.052825
[12:37:42.696] Evaluation Started ==>
[12:37:54.114] ==> valid iteration 4400: unet metrics: {'dc': 0.5964091807594071, 'jc': 0.46606863986882047, 'pre': 0.6568314481968204, 'hd': 6.51904550994078}, ynet metrics: {'dc': 0.518043942284647, 'jc': 0.3940761652232232, 'pre': 0.5631637832864116, 'hd': 7.207017700067572}.
[12:37:54.170] ==> New best valid dice for unet: 0.596409, at iteration 4400
[12:37:54.171] Evaluation Finished!⏹️
[12:37:54.322] iteration 4401: total_loss: 0.169867, loss_sup: 0.096855, loss_mps: 0.027176, loss_cps: 0.045836
[12:37:54.469] iteration 4402: total_loss: 0.109003, loss_sup: 0.035399, loss_mps: 0.025793, loss_cps: 0.047811
[12:37:54.616] iteration 4403: total_loss: 0.360861, loss_sup: 0.256911, loss_mps: 0.036228, loss_cps: 0.067723
[12:37:54.762] iteration 4404: total_loss: 0.334378, loss_sup: 0.236926, loss_mps: 0.032948, loss_cps: 0.064503
[12:37:54.908] iteration 4405: total_loss: 0.283889, loss_sup: 0.180300, loss_mps: 0.035459, loss_cps: 0.068130
[12:37:55.055] iteration 4406: total_loss: 0.216989, loss_sup: 0.140033, loss_mps: 0.027890, loss_cps: 0.049066
[12:37:55.201] iteration 4407: total_loss: 0.140597, loss_sup: 0.068910, loss_mps: 0.025617, loss_cps: 0.046069
[12:37:55.346] iteration 4408: total_loss: 0.441436, loss_sup: 0.311368, loss_mps: 0.042391, loss_cps: 0.087677
[12:37:55.493] iteration 4409: total_loss: 0.386785, loss_sup: 0.267164, loss_mps: 0.040266, loss_cps: 0.079355
[12:37:55.641] iteration 4410: total_loss: 0.155479, loss_sup: 0.096265, loss_mps: 0.021959, loss_cps: 0.037255
[12:37:55.791] iteration 4411: total_loss: 0.107578, loss_sup: 0.038593, loss_mps: 0.025847, loss_cps: 0.043137
[12:37:55.937] iteration 4412: total_loss: 0.291553, loss_sup: 0.177699, loss_mps: 0.037872, loss_cps: 0.075982
[12:37:56.086] iteration 4413: total_loss: 0.249824, loss_sup: 0.164693, loss_mps: 0.029586, loss_cps: 0.055544
[12:37:56.232] iteration 4414: total_loss: 0.305878, loss_sup: 0.211628, loss_mps: 0.032729, loss_cps: 0.061521
[12:37:56.378] iteration 4415: total_loss: 0.264966, loss_sup: 0.191569, loss_mps: 0.027581, loss_cps: 0.045816
[12:37:56.524] iteration 4416: total_loss: 0.263403, loss_sup: 0.206207, loss_mps: 0.021285, loss_cps: 0.035911
[12:37:56.670] iteration 4417: total_loss: 0.297354, loss_sup: 0.186772, loss_mps: 0.037003, loss_cps: 0.073578
[12:37:56.817] iteration 4418: total_loss: 0.427455, loss_sup: 0.329457, loss_mps: 0.034962, loss_cps: 0.063036
[12:37:56.963] iteration 4419: total_loss: 0.173703, loss_sup: 0.122007, loss_mps: 0.020534, loss_cps: 0.031162
[12:37:57.109] iteration 4420: total_loss: 0.554486, loss_sup: 0.477935, loss_mps: 0.028706, loss_cps: 0.047845
[12:37:57.254] iteration 4421: total_loss: 0.212227, loss_sup: 0.146091, loss_mps: 0.025198, loss_cps: 0.040937
[12:37:57.400] iteration 4422: total_loss: 0.222237, loss_sup: 0.150739, loss_mps: 0.026162, loss_cps: 0.045336
[12:37:57.545] iteration 4423: total_loss: 0.237763, loss_sup: 0.170426, loss_mps: 0.025318, loss_cps: 0.042020
[12:37:57.690] iteration 4424: total_loss: 0.193521, loss_sup: 0.117257, loss_mps: 0.027440, loss_cps: 0.048824
[12:37:57.835] iteration 4425: total_loss: 0.247008, loss_sup: 0.148804, loss_mps: 0.034458, loss_cps: 0.063746
[12:37:57.981] iteration 4426: total_loss: 0.249327, loss_sup: 0.161072, loss_mps: 0.032497, loss_cps: 0.055757
[12:37:58.130] iteration 4427: total_loss: 0.358633, loss_sup: 0.221786, loss_mps: 0.045816, loss_cps: 0.091030
[12:37:58.277] iteration 4428: total_loss: 0.404738, loss_sup: 0.304447, loss_mps: 0.036444, loss_cps: 0.063847
[12:37:58.423] iteration 4429: total_loss: 0.264053, loss_sup: 0.170473, loss_mps: 0.034275, loss_cps: 0.059305
[12:37:58.572] iteration 4430: total_loss: 0.158277, loss_sup: 0.093651, loss_mps: 0.025033, loss_cps: 0.039593
[12:37:58.718] iteration 4431: total_loss: 0.313960, loss_sup: 0.198897, loss_mps: 0.039194, loss_cps: 0.075870
[12:37:58.864] iteration 4432: total_loss: 0.480983, loss_sup: 0.373162, loss_mps: 0.037734, loss_cps: 0.070087
[12:37:59.009] iteration 4433: total_loss: 0.284780, loss_sup: 0.164278, loss_mps: 0.040892, loss_cps: 0.079610
[12:37:59.156] iteration 4434: total_loss: 0.220025, loss_sup: 0.123398, loss_mps: 0.033070, loss_cps: 0.063558
[12:37:59.303] iteration 4435: total_loss: 0.137004, loss_sup: 0.060378, loss_mps: 0.028374, loss_cps: 0.048252
[12:37:59.449] iteration 4436: total_loss: 0.369730, loss_sup: 0.247194, loss_mps: 0.041022, loss_cps: 0.081514
[12:37:59.594] iteration 4437: total_loss: 0.209860, loss_sup: 0.133417, loss_mps: 0.028366, loss_cps: 0.048078
[12:37:59.740] iteration 4438: total_loss: 0.353382, loss_sup: 0.252033, loss_mps: 0.036012, loss_cps: 0.065337
[12:37:59.887] iteration 4439: total_loss: 0.281283, loss_sup: 0.147681, loss_mps: 0.045281, loss_cps: 0.088320
[12:38:00.033] iteration 4440: total_loss: 0.264887, loss_sup: 0.168598, loss_mps: 0.033945, loss_cps: 0.062344
[12:38:00.178] iteration 4441: total_loss: 0.439176, loss_sup: 0.355849, loss_mps: 0.030980, loss_cps: 0.052348
[12:38:00.324] iteration 4442: total_loss: 0.515257, loss_sup: 0.408476, loss_mps: 0.037634, loss_cps: 0.069147
[12:38:00.469] iteration 4443: total_loss: 0.246008, loss_sup: 0.159848, loss_mps: 0.032564, loss_cps: 0.053596
[12:38:00.616] iteration 4444: total_loss: 0.369985, loss_sup: 0.285892, loss_mps: 0.030489, loss_cps: 0.053604
[12:38:00.764] iteration 4445: total_loss: 0.165080, loss_sup: 0.084460, loss_mps: 0.031285, loss_cps: 0.049335
[12:38:00.912] iteration 4446: total_loss: 0.270457, loss_sup: 0.194407, loss_mps: 0.029737, loss_cps: 0.046312
[12:38:01.058] iteration 4447: total_loss: 0.181940, loss_sup: 0.121003, loss_mps: 0.024218, loss_cps: 0.036720
[12:38:01.205] iteration 4448: total_loss: 0.543433, loss_sup: 0.384573, loss_mps: 0.053379, loss_cps: 0.105481
[12:38:01.353] iteration 4449: total_loss: 0.183873, loss_sup: 0.092215, loss_mps: 0.034342, loss_cps: 0.057317
[12:38:01.502] iteration 4450: total_loss: 0.356238, loss_sup: 0.221105, loss_mps: 0.045959, loss_cps: 0.089175
[12:38:01.648] iteration 4451: total_loss: 0.467222, loss_sup: 0.362824, loss_mps: 0.038176, loss_cps: 0.066221
[12:38:01.794] iteration 4452: total_loss: 0.234349, loss_sup: 0.119553, loss_mps: 0.040599, loss_cps: 0.074197
[12:38:01.941] iteration 4453: total_loss: 0.433788, loss_sup: 0.277223, loss_mps: 0.053764, loss_cps: 0.102801
[12:38:02.088] iteration 4454: total_loss: 0.193732, loss_sup: 0.099545, loss_mps: 0.033687, loss_cps: 0.060500
[12:38:02.233] iteration 4455: total_loss: 0.287793, loss_sup: 0.225732, loss_mps: 0.024418, loss_cps: 0.037643
[12:38:02.379] iteration 4456: total_loss: 0.387380, loss_sup: 0.283754, loss_mps: 0.035458, loss_cps: 0.068168
[12:38:02.526] iteration 4457: total_loss: 0.273718, loss_sup: 0.140262, loss_mps: 0.043542, loss_cps: 0.089915
[12:38:02.672] iteration 4458: total_loss: 0.283984, loss_sup: 0.184252, loss_mps: 0.034661, loss_cps: 0.065071
[12:38:02.818] iteration 4459: total_loss: 0.297076, loss_sup: 0.200472, loss_mps: 0.033553, loss_cps: 0.063051
[12:38:02.963] iteration 4460: total_loss: 0.354769, loss_sup: 0.224230, loss_mps: 0.045257, loss_cps: 0.085283
[12:38:03.110] iteration 4461: total_loss: 0.164645, loss_sup: 0.092069, loss_mps: 0.027853, loss_cps: 0.044724
[12:38:03.256] iteration 4462: total_loss: 0.230497, loss_sup: 0.168184, loss_mps: 0.024909, loss_cps: 0.037404
[12:38:03.404] iteration 4463: total_loss: 0.331747, loss_sup: 0.276676, loss_mps: 0.021740, loss_cps: 0.033332
[12:38:03.551] iteration 4464: total_loss: 0.154201, loss_sup: 0.076570, loss_mps: 0.028032, loss_cps: 0.049599
[12:38:03.699] iteration 4465: total_loss: 0.322439, loss_sup: 0.230792, loss_mps: 0.032677, loss_cps: 0.058969
[12:38:03.845] iteration 4466: total_loss: 0.235706, loss_sup: 0.157405, loss_mps: 0.029320, loss_cps: 0.048982
[12:38:03.991] iteration 4467: total_loss: 0.121117, loss_sup: 0.061206, loss_mps: 0.023025, loss_cps: 0.036887
[12:38:04.137] iteration 4468: total_loss: 0.197102, loss_sup: 0.114560, loss_mps: 0.030884, loss_cps: 0.051657
[12:38:04.283] iteration 4469: total_loss: 0.152124, loss_sup: 0.062167, loss_mps: 0.031938, loss_cps: 0.058019
[12:38:04.429] iteration 4470: total_loss: 0.164922, loss_sup: 0.091138, loss_mps: 0.028470, loss_cps: 0.045313
[12:38:04.574] iteration 4471: total_loss: 0.216382, loss_sup: 0.118785, loss_mps: 0.035912, loss_cps: 0.061685
[12:38:04.720] iteration 4472: total_loss: 0.246406, loss_sup: 0.142688, loss_mps: 0.036188, loss_cps: 0.067530
[12:38:04.867] iteration 4473: total_loss: 0.207106, loss_sup: 0.129755, loss_mps: 0.028674, loss_cps: 0.048677
[12:38:05.015] iteration 4474: total_loss: 0.480879, loss_sup: 0.347074, loss_mps: 0.046292, loss_cps: 0.087513
[12:38:05.161] iteration 4475: total_loss: 0.536990, loss_sup: 0.447132, loss_mps: 0.032386, loss_cps: 0.057473
[12:38:05.307] iteration 4476: total_loss: 0.168948, loss_sup: 0.095689, loss_mps: 0.027906, loss_cps: 0.045353
[12:38:05.453] iteration 4477: total_loss: 0.157587, loss_sup: 0.086278, loss_mps: 0.026638, loss_cps: 0.044671
[12:38:05.601] iteration 4478: total_loss: 0.309330, loss_sup: 0.222735, loss_mps: 0.031974, loss_cps: 0.054621
[12:38:05.747] iteration 4479: total_loss: 0.168669, loss_sup: 0.102949, loss_mps: 0.025543, loss_cps: 0.040177
[12:38:05.892] iteration 4480: total_loss: 0.179480, loss_sup: 0.124382, loss_mps: 0.022378, loss_cps: 0.032721
[12:38:06.038] iteration 4481: total_loss: 0.237367, loss_sup: 0.143310, loss_mps: 0.032967, loss_cps: 0.061090
[12:38:06.184] iteration 4482: total_loss: 0.243287, loss_sup: 0.139912, loss_mps: 0.036172, loss_cps: 0.067203
[12:38:06.330] iteration 4483: total_loss: 0.245532, loss_sup: 0.152413, loss_mps: 0.033394, loss_cps: 0.059725
[12:38:06.477] iteration 4484: total_loss: 0.211902, loss_sup: 0.138345, loss_mps: 0.026790, loss_cps: 0.046768
[12:38:06.622] iteration 4485: total_loss: 0.139969, loss_sup: 0.053585, loss_mps: 0.030286, loss_cps: 0.056098
[12:38:06.768] iteration 4486: total_loss: 0.156537, loss_sup: 0.067444, loss_mps: 0.030737, loss_cps: 0.058356
[12:38:06.914] iteration 4487: total_loss: 0.198323, loss_sup: 0.105077, loss_mps: 0.033010, loss_cps: 0.060236
[12:38:07.059] iteration 4488: total_loss: 0.257945, loss_sup: 0.161729, loss_mps: 0.033233, loss_cps: 0.062984
[12:38:07.206] iteration 4489: total_loss: 0.426918, loss_sup: 0.305133, loss_mps: 0.040150, loss_cps: 0.081635
[12:38:07.352] iteration 4490: total_loss: 0.187815, loss_sup: 0.087212, loss_mps: 0.035480, loss_cps: 0.065123
[12:38:07.499] iteration 4491: total_loss: 0.328373, loss_sup: 0.223634, loss_mps: 0.033703, loss_cps: 0.071035
[12:38:07.645] iteration 4492: total_loss: 0.196941, loss_sup: 0.078726, loss_mps: 0.038623, loss_cps: 0.079592
[12:38:07.792] iteration 4493: total_loss: 0.174248, loss_sup: 0.083189, loss_mps: 0.031132, loss_cps: 0.059927
[12:38:07.939] iteration 4494: total_loss: 0.253034, loss_sup: 0.158803, loss_mps: 0.032654, loss_cps: 0.061577
[12:38:08.087] iteration 4495: total_loss: 0.237423, loss_sup: 0.139732, loss_mps: 0.033365, loss_cps: 0.064326
[12:38:08.236] iteration 4496: total_loss: 0.189883, loss_sup: 0.114174, loss_mps: 0.026983, loss_cps: 0.048727
[12:38:08.382] iteration 4497: total_loss: 0.179828, loss_sup: 0.093819, loss_mps: 0.029290, loss_cps: 0.056719
[12:38:08.529] iteration 4498: total_loss: 0.149578, loss_sup: 0.079624, loss_mps: 0.024945, loss_cps: 0.045008
[12:38:08.675] iteration 4499: total_loss: 0.287672, loss_sup: 0.214437, loss_mps: 0.026985, loss_cps: 0.046251
[12:38:08.821] iteration 4500: total_loss: 0.377096, loss_sup: 0.288914, loss_mps: 0.031058, loss_cps: 0.057124
[12:38:08.821] Evaluation Started ==>
[12:38:20.200] ==> valid iteration 4500: unet metrics: {'dc': 0.608718479622528, 'jc': 0.48073895754028045, 'pre': 0.6828604595292567, 'hd': 6.395974923931275}, ynet metrics: {'dc': 0.5232306898951049, 'jc': 0.3988081744306592, 'pre': 0.6794784862560105, 'hd': 6.536872861594302}.
[12:38:20.257] ==> New best valid dice for unet: 0.608718, at iteration 4500
[12:38:20.259] Evaluation Finished!⏹️
[12:38:20.413] iteration 4501: total_loss: 0.132860, loss_sup: 0.051229, loss_mps: 0.028198, loss_cps: 0.053434
[12:38:20.563] iteration 4502: total_loss: 0.243275, loss_sup: 0.152599, loss_mps: 0.031553, loss_cps: 0.059123
[12:38:20.709] iteration 4503: total_loss: 0.255178, loss_sup: 0.197732, loss_mps: 0.021843, loss_cps: 0.035604
[12:38:20.854] iteration 4504: total_loss: 0.158219, loss_sup: 0.092942, loss_mps: 0.023466, loss_cps: 0.041811
[12:38:20.999] iteration 4505: total_loss: 0.181178, loss_sup: 0.102389, loss_mps: 0.028042, loss_cps: 0.050747
[12:38:21.144] iteration 4506: total_loss: 0.291823, loss_sup: 0.211454, loss_mps: 0.028781, loss_cps: 0.051587
[12:38:21.289] iteration 4507: total_loss: 0.212415, loss_sup: 0.130838, loss_mps: 0.029050, loss_cps: 0.052527
[12:38:21.436] iteration 4508: total_loss: 0.254230, loss_sup: 0.171643, loss_mps: 0.028255, loss_cps: 0.054332
[12:38:21.582] iteration 4509: total_loss: 0.282577, loss_sup: 0.188847, loss_mps: 0.031333, loss_cps: 0.062397
[12:38:21.729] iteration 4510: total_loss: 0.163710, loss_sup: 0.087719, loss_mps: 0.027420, loss_cps: 0.048571
[12:38:21.875] iteration 4511: total_loss: 0.384146, loss_sup: 0.296259, loss_mps: 0.030529, loss_cps: 0.057358
[12:38:22.020] iteration 4512: total_loss: 0.288833, loss_sup: 0.216445, loss_mps: 0.026377, loss_cps: 0.046011
[12:38:22.166] iteration 4513: total_loss: 0.216405, loss_sup: 0.145711, loss_mps: 0.026751, loss_cps: 0.043943
[12:38:22.314] iteration 4514: total_loss: 0.101402, loss_sup: 0.029545, loss_mps: 0.026635, loss_cps: 0.045223
[12:38:22.459] iteration 4515: total_loss: 0.162922, loss_sup: 0.097300, loss_mps: 0.025175, loss_cps: 0.040447
[12:38:22.606] iteration 4516: total_loss: 0.136712, loss_sup: 0.055044, loss_mps: 0.029224, loss_cps: 0.052444
[12:38:22.752] iteration 4517: total_loss: 0.127088, loss_sup: 0.065178, loss_mps: 0.023961, loss_cps: 0.037949
[12:38:22.899] iteration 4518: total_loss: 0.342658, loss_sup: 0.244052, loss_mps: 0.033950, loss_cps: 0.064656
[12:38:23.045] iteration 4519: total_loss: 0.256201, loss_sup: 0.189256, loss_mps: 0.024778, loss_cps: 0.042167
[12:38:23.190] iteration 4520: total_loss: 0.225270, loss_sup: 0.175114, loss_mps: 0.020759, loss_cps: 0.029397
[12:38:23.336] iteration 4521: total_loss: 0.437977, loss_sup: 0.322328, loss_mps: 0.039033, loss_cps: 0.076615
[12:38:23.482] iteration 4522: total_loss: 0.294141, loss_sup: 0.217967, loss_mps: 0.027215, loss_cps: 0.048959
[12:38:23.627] iteration 4523: total_loss: 0.201104, loss_sup: 0.137562, loss_mps: 0.024218, loss_cps: 0.039324
[12:38:23.778] iteration 4524: total_loss: 0.180460, loss_sup: 0.136336, loss_mps: 0.017744, loss_cps: 0.026380
[12:38:23.924] iteration 4525: total_loss: 0.310387, loss_sup: 0.228250, loss_mps: 0.030230, loss_cps: 0.051907
[12:38:24.070] iteration 4526: total_loss: 0.453662, loss_sup: 0.328931, loss_mps: 0.041660, loss_cps: 0.083071
[12:38:24.216] iteration 4527: total_loss: 0.434300, loss_sup: 0.319711, loss_mps: 0.039554, loss_cps: 0.075035
[12:38:24.363] iteration 4528: total_loss: 0.186827, loss_sup: 0.110334, loss_mps: 0.027919, loss_cps: 0.048574
[12:38:24.509] iteration 4529: total_loss: 0.201437, loss_sup: 0.138969, loss_mps: 0.023633, loss_cps: 0.038834
[12:38:24.655] iteration 4530: total_loss: 0.171905, loss_sup: 0.106989, loss_mps: 0.024882, loss_cps: 0.040034
[12:38:24.801] iteration 4531: total_loss: 0.557708, loss_sup: 0.451813, loss_mps: 0.035797, loss_cps: 0.070098
[12:38:24.946] iteration 4532: total_loss: 0.497629, loss_sup: 0.372758, loss_mps: 0.039964, loss_cps: 0.084907
[12:38:25.091] iteration 4533: total_loss: 0.320566, loss_sup: 0.229476, loss_mps: 0.032367, loss_cps: 0.058724
[12:38:25.237] iteration 4534: total_loss: 0.431559, loss_sup: 0.273827, loss_mps: 0.051212, loss_cps: 0.106520
[12:38:25.383] iteration 4535: total_loss: 0.235965, loss_sup: 0.152176, loss_mps: 0.031556, loss_cps: 0.052233
[12:38:25.528] iteration 4536: total_loss: 0.372317, loss_sup: 0.286471, loss_mps: 0.032219, loss_cps: 0.053628
[12:38:25.674] iteration 4537: total_loss: 0.321207, loss_sup: 0.241650, loss_mps: 0.029645, loss_cps: 0.049913
[12:38:25.820] iteration 4538: total_loss: 0.364487, loss_sup: 0.278425, loss_mps: 0.032415, loss_cps: 0.053648
[12:38:25.966] iteration 4539: total_loss: 0.365198, loss_sup: 0.272693, loss_mps: 0.034267, loss_cps: 0.058238
[12:38:26.111] iteration 4540: total_loss: 0.264580, loss_sup: 0.180989, loss_mps: 0.031580, loss_cps: 0.052011
[12:38:26.257] iteration 4541: total_loss: 0.255644, loss_sup: 0.176258, loss_mps: 0.030402, loss_cps: 0.048984
[12:38:26.402] iteration 4542: total_loss: 0.224458, loss_sup: 0.118968, loss_mps: 0.039397, loss_cps: 0.066093
[12:38:26.547] iteration 4543: total_loss: 0.318258, loss_sup: 0.246707, loss_mps: 0.028559, loss_cps: 0.042992
[12:38:26.693] iteration 4544: total_loss: 0.386690, loss_sup: 0.298605, loss_mps: 0.034050, loss_cps: 0.054035
[12:38:26.838] iteration 4545: total_loss: 0.220407, loss_sup: 0.155019, loss_mps: 0.027239, loss_cps: 0.038148
[12:38:26.984] iteration 4546: total_loss: 0.733948, loss_sup: 0.612716, loss_mps: 0.043479, loss_cps: 0.077752
[12:38:27.129] iteration 4547: total_loss: 0.469902, loss_sup: 0.389093, loss_mps: 0.031245, loss_cps: 0.049564
[12:38:27.275] iteration 4548: total_loss: 0.140894, loss_sup: 0.061072, loss_mps: 0.032391, loss_cps: 0.047430
[12:38:27.421] iteration 4549: total_loss: 0.396251, loss_sup: 0.268454, loss_mps: 0.047278, loss_cps: 0.080520
[12:38:27.566] iteration 4550: total_loss: 0.162478, loss_sup: 0.073393, loss_mps: 0.034367, loss_cps: 0.054719
[12:38:27.712] iteration 4551: total_loss: 0.335812, loss_sup: 0.239860, loss_mps: 0.036577, loss_cps: 0.059374
[12:38:27.859] iteration 4552: total_loss: 0.217726, loss_sup: 0.113192, loss_mps: 0.039096, loss_cps: 0.065437
[12:38:28.005] iteration 4553: total_loss: 0.322317, loss_sup: 0.212318, loss_mps: 0.041564, loss_cps: 0.068435
[12:38:28.151] iteration 4554: total_loss: 0.157201, loss_sup: 0.084209, loss_mps: 0.030316, loss_cps: 0.042677
[12:38:28.296] iteration 4555: total_loss: 0.221973, loss_sup: 0.147005, loss_mps: 0.030615, loss_cps: 0.044354
[12:38:28.442] iteration 4556: total_loss: 0.191358, loss_sup: 0.101912, loss_mps: 0.036219, loss_cps: 0.053228
[12:38:28.588] iteration 4557: total_loss: 0.316332, loss_sup: 0.223694, loss_mps: 0.036047, loss_cps: 0.056590
[12:38:28.733] iteration 4558: total_loss: 0.248949, loss_sup: 0.168307, loss_mps: 0.031712, loss_cps: 0.048930
[12:38:28.880] iteration 4559: total_loss: 0.414021, loss_sup: 0.321190, loss_mps: 0.035356, loss_cps: 0.057474
[12:38:29.027] iteration 4560: total_loss: 0.252059, loss_sup: 0.137856, loss_mps: 0.041774, loss_cps: 0.072429
[12:38:29.172] iteration 4561: total_loss: 0.276536, loss_sup: 0.179564, loss_mps: 0.037354, loss_cps: 0.059617
[12:38:29.318] iteration 4562: total_loss: 0.167468, loss_sup: 0.085874, loss_mps: 0.030653, loss_cps: 0.050941
[12:38:29.464] iteration 4563: total_loss: 0.228027, loss_sup: 0.162206, loss_mps: 0.026001, loss_cps: 0.039821
[12:38:29.609] iteration 4564: total_loss: 0.283602, loss_sup: 0.176524, loss_mps: 0.038530, loss_cps: 0.068548
[12:38:29.755] iteration 4565: total_loss: 0.153579, loss_sup: 0.075021, loss_mps: 0.030107, loss_cps: 0.048451
[12:38:29.900] iteration 4566: total_loss: 0.464300, loss_sup: 0.366512, loss_mps: 0.035912, loss_cps: 0.061876
[12:38:30.047] iteration 4567: total_loss: 0.349766, loss_sup: 0.225815, loss_mps: 0.042652, loss_cps: 0.081299
[12:38:30.193] iteration 4568: total_loss: 0.151191, loss_sup: 0.069443, loss_mps: 0.030621, loss_cps: 0.051127
[12:38:30.339] iteration 4569: total_loss: 0.258524, loss_sup: 0.132641, loss_mps: 0.042977, loss_cps: 0.082905
[12:38:30.485] iteration 4570: total_loss: 0.270624, loss_sup: 0.176580, loss_mps: 0.034290, loss_cps: 0.059753
[12:38:30.631] iteration 4571: total_loss: 0.146588, loss_sup: 0.083769, loss_mps: 0.024535, loss_cps: 0.038285
[12:38:30.778] iteration 4572: total_loss: 0.248165, loss_sup: 0.173576, loss_mps: 0.030061, loss_cps: 0.044528
[12:38:30.924] iteration 4573: total_loss: 0.317898, loss_sup: 0.227718, loss_mps: 0.033095, loss_cps: 0.057085
[12:38:31.070] iteration 4574: total_loss: 0.333981, loss_sup: 0.229348, loss_mps: 0.036476, loss_cps: 0.068156
[12:38:31.216] iteration 4575: total_loss: 0.161076, loss_sup: 0.086883, loss_mps: 0.027833, loss_cps: 0.046360
[12:38:31.362] iteration 4576: total_loss: 0.288558, loss_sup: 0.205583, loss_mps: 0.030935, loss_cps: 0.052040
[12:38:31.508] iteration 4577: total_loss: 0.313649, loss_sup: 0.228891, loss_mps: 0.030307, loss_cps: 0.054451
[12:38:31.654] iteration 4578: total_loss: 0.286389, loss_sup: 0.187501, loss_mps: 0.034470, loss_cps: 0.064418
[12:38:31.801] iteration 4579: total_loss: 0.268738, loss_sup: 0.149605, loss_mps: 0.040253, loss_cps: 0.078880
[12:38:31.946] iteration 4580: total_loss: 0.303787, loss_sup: 0.210311, loss_mps: 0.033357, loss_cps: 0.060119
[12:38:32.092] iteration 4581: total_loss: 0.203645, loss_sup: 0.126516, loss_mps: 0.027876, loss_cps: 0.049252
[12:38:32.240] iteration 4582: total_loss: 0.526004, loss_sup: 0.386597, loss_mps: 0.045374, loss_cps: 0.094033
[12:38:32.385] iteration 4583: total_loss: 0.272380, loss_sup: 0.187596, loss_mps: 0.029947, loss_cps: 0.054837
[12:38:32.532] iteration 4584: total_loss: 0.398823, loss_sup: 0.267198, loss_mps: 0.044127, loss_cps: 0.087499
[12:38:32.678] iteration 4585: total_loss: 0.348769, loss_sup: 0.253784, loss_mps: 0.034249, loss_cps: 0.060736
[12:38:32.824] iteration 4586: total_loss: 0.153488, loss_sup: 0.084287, loss_mps: 0.025168, loss_cps: 0.044033
[12:38:32.970] iteration 4587: total_loss: 0.333604, loss_sup: 0.220080, loss_mps: 0.039039, loss_cps: 0.074485
[12:38:33.115] iteration 4588: total_loss: 0.285683, loss_sup: 0.172874, loss_mps: 0.039068, loss_cps: 0.073740
[12:38:33.261] iteration 4589: total_loss: 0.341035, loss_sup: 0.191557, loss_mps: 0.049658, loss_cps: 0.099821
[12:38:33.406] iteration 4590: total_loss: 0.321747, loss_sup: 0.196947, loss_mps: 0.041573, loss_cps: 0.083227
[12:38:33.552] iteration 4591: total_loss: 0.198957, loss_sup: 0.082318, loss_mps: 0.040863, loss_cps: 0.075776
[12:38:33.698] iteration 4592: total_loss: 0.201060, loss_sup: 0.111963, loss_mps: 0.033190, loss_cps: 0.055906
[12:38:33.844] iteration 4593: total_loss: 0.216476, loss_sup: 0.105752, loss_mps: 0.038072, loss_cps: 0.072652
[12:38:33.989] iteration 4594: total_loss: 0.202297, loss_sup: 0.100193, loss_mps: 0.036779, loss_cps: 0.065324
[12:38:34.135] iteration 4595: total_loss: 0.182096, loss_sup: 0.107336, loss_mps: 0.029157, loss_cps: 0.045603
[12:38:34.281] iteration 4596: total_loss: 0.185243, loss_sup: 0.093389, loss_mps: 0.034213, loss_cps: 0.057642
[12:38:34.426] iteration 4597: total_loss: 0.196461, loss_sup: 0.100676, loss_mps: 0.034746, loss_cps: 0.061039
[12:38:34.491] iteration 4598: total_loss: 0.231261, loss_sup: 0.147469, loss_mps: 0.031635, loss_cps: 0.052157
[12:38:35.690] iteration 4599: total_loss: 0.183180, loss_sup: 0.076672, loss_mps: 0.037741, loss_cps: 0.068766
[12:38:35.839] iteration 4600: total_loss: 0.328824, loss_sup: 0.207087, loss_mps: 0.042423, loss_cps: 0.079314
[12:38:35.840] Evaluation Started ==>
[12:38:47.230] ==> valid iteration 4600: unet metrics: {'dc': 0.5564229931905478, 'jc': 0.4265255880087767, 'pre': 0.6012339126367336, 'hd': 7.057127993630156}, ynet metrics: {'dc': 0.4583884811449997, 'jc': 0.3394742088261834, 'pre': 0.5620898342199556, 'hd': 7.1771393128454815}.
[12:38:47.232] Evaluation Finished!⏹️
[12:38:47.384] iteration 4601: total_loss: 0.358484, loss_sup: 0.276284, loss_mps: 0.031033, loss_cps: 0.051167
[12:38:47.531] iteration 4602: total_loss: 0.130056, loss_sup: 0.043905, loss_mps: 0.031104, loss_cps: 0.055046
[12:38:47.677] iteration 4603: total_loss: 0.437671, loss_sup: 0.345041, loss_mps: 0.033040, loss_cps: 0.059591
[12:38:47.823] iteration 4604: total_loss: 0.183674, loss_sup: 0.118213, loss_mps: 0.025874, loss_cps: 0.039588
[12:38:47.968] iteration 4605: total_loss: 0.176637, loss_sup: 0.086511, loss_mps: 0.032594, loss_cps: 0.057532
[12:38:48.113] iteration 4606: total_loss: 0.115653, loss_sup: 0.057011, loss_mps: 0.022859, loss_cps: 0.035783
[12:38:48.258] iteration 4607: total_loss: 0.198766, loss_sup: 0.090244, loss_mps: 0.037299, loss_cps: 0.071223
[12:38:48.404] iteration 4608: total_loss: 0.217295, loss_sup: 0.115410, loss_mps: 0.035761, loss_cps: 0.066123
[12:38:48.550] iteration 4609: total_loss: 0.345475, loss_sup: 0.252574, loss_mps: 0.031876, loss_cps: 0.061025
[12:38:48.696] iteration 4610: total_loss: 0.309575, loss_sup: 0.180926, loss_mps: 0.042694, loss_cps: 0.085955
[12:38:48.844] iteration 4611: total_loss: 0.266678, loss_sup: 0.121796, loss_mps: 0.046678, loss_cps: 0.098204
[12:38:48.990] iteration 4612: total_loss: 0.247315, loss_sup: 0.150118, loss_mps: 0.033244, loss_cps: 0.063954
[12:38:49.137] iteration 4613: total_loss: 0.183472, loss_sup: 0.111359, loss_mps: 0.025591, loss_cps: 0.046523
[12:38:49.282] iteration 4614: total_loss: 0.191751, loss_sup: 0.100274, loss_mps: 0.031741, loss_cps: 0.059736
[12:38:49.428] iteration 4615: total_loss: 0.285794, loss_sup: 0.186499, loss_mps: 0.034938, loss_cps: 0.064357
[12:38:49.574] iteration 4616: total_loss: 0.374533, loss_sup: 0.259712, loss_mps: 0.038771, loss_cps: 0.076051
[12:38:49.719] iteration 4617: total_loss: 0.189192, loss_sup: 0.108321, loss_mps: 0.029780, loss_cps: 0.051091
[12:38:49.865] iteration 4618: total_loss: 0.510024, loss_sup: 0.379368, loss_mps: 0.043546, loss_cps: 0.087110
[12:38:50.011] iteration 4619: total_loss: 0.168886, loss_sup: 0.084417, loss_mps: 0.030472, loss_cps: 0.053997
[12:38:50.157] iteration 4620: total_loss: 0.241879, loss_sup: 0.169978, loss_mps: 0.027019, loss_cps: 0.044882
[12:38:50.303] iteration 4621: total_loss: 0.231897, loss_sup: 0.115700, loss_mps: 0.040793, loss_cps: 0.075404
[12:38:50.449] iteration 4622: total_loss: 0.116757, loss_sup: 0.053302, loss_mps: 0.024853, loss_cps: 0.038601
[12:38:50.595] iteration 4623: total_loss: 0.289877, loss_sup: 0.150966, loss_mps: 0.045535, loss_cps: 0.093376
[12:38:50.742] iteration 4624: total_loss: 0.152603, loss_sup: 0.061909, loss_mps: 0.032569, loss_cps: 0.058125
[12:38:50.888] iteration 4625: total_loss: 0.257557, loss_sup: 0.138963, loss_mps: 0.040189, loss_cps: 0.078405
[12:38:51.034] iteration 4626: total_loss: 0.211114, loss_sup: 0.099452, loss_mps: 0.038931, loss_cps: 0.072731
[12:38:51.179] iteration 4627: total_loss: 0.286759, loss_sup: 0.177462, loss_mps: 0.038916, loss_cps: 0.070381
[12:38:51.325] iteration 4628: total_loss: 0.292103, loss_sup: 0.169678, loss_mps: 0.040995, loss_cps: 0.081430
[12:38:51.472] iteration 4629: total_loss: 0.206117, loss_sup: 0.124673, loss_mps: 0.028480, loss_cps: 0.052964
[12:38:51.617] iteration 4630: total_loss: 0.311295, loss_sup: 0.216573, loss_mps: 0.033762, loss_cps: 0.060961
[12:38:51.764] iteration 4631: total_loss: 0.239337, loss_sup: 0.164960, loss_mps: 0.027860, loss_cps: 0.046518
[12:38:51.911] iteration 4632: total_loss: 0.197895, loss_sup: 0.125202, loss_mps: 0.027636, loss_cps: 0.045057
[12:38:52.056] iteration 4633: total_loss: 0.219899, loss_sup: 0.144908, loss_mps: 0.027562, loss_cps: 0.047429
[12:38:52.202] iteration 4634: total_loss: 0.232558, loss_sup: 0.147701, loss_mps: 0.030768, loss_cps: 0.054089
[12:38:52.347] iteration 4635: total_loss: 0.259823, loss_sup: 0.177366, loss_mps: 0.029713, loss_cps: 0.052744
[12:38:52.493] iteration 4636: total_loss: 0.127024, loss_sup: 0.062427, loss_mps: 0.025534, loss_cps: 0.039064
[12:38:52.638] iteration 4637: total_loss: 0.283338, loss_sup: 0.202521, loss_mps: 0.029883, loss_cps: 0.050935
[12:38:52.783] iteration 4638: total_loss: 0.314451, loss_sup: 0.209542, loss_mps: 0.036301, loss_cps: 0.068608
[12:38:52.929] iteration 4639: total_loss: 0.218388, loss_sup: 0.125704, loss_mps: 0.033016, loss_cps: 0.059668
[12:38:53.074] iteration 4640: total_loss: 0.145263, loss_sup: 0.065020, loss_mps: 0.029354, loss_cps: 0.050889
[12:38:53.224] iteration 4641: total_loss: 0.208620, loss_sup: 0.113286, loss_mps: 0.032799, loss_cps: 0.062536
[12:38:53.370] iteration 4642: total_loss: 0.295351, loss_sup: 0.168879, loss_mps: 0.042284, loss_cps: 0.084189
[12:38:53.518] iteration 4643: total_loss: 0.427628, loss_sup: 0.332392, loss_mps: 0.033090, loss_cps: 0.062146
[12:38:53.664] iteration 4644: total_loss: 0.408624, loss_sup: 0.277199, loss_mps: 0.042478, loss_cps: 0.088947
[12:38:53.810] iteration 4645: total_loss: 0.182343, loss_sup: 0.101432, loss_mps: 0.030076, loss_cps: 0.050835
[12:38:53.957] iteration 4646: total_loss: 0.204414, loss_sup: 0.109688, loss_mps: 0.031783, loss_cps: 0.062943
[12:38:54.102] iteration 4647: total_loss: 0.192935, loss_sup: 0.101665, loss_mps: 0.031558, loss_cps: 0.059713
[12:38:54.248] iteration 4648: total_loss: 0.161344, loss_sup: 0.087925, loss_mps: 0.026423, loss_cps: 0.046996
[12:38:54.395] iteration 4649: total_loss: 0.263994, loss_sup: 0.151166, loss_mps: 0.039229, loss_cps: 0.073599
[12:38:54.542] iteration 4650: total_loss: 0.258202, loss_sup: 0.166106, loss_mps: 0.031464, loss_cps: 0.060631
[12:38:54.688] iteration 4651: total_loss: 0.377497, loss_sup: 0.291420, loss_mps: 0.030936, loss_cps: 0.055142
[12:38:54.834] iteration 4652: total_loss: 0.295867, loss_sup: 0.212060, loss_mps: 0.031624, loss_cps: 0.052182
[12:38:54.980] iteration 4653: total_loss: 0.322849, loss_sup: 0.222072, loss_mps: 0.034600, loss_cps: 0.066176
[12:38:55.125] iteration 4654: total_loss: 0.233485, loss_sup: 0.162439, loss_mps: 0.027309, loss_cps: 0.043737
[12:38:55.271] iteration 4655: total_loss: 0.212688, loss_sup: 0.124045, loss_mps: 0.032562, loss_cps: 0.056081
[12:38:55.416] iteration 4656: total_loss: 0.351229, loss_sup: 0.264560, loss_mps: 0.031426, loss_cps: 0.055243
[12:38:55.562] iteration 4657: total_loss: 0.320315, loss_sup: 0.197003, loss_mps: 0.042235, loss_cps: 0.081077
[12:38:55.712] iteration 4658: total_loss: 0.265599, loss_sup: 0.160862, loss_mps: 0.036560, loss_cps: 0.068178
[12:38:55.858] iteration 4659: total_loss: 0.158339, loss_sup: 0.079099, loss_mps: 0.029866, loss_cps: 0.049374
[12:38:56.007] iteration 4660: total_loss: 0.253694, loss_sup: 0.177012, loss_mps: 0.029066, loss_cps: 0.047615
[12:38:56.153] iteration 4661: total_loss: 0.237192, loss_sup: 0.129532, loss_mps: 0.036493, loss_cps: 0.071167
[12:38:56.299] iteration 4662: total_loss: 0.224247, loss_sup: 0.152676, loss_mps: 0.026309, loss_cps: 0.045262
[12:38:56.446] iteration 4663: total_loss: 0.207031, loss_sup: 0.087254, loss_mps: 0.042329, loss_cps: 0.077449
[12:38:56.594] iteration 4664: total_loss: 0.237207, loss_sup: 0.164159, loss_mps: 0.027638, loss_cps: 0.045411
[12:38:56.741] iteration 4665: total_loss: 0.227678, loss_sup: 0.136989, loss_mps: 0.032968, loss_cps: 0.057721
[12:38:56.887] iteration 4666: total_loss: 0.188167, loss_sup: 0.130598, loss_mps: 0.022379, loss_cps: 0.035190
[12:38:57.034] iteration 4667: total_loss: 0.154946, loss_sup: 0.086945, loss_mps: 0.026121, loss_cps: 0.041879
[12:38:57.180] iteration 4668: total_loss: 0.238084, loss_sup: 0.159643, loss_mps: 0.028440, loss_cps: 0.050001
[12:38:57.326] iteration 4669: total_loss: 0.165739, loss_sup: 0.090766, loss_mps: 0.028539, loss_cps: 0.046434
[12:38:57.471] iteration 4670: total_loss: 0.446887, loss_sup: 0.331850, loss_mps: 0.039195, loss_cps: 0.075842
[12:38:57.616] iteration 4671: total_loss: 0.363290, loss_sup: 0.221764, loss_mps: 0.048037, loss_cps: 0.093489
[12:38:57.762] iteration 4672: total_loss: 0.126305, loss_sup: 0.048732, loss_mps: 0.028105, loss_cps: 0.049467
[12:38:57.908] iteration 4673: total_loss: 0.154116, loss_sup: 0.063723, loss_mps: 0.032851, loss_cps: 0.057542
[12:38:58.055] iteration 4674: total_loss: 0.267127, loss_sup: 0.189884, loss_mps: 0.027410, loss_cps: 0.049833
[12:38:58.201] iteration 4675: total_loss: 0.227237, loss_sup: 0.137940, loss_mps: 0.032614, loss_cps: 0.056684
[12:38:58.352] iteration 4676: total_loss: 0.344426, loss_sup: 0.258794, loss_mps: 0.029959, loss_cps: 0.055673
[12:38:58.499] iteration 4677: total_loss: 0.238310, loss_sup: 0.144567, loss_mps: 0.033239, loss_cps: 0.060504
[12:38:58.645] iteration 4678: total_loss: 0.378781, loss_sup: 0.262086, loss_mps: 0.039452, loss_cps: 0.077243
[12:38:58.792] iteration 4679: total_loss: 0.291758, loss_sup: 0.201625, loss_mps: 0.031450, loss_cps: 0.058683
[12:38:58.937] iteration 4680: total_loss: 0.198605, loss_sup: 0.123291, loss_mps: 0.028013, loss_cps: 0.047302
[12:38:59.083] iteration 4681: total_loss: 0.329132, loss_sup: 0.216457, loss_mps: 0.037767, loss_cps: 0.074908
[12:38:59.229] iteration 4682: total_loss: 0.155482, loss_sup: 0.068604, loss_mps: 0.031261, loss_cps: 0.055617
[12:38:59.374] iteration 4683: total_loss: 0.278692, loss_sup: 0.171868, loss_mps: 0.037469, loss_cps: 0.069354
[12:38:59.520] iteration 4684: total_loss: 0.116355, loss_sup: 0.055686, loss_mps: 0.023036, loss_cps: 0.037632
[12:38:59.666] iteration 4685: total_loss: 0.376142, loss_sup: 0.259166, loss_mps: 0.039345, loss_cps: 0.077632
[12:38:59.814] iteration 4686: total_loss: 0.351130, loss_sup: 0.256966, loss_mps: 0.033837, loss_cps: 0.060327
[12:38:59.960] iteration 4687: total_loss: 0.331622, loss_sup: 0.257907, loss_mps: 0.027200, loss_cps: 0.046515
[12:39:00.106] iteration 4688: total_loss: 0.163154, loss_sup: 0.071303, loss_mps: 0.032612, loss_cps: 0.059239
[12:39:00.252] iteration 4689: total_loss: 0.097095, loss_sup: 0.048325, loss_mps: 0.020563, loss_cps: 0.028206
[12:39:00.402] iteration 4690: total_loss: 0.180574, loss_sup: 0.098521, loss_mps: 0.030758, loss_cps: 0.051295
[12:39:00.549] iteration 4691: total_loss: 0.135593, loss_sup: 0.053232, loss_mps: 0.029950, loss_cps: 0.052411
[12:39:00.697] iteration 4692: total_loss: 0.205478, loss_sup: 0.102534, loss_mps: 0.036759, loss_cps: 0.066184
[12:39:00.843] iteration 4693: total_loss: 0.400805, loss_sup: 0.271360, loss_mps: 0.043697, loss_cps: 0.085749
[12:39:00.989] iteration 4694: total_loss: 0.378412, loss_sup: 0.253794, loss_mps: 0.041993, loss_cps: 0.082625
[12:39:01.136] iteration 4695: total_loss: 0.169533, loss_sup: 0.067994, loss_mps: 0.035359, loss_cps: 0.066180
[12:39:01.283] iteration 4696: total_loss: 0.121893, loss_sup: 0.059414, loss_mps: 0.024461, loss_cps: 0.038019
[12:39:01.429] iteration 4697: total_loss: 0.237887, loss_sup: 0.153136, loss_mps: 0.030488, loss_cps: 0.054262
[12:39:01.576] iteration 4698: total_loss: 0.155773, loss_sup: 0.096503, loss_mps: 0.022465, loss_cps: 0.036805
[12:39:01.722] iteration 4699: total_loss: 0.246224, loss_sup: 0.172906, loss_mps: 0.027468, loss_cps: 0.045849
[12:39:01.868] iteration 4700: total_loss: 0.173349, loss_sup: 0.090043, loss_mps: 0.030245, loss_cps: 0.053061
[12:39:01.868] Evaluation Started ==>
[12:39:13.214] ==> valid iteration 4700: unet metrics: {'dc': 0.5743137744322271, 'jc': 0.45312498384235184, 'pre': 0.6102186012821773, 'hd': 6.911667101577921}, ynet metrics: {'dc': 0.5301165413157601, 'jc': 0.41766588401110855, 'pre': 0.5797467674959647, 'hd': 6.931315745804947}.
[12:39:13.216] Evaluation Finished!⏹️
[12:39:13.370] iteration 4701: total_loss: 0.128838, loss_sup: 0.059127, loss_mps: 0.025901, loss_cps: 0.043809
[12:39:13.518] iteration 4702: total_loss: 0.204464, loss_sup: 0.110912, loss_mps: 0.033955, loss_cps: 0.059597
[12:39:13.664] iteration 4703: total_loss: 0.265609, loss_sup: 0.182987, loss_mps: 0.030003, loss_cps: 0.052619
[12:39:13.811] iteration 4704: total_loss: 0.176996, loss_sup: 0.073685, loss_mps: 0.036326, loss_cps: 0.066985
[12:39:13.957] iteration 4705: total_loss: 0.173239, loss_sup: 0.095306, loss_mps: 0.029000, loss_cps: 0.048932
[12:39:14.103] iteration 4706: total_loss: 0.178708, loss_sup: 0.114922, loss_mps: 0.023949, loss_cps: 0.039837
[12:39:14.249] iteration 4707: total_loss: 0.371615, loss_sup: 0.286969, loss_mps: 0.029082, loss_cps: 0.055564
[12:39:14.397] iteration 4708: total_loss: 0.196139, loss_sup: 0.116187, loss_mps: 0.029319, loss_cps: 0.050633
[12:39:14.543] iteration 4709: total_loss: 0.482312, loss_sup: 0.367127, loss_mps: 0.038339, loss_cps: 0.076846
[12:39:14.689] iteration 4710: total_loss: 0.191944, loss_sup: 0.086302, loss_mps: 0.035263, loss_cps: 0.070379
[12:39:14.834] iteration 4711: total_loss: 0.200143, loss_sup: 0.086656, loss_mps: 0.037801, loss_cps: 0.075685
[12:39:14.982] iteration 4712: total_loss: 0.188262, loss_sup: 0.110637, loss_mps: 0.027115, loss_cps: 0.050510
[12:39:15.128] iteration 4713: total_loss: 0.167930, loss_sup: 0.114847, loss_mps: 0.019948, loss_cps: 0.033136
[12:39:15.273] iteration 4714: total_loss: 0.225688, loss_sup: 0.146699, loss_mps: 0.028082, loss_cps: 0.050906
[12:39:15.420] iteration 4715: total_loss: 0.207422, loss_sup: 0.140418, loss_mps: 0.023234, loss_cps: 0.043771
[12:39:15.566] iteration 4716: total_loss: 0.236421, loss_sup: 0.160157, loss_mps: 0.027007, loss_cps: 0.049257
[12:39:15.711] iteration 4717: total_loss: 0.239184, loss_sup: 0.143056, loss_mps: 0.032788, loss_cps: 0.063340
[12:39:15.859] iteration 4718: total_loss: 0.166057, loss_sup: 0.080659, loss_mps: 0.029512, loss_cps: 0.055886
[12:39:16.006] iteration 4719: total_loss: 0.377550, loss_sup: 0.202365, loss_mps: 0.055546, loss_cps: 0.119639
[12:39:16.153] iteration 4720: total_loss: 0.246622, loss_sup: 0.128842, loss_mps: 0.039680, loss_cps: 0.078101
[12:39:16.298] iteration 4721: total_loss: 0.226920, loss_sup: 0.140838, loss_mps: 0.030005, loss_cps: 0.056078
[12:39:16.444] iteration 4722: total_loss: 0.153872, loss_sup: 0.064929, loss_mps: 0.030985, loss_cps: 0.057958
[12:39:16.589] iteration 4723: total_loss: 0.377621, loss_sup: 0.267190, loss_mps: 0.036297, loss_cps: 0.074134
[12:39:16.735] iteration 4724: total_loss: 0.301358, loss_sup: 0.169226, loss_mps: 0.042672, loss_cps: 0.089460
[12:39:16.882] iteration 4725: total_loss: 0.123571, loss_sup: 0.051977, loss_mps: 0.026163, loss_cps: 0.045431
[12:39:17.029] iteration 4726: total_loss: 0.342460, loss_sup: 0.243872, loss_mps: 0.033797, loss_cps: 0.064791
[12:39:17.174] iteration 4727: total_loss: 0.168285, loss_sup: 0.083981, loss_mps: 0.030350, loss_cps: 0.053954
[12:39:17.320] iteration 4728: total_loss: 0.237578, loss_sup: 0.163101, loss_mps: 0.029182, loss_cps: 0.045295
[12:39:17.467] iteration 4729: total_loss: 0.239408, loss_sup: 0.166943, loss_mps: 0.026712, loss_cps: 0.045753
[12:39:17.613] iteration 4730: total_loss: 0.539315, loss_sup: 0.468227, loss_mps: 0.028060, loss_cps: 0.043028
[12:39:17.759] iteration 4731: total_loss: 0.175163, loss_sup: 0.101868, loss_mps: 0.028423, loss_cps: 0.044872
[12:39:17.905] iteration 4732: total_loss: 0.187821, loss_sup: 0.127241, loss_mps: 0.023879, loss_cps: 0.036701
[12:39:18.050] iteration 4733: total_loss: 0.255009, loss_sup: 0.143389, loss_mps: 0.040134, loss_cps: 0.071486
[12:39:18.196] iteration 4734: total_loss: 0.236888, loss_sup: 0.140967, loss_mps: 0.036214, loss_cps: 0.059707
[12:39:18.341] iteration 4735: total_loss: 0.174161, loss_sup: 0.086625, loss_mps: 0.031829, loss_cps: 0.055706
[12:39:18.487] iteration 4736: total_loss: 0.518047, loss_sup: 0.383236, loss_mps: 0.047029, loss_cps: 0.087782
[12:39:18.633] iteration 4737: total_loss: 0.258754, loss_sup: 0.154924, loss_mps: 0.037977, loss_cps: 0.065853
[12:39:18.779] iteration 4738: total_loss: 0.262779, loss_sup: 0.137376, loss_mps: 0.044239, loss_cps: 0.081164
[12:39:18.924] iteration 4739: total_loss: 0.295675, loss_sup: 0.199905, loss_mps: 0.034556, loss_cps: 0.061215
[12:39:19.069] iteration 4740: total_loss: 0.304570, loss_sup: 0.182748, loss_mps: 0.042053, loss_cps: 0.079769
[12:39:19.215] iteration 4741: total_loss: 0.143529, loss_sup: 0.043186, loss_mps: 0.035392, loss_cps: 0.064950
[12:39:19.360] iteration 4742: total_loss: 0.173736, loss_sup: 0.083685, loss_mps: 0.032779, loss_cps: 0.057273
[12:39:19.505] iteration 4743: total_loss: 0.170954, loss_sup: 0.043325, loss_mps: 0.043041, loss_cps: 0.084588
[12:39:19.652] iteration 4744: total_loss: 0.207596, loss_sup: 0.134436, loss_mps: 0.027016, loss_cps: 0.046145
[12:39:19.800] iteration 4745: total_loss: 0.208842, loss_sup: 0.156859, loss_mps: 0.020394, loss_cps: 0.031589
[12:39:19.945] iteration 4746: total_loss: 0.141450, loss_sup: 0.069438, loss_mps: 0.026583, loss_cps: 0.045428
[12:39:20.092] iteration 4747: total_loss: 0.308077, loss_sup: 0.214966, loss_mps: 0.034094, loss_cps: 0.059017
[12:39:20.238] iteration 4748: total_loss: 0.149682, loss_sup: 0.077324, loss_mps: 0.026772, loss_cps: 0.045586
[12:39:20.388] iteration 4749: total_loss: 0.227454, loss_sup: 0.156754, loss_mps: 0.025338, loss_cps: 0.045362
[12:39:20.534] iteration 4750: total_loss: 0.128382, loss_sup: 0.065827, loss_mps: 0.022996, loss_cps: 0.039558
[12:39:20.680] iteration 4751: total_loss: 0.260002, loss_sup: 0.172307, loss_mps: 0.030653, loss_cps: 0.057042
[12:39:20.825] iteration 4752: total_loss: 0.460911, loss_sup: 0.375178, loss_mps: 0.031287, loss_cps: 0.054446
[12:39:20.971] iteration 4753: total_loss: 0.321228, loss_sup: 0.212594, loss_mps: 0.035143, loss_cps: 0.073491
[12:39:21.116] iteration 4754: total_loss: 0.408868, loss_sup: 0.287969, loss_mps: 0.039151, loss_cps: 0.081749
[12:39:21.264] iteration 4755: total_loss: 0.174694, loss_sup: 0.075425, loss_mps: 0.034352, loss_cps: 0.064917
[12:39:21.409] iteration 4756: total_loss: 0.217033, loss_sup: 0.134232, loss_mps: 0.029025, loss_cps: 0.053776
[12:39:21.555] iteration 4757: total_loss: 0.238179, loss_sup: 0.171562, loss_mps: 0.024086, loss_cps: 0.042531
[12:39:21.701] iteration 4758: total_loss: 0.337010, loss_sup: 0.244993, loss_mps: 0.031857, loss_cps: 0.060161
[12:39:21.846] iteration 4759: total_loss: 0.281373, loss_sup: 0.184429, loss_mps: 0.033126, loss_cps: 0.063818
[12:39:21.992] iteration 4760: total_loss: 0.142217, loss_sup: 0.060737, loss_mps: 0.029241, loss_cps: 0.052239
[12:39:22.137] iteration 4761: total_loss: 0.446343, loss_sup: 0.342158, loss_mps: 0.035692, loss_cps: 0.068493
[12:39:22.283] iteration 4762: total_loss: 0.187594, loss_sup: 0.079901, loss_mps: 0.035928, loss_cps: 0.071765
[12:39:22.428] iteration 4763: total_loss: 0.143856, loss_sup: 0.062540, loss_mps: 0.029580, loss_cps: 0.051735
[12:39:22.574] iteration 4764: total_loss: 0.241472, loss_sup: 0.177457, loss_mps: 0.025522, loss_cps: 0.038494
[12:39:22.719] iteration 4765: total_loss: 0.422823, loss_sup: 0.283337, loss_mps: 0.046353, loss_cps: 0.093133
[12:39:22.864] iteration 4766: total_loss: 0.305326, loss_sup: 0.219082, loss_mps: 0.031717, loss_cps: 0.054527
[12:39:23.009] iteration 4767: total_loss: 0.310546, loss_sup: 0.206675, loss_mps: 0.035113, loss_cps: 0.068758
[12:39:23.155] iteration 4768: total_loss: 0.208976, loss_sup: 0.133517, loss_mps: 0.027067, loss_cps: 0.048392
[12:39:23.301] iteration 4769: total_loss: 0.294427, loss_sup: 0.205270, loss_mps: 0.032296, loss_cps: 0.056861
[12:39:23.448] iteration 4770: total_loss: 0.246075, loss_sup: 0.153471, loss_mps: 0.033452, loss_cps: 0.059152
[12:39:23.595] iteration 4771: total_loss: 0.290280, loss_sup: 0.210322, loss_mps: 0.028556, loss_cps: 0.051402
[12:39:23.742] iteration 4772: total_loss: 0.354653, loss_sup: 0.248460, loss_mps: 0.038057, loss_cps: 0.068136
[12:39:23.889] iteration 4773: total_loss: 0.144369, loss_sup: 0.070663, loss_mps: 0.028153, loss_cps: 0.045553
[12:39:24.035] iteration 4774: total_loss: 0.148936, loss_sup: 0.040841, loss_mps: 0.037498, loss_cps: 0.070597
[12:39:24.181] iteration 4775: total_loss: 0.184969, loss_sup: 0.101063, loss_mps: 0.031331, loss_cps: 0.052574
[12:39:24.328] iteration 4776: total_loss: 0.298440, loss_sup: 0.169412, loss_mps: 0.044222, loss_cps: 0.084806
[12:39:24.474] iteration 4777: total_loss: 0.234628, loss_sup: 0.136531, loss_mps: 0.034985, loss_cps: 0.063112
[12:39:24.622] iteration 4778: total_loss: 0.127130, loss_sup: 0.073092, loss_mps: 0.022060, loss_cps: 0.031978
[12:39:24.769] iteration 4779: total_loss: 0.230214, loss_sup: 0.103179, loss_mps: 0.044566, loss_cps: 0.082469
[12:39:24.915] iteration 4780: total_loss: 0.175696, loss_sup: 0.066961, loss_mps: 0.040087, loss_cps: 0.068649
[12:39:25.061] iteration 4781: total_loss: 0.330823, loss_sup: 0.225524, loss_mps: 0.037149, loss_cps: 0.068150
[12:39:25.208] iteration 4782: total_loss: 0.186747, loss_sup: 0.099306, loss_mps: 0.032103, loss_cps: 0.055338
[12:39:25.354] iteration 4783: total_loss: 0.229930, loss_sup: 0.121853, loss_mps: 0.037886, loss_cps: 0.070191
[12:39:25.500] iteration 4784: total_loss: 0.191168, loss_sup: 0.102325, loss_mps: 0.031930, loss_cps: 0.056914
[12:39:25.647] iteration 4785: total_loss: 0.358447, loss_sup: 0.270209, loss_mps: 0.032105, loss_cps: 0.056132
[12:39:25.793] iteration 4786: total_loss: 0.242659, loss_sup: 0.183007, loss_mps: 0.023505, loss_cps: 0.036147
[12:39:25.940] iteration 4787: total_loss: 0.185205, loss_sup: 0.073607, loss_mps: 0.038734, loss_cps: 0.072865
[12:39:26.087] iteration 4788: total_loss: 0.544051, loss_sup: 0.449837, loss_mps: 0.034758, loss_cps: 0.059457
[12:39:26.233] iteration 4789: total_loss: 0.189597, loss_sup: 0.115364, loss_mps: 0.027487, loss_cps: 0.046746
[12:39:26.379] iteration 4790: total_loss: 0.137711, loss_sup: 0.065275, loss_mps: 0.027659, loss_cps: 0.044777
[12:39:26.524] iteration 4791: total_loss: 0.236474, loss_sup: 0.157659, loss_mps: 0.028464, loss_cps: 0.050351
[12:39:26.670] iteration 4792: total_loss: 0.130708, loss_sup: 0.050966, loss_mps: 0.028696, loss_cps: 0.051047
[12:39:26.816] iteration 4793: total_loss: 0.214685, loss_sup: 0.150397, loss_mps: 0.025113, loss_cps: 0.039175
[12:39:26.962] iteration 4794: total_loss: 0.233658, loss_sup: 0.141290, loss_mps: 0.032243, loss_cps: 0.060124
[12:39:27.108] iteration 4795: total_loss: 0.256737, loss_sup: 0.158809, loss_mps: 0.034639, loss_cps: 0.063289
[12:39:27.255] iteration 4796: total_loss: 0.151388, loss_sup: 0.092255, loss_mps: 0.023248, loss_cps: 0.035885
[12:39:27.401] iteration 4797: total_loss: 0.251228, loss_sup: 0.153353, loss_mps: 0.034982, loss_cps: 0.062894
[12:39:27.547] iteration 4798: total_loss: 0.296545, loss_sup: 0.213378, loss_mps: 0.029682, loss_cps: 0.053485
[12:39:27.692] iteration 4799: total_loss: 0.162820, loss_sup: 0.090691, loss_mps: 0.026921, loss_cps: 0.045208
[12:39:27.838] iteration 4800: total_loss: 0.198422, loss_sup: 0.134032, loss_mps: 0.024977, loss_cps: 0.039413
[12:39:27.838] Evaluation Started ==>
[12:39:39.204] ==> valid iteration 4800: unet metrics: {'dc': 0.6070861178445341, 'jc': 0.4743920554567585, 'pre': 0.6405228602379796, 'hd': 6.684949774082808}, ynet metrics: {'dc': 0.5405802549000943, 'jc': 0.4195031889114324, 'pre': 0.634516846484894, 'hd': 6.732231965952924}.
[12:39:39.364] ==> New best valid dice for ynet: 0.540580, at iteration 4800
[12:39:39.366] Evaluation Finished!⏹️
[12:39:39.520] iteration 4801: total_loss: 0.167617, loss_sup: 0.086707, loss_mps: 0.030463, loss_cps: 0.050447
[12:39:39.667] iteration 4802: total_loss: 0.366709, loss_sup: 0.264710, loss_mps: 0.036483, loss_cps: 0.065516
[12:39:39.814] iteration 4803: total_loss: 0.382901, loss_sup: 0.284676, loss_mps: 0.035232, loss_cps: 0.062992
[12:39:39.959] iteration 4804: total_loss: 0.225935, loss_sup: 0.128321, loss_mps: 0.034252, loss_cps: 0.063363
[12:39:40.106] iteration 4805: total_loss: 0.217183, loss_sup: 0.127345, loss_mps: 0.031604, loss_cps: 0.058234
[12:39:40.252] iteration 4806: total_loss: 0.341797, loss_sup: 0.253156, loss_mps: 0.030946, loss_cps: 0.057696
[12:39:40.397] iteration 4807: total_loss: 0.164176, loss_sup: 0.076049, loss_mps: 0.031948, loss_cps: 0.056180
[12:39:40.543] iteration 4808: total_loss: 0.195156, loss_sup: 0.121137, loss_mps: 0.027372, loss_cps: 0.046646
[12:39:40.688] iteration 4809: total_loss: 0.365903, loss_sup: 0.298581, loss_mps: 0.026181, loss_cps: 0.041141
[12:39:40.834] iteration 4810: total_loss: 0.161131, loss_sup: 0.101025, loss_mps: 0.023497, loss_cps: 0.036610
[12:39:40.979] iteration 4811: total_loss: 0.339896, loss_sup: 0.268275, loss_mps: 0.026936, loss_cps: 0.044685
[12:39:41.132] iteration 4812: total_loss: 0.165822, loss_sup: 0.092338, loss_mps: 0.029325, loss_cps: 0.044159
[12:39:41.278] iteration 4813: total_loss: 0.178097, loss_sup: 0.092199, loss_mps: 0.031829, loss_cps: 0.054069
[12:39:41.424] iteration 4814: total_loss: 0.144855, loss_sup: 0.076150, loss_mps: 0.025167, loss_cps: 0.043539
[12:39:41.570] iteration 4815: total_loss: 0.397961, loss_sup: 0.291261, loss_mps: 0.037655, loss_cps: 0.069045
[12:39:41.718] iteration 4816: total_loss: 0.258563, loss_sup: 0.157997, loss_mps: 0.035810, loss_cps: 0.064756
[12:39:41.867] iteration 4817: total_loss: 0.179590, loss_sup: 0.069090, loss_mps: 0.038164, loss_cps: 0.072337
[12:39:42.013] iteration 4818: total_loss: 0.176466, loss_sup: 0.099858, loss_mps: 0.028508, loss_cps: 0.048099
[12:39:42.160] iteration 4819: total_loss: 0.209964, loss_sup: 0.093171, loss_mps: 0.040188, loss_cps: 0.076605
[12:39:42.307] iteration 4820: total_loss: 0.421249, loss_sup: 0.329801, loss_mps: 0.031631, loss_cps: 0.059817
[12:39:42.453] iteration 4821: total_loss: 0.138526, loss_sup: 0.069223, loss_mps: 0.025439, loss_cps: 0.043865
[12:39:42.601] iteration 4822: total_loss: 0.142572, loss_sup: 0.060552, loss_mps: 0.029677, loss_cps: 0.052343
[12:39:42.747] iteration 4823: total_loss: 0.381748, loss_sup: 0.282698, loss_mps: 0.034026, loss_cps: 0.065024
[12:39:42.892] iteration 4824: total_loss: 0.409072, loss_sup: 0.329253, loss_mps: 0.029375, loss_cps: 0.050443
[12:39:43.042] iteration 4825: total_loss: 0.210197, loss_sup: 0.121663, loss_mps: 0.031136, loss_cps: 0.057397
[12:39:43.189] iteration 4826: total_loss: 0.241618, loss_sup: 0.160796, loss_mps: 0.028957, loss_cps: 0.051864
[12:39:43.334] iteration 4827: total_loss: 0.307654, loss_sup: 0.224391, loss_mps: 0.031024, loss_cps: 0.052239
[12:39:43.480] iteration 4828: total_loss: 0.164933, loss_sup: 0.095211, loss_mps: 0.026974, loss_cps: 0.042748
[12:39:43.627] iteration 4829: total_loss: 0.232595, loss_sup: 0.108309, loss_mps: 0.042082, loss_cps: 0.082204
[12:39:43.772] iteration 4830: total_loss: 0.216735, loss_sup: 0.123270, loss_mps: 0.033125, loss_cps: 0.060340
[12:39:43.918] iteration 4831: total_loss: 0.284891, loss_sup: 0.142727, loss_mps: 0.046865, loss_cps: 0.095299
[12:39:44.063] iteration 4832: total_loss: 0.232150, loss_sup: 0.161080, loss_mps: 0.027355, loss_cps: 0.043715
[12:39:44.210] iteration 4833: total_loss: 0.417981, loss_sup: 0.273960, loss_mps: 0.048376, loss_cps: 0.095646
[12:39:44.356] iteration 4834: total_loss: 0.328764, loss_sup: 0.244477, loss_mps: 0.030986, loss_cps: 0.053301
[12:39:44.501] iteration 4835: total_loss: 0.150602, loss_sup: 0.063144, loss_mps: 0.032144, loss_cps: 0.055314
[12:39:44.648] iteration 4836: total_loss: 0.303417, loss_sup: 0.178698, loss_mps: 0.042055, loss_cps: 0.082665
[12:39:44.794] iteration 4837: total_loss: 0.326205, loss_sup: 0.214705, loss_mps: 0.038823, loss_cps: 0.072677
[12:39:44.939] iteration 4838: total_loss: 0.237403, loss_sup: 0.133401, loss_mps: 0.037417, loss_cps: 0.066586
[12:39:45.088] iteration 4839: total_loss: 0.154712, loss_sup: 0.068390, loss_mps: 0.032187, loss_cps: 0.054135
[12:39:45.235] iteration 4840: total_loss: 0.158327, loss_sup: 0.100049, loss_mps: 0.023270, loss_cps: 0.035009
[12:39:45.381] iteration 4841: total_loss: 0.137948, loss_sup: 0.060819, loss_mps: 0.029325, loss_cps: 0.047804
[12:39:45.527] iteration 4842: total_loss: 0.141945, loss_sup: 0.059539, loss_mps: 0.030465, loss_cps: 0.051941
[12:39:45.673] iteration 4843: total_loss: 0.258347, loss_sup: 0.147445, loss_mps: 0.037863, loss_cps: 0.073040
[12:39:45.819] iteration 4844: total_loss: 0.152041, loss_sup: 0.049284, loss_mps: 0.035507, loss_cps: 0.067250
[12:39:45.966] iteration 4845: total_loss: 0.270839, loss_sup: 0.194738, loss_mps: 0.028847, loss_cps: 0.047254
[12:39:46.112] iteration 4846: total_loss: 0.254027, loss_sup: 0.152027, loss_mps: 0.035157, loss_cps: 0.066843
[12:39:46.258] iteration 4847: total_loss: 0.134911, loss_sup: 0.065264, loss_mps: 0.026454, loss_cps: 0.043193
[12:39:46.403] iteration 4848: total_loss: 0.378002, loss_sup: 0.270809, loss_mps: 0.037134, loss_cps: 0.070058
[12:39:46.550] iteration 4849: total_loss: 0.138122, loss_sup: 0.063865, loss_mps: 0.027396, loss_cps: 0.046861
[12:39:46.696] iteration 4850: total_loss: 0.233642, loss_sup: 0.154127, loss_mps: 0.029842, loss_cps: 0.049673
[12:39:46.843] iteration 4851: total_loss: 0.192703, loss_sup: 0.110539, loss_mps: 0.030467, loss_cps: 0.051698
[12:39:46.989] iteration 4852: total_loss: 0.230264, loss_sup: 0.160413, loss_mps: 0.026568, loss_cps: 0.043283
[12:39:47.136] iteration 4853: total_loss: 0.121690, loss_sup: 0.035568, loss_mps: 0.029232, loss_cps: 0.056890
[12:39:47.282] iteration 4854: total_loss: 0.155825, loss_sup: 0.086666, loss_mps: 0.025496, loss_cps: 0.043663
[12:39:47.434] iteration 4855: total_loss: 0.183365, loss_sup: 0.109264, loss_mps: 0.027051, loss_cps: 0.047050
[12:39:47.580] iteration 4856: total_loss: 0.200458, loss_sup: 0.144878, loss_mps: 0.021094, loss_cps: 0.034486
[12:39:47.726] iteration 4857: total_loss: 0.274891, loss_sup: 0.190512, loss_mps: 0.030319, loss_cps: 0.054060
[12:39:47.872] iteration 4858: total_loss: 0.350992, loss_sup: 0.228768, loss_mps: 0.040743, loss_cps: 0.081481
[12:39:48.020] iteration 4859: total_loss: 0.129851, loss_sup: 0.056639, loss_mps: 0.026295, loss_cps: 0.046917
[12:39:48.168] iteration 4860: total_loss: 0.360960, loss_sup: 0.242086, loss_mps: 0.039626, loss_cps: 0.079248
[12:39:48.314] iteration 4861: total_loss: 0.424869, loss_sup: 0.364085, loss_mps: 0.023364, loss_cps: 0.037419
[12:39:48.459] iteration 4862: total_loss: 0.162338, loss_sup: 0.095263, loss_mps: 0.025123, loss_cps: 0.041952
[12:39:48.607] iteration 4863: total_loss: 0.164185, loss_sup: 0.086325, loss_mps: 0.027512, loss_cps: 0.050348
[12:39:48.755] iteration 4864: total_loss: 0.150020, loss_sup: 0.062969, loss_mps: 0.029868, loss_cps: 0.057183
[12:39:48.901] iteration 4865: total_loss: 0.288917, loss_sup: 0.201039, loss_mps: 0.030086, loss_cps: 0.057791
[12:39:49.047] iteration 4866: total_loss: 0.244145, loss_sup: 0.178057, loss_mps: 0.023948, loss_cps: 0.042140
[12:39:49.193] iteration 4867: total_loss: 0.095655, loss_sup: 0.037559, loss_mps: 0.021830, loss_cps: 0.036266
[12:39:49.338] iteration 4868: total_loss: 0.254720, loss_sup: 0.137779, loss_mps: 0.038236, loss_cps: 0.078705
[12:39:49.486] iteration 4869: total_loss: 0.188409, loss_sup: 0.093445, loss_mps: 0.032415, loss_cps: 0.062549
[12:39:49.632] iteration 4870: total_loss: 0.619917, loss_sup: 0.499425, loss_mps: 0.039216, loss_cps: 0.081276
[12:39:49.778] iteration 4871: total_loss: 0.199927, loss_sup: 0.112616, loss_mps: 0.029877, loss_cps: 0.057434
[12:39:49.924] iteration 4872: total_loss: 0.260165, loss_sup: 0.157677, loss_mps: 0.034377, loss_cps: 0.068111
[12:39:50.072] iteration 4873: total_loss: 0.201512, loss_sup: 0.124163, loss_mps: 0.028458, loss_cps: 0.048892
[12:39:50.218] iteration 4874: total_loss: 0.106696, loss_sup: 0.038144, loss_mps: 0.024853, loss_cps: 0.043698
[12:39:50.363] iteration 4875: total_loss: 0.199357, loss_sup: 0.122761, loss_mps: 0.028803, loss_cps: 0.047794
[12:39:50.509] iteration 4876: total_loss: 0.250177, loss_sup: 0.171359, loss_mps: 0.028790, loss_cps: 0.050029
[12:39:50.655] iteration 4877: total_loss: 0.302453, loss_sup: 0.201782, loss_mps: 0.034931, loss_cps: 0.065740
[12:39:50.800] iteration 4878: total_loss: 0.400730, loss_sup: 0.271200, loss_mps: 0.043041, loss_cps: 0.086489
[12:39:50.947] iteration 4879: total_loss: 0.146163, loss_sup: 0.061987, loss_mps: 0.029686, loss_cps: 0.054490
[12:39:51.093] iteration 4880: total_loss: 0.348589, loss_sup: 0.194396, loss_mps: 0.049923, loss_cps: 0.104269
[12:39:51.240] iteration 4881: total_loss: 0.188311, loss_sup: 0.095047, loss_mps: 0.031387, loss_cps: 0.061877
[12:39:51.387] iteration 4882: total_loss: 0.155355, loss_sup: 0.073988, loss_mps: 0.028283, loss_cps: 0.053084
[12:39:51.533] iteration 4883: total_loss: 0.162428, loss_sup: 0.085812, loss_mps: 0.028123, loss_cps: 0.048493
[12:39:51.678] iteration 4884: total_loss: 0.224738, loss_sup: 0.150559, loss_mps: 0.026984, loss_cps: 0.047195
[12:39:51.824] iteration 4885: total_loss: 0.176842, loss_sup: 0.091424, loss_mps: 0.030028, loss_cps: 0.055390
[12:39:51.970] iteration 4886: total_loss: 0.296792, loss_sup: 0.155712, loss_mps: 0.046061, loss_cps: 0.095019
[12:39:52.116] iteration 4887: total_loss: 0.172065, loss_sup: 0.093432, loss_mps: 0.027653, loss_cps: 0.050980
[12:39:52.263] iteration 4888: total_loss: 0.201055, loss_sup: 0.124715, loss_mps: 0.028182, loss_cps: 0.048158
[12:39:52.409] iteration 4889: total_loss: 0.202906, loss_sup: 0.115159, loss_mps: 0.030967, loss_cps: 0.056780
[12:39:52.557] iteration 4890: total_loss: 0.213364, loss_sup: 0.126260, loss_mps: 0.031239, loss_cps: 0.055865
[12:39:52.703] iteration 4891: total_loss: 0.219067, loss_sup: 0.130484, loss_mps: 0.031176, loss_cps: 0.057406
[12:39:52.849] iteration 4892: total_loss: 0.160325, loss_sup: 0.070456, loss_mps: 0.031735, loss_cps: 0.058133
[12:39:52.996] iteration 4893: total_loss: 0.200367, loss_sup: 0.133858, loss_mps: 0.024133, loss_cps: 0.042376
[12:39:53.141] iteration 4894: total_loss: 0.165474, loss_sup: 0.084712, loss_mps: 0.027171, loss_cps: 0.053591
[12:39:53.290] iteration 4895: total_loss: 0.120333, loss_sup: 0.038344, loss_mps: 0.028526, loss_cps: 0.053463
[12:39:53.436] iteration 4896: total_loss: 0.183952, loss_sup: 0.088977, loss_mps: 0.032587, loss_cps: 0.062389
[12:39:53.581] iteration 4897: total_loss: 0.188228, loss_sup: 0.114476, loss_mps: 0.026790, loss_cps: 0.046963
[12:39:53.727] iteration 4898: total_loss: 0.250274, loss_sup: 0.192641, loss_mps: 0.021829, loss_cps: 0.035804
[12:39:53.873] iteration 4899: total_loss: 0.138007, loss_sup: 0.065459, loss_mps: 0.025438, loss_cps: 0.047110
[12:39:54.018] iteration 4900: total_loss: 0.183532, loss_sup: 0.115693, loss_mps: 0.024164, loss_cps: 0.043675
[12:39:54.018] Evaluation Started ==>
[12:40:05.451] ==> valid iteration 4900: unet metrics: {'dc': 0.6262553444045026, 'jc': 0.49489055503974444, 'pre': 0.6559874681776124, 'hd': 6.576992766431822}, ynet metrics: {'dc': 0.5350919638307056, 'jc': 0.41312501612842595, 'pre': 0.6853209576975916, 'hd': 6.393809496444872}.
[12:40:05.498] ==> New best valid dice for unet: 0.626255, at iteration 4900
[12:40:05.499] Evaluation Finished!⏹️
[12:40:05.648] iteration 4901: total_loss: 0.223397, loss_sup: 0.142322, loss_mps: 0.027263, loss_cps: 0.053812
[12:40:05.795] iteration 4902: total_loss: 0.345416, loss_sup: 0.276474, loss_mps: 0.024469, loss_cps: 0.044473
[12:40:05.941] iteration 4903: total_loss: 0.305506, loss_sup: 0.244459, loss_mps: 0.022881, loss_cps: 0.038166
[12:40:06.086] iteration 4904: total_loss: 0.224511, loss_sup: 0.143915, loss_mps: 0.029086, loss_cps: 0.051510
[12:40:06.233] iteration 4905: total_loss: 0.160327, loss_sup: 0.070726, loss_mps: 0.030841, loss_cps: 0.058760
[12:40:06.378] iteration 4906: total_loss: 0.174093, loss_sup: 0.102629, loss_mps: 0.027125, loss_cps: 0.044339
[12:40:06.525] iteration 4907: total_loss: 0.198480, loss_sup: 0.130771, loss_mps: 0.025307, loss_cps: 0.042402
[12:40:06.670] iteration 4908: total_loss: 0.171274, loss_sup: 0.106910, loss_mps: 0.023995, loss_cps: 0.040369
[12:40:06.817] iteration 4909: total_loss: 0.206562, loss_sup: 0.132618, loss_mps: 0.026507, loss_cps: 0.047437
[12:40:06.964] iteration 4910: total_loss: 0.220393, loss_sup: 0.134934, loss_mps: 0.030036, loss_cps: 0.055423
[12:40:07.112] iteration 4911: total_loss: 0.207394, loss_sup: 0.135835, loss_mps: 0.025609, loss_cps: 0.045950
[12:40:07.258] iteration 4912: total_loss: 0.296444, loss_sup: 0.186424, loss_mps: 0.038064, loss_cps: 0.071955
[12:40:07.404] iteration 4913: total_loss: 0.181507, loss_sup: 0.076737, loss_mps: 0.035826, loss_cps: 0.068944
[12:40:07.549] iteration 4914: total_loss: 0.161754, loss_sup: 0.098109, loss_mps: 0.024488, loss_cps: 0.039158
[12:40:07.694] iteration 4915: total_loss: 0.223819, loss_sup: 0.138583, loss_mps: 0.030150, loss_cps: 0.055086
[12:40:07.843] iteration 4916: total_loss: 0.123195, loss_sup: 0.050668, loss_mps: 0.026452, loss_cps: 0.046075
[12:40:07.988] iteration 4917: total_loss: 0.186441, loss_sup: 0.110410, loss_mps: 0.026690, loss_cps: 0.049341
[12:40:08.133] iteration 4918: total_loss: 0.168025, loss_sup: 0.023978, loss_mps: 0.046585, loss_cps: 0.097462
[12:40:08.279] iteration 4919: total_loss: 0.228157, loss_sup: 0.130500, loss_mps: 0.032768, loss_cps: 0.064889
[12:40:08.426] iteration 4920: total_loss: 0.128034, loss_sup: 0.057701, loss_mps: 0.026131, loss_cps: 0.044202
[12:40:08.571] iteration 4921: total_loss: 0.096676, loss_sup: 0.041597, loss_mps: 0.021530, loss_cps: 0.033549
[12:40:08.716] iteration 4922: total_loss: 0.133366, loss_sup: 0.030485, loss_mps: 0.035918, loss_cps: 0.066963
[12:40:08.862] iteration 4923: total_loss: 0.215882, loss_sup: 0.095889, loss_mps: 0.039972, loss_cps: 0.080021
[12:40:09.010] iteration 4924: total_loss: 0.247272, loss_sup: 0.148484, loss_mps: 0.034041, loss_cps: 0.064747
[12:40:09.155] iteration 4925: total_loss: 0.158771, loss_sup: 0.078527, loss_mps: 0.028638, loss_cps: 0.051606
[12:40:09.300] iteration 4926: total_loss: 0.218042, loss_sup: 0.139572, loss_mps: 0.026853, loss_cps: 0.051617
[12:40:09.446] iteration 4927: total_loss: 0.541046, loss_sup: 0.449614, loss_mps: 0.032321, loss_cps: 0.059111
[12:40:09.592] iteration 4928: total_loss: 0.166512, loss_sup: 0.092396, loss_mps: 0.025937, loss_cps: 0.048179
[12:40:09.738] iteration 4929: total_loss: 0.375489, loss_sup: 0.287562, loss_mps: 0.031147, loss_cps: 0.056780
[12:40:09.885] iteration 4930: total_loss: 0.255790, loss_sup: 0.121768, loss_mps: 0.043968, loss_cps: 0.090054
[12:40:10.031] iteration 4931: total_loss: 0.187516, loss_sup: 0.087105, loss_mps: 0.034340, loss_cps: 0.066071
[12:40:10.178] iteration 4932: total_loss: 0.442869, loss_sup: 0.304918, loss_mps: 0.045017, loss_cps: 0.092935
[12:40:10.323] iteration 4933: total_loss: 0.184037, loss_sup: 0.113219, loss_mps: 0.025843, loss_cps: 0.044975
[12:40:10.469] iteration 4934: total_loss: 0.260297, loss_sup: 0.204215, loss_mps: 0.022115, loss_cps: 0.033968
[12:40:10.614] iteration 4935: total_loss: 0.218984, loss_sup: 0.101478, loss_mps: 0.040118, loss_cps: 0.077388
[12:40:10.760] iteration 4936: total_loss: 0.207352, loss_sup: 0.102097, loss_mps: 0.037480, loss_cps: 0.067775
[12:40:10.915] iteration 4937: total_loss: 0.181741, loss_sup: 0.097045, loss_mps: 0.030029, loss_cps: 0.054667
[12:40:11.061] iteration 4938: total_loss: 0.445983, loss_sup: 0.342138, loss_mps: 0.037503, loss_cps: 0.066343
[12:40:11.210] iteration 4939: total_loss: 0.249405, loss_sup: 0.169742, loss_mps: 0.029720, loss_cps: 0.049943
[12:40:11.357] iteration 4940: total_loss: 0.212775, loss_sup: 0.133515, loss_mps: 0.029627, loss_cps: 0.049633
[12:40:11.503] iteration 4941: total_loss: 0.232323, loss_sup: 0.152726, loss_mps: 0.029345, loss_cps: 0.050252
[12:40:11.649] iteration 4942: total_loss: 0.158695, loss_sup: 0.046960, loss_mps: 0.038103, loss_cps: 0.073632
[12:40:11.797] iteration 4943: total_loss: 0.254461, loss_sup: 0.181745, loss_mps: 0.027094, loss_cps: 0.045622
[12:40:11.944] iteration 4944: total_loss: 0.303184, loss_sup: 0.219257, loss_mps: 0.030308, loss_cps: 0.053619
[12:40:12.090] iteration 4945: total_loss: 0.103771, loss_sup: 0.044374, loss_mps: 0.023670, loss_cps: 0.035728
[12:40:12.236] iteration 4946: total_loss: 0.137754, loss_sup: 0.076034, loss_mps: 0.023926, loss_cps: 0.037794
[12:40:12.382] iteration 4947: total_loss: 0.261834, loss_sup: 0.170553, loss_mps: 0.032052, loss_cps: 0.059229
[12:40:12.528] iteration 4948: total_loss: 0.470627, loss_sup: 0.385357, loss_mps: 0.030841, loss_cps: 0.054429
[12:40:12.674] iteration 4949: total_loss: 0.416408, loss_sup: 0.313220, loss_mps: 0.036897, loss_cps: 0.066291
[12:40:12.820] iteration 4950: total_loss: 0.252723, loss_sup: 0.190008, loss_mps: 0.023946, loss_cps: 0.038770
[12:40:12.965] iteration 4951: total_loss: 0.172948, loss_sup: 0.059372, loss_mps: 0.039134, loss_cps: 0.074442
[12:40:13.111] iteration 4952: total_loss: 0.411828, loss_sup: 0.331547, loss_mps: 0.028696, loss_cps: 0.051585
[12:40:13.257] iteration 4953: total_loss: 0.185921, loss_sup: 0.080036, loss_mps: 0.037758, loss_cps: 0.068127
[12:40:13.404] iteration 4954: total_loss: 0.362659, loss_sup: 0.269524, loss_mps: 0.033266, loss_cps: 0.059869
[12:40:13.549] iteration 4955: total_loss: 0.178555, loss_sup: 0.092011, loss_mps: 0.032744, loss_cps: 0.053800
[12:40:13.695] iteration 4956: total_loss: 0.121257, loss_sup: 0.069926, loss_mps: 0.020452, loss_cps: 0.030879
[12:40:13.840] iteration 4957: total_loss: 0.200722, loss_sup: 0.120749, loss_mps: 0.029970, loss_cps: 0.050002
[12:40:13.986] iteration 4958: total_loss: 0.204618, loss_sup: 0.125309, loss_mps: 0.029468, loss_cps: 0.049841
[12:40:14.132] iteration 4959: total_loss: 0.213182, loss_sup: 0.115942, loss_mps: 0.034608, loss_cps: 0.062633
[12:40:14.280] iteration 4960: total_loss: 0.506387, loss_sup: 0.427323, loss_mps: 0.030129, loss_cps: 0.048935
[12:40:14.426] iteration 4961: total_loss: 0.242468, loss_sup: 0.124247, loss_mps: 0.040234, loss_cps: 0.077987
[12:40:14.571] iteration 4962: total_loss: 0.179664, loss_sup: 0.098218, loss_mps: 0.030532, loss_cps: 0.050914
[12:40:14.720] iteration 4963: total_loss: 0.296857, loss_sup: 0.199752, loss_mps: 0.034315, loss_cps: 0.062790
[12:40:14.866] iteration 4964: total_loss: 0.216136, loss_sup: 0.112173, loss_mps: 0.036860, loss_cps: 0.067102
[12:40:15.012] iteration 4965: total_loss: 0.143840, loss_sup: 0.065383, loss_mps: 0.029561, loss_cps: 0.048895
[12:40:15.159] iteration 4966: total_loss: 0.224584, loss_sup: 0.149981, loss_mps: 0.028052, loss_cps: 0.046551
[12:40:15.305] iteration 4967: total_loss: 0.278210, loss_sup: 0.201598, loss_mps: 0.027669, loss_cps: 0.048943
[12:40:15.454] iteration 4968: total_loss: 0.292483, loss_sup: 0.161274, loss_mps: 0.043777, loss_cps: 0.087431
[12:40:15.600] iteration 4969: total_loss: 0.249950, loss_sup: 0.131425, loss_mps: 0.039383, loss_cps: 0.079142
[12:40:15.747] iteration 4970: total_loss: 0.419842, loss_sup: 0.302419, loss_mps: 0.039812, loss_cps: 0.077611
[12:40:15.893] iteration 4971: total_loss: 0.429214, loss_sup: 0.321850, loss_mps: 0.036718, loss_cps: 0.070645
[12:40:16.042] iteration 4972: total_loss: 0.236471, loss_sup: 0.151391, loss_mps: 0.030859, loss_cps: 0.054221
[12:40:16.187] iteration 4973: total_loss: 0.254719, loss_sup: 0.156355, loss_mps: 0.035377, loss_cps: 0.062987
[12:40:16.333] iteration 4974: total_loss: 0.227184, loss_sup: 0.141752, loss_mps: 0.031040, loss_cps: 0.054392
[12:40:16.479] iteration 4975: total_loss: 0.189161, loss_sup: 0.089879, loss_mps: 0.035903, loss_cps: 0.063379
[12:40:16.626] iteration 4976: total_loss: 0.162024, loss_sup: 0.085254, loss_mps: 0.029807, loss_cps: 0.046963
[12:40:16.771] iteration 4977: total_loss: 0.160570, loss_sup: 0.063133, loss_mps: 0.035284, loss_cps: 0.062152
[12:40:16.918] iteration 4978: total_loss: 0.305381, loss_sup: 0.228713, loss_mps: 0.028811, loss_cps: 0.047856
[12:40:17.067] iteration 4979: total_loss: 0.179159, loss_sup: 0.057413, loss_mps: 0.043025, loss_cps: 0.078721
[12:40:17.213] iteration 4980: total_loss: 0.220580, loss_sup: 0.149916, loss_mps: 0.027401, loss_cps: 0.043263
[12:40:17.360] iteration 4981: total_loss: 0.193194, loss_sup: 0.113741, loss_mps: 0.030987, loss_cps: 0.048466
[12:40:17.506] iteration 4982: total_loss: 0.296520, loss_sup: 0.174519, loss_mps: 0.044097, loss_cps: 0.077903
[12:40:17.652] iteration 4983: total_loss: 0.157174, loss_sup: 0.079634, loss_mps: 0.031290, loss_cps: 0.046250
[12:40:17.799] iteration 4984: total_loss: 0.227131, loss_sup: 0.136977, loss_mps: 0.033753, loss_cps: 0.056401
[12:40:17.945] iteration 4985: total_loss: 0.183808, loss_sup: 0.094865, loss_mps: 0.033898, loss_cps: 0.055046
[12:40:18.091] iteration 4986: total_loss: 0.264380, loss_sup: 0.156743, loss_mps: 0.038087, loss_cps: 0.069550
[12:40:18.237] iteration 4987: total_loss: 0.194217, loss_sup: 0.100534, loss_mps: 0.034804, loss_cps: 0.058879
[12:40:18.383] iteration 4988: total_loss: 0.119212, loss_sup: 0.027544, loss_mps: 0.032842, loss_cps: 0.058826
[12:40:18.530] iteration 4989: total_loss: 0.499711, loss_sup: 0.378822, loss_mps: 0.041426, loss_cps: 0.079463
[12:40:18.676] iteration 4990: total_loss: 0.292605, loss_sup: 0.172586, loss_mps: 0.040854, loss_cps: 0.079165
[12:40:18.822] iteration 4991: total_loss: 0.183534, loss_sup: 0.087291, loss_mps: 0.034196, loss_cps: 0.062047
[12:40:18.967] iteration 4992: total_loss: 0.150454, loss_sup: 0.089270, loss_mps: 0.023235, loss_cps: 0.037948
[12:40:19.113] iteration 4993: total_loss: 0.146993, loss_sup: 0.078612, loss_mps: 0.025659, loss_cps: 0.042722
[12:40:19.260] iteration 4994: total_loss: 0.180042, loss_sup: 0.079817, loss_mps: 0.034516, loss_cps: 0.065709
[12:40:19.406] iteration 4995: total_loss: 0.130464, loss_sup: 0.050439, loss_mps: 0.029353, loss_cps: 0.050671
[12:40:19.556] iteration 4996: total_loss: 0.341577, loss_sup: 0.215486, loss_mps: 0.041307, loss_cps: 0.084783
[12:40:19.703] iteration 4997: total_loss: 0.123723, loss_sup: 0.036650, loss_mps: 0.030728, loss_cps: 0.056346
[12:40:19.850] iteration 4998: total_loss: 0.210364, loss_sup: 0.109365, loss_mps: 0.033935, loss_cps: 0.067064
[12:40:19.997] iteration 4999: total_loss: 0.607475, loss_sup: 0.465112, loss_mps: 0.046452, loss_cps: 0.095911
[12:40:20.144] iteration 5000: total_loss: 0.348832, loss_sup: 0.244771, loss_mps: 0.034847, loss_cps: 0.069214
[12:40:20.144] Evaluation Started ==>
[12:40:31.497] ==> valid iteration 5000: unet metrics: {'dc': 0.6368512386585284, 'jc': 0.498559469177693, 'pre': 0.6577603797376675, 'hd': 6.598722398016255}, ynet metrics: {'dc': 0.5545234712417091, 'jc': 0.4198796243430933, 'pre': 0.6284653868983031, 'hd': 6.81035319384086}.
[12:40:31.559] ==> New best valid dice for unet: 0.636851, at iteration 5000
[12:40:31.717] ==> New best valid dice for ynet: 0.554523, at iteration 5000
[12:40:31.719] Evaluation Finished!⏹️
[12:40:31.870] iteration 5001: total_loss: 0.467957, loss_sup: 0.328168, loss_mps: 0.045735, loss_cps: 0.094054
[12:40:32.018] iteration 5002: total_loss: 0.266987, loss_sup: 0.164192, loss_mps: 0.035373, loss_cps: 0.067422
[12:40:32.164] iteration 5003: total_loss: 0.257219, loss_sup: 0.194215, loss_mps: 0.023493, loss_cps: 0.039511
[12:40:32.309] iteration 5004: total_loss: 0.282614, loss_sup: 0.125799, loss_mps: 0.050464, loss_cps: 0.106351
[12:40:32.455] iteration 5005: total_loss: 0.379798, loss_sup: 0.292589, loss_mps: 0.030185, loss_cps: 0.057024
[12:40:32.600] iteration 5006: total_loss: 0.131538, loss_sup: 0.055720, loss_mps: 0.028497, loss_cps: 0.047321
[12:40:32.745] iteration 5007: total_loss: 0.227440, loss_sup: 0.114414, loss_mps: 0.038821, loss_cps: 0.074204
[12:40:32.891] iteration 5008: total_loss: 0.719314, loss_sup: 0.630599, loss_mps: 0.031427, loss_cps: 0.057288
[12:40:33.038] iteration 5009: total_loss: 0.194853, loss_sup: 0.124523, loss_mps: 0.027060, loss_cps: 0.043270
[12:40:33.183] iteration 5010: total_loss: 0.226293, loss_sup: 0.108644, loss_mps: 0.040971, loss_cps: 0.076678
[12:40:33.329] iteration 5011: total_loss: 0.166478, loss_sup: 0.096362, loss_mps: 0.026871, loss_cps: 0.043245
[12:40:33.474] iteration 5012: total_loss: 0.282118, loss_sup: 0.153485, loss_mps: 0.044192, loss_cps: 0.084440
[12:40:33.619] iteration 5013: total_loss: 0.467785, loss_sup: 0.384719, loss_mps: 0.031067, loss_cps: 0.052000
[12:40:33.765] iteration 5014: total_loss: 0.334424, loss_sup: 0.164904, loss_mps: 0.056386, loss_cps: 0.113133
[12:40:33.910] iteration 5015: total_loss: 0.144380, loss_sup: 0.055898, loss_mps: 0.033897, loss_cps: 0.054585
[12:40:33.975] iteration 5016: total_loss: 0.193081, loss_sup: 0.062873, loss_mps: 0.045046, loss_cps: 0.085162
[12:40:35.162] iteration 5017: total_loss: 0.367808, loss_sup: 0.232825, loss_mps: 0.046161, loss_cps: 0.088823
[12:40:35.311] iteration 5018: total_loss: 0.308367, loss_sup: 0.162789, loss_mps: 0.050367, loss_cps: 0.095211
[12:40:35.457] iteration 5019: total_loss: 0.241657, loss_sup: 0.149160, loss_mps: 0.034883, loss_cps: 0.057614
[12:40:35.605] iteration 5020: total_loss: 0.363003, loss_sup: 0.217121, loss_mps: 0.050440, loss_cps: 0.095441
[12:40:35.752] iteration 5021: total_loss: 0.285855, loss_sup: 0.146266, loss_mps: 0.048078, loss_cps: 0.091510
[12:40:35.898] iteration 5022: total_loss: 0.389105, loss_sup: 0.251619, loss_mps: 0.047725, loss_cps: 0.089761
[12:40:36.044] iteration 5023: total_loss: 0.521844, loss_sup: 0.336398, loss_mps: 0.060889, loss_cps: 0.124557
[12:40:36.191] iteration 5024: total_loss: 0.366997, loss_sup: 0.251036, loss_mps: 0.041674, loss_cps: 0.074287
[12:40:36.338] iteration 5025: total_loss: 0.235997, loss_sup: 0.103284, loss_mps: 0.045705, loss_cps: 0.087008
[12:40:36.486] iteration 5026: total_loss: 0.256157, loss_sup: 0.158620, loss_mps: 0.036761, loss_cps: 0.060776
[12:40:36.632] iteration 5027: total_loss: 0.292004, loss_sup: 0.156379, loss_mps: 0.047554, loss_cps: 0.088071
[12:40:36.779] iteration 5028: total_loss: 0.275493, loss_sup: 0.146289, loss_mps: 0.043611, loss_cps: 0.085594
[12:40:36.926] iteration 5029: total_loss: 0.345316, loss_sup: 0.220245, loss_mps: 0.044314, loss_cps: 0.080757
[12:40:37.075] iteration 5030: total_loss: 0.157347, loss_sup: 0.048829, loss_mps: 0.038811, loss_cps: 0.069707
[12:40:37.225] iteration 5031: total_loss: 0.167981, loss_sup: 0.085592, loss_mps: 0.031619, loss_cps: 0.050770
[12:40:37.374] iteration 5032: total_loss: 0.186284, loss_sup: 0.072779, loss_mps: 0.040488, loss_cps: 0.073018
[12:40:37.521] iteration 5033: total_loss: 0.187781, loss_sup: 0.100694, loss_mps: 0.032276, loss_cps: 0.054811
[12:40:37.669] iteration 5034: total_loss: 0.272041, loss_sup: 0.177507, loss_mps: 0.034313, loss_cps: 0.060220
[12:40:37.816] iteration 5035: total_loss: 0.607651, loss_sup: 0.511851, loss_mps: 0.034498, loss_cps: 0.061302
[12:40:37.963] iteration 5036: total_loss: 0.095885, loss_sup: 0.040650, loss_mps: 0.022089, loss_cps: 0.033147
[12:40:38.110] iteration 5037: total_loss: 0.263156, loss_sup: 0.130688, loss_mps: 0.046092, loss_cps: 0.086376
[12:40:38.256] iteration 5038: total_loss: 0.143867, loss_sup: 0.055781, loss_mps: 0.032856, loss_cps: 0.055230
[12:40:38.402] iteration 5039: total_loss: 0.215886, loss_sup: 0.081131, loss_mps: 0.043813, loss_cps: 0.090941
[12:40:38.549] iteration 5040: total_loss: 0.292950, loss_sup: 0.192916, loss_mps: 0.034912, loss_cps: 0.065121
[12:40:38.697] iteration 5041: total_loss: 0.304875, loss_sup: 0.203136, loss_mps: 0.036198, loss_cps: 0.065541
[12:40:38.844] iteration 5042: total_loss: 0.368498, loss_sup: 0.259188, loss_mps: 0.037912, loss_cps: 0.071399
[12:40:38.992] iteration 5043: total_loss: 0.168622, loss_sup: 0.078775, loss_mps: 0.031557, loss_cps: 0.058290
[12:40:39.139] iteration 5044: total_loss: 0.158066, loss_sup: 0.080659, loss_mps: 0.028756, loss_cps: 0.048650
[12:40:39.286] iteration 5045: total_loss: 0.418078, loss_sup: 0.320600, loss_mps: 0.034506, loss_cps: 0.062973
[12:40:39.434] iteration 5046: total_loss: 0.272064, loss_sup: 0.180750, loss_mps: 0.032523, loss_cps: 0.058792
[12:40:39.580] iteration 5047: total_loss: 0.387621, loss_sup: 0.233214, loss_mps: 0.051340, loss_cps: 0.103067
[12:40:39.728] iteration 5048: total_loss: 0.390791, loss_sup: 0.287640, loss_mps: 0.035864, loss_cps: 0.067287
[12:40:39.876] iteration 5049: total_loss: 0.249603, loss_sup: 0.167806, loss_mps: 0.029447, loss_cps: 0.052350
[12:40:40.024] iteration 5050: total_loss: 0.225538, loss_sup: 0.149689, loss_mps: 0.027637, loss_cps: 0.048212
[12:40:40.171] iteration 5051: total_loss: 0.256345, loss_sup: 0.173687, loss_mps: 0.031489, loss_cps: 0.051168
[12:40:40.317] iteration 5052: total_loss: 0.687198, loss_sup: 0.580252, loss_mps: 0.037778, loss_cps: 0.069168
[12:40:40.463] iteration 5053: total_loss: 0.231495, loss_sup: 0.118176, loss_mps: 0.039799, loss_cps: 0.073521
[12:40:40.609] iteration 5054: total_loss: 0.326087, loss_sup: 0.211229, loss_mps: 0.039138, loss_cps: 0.075720
[12:40:40.756] iteration 5055: total_loss: 0.248243, loss_sup: 0.089996, loss_mps: 0.053316, loss_cps: 0.104930
[12:40:40.903] iteration 5056: total_loss: 0.222849, loss_sup: 0.112048, loss_mps: 0.039346, loss_cps: 0.071455
[12:40:41.050] iteration 5057: total_loss: 0.242314, loss_sup: 0.151688, loss_mps: 0.034309, loss_cps: 0.056316
[12:40:41.197] iteration 5058: total_loss: 0.247222, loss_sup: 0.104126, loss_mps: 0.048665, loss_cps: 0.094430
[12:40:41.347] iteration 5059: total_loss: 0.253042, loss_sup: 0.129856, loss_mps: 0.042120, loss_cps: 0.081067
[12:40:41.494] iteration 5060: total_loss: 0.244343, loss_sup: 0.152041, loss_mps: 0.034300, loss_cps: 0.058003
[12:40:41.641] iteration 5061: total_loss: 0.291392, loss_sup: 0.182057, loss_mps: 0.037618, loss_cps: 0.071718
[12:40:41.787] iteration 5062: total_loss: 0.193155, loss_sup: 0.073889, loss_mps: 0.041617, loss_cps: 0.077649
[12:40:41.934] iteration 5063: total_loss: 0.428296, loss_sup: 0.326418, loss_mps: 0.038227, loss_cps: 0.063651
[12:40:42.080] iteration 5064: total_loss: 0.212515, loss_sup: 0.129506, loss_mps: 0.031471, loss_cps: 0.051538
[12:40:42.227] iteration 5065: total_loss: 0.267676, loss_sup: 0.157052, loss_mps: 0.040360, loss_cps: 0.070263
[12:40:42.373] iteration 5066: total_loss: 0.135489, loss_sup: 0.076646, loss_mps: 0.024325, loss_cps: 0.034518
[12:40:42.520] iteration 5067: total_loss: 0.151349, loss_sup: 0.066768, loss_mps: 0.031548, loss_cps: 0.053033
[12:40:42.666] iteration 5068: total_loss: 0.320727, loss_sup: 0.228699, loss_mps: 0.034284, loss_cps: 0.057744
[12:40:42.818] iteration 5069: total_loss: 0.348651, loss_sup: 0.199459, loss_mps: 0.051720, loss_cps: 0.097472
[12:40:42.967] iteration 5070: total_loss: 0.313875, loss_sup: 0.227513, loss_mps: 0.033118, loss_cps: 0.053243
[12:40:43.115] iteration 5071: total_loss: 0.329135, loss_sup: 0.194941, loss_mps: 0.046039, loss_cps: 0.088155
[12:40:43.261] iteration 5072: total_loss: 0.169808, loss_sup: 0.084846, loss_mps: 0.030631, loss_cps: 0.054331
[12:40:43.412] iteration 5073: total_loss: 0.146293, loss_sup: 0.077997, loss_mps: 0.027185, loss_cps: 0.041111
[12:40:43.559] iteration 5074: total_loss: 0.140938, loss_sup: 0.083913, loss_mps: 0.023247, loss_cps: 0.033777
[12:40:43.706] iteration 5075: total_loss: 0.160890, loss_sup: 0.081296, loss_mps: 0.028895, loss_cps: 0.050698
[12:40:43.854] iteration 5076: total_loss: 0.271843, loss_sup: 0.196233, loss_mps: 0.028374, loss_cps: 0.047236
[12:40:44.001] iteration 5077: total_loss: 0.199947, loss_sup: 0.127296, loss_mps: 0.027041, loss_cps: 0.045610
[12:40:44.148] iteration 5078: total_loss: 0.147190, loss_sup: 0.076234, loss_mps: 0.025885, loss_cps: 0.045070
[12:40:44.294] iteration 5079: total_loss: 0.176516, loss_sup: 0.128655, loss_mps: 0.019357, loss_cps: 0.028504
[12:40:44.442] iteration 5080: total_loss: 0.146834, loss_sup: 0.073807, loss_mps: 0.026349, loss_cps: 0.046678
[12:40:44.588] iteration 5081: total_loss: 0.154516, loss_sup: 0.050383, loss_mps: 0.036634, loss_cps: 0.067499
[12:40:44.736] iteration 5082: total_loss: 0.153331, loss_sup: 0.061246, loss_mps: 0.031825, loss_cps: 0.060260
[12:40:44.882] iteration 5083: total_loss: 0.116405, loss_sup: 0.043317, loss_mps: 0.027099, loss_cps: 0.045989
[12:40:45.030] iteration 5084: total_loss: 0.268996, loss_sup: 0.176520, loss_mps: 0.032395, loss_cps: 0.060081
[12:40:45.178] iteration 5085: total_loss: 0.265308, loss_sup: 0.202298, loss_mps: 0.024052, loss_cps: 0.038958
[12:40:45.325] iteration 5086: total_loss: 0.351457, loss_sup: 0.275232, loss_mps: 0.028583, loss_cps: 0.047642
[12:40:45.471] iteration 5087: total_loss: 0.247313, loss_sup: 0.144360, loss_mps: 0.035333, loss_cps: 0.067620
[12:40:45.618] iteration 5088: total_loss: 0.176904, loss_sup: 0.074252, loss_mps: 0.034775, loss_cps: 0.067877
[12:40:45.765] iteration 5089: total_loss: 0.077639, loss_sup: 0.015862, loss_mps: 0.022384, loss_cps: 0.039392
[12:40:45.913] iteration 5090: total_loss: 0.154029, loss_sup: 0.086562, loss_mps: 0.025298, loss_cps: 0.042170
[12:40:46.059] iteration 5091: total_loss: 0.239951, loss_sup: 0.135064, loss_mps: 0.035816, loss_cps: 0.069071
[12:40:46.208] iteration 5092: total_loss: 0.257344, loss_sup: 0.137359, loss_mps: 0.041001, loss_cps: 0.078984
[12:40:46.356] iteration 5093: total_loss: 0.188632, loss_sup: 0.093655, loss_mps: 0.034004, loss_cps: 0.060973
[12:40:46.505] iteration 5094: total_loss: 0.535404, loss_sup: 0.374986, loss_mps: 0.050362, loss_cps: 0.110056
[12:40:46.654] iteration 5095: total_loss: 0.186708, loss_sup: 0.105405, loss_mps: 0.029438, loss_cps: 0.051865
[12:40:46.801] iteration 5096: total_loss: 0.231980, loss_sup: 0.145299, loss_mps: 0.030398, loss_cps: 0.056282
[12:40:46.948] iteration 5097: total_loss: 0.392721, loss_sup: 0.304467, loss_mps: 0.030119, loss_cps: 0.058136
[12:40:47.096] iteration 5098: total_loss: 0.266662, loss_sup: 0.192572, loss_mps: 0.026591, loss_cps: 0.047499
[12:40:47.243] iteration 5099: total_loss: 0.224838, loss_sup: 0.134553, loss_mps: 0.031977, loss_cps: 0.058307
[12:40:47.392] iteration 5100: total_loss: 0.244747, loss_sup: 0.158323, loss_mps: 0.031447, loss_cps: 0.054977
[12:40:47.392] Evaluation Started ==>
[12:40:58.667] ==> valid iteration 5100: unet metrics: {'dc': 0.5897594525646026, 'jc': 0.46461046064753164, 'pre': 0.672517455779985, 'hd': 6.163828173099833}, ynet metrics: {'dc': 0.5714120985548847, 'jc': 0.4428157939677131, 'pre': 0.6386647044473148, 'hd': 6.732292650071784}.
[12:40:58.826] ==> New best valid dice for ynet: 0.571412, at iteration 5100
[12:40:58.828] Evaluation Finished!⏹️
[12:40:58.980] iteration 5101: total_loss: 0.359115, loss_sup: 0.271169, loss_mps: 0.031299, loss_cps: 0.056648
[12:40:59.128] iteration 5102: total_loss: 0.286720, loss_sup: 0.191541, loss_mps: 0.033843, loss_cps: 0.061336
[12:40:59.276] iteration 5103: total_loss: 0.315262, loss_sup: 0.254991, loss_mps: 0.022941, loss_cps: 0.037330
[12:40:59.422] iteration 5104: total_loss: 0.136645, loss_sup: 0.049204, loss_mps: 0.030543, loss_cps: 0.056898
[12:40:59.569] iteration 5105: total_loss: 0.161296, loss_sup: 0.091144, loss_mps: 0.026863, loss_cps: 0.043289
[12:40:59.715] iteration 5106: total_loss: 0.182824, loss_sup: 0.100288, loss_mps: 0.029397, loss_cps: 0.053140
[12:40:59.861] iteration 5107: total_loss: 0.229272, loss_sup: 0.141245, loss_mps: 0.031029, loss_cps: 0.056998
[12:41:00.009] iteration 5108: total_loss: 0.107584, loss_sup: 0.042815, loss_mps: 0.024225, loss_cps: 0.040544
[12:41:00.156] iteration 5109: total_loss: 0.123986, loss_sup: 0.057779, loss_mps: 0.024160, loss_cps: 0.042047
[12:41:00.307] iteration 5110: total_loss: 0.148246, loss_sup: 0.042253, loss_mps: 0.036118, loss_cps: 0.069875
[12:41:00.453] iteration 5111: total_loss: 0.165037, loss_sup: 0.089825, loss_mps: 0.026763, loss_cps: 0.048449
[12:41:00.599] iteration 5112: total_loss: 0.231564, loss_sup: 0.073488, loss_mps: 0.050155, loss_cps: 0.107921
[12:41:00.747] iteration 5113: total_loss: 0.314341, loss_sup: 0.239133, loss_mps: 0.026498, loss_cps: 0.048709
[12:41:00.894] iteration 5114: total_loss: 0.514492, loss_sup: 0.323764, loss_mps: 0.060311, loss_cps: 0.130417
[12:41:01.045] iteration 5115: total_loss: 0.183243, loss_sup: 0.109121, loss_mps: 0.027399, loss_cps: 0.046722
[12:41:01.192] iteration 5116: total_loss: 0.189121, loss_sup: 0.115498, loss_mps: 0.027187, loss_cps: 0.046436
[12:41:01.338] iteration 5117: total_loss: 0.196423, loss_sup: 0.096565, loss_mps: 0.033181, loss_cps: 0.066677
[12:41:01.484] iteration 5118: total_loss: 0.180936, loss_sup: 0.102409, loss_mps: 0.028639, loss_cps: 0.049888
[12:41:01.631] iteration 5119: total_loss: 0.221660, loss_sup: 0.111675, loss_mps: 0.037810, loss_cps: 0.072175
[12:41:01.777] iteration 5120: total_loss: 0.166489, loss_sup: 0.087638, loss_mps: 0.028965, loss_cps: 0.049885
[12:41:01.925] iteration 5121: total_loss: 0.295024, loss_sup: 0.230286, loss_mps: 0.023679, loss_cps: 0.041059
[12:41:02.071] iteration 5122: total_loss: 0.115850, loss_sup: 0.041452, loss_mps: 0.028203, loss_cps: 0.046196
[12:41:02.218] iteration 5123: total_loss: 0.196444, loss_sup: 0.088487, loss_mps: 0.036899, loss_cps: 0.071058
[12:41:02.366] iteration 5124: total_loss: 0.324339, loss_sup: 0.233022, loss_mps: 0.032250, loss_cps: 0.059068
[12:41:02.513] iteration 5125: total_loss: 0.123556, loss_sup: 0.065487, loss_mps: 0.021970, loss_cps: 0.036099
[12:41:02.660] iteration 5126: total_loss: 0.098182, loss_sup: 0.025024, loss_mps: 0.027069, loss_cps: 0.046090
[12:41:02.805] iteration 5127: total_loss: 0.167412, loss_sup: 0.101670, loss_mps: 0.024862, loss_cps: 0.040881
[12:41:02.951] iteration 5128: total_loss: 0.208103, loss_sup: 0.128448, loss_mps: 0.028740, loss_cps: 0.050915
[12:41:03.098] iteration 5129: total_loss: 0.182249, loss_sup: 0.114944, loss_mps: 0.024937, loss_cps: 0.042368
[12:41:03.244] iteration 5130: total_loss: 0.308802, loss_sup: 0.207552, loss_mps: 0.035188, loss_cps: 0.066062
[12:41:03.391] iteration 5131: total_loss: 0.261100, loss_sup: 0.201311, loss_mps: 0.023712, loss_cps: 0.036077
[12:41:03.540] iteration 5132: total_loss: 0.173592, loss_sup: 0.089873, loss_mps: 0.030667, loss_cps: 0.053052
[12:41:03.687] iteration 5133: total_loss: 0.108491, loss_sup: 0.037534, loss_mps: 0.025755, loss_cps: 0.045202
[12:41:03.833] iteration 5134: total_loss: 0.242935, loss_sup: 0.110525, loss_mps: 0.044015, loss_cps: 0.088396
[12:41:03.979] iteration 5135: total_loss: 0.140901, loss_sup: 0.058838, loss_mps: 0.029907, loss_cps: 0.052157
[12:41:04.125] iteration 5136: total_loss: 0.351148, loss_sup: 0.266138, loss_mps: 0.030446, loss_cps: 0.054563
[12:41:04.271] iteration 5137: total_loss: 0.512668, loss_sup: 0.376034, loss_mps: 0.045020, loss_cps: 0.091614
[12:41:04.416] iteration 5138: total_loss: 0.176152, loss_sup: 0.081676, loss_mps: 0.033156, loss_cps: 0.061320
[12:41:04.563] iteration 5139: total_loss: 0.164173, loss_sup: 0.073013, loss_mps: 0.032594, loss_cps: 0.058565
[12:41:04.710] iteration 5140: total_loss: 0.216107, loss_sup: 0.105080, loss_mps: 0.037021, loss_cps: 0.074006
[12:41:04.858] iteration 5141: total_loss: 0.220785, loss_sup: 0.130157, loss_mps: 0.031544, loss_cps: 0.059084
[12:41:05.005] iteration 5142: total_loss: 0.089747, loss_sup: 0.021053, loss_mps: 0.025222, loss_cps: 0.043472
[12:41:05.151] iteration 5143: total_loss: 0.187710, loss_sup: 0.109499, loss_mps: 0.027656, loss_cps: 0.050555
[12:41:05.297] iteration 5144: total_loss: 0.111730, loss_sup: 0.048041, loss_mps: 0.024390, loss_cps: 0.039298
[12:41:05.442] iteration 5145: total_loss: 0.291784, loss_sup: 0.184997, loss_mps: 0.035040, loss_cps: 0.071746
[12:41:05.588] iteration 5146: total_loss: 0.129959, loss_sup: 0.050783, loss_mps: 0.028752, loss_cps: 0.050424
[12:41:05.736] iteration 5147: total_loss: 0.323990, loss_sup: 0.241618, loss_mps: 0.029756, loss_cps: 0.052617
[12:41:05.883] iteration 5148: total_loss: 0.187614, loss_sup: 0.124212, loss_mps: 0.023938, loss_cps: 0.039464
[12:41:06.029] iteration 5149: total_loss: 0.251192, loss_sup: 0.124071, loss_mps: 0.042218, loss_cps: 0.084903
[12:41:06.175] iteration 5150: total_loss: 0.298502, loss_sup: 0.210222, loss_mps: 0.030415, loss_cps: 0.057866
[12:41:06.323] iteration 5151: total_loss: 0.211644, loss_sup: 0.127503, loss_mps: 0.030032, loss_cps: 0.054109
[12:41:06.470] iteration 5152: total_loss: 0.359667, loss_sup: 0.243701, loss_mps: 0.038561, loss_cps: 0.077404
[12:41:06.618] iteration 5153: total_loss: 0.133828, loss_sup: 0.061340, loss_mps: 0.026802, loss_cps: 0.045685
[12:41:06.765] iteration 5154: total_loss: 0.131821, loss_sup: 0.055232, loss_mps: 0.027653, loss_cps: 0.048936
[12:41:06.911] iteration 5155: total_loss: 0.187430, loss_sup: 0.084102, loss_mps: 0.035182, loss_cps: 0.068146
[12:41:07.058] iteration 5156: total_loss: 0.257535, loss_sup: 0.149735, loss_mps: 0.036763, loss_cps: 0.071037
[12:41:07.206] iteration 5157: total_loss: 0.131240, loss_sup: 0.016317, loss_mps: 0.039454, loss_cps: 0.075469
[12:41:07.353] iteration 5158: total_loss: 0.330001, loss_sup: 0.260725, loss_mps: 0.025654, loss_cps: 0.043622
[12:41:07.499] iteration 5159: total_loss: 0.206806, loss_sup: 0.109043, loss_mps: 0.035397, loss_cps: 0.062366
[12:41:07.646] iteration 5160: total_loss: 0.153496, loss_sup: 0.069781, loss_mps: 0.029775, loss_cps: 0.053940
[12:41:07.793] iteration 5161: total_loss: 0.286777, loss_sup: 0.200072, loss_mps: 0.031189, loss_cps: 0.055516
[12:41:07.941] iteration 5162: total_loss: 0.119799, loss_sup: 0.028905, loss_mps: 0.032190, loss_cps: 0.058704
[12:41:08.087] iteration 5163: total_loss: 0.232579, loss_sup: 0.100218, loss_mps: 0.043181, loss_cps: 0.089180
[12:41:08.238] iteration 5164: total_loss: 0.291258, loss_sup: 0.154386, loss_mps: 0.046569, loss_cps: 0.090304
[12:41:08.383] iteration 5165: total_loss: 0.340133, loss_sup: 0.192206, loss_mps: 0.048196, loss_cps: 0.099731
[12:41:08.530] iteration 5166: total_loss: 0.128145, loss_sup: 0.068737, loss_mps: 0.022216, loss_cps: 0.037191
[12:41:08.676] iteration 5167: total_loss: 0.263551, loss_sup: 0.152971, loss_mps: 0.038761, loss_cps: 0.071819
[12:41:08.824] iteration 5168: total_loss: 0.238464, loss_sup: 0.159901, loss_mps: 0.029497, loss_cps: 0.049067
[12:41:08.970] iteration 5169: total_loss: 0.170972, loss_sup: 0.089329, loss_mps: 0.029767, loss_cps: 0.051876
[12:41:09.116] iteration 5170: total_loss: 0.400499, loss_sup: 0.327914, loss_mps: 0.026968, loss_cps: 0.045617
[12:41:09.262] iteration 5171: total_loss: 0.151502, loss_sup: 0.082990, loss_mps: 0.025220, loss_cps: 0.043293
[12:41:09.408] iteration 5172: total_loss: 0.185694, loss_sup: 0.095832, loss_mps: 0.032254, loss_cps: 0.057609
[12:41:09.554] iteration 5173: total_loss: 0.143728, loss_sup: 0.052852, loss_mps: 0.033097, loss_cps: 0.057779
[12:41:09.699] iteration 5174: total_loss: 0.158093, loss_sup: 0.075829, loss_mps: 0.029395, loss_cps: 0.052869
[12:41:09.845] iteration 5175: total_loss: 0.246652, loss_sup: 0.167106, loss_mps: 0.029284, loss_cps: 0.050262
[12:41:09.991] iteration 5176: total_loss: 0.248632, loss_sup: 0.151676, loss_mps: 0.034008, loss_cps: 0.062948
[12:41:10.137] iteration 5177: total_loss: 0.188920, loss_sup: 0.111373, loss_mps: 0.028985, loss_cps: 0.048561
[12:41:10.283] iteration 5178: total_loss: 0.150017, loss_sup: 0.076865, loss_mps: 0.027544, loss_cps: 0.045608
[12:41:10.429] iteration 5179: total_loss: 0.169723, loss_sup: 0.076834, loss_mps: 0.032049, loss_cps: 0.060840
[12:41:10.581] iteration 5180: total_loss: 0.361127, loss_sup: 0.240220, loss_mps: 0.039765, loss_cps: 0.081142
[12:41:10.729] iteration 5181: total_loss: 0.269978, loss_sup: 0.191670, loss_mps: 0.027857, loss_cps: 0.050450
[12:41:10.876] iteration 5182: total_loss: 0.288551, loss_sup: 0.163228, loss_mps: 0.041758, loss_cps: 0.083565
[12:41:11.022] iteration 5183: total_loss: 0.284919, loss_sup: 0.146695, loss_mps: 0.046724, loss_cps: 0.091500
[12:41:11.169] iteration 5184: total_loss: 0.172415, loss_sup: 0.085788, loss_mps: 0.031241, loss_cps: 0.055386
[12:41:11.315] iteration 5185: total_loss: 0.193941, loss_sup: 0.117606, loss_mps: 0.026541, loss_cps: 0.049794
[12:41:11.461] iteration 5186: total_loss: 0.199398, loss_sup: 0.090650, loss_mps: 0.037489, loss_cps: 0.071259
[12:41:11.610] iteration 5187: total_loss: 0.163812, loss_sup: 0.063587, loss_mps: 0.035810, loss_cps: 0.064415
[12:41:11.756] iteration 5188: total_loss: 0.315839, loss_sup: 0.234064, loss_mps: 0.028947, loss_cps: 0.052828
[12:41:11.902] iteration 5189: total_loss: 0.271194, loss_sup: 0.165963, loss_mps: 0.035832, loss_cps: 0.069398
[12:41:12.048] iteration 5190: total_loss: 0.255434, loss_sup: 0.139431, loss_mps: 0.040100, loss_cps: 0.075904
[12:41:12.195] iteration 5191: total_loss: 0.218311, loss_sup: 0.148313, loss_mps: 0.025878, loss_cps: 0.044121
[12:41:12.343] iteration 5192: total_loss: 0.154028, loss_sup: 0.046294, loss_mps: 0.037174, loss_cps: 0.070561
[12:41:12.489] iteration 5193: total_loss: 0.281290, loss_sup: 0.191642, loss_mps: 0.031661, loss_cps: 0.057987
[12:41:12.637] iteration 5194: total_loss: 0.303949, loss_sup: 0.191839, loss_mps: 0.038988, loss_cps: 0.073121
[12:41:12.785] iteration 5195: total_loss: 0.150851, loss_sup: 0.074185, loss_mps: 0.028442, loss_cps: 0.048224
[12:41:12.931] iteration 5196: total_loss: 0.170336, loss_sup: 0.088887, loss_mps: 0.028946, loss_cps: 0.052502
[12:41:13.078] iteration 5197: total_loss: 0.210252, loss_sup: 0.133650, loss_mps: 0.028505, loss_cps: 0.048098
[12:41:13.225] iteration 5198: total_loss: 0.315389, loss_sup: 0.233195, loss_mps: 0.030027, loss_cps: 0.052167
[12:41:13.372] iteration 5199: total_loss: 0.311442, loss_sup: 0.212602, loss_mps: 0.034940, loss_cps: 0.063901
[12:41:13.518] iteration 5200: total_loss: 0.114816, loss_sup: 0.040030, loss_mps: 0.027387, loss_cps: 0.047399
[12:41:13.519] Evaluation Started ==>
[12:41:24.858] ==> valid iteration 5200: unet metrics: {'dc': 0.6157568288812273, 'jc': 0.4873593028442785, 'pre': 0.6886303030405073, 'hd': 6.3656690570076}, ynet metrics: {'dc': 0.523630924184414, 'jc': 0.4013153997226093, 'pre': 0.7121085593714331, 'hd': 6.26022496436915}.
[12:41:24.859] Evaluation Finished!⏹️
[12:41:25.009] iteration 5201: total_loss: 0.254502, loss_sup: 0.160336, loss_mps: 0.032590, loss_cps: 0.061577
[12:41:25.158] iteration 5202: total_loss: 0.484048, loss_sup: 0.347871, loss_mps: 0.044921, loss_cps: 0.091256
[12:41:25.304] iteration 5203: total_loss: 0.386820, loss_sup: 0.267008, loss_mps: 0.041008, loss_cps: 0.078804
[12:41:25.449] iteration 5204: total_loss: 0.248991, loss_sup: 0.121629, loss_mps: 0.045137, loss_cps: 0.082226
[12:41:25.595] iteration 5205: total_loss: 0.207713, loss_sup: 0.120092, loss_mps: 0.032176, loss_cps: 0.055445
[12:41:25.745] iteration 5206: total_loss: 0.125713, loss_sup: 0.061700, loss_mps: 0.025687, loss_cps: 0.038325
[12:41:25.890] iteration 5207: total_loss: 0.353365, loss_sup: 0.210599, loss_mps: 0.047668, loss_cps: 0.095098
[12:41:26.037] iteration 5208: total_loss: 0.164107, loss_sup: 0.101317, loss_mps: 0.024632, loss_cps: 0.038159
[12:41:26.183] iteration 5209: total_loss: 0.160181, loss_sup: 0.080233, loss_mps: 0.029790, loss_cps: 0.050157
[12:41:26.327] iteration 5210: total_loss: 0.276526, loss_sup: 0.183351, loss_mps: 0.034249, loss_cps: 0.058926
[12:41:26.473] iteration 5211: total_loss: 0.185002, loss_sup: 0.109653, loss_mps: 0.027963, loss_cps: 0.047386
[12:41:26.619] iteration 5212: total_loss: 0.359481, loss_sup: 0.212909, loss_mps: 0.050202, loss_cps: 0.096371
[12:41:26.767] iteration 5213: total_loss: 0.098253, loss_sup: 0.033342, loss_mps: 0.025919, loss_cps: 0.038991
[12:41:26.913] iteration 5214: total_loss: 0.248545, loss_sup: 0.141683, loss_mps: 0.038488, loss_cps: 0.068373
[12:41:27.059] iteration 5215: total_loss: 0.365990, loss_sup: 0.238236, loss_mps: 0.043293, loss_cps: 0.084462
[12:41:27.205] iteration 5216: total_loss: 0.146362, loss_sup: 0.067178, loss_mps: 0.030224, loss_cps: 0.048960
[12:41:27.353] iteration 5217: total_loss: 0.317642, loss_sup: 0.213216, loss_mps: 0.038661, loss_cps: 0.065765
[12:41:27.498] iteration 5218: total_loss: 0.179283, loss_sup: 0.098357, loss_mps: 0.030020, loss_cps: 0.050906
[12:41:27.644] iteration 5219: total_loss: 0.150613, loss_sup: 0.085883, loss_mps: 0.024944, loss_cps: 0.039785
[12:41:27.790] iteration 5220: total_loss: 0.374381, loss_sup: 0.270391, loss_mps: 0.037297, loss_cps: 0.066693
[12:41:27.937] iteration 5221: total_loss: 0.219654, loss_sup: 0.108625, loss_mps: 0.039986, loss_cps: 0.071043
[12:41:28.084] iteration 5222: total_loss: 0.125834, loss_sup: 0.049023, loss_mps: 0.030352, loss_cps: 0.046459
[12:41:28.235] iteration 5223: total_loss: 0.177729, loss_sup: 0.109995, loss_mps: 0.026257, loss_cps: 0.041477
[12:41:28.380] iteration 5224: total_loss: 0.262764, loss_sup: 0.177403, loss_mps: 0.030936, loss_cps: 0.054425
[12:41:28.526] iteration 5225: total_loss: 0.104938, loss_sup: 0.038702, loss_mps: 0.025925, loss_cps: 0.040310
[12:41:28.673] iteration 5226: total_loss: 0.138087, loss_sup: 0.055134, loss_mps: 0.030616, loss_cps: 0.052336
[12:41:28.819] iteration 5227: total_loss: 0.271167, loss_sup: 0.216770, loss_mps: 0.022683, loss_cps: 0.031714
[12:41:28.966] iteration 5228: total_loss: 0.131378, loss_sup: 0.065393, loss_mps: 0.026034, loss_cps: 0.039950
[12:41:29.112] iteration 5229: total_loss: 0.400476, loss_sup: 0.342911, loss_mps: 0.022711, loss_cps: 0.034855
[12:41:29.258] iteration 5230: total_loss: 0.218652, loss_sup: 0.092834, loss_mps: 0.041946, loss_cps: 0.083872
[12:41:29.403] iteration 5231: total_loss: 0.157184, loss_sup: 0.063157, loss_mps: 0.033243, loss_cps: 0.060783
[12:41:29.549] iteration 5232: total_loss: 0.585736, loss_sup: 0.456925, loss_mps: 0.043715, loss_cps: 0.085096
[12:41:29.696] iteration 5233: total_loss: 0.233947, loss_sup: 0.126586, loss_mps: 0.036961, loss_cps: 0.070399
[12:41:29.842] iteration 5234: total_loss: 0.331255, loss_sup: 0.159274, loss_mps: 0.055668, loss_cps: 0.116313
[12:41:29.989] iteration 5235: total_loss: 0.142030, loss_sup: 0.054605, loss_mps: 0.031603, loss_cps: 0.055822
[12:41:30.135] iteration 5236: total_loss: 0.143677, loss_sup: 0.066690, loss_mps: 0.028592, loss_cps: 0.048395
[12:41:30.283] iteration 5237: total_loss: 0.256315, loss_sup: 0.116530, loss_mps: 0.045828, loss_cps: 0.093957
[12:41:30.428] iteration 5238: total_loss: 0.233797, loss_sup: 0.165964, loss_mps: 0.025334, loss_cps: 0.042499
[12:41:30.574] iteration 5239: total_loss: 0.242431, loss_sup: 0.136028, loss_mps: 0.037553, loss_cps: 0.068849
[12:41:30.721] iteration 5240: total_loss: 0.202196, loss_sup: 0.134190, loss_mps: 0.025248, loss_cps: 0.042759
[12:41:30.868] iteration 5241: total_loss: 0.281831, loss_sup: 0.144976, loss_mps: 0.044151, loss_cps: 0.092704
[12:41:31.013] iteration 5242: total_loss: 0.210187, loss_sup: 0.102458, loss_mps: 0.037202, loss_cps: 0.070528
[12:41:31.158] iteration 5243: total_loss: 0.407561, loss_sup: 0.249712, loss_mps: 0.053493, loss_cps: 0.104356
[12:41:31.306] iteration 5244: total_loss: 0.309703, loss_sup: 0.204700, loss_mps: 0.037428, loss_cps: 0.067576
[12:41:31.452] iteration 5245: total_loss: 0.145749, loss_sup: 0.053064, loss_mps: 0.033361, loss_cps: 0.059324
[12:41:31.601] iteration 5246: total_loss: 0.207562, loss_sup: 0.114395, loss_mps: 0.034623, loss_cps: 0.058544
[12:41:31.749] iteration 5247: total_loss: 0.229262, loss_sup: 0.153966, loss_mps: 0.028941, loss_cps: 0.046355
[12:41:31.895] iteration 5248: total_loss: 0.314104, loss_sup: 0.214599, loss_mps: 0.034846, loss_cps: 0.064659
[12:41:32.041] iteration 5249: total_loss: 0.260315, loss_sup: 0.178372, loss_mps: 0.030994, loss_cps: 0.050949
[12:41:32.187] iteration 5250: total_loss: 0.260735, loss_sup: 0.191351, loss_mps: 0.027056, loss_cps: 0.042328
[12:41:32.336] iteration 5251: total_loss: 0.214908, loss_sup: 0.111467, loss_mps: 0.037380, loss_cps: 0.066060
[12:41:32.483] iteration 5252: total_loss: 0.216959, loss_sup: 0.156040, loss_mps: 0.023712, loss_cps: 0.037207
[12:41:32.629] iteration 5253: total_loss: 0.353058, loss_sup: 0.246978, loss_mps: 0.038475, loss_cps: 0.067606
[12:41:32.776] iteration 5254: total_loss: 0.432257, loss_sup: 0.324343, loss_mps: 0.038308, loss_cps: 0.069606
[12:41:32.922] iteration 5255: total_loss: 0.149240, loss_sup: 0.087283, loss_mps: 0.024191, loss_cps: 0.037765
[12:41:33.068] iteration 5256: total_loss: 0.268587, loss_sup: 0.177247, loss_mps: 0.033529, loss_cps: 0.057810
[12:41:33.214] iteration 5257: total_loss: 0.362694, loss_sup: 0.227622, loss_mps: 0.044759, loss_cps: 0.090312
[12:41:33.359] iteration 5258: total_loss: 0.188511, loss_sup: 0.091903, loss_mps: 0.034381, loss_cps: 0.062227
[12:41:33.507] iteration 5259: total_loss: 0.179621, loss_sup: 0.073350, loss_mps: 0.035862, loss_cps: 0.070409
[12:41:33.654] iteration 5260: total_loss: 0.172245, loss_sup: 0.078223, loss_mps: 0.034531, loss_cps: 0.059491
[12:41:33.801] iteration 5261: total_loss: 0.266210, loss_sup: 0.140046, loss_mps: 0.042355, loss_cps: 0.083808
[12:41:33.947] iteration 5262: total_loss: 0.106515, loss_sup: 0.035894, loss_mps: 0.027825, loss_cps: 0.042796
[12:41:34.093] iteration 5263: total_loss: 0.167758, loss_sup: 0.105517, loss_mps: 0.023144, loss_cps: 0.039097
[12:41:34.239] iteration 5264: total_loss: 0.219562, loss_sup: 0.114906, loss_mps: 0.035465, loss_cps: 0.069191
[12:41:34.385] iteration 5265: total_loss: 0.132417, loss_sup: 0.063724, loss_mps: 0.026328, loss_cps: 0.042364
[12:41:34.530] iteration 5266: total_loss: 0.239275, loss_sup: 0.149547, loss_mps: 0.033394, loss_cps: 0.056334
[12:41:34.676] iteration 5267: total_loss: 0.642029, loss_sup: 0.492330, loss_mps: 0.048849, loss_cps: 0.100850
[12:41:34.824] iteration 5268: total_loss: 0.239887, loss_sup: 0.117400, loss_mps: 0.041257, loss_cps: 0.081231
[12:41:34.970] iteration 5269: total_loss: 0.265161, loss_sup: 0.170202, loss_mps: 0.033421, loss_cps: 0.061538
[12:41:35.120] iteration 5270: total_loss: 0.183552, loss_sup: 0.123491, loss_mps: 0.023303, loss_cps: 0.036758
[12:41:35.266] iteration 5271: total_loss: 0.163188, loss_sup: 0.071975, loss_mps: 0.033580, loss_cps: 0.057634
[12:41:35.411] iteration 5272: total_loss: 0.108392, loss_sup: 0.024736, loss_mps: 0.029993, loss_cps: 0.053664
[12:41:35.557] iteration 5273: total_loss: 0.215801, loss_sup: 0.083753, loss_mps: 0.044035, loss_cps: 0.088013
[12:41:35.705] iteration 5274: total_loss: 0.258191, loss_sup: 0.132268, loss_mps: 0.043384, loss_cps: 0.082539
[12:41:35.854] iteration 5275: total_loss: 0.195376, loss_sup: 0.104170, loss_mps: 0.033080, loss_cps: 0.058125
[12:41:36.000] iteration 5276: total_loss: 0.291062, loss_sup: 0.183018, loss_mps: 0.037335, loss_cps: 0.070710
[12:41:36.146] iteration 5277: total_loss: 0.201407, loss_sup: 0.097794, loss_mps: 0.037197, loss_cps: 0.066416
[12:41:36.293] iteration 5278: total_loss: 0.185381, loss_sup: 0.094035, loss_mps: 0.032370, loss_cps: 0.058976
[12:41:36.438] iteration 5279: total_loss: 0.222468, loss_sup: 0.150116, loss_mps: 0.026820, loss_cps: 0.045532
[12:41:36.584] iteration 5280: total_loss: 0.221348, loss_sup: 0.129687, loss_mps: 0.031365, loss_cps: 0.060296
[12:41:36.730] iteration 5281: total_loss: 0.210730, loss_sup: 0.116353, loss_mps: 0.033007, loss_cps: 0.061370
[12:41:36.876] iteration 5282: total_loss: 0.219004, loss_sup: 0.112704, loss_mps: 0.036405, loss_cps: 0.069895
[12:41:37.023] iteration 5283: total_loss: 0.256138, loss_sup: 0.156053, loss_mps: 0.036025, loss_cps: 0.064060
[12:41:37.169] iteration 5284: total_loss: 0.332609, loss_sup: 0.210141, loss_mps: 0.042786, loss_cps: 0.079682
[12:41:37.317] iteration 5285: total_loss: 0.169199, loss_sup: 0.088993, loss_mps: 0.029725, loss_cps: 0.050481
[12:41:37.463] iteration 5286: total_loss: 0.211782, loss_sup: 0.132369, loss_mps: 0.029637, loss_cps: 0.049776
[12:41:37.609] iteration 5287: total_loss: 0.188099, loss_sup: 0.081095, loss_mps: 0.036926, loss_cps: 0.070078
[12:41:37.755] iteration 5288: total_loss: 0.190402, loss_sup: 0.075254, loss_mps: 0.040328, loss_cps: 0.074819
[12:41:37.902] iteration 5289: total_loss: 0.251183, loss_sup: 0.189961, loss_mps: 0.023873, loss_cps: 0.037350
[12:41:38.049] iteration 5290: total_loss: 0.342522, loss_sup: 0.210340, loss_mps: 0.044274, loss_cps: 0.087909
[12:41:38.195] iteration 5291: total_loss: 0.176801, loss_sup: 0.086990, loss_mps: 0.032263, loss_cps: 0.057547
[12:41:38.344] iteration 5292: total_loss: 0.260261, loss_sup: 0.172141, loss_mps: 0.032717, loss_cps: 0.055403
[12:41:38.490] iteration 5293: total_loss: 0.138910, loss_sup: 0.051322, loss_mps: 0.031404, loss_cps: 0.056183
[12:41:38.637] iteration 5294: total_loss: 0.397771, loss_sup: 0.283778, loss_mps: 0.039280, loss_cps: 0.074713
[12:41:38.783] iteration 5295: total_loss: 0.203478, loss_sup: 0.118231, loss_mps: 0.031151, loss_cps: 0.054097
[12:41:38.930] iteration 5296: total_loss: 0.309688, loss_sup: 0.213840, loss_mps: 0.033814, loss_cps: 0.062034
[12:41:39.076] iteration 5297: total_loss: 0.436174, loss_sup: 0.290704, loss_mps: 0.049908, loss_cps: 0.095562
[12:41:39.223] iteration 5298: total_loss: 0.287231, loss_sup: 0.179149, loss_mps: 0.040542, loss_cps: 0.067540
[12:41:39.372] iteration 5299: total_loss: 0.131375, loss_sup: 0.056067, loss_mps: 0.028546, loss_cps: 0.046763
[12:41:39.518] iteration 5300: total_loss: 0.195254, loss_sup: 0.081296, loss_mps: 0.039674, loss_cps: 0.074285
[12:41:39.518] Evaluation Started ==>
[12:41:50.901] ==> valid iteration 5300: unet metrics: {'dc': 0.5575998566224873, 'jc': 0.4243403478591391, 'pre': 0.6276753422938967, 'hd': 6.679778763901445}, ynet metrics: {'dc': 0.5159493722054845, 'jc': 0.39553919871472704, 'pre': 0.6060122246544422, 'hd': 6.628186397090783}.
[12:41:50.903] Evaluation Finished!⏹️
[12:41:51.054] iteration 5301: total_loss: 0.193311, loss_sup: 0.129296, loss_mps: 0.025158, loss_cps: 0.038857
[12:41:51.201] iteration 5302: total_loss: 0.207740, loss_sup: 0.128015, loss_mps: 0.029232, loss_cps: 0.050493
[12:41:51.347] iteration 5303: total_loss: 0.325799, loss_sup: 0.218966, loss_mps: 0.036879, loss_cps: 0.069954
[12:41:51.494] iteration 5304: total_loss: 0.226701, loss_sup: 0.113544, loss_mps: 0.039080, loss_cps: 0.074077
[12:41:51.640] iteration 5305: total_loss: 0.286767, loss_sup: 0.164139, loss_mps: 0.043387, loss_cps: 0.079241
[12:41:51.788] iteration 5306: total_loss: 0.156830, loss_sup: 0.088704, loss_mps: 0.025026, loss_cps: 0.043100
[12:41:51.935] iteration 5307: total_loss: 0.220961, loss_sup: 0.122596, loss_mps: 0.034879, loss_cps: 0.063485
[12:41:52.080] iteration 5308: total_loss: 0.219796, loss_sup: 0.108914, loss_mps: 0.038331, loss_cps: 0.072551
[12:41:52.226] iteration 5309: total_loss: 0.221966, loss_sup: 0.144560, loss_mps: 0.028130, loss_cps: 0.049276
[12:41:52.372] iteration 5310: total_loss: 0.152633, loss_sup: 0.090210, loss_mps: 0.023960, loss_cps: 0.038464
[12:41:52.519] iteration 5311: total_loss: 0.329094, loss_sup: 0.241519, loss_mps: 0.031670, loss_cps: 0.055904
[12:41:52.665] iteration 5312: total_loss: 0.168970, loss_sup: 0.060980, loss_mps: 0.037062, loss_cps: 0.070928
[12:41:52.810] iteration 5313: total_loss: 0.198004, loss_sup: 0.098220, loss_mps: 0.036434, loss_cps: 0.063350
[12:41:52.955] iteration 5314: total_loss: 0.352864, loss_sup: 0.276359, loss_mps: 0.028408, loss_cps: 0.048097
[12:41:53.100] iteration 5315: total_loss: 0.299225, loss_sup: 0.216588, loss_mps: 0.029941, loss_cps: 0.052696
[12:41:53.246] iteration 5316: total_loss: 0.197122, loss_sup: 0.103012, loss_mps: 0.034830, loss_cps: 0.059280
[12:41:53.395] iteration 5317: total_loss: 0.251769, loss_sup: 0.137372, loss_mps: 0.038046, loss_cps: 0.076352
[12:41:53.541] iteration 5318: total_loss: 0.274168, loss_sup: 0.164266, loss_mps: 0.038559, loss_cps: 0.071343
[12:41:53.687] iteration 5319: total_loss: 0.167668, loss_sup: 0.085645, loss_mps: 0.029016, loss_cps: 0.053007
[12:41:53.833] iteration 5320: total_loss: 0.216484, loss_sup: 0.138242, loss_mps: 0.028774, loss_cps: 0.049468
[12:41:53.979] iteration 5321: total_loss: 0.138374, loss_sup: 0.071981, loss_mps: 0.025476, loss_cps: 0.040917
[12:41:54.125] iteration 5322: total_loss: 0.418151, loss_sup: 0.299282, loss_mps: 0.040713, loss_cps: 0.078156
[12:41:54.271] iteration 5323: total_loss: 0.110848, loss_sup: 0.038155, loss_mps: 0.027888, loss_cps: 0.044805
[12:41:54.419] iteration 5324: total_loss: 0.288090, loss_sup: 0.156654, loss_mps: 0.043699, loss_cps: 0.087737
[12:41:54.565] iteration 5325: total_loss: 0.139712, loss_sup: 0.051306, loss_mps: 0.031605, loss_cps: 0.056802
[12:41:54.710] iteration 5326: total_loss: 0.229710, loss_sup: 0.088501, loss_mps: 0.046640, loss_cps: 0.094569
[12:41:54.858] iteration 5327: total_loss: 0.296518, loss_sup: 0.200383, loss_mps: 0.035130, loss_cps: 0.061005
[12:41:55.006] iteration 5328: total_loss: 0.193279, loss_sup: 0.106244, loss_mps: 0.030784, loss_cps: 0.056251
[12:41:55.152] iteration 5329: total_loss: 0.178992, loss_sup: 0.082109, loss_mps: 0.035163, loss_cps: 0.061720
[12:41:55.297] iteration 5330: total_loss: 0.280317, loss_sup: 0.175211, loss_mps: 0.036288, loss_cps: 0.068818
[12:41:55.443] iteration 5331: total_loss: 0.291586, loss_sup: 0.203157, loss_mps: 0.031989, loss_cps: 0.056441
[12:41:55.588] iteration 5332: total_loss: 0.309745, loss_sup: 0.203402, loss_mps: 0.038178, loss_cps: 0.068164
[12:41:55.734] iteration 5333: total_loss: 0.187099, loss_sup: 0.083193, loss_mps: 0.035592, loss_cps: 0.068313
[12:41:55.879] iteration 5334: total_loss: 0.219554, loss_sup: 0.098711, loss_mps: 0.042066, loss_cps: 0.078777
[12:41:56.026] iteration 5335: total_loss: 0.148301, loss_sup: 0.038535, loss_mps: 0.036872, loss_cps: 0.072894
[12:41:56.177] iteration 5336: total_loss: 0.124653, loss_sup: 0.042278, loss_mps: 0.029575, loss_cps: 0.052800
[12:41:56.323] iteration 5337: total_loss: 0.412202, loss_sup: 0.326742, loss_mps: 0.031459, loss_cps: 0.054002
[12:41:56.472] iteration 5338: total_loss: 0.219201, loss_sup: 0.138756, loss_mps: 0.029225, loss_cps: 0.051221
[12:41:56.617] iteration 5339: total_loss: 0.230148, loss_sup: 0.093853, loss_mps: 0.044651, loss_cps: 0.091644
[12:41:56.763] iteration 5340: total_loss: 0.177737, loss_sup: 0.093143, loss_mps: 0.031267, loss_cps: 0.053327
[12:41:56.909] iteration 5341: total_loss: 0.195488, loss_sup: 0.121911, loss_mps: 0.027663, loss_cps: 0.045914
[12:41:57.054] iteration 5342: total_loss: 0.300360, loss_sup: 0.213495, loss_mps: 0.031905, loss_cps: 0.054961
[12:41:57.199] iteration 5343: total_loss: 0.392175, loss_sup: 0.292437, loss_mps: 0.035110, loss_cps: 0.064628
[12:41:57.347] iteration 5344: total_loss: 0.448869, loss_sup: 0.306496, loss_mps: 0.046656, loss_cps: 0.095718
[12:41:57.492] iteration 5345: total_loss: 0.142810, loss_sup: 0.046814, loss_mps: 0.034045, loss_cps: 0.061951
[12:41:57.638] iteration 5346: total_loss: 0.254148, loss_sup: 0.156699, loss_mps: 0.034176, loss_cps: 0.063273
[12:41:57.785] iteration 5347: total_loss: 0.331818, loss_sup: 0.252465, loss_mps: 0.028729, loss_cps: 0.050624
[12:41:57.932] iteration 5348: total_loss: 0.427669, loss_sup: 0.281905, loss_mps: 0.047888, loss_cps: 0.097876
[12:41:58.078] iteration 5349: total_loss: 0.203516, loss_sup: 0.110722, loss_mps: 0.033615, loss_cps: 0.059179
[12:41:58.226] iteration 5350: total_loss: 0.168062, loss_sup: 0.085602, loss_mps: 0.030181, loss_cps: 0.052279
[12:41:58.372] iteration 5351: total_loss: 0.187283, loss_sup: 0.120580, loss_mps: 0.025180, loss_cps: 0.041522
[12:41:58.518] iteration 5352: total_loss: 0.217295, loss_sup: 0.108292, loss_mps: 0.038608, loss_cps: 0.070395
[12:41:58.663] iteration 5353: total_loss: 0.143503, loss_sup: 0.069474, loss_mps: 0.027406, loss_cps: 0.046622
[12:41:58.810] iteration 5354: total_loss: 0.275782, loss_sup: 0.170068, loss_mps: 0.036704, loss_cps: 0.069010
[12:41:58.956] iteration 5355: total_loss: 0.150070, loss_sup: 0.057057, loss_mps: 0.033146, loss_cps: 0.059867
[12:41:59.101] iteration 5356: total_loss: 0.301581, loss_sup: 0.181056, loss_mps: 0.042017, loss_cps: 0.078509
[12:41:59.248] iteration 5357: total_loss: 0.196199, loss_sup: 0.109657, loss_mps: 0.030445, loss_cps: 0.056098
[12:41:59.394] iteration 5358: total_loss: 0.362869, loss_sup: 0.223631, loss_mps: 0.046678, loss_cps: 0.092560
[12:41:59.540] iteration 5359: total_loss: 0.235799, loss_sup: 0.143827, loss_mps: 0.034886, loss_cps: 0.057086
[12:41:59.686] iteration 5360: total_loss: 0.206865, loss_sup: 0.099122, loss_mps: 0.038313, loss_cps: 0.069431
[12:41:59.832] iteration 5361: total_loss: 0.190963, loss_sup: 0.115484, loss_mps: 0.027447, loss_cps: 0.048031
[12:41:59.978] iteration 5362: total_loss: 0.294234, loss_sup: 0.161532, loss_mps: 0.044896, loss_cps: 0.087805
[12:42:00.125] iteration 5363: total_loss: 0.206602, loss_sup: 0.100067, loss_mps: 0.039140, loss_cps: 0.067395
[12:42:00.271] iteration 5364: total_loss: 0.326682, loss_sup: 0.176794, loss_mps: 0.049668, loss_cps: 0.100219
[12:42:00.418] iteration 5365: total_loss: 0.249712, loss_sup: 0.164077, loss_mps: 0.031986, loss_cps: 0.053649
[12:42:00.564] iteration 5366: total_loss: 0.162623, loss_sup: 0.078560, loss_mps: 0.031206, loss_cps: 0.052856
[12:42:00.713] iteration 5367: total_loss: 0.347241, loss_sup: 0.225742, loss_mps: 0.043805, loss_cps: 0.077695
[12:42:00.861] iteration 5368: total_loss: 0.165934, loss_sup: 0.079008, loss_mps: 0.032251, loss_cps: 0.054675
[12:42:01.010] iteration 5369: total_loss: 0.169112, loss_sup: 0.089646, loss_mps: 0.029828, loss_cps: 0.049638
[12:42:01.158] iteration 5370: total_loss: 0.371175, loss_sup: 0.254996, loss_mps: 0.039915, loss_cps: 0.076264
[12:42:01.304] iteration 5371: total_loss: 0.116445, loss_sup: 0.046549, loss_mps: 0.025689, loss_cps: 0.044207
[12:42:01.449] iteration 5372: total_loss: 0.186982, loss_sup: 0.068551, loss_mps: 0.040078, loss_cps: 0.078353
[12:42:01.595] iteration 5373: total_loss: 0.254151, loss_sup: 0.171181, loss_mps: 0.029401, loss_cps: 0.053569
[12:42:01.740] iteration 5374: total_loss: 0.148835, loss_sup: 0.054603, loss_mps: 0.034175, loss_cps: 0.060057
[12:42:01.889] iteration 5375: total_loss: 0.116082, loss_sup: 0.047175, loss_mps: 0.024734, loss_cps: 0.044173
[12:42:02.034] iteration 5376: total_loss: 0.127233, loss_sup: 0.054294, loss_mps: 0.026642, loss_cps: 0.046297
[12:42:02.181] iteration 5377: total_loss: 0.196881, loss_sup: 0.074007, loss_mps: 0.042066, loss_cps: 0.080808
[12:42:02.327] iteration 5378: total_loss: 0.082032, loss_sup: 0.015319, loss_mps: 0.025356, loss_cps: 0.041356
[12:42:02.473] iteration 5379: total_loss: 0.357456, loss_sup: 0.250820, loss_mps: 0.036982, loss_cps: 0.069654
[12:42:02.619] iteration 5380: total_loss: 0.385308, loss_sup: 0.302452, loss_mps: 0.029558, loss_cps: 0.053298
[12:42:02.766] iteration 5381: total_loss: 0.329942, loss_sup: 0.240246, loss_mps: 0.031268, loss_cps: 0.058428
[12:42:02.915] iteration 5382: total_loss: 0.324906, loss_sup: 0.242826, loss_mps: 0.029912, loss_cps: 0.052168
[12:42:03.061] iteration 5383: total_loss: 0.171879, loss_sup: 0.100707, loss_mps: 0.026183, loss_cps: 0.044989
[12:42:03.207] iteration 5384: total_loss: 0.283128, loss_sup: 0.213280, loss_mps: 0.025726, loss_cps: 0.044122
[12:42:03.352] iteration 5385: total_loss: 0.103177, loss_sup: 0.031722, loss_mps: 0.025817, loss_cps: 0.045639
[12:42:03.497] iteration 5386: total_loss: 0.203229, loss_sup: 0.109373, loss_mps: 0.032684, loss_cps: 0.061172
[12:42:03.645] iteration 5387: total_loss: 0.306171, loss_sup: 0.190101, loss_mps: 0.040104, loss_cps: 0.075966
[12:42:03.792] iteration 5388: total_loss: 0.157055, loss_sup: 0.083615, loss_mps: 0.027380, loss_cps: 0.046060
[12:42:03.937] iteration 5389: total_loss: 0.190164, loss_sup: 0.119950, loss_mps: 0.026463, loss_cps: 0.043752
[12:42:04.084] iteration 5390: total_loss: 0.588666, loss_sup: 0.482649, loss_mps: 0.036340, loss_cps: 0.069677
[12:42:04.230] iteration 5391: total_loss: 0.168818, loss_sup: 0.044261, loss_mps: 0.043320, loss_cps: 0.081237
[12:42:04.376] iteration 5392: total_loss: 0.149246, loss_sup: 0.081908, loss_mps: 0.025244, loss_cps: 0.042094
[12:42:04.522] iteration 5393: total_loss: 0.137847, loss_sup: 0.036875, loss_mps: 0.036233, loss_cps: 0.064739
[12:42:04.670] iteration 5394: total_loss: 0.219141, loss_sup: 0.089082, loss_mps: 0.044276, loss_cps: 0.085783
[12:42:04.816] iteration 5395: total_loss: 0.200079, loss_sup: 0.124872, loss_mps: 0.027008, loss_cps: 0.048199
[12:42:04.962] iteration 5396: total_loss: 0.266251, loss_sup: 0.182689, loss_mps: 0.029602, loss_cps: 0.053961
[12:42:05.109] iteration 5397: total_loss: 0.248809, loss_sup: 0.170890, loss_mps: 0.028821, loss_cps: 0.049098
[12:42:05.254] iteration 5398: total_loss: 0.184930, loss_sup: 0.091957, loss_mps: 0.032630, loss_cps: 0.060343
[12:42:05.400] iteration 5399: total_loss: 0.230322, loss_sup: 0.124736, loss_mps: 0.035964, loss_cps: 0.069622
[12:42:05.547] iteration 5400: total_loss: 0.163718, loss_sup: 0.072356, loss_mps: 0.033161, loss_cps: 0.058201
[12:42:05.548] Evaluation Started ==>
[12:42:16.941] ==> valid iteration 5400: unet metrics: {'dc': 0.5916143412901826, 'jc': 0.4707976404614907, 'pre': 0.6625546857343613, 'hd': 6.016672282826447}, ynet metrics: {'dc': 0.5691132934267465, 'jc': 0.4444209165559002, 'pre': 0.6736526031141231, 'hd': 6.401425360384055}.
[12:42:16.943] Evaluation Finished!⏹️
[12:42:17.094] iteration 5401: total_loss: 0.099375, loss_sup: 0.046069, loss_mps: 0.021396, loss_cps: 0.031910
[12:42:17.242] iteration 5402: total_loss: 0.158226, loss_sup: 0.081956, loss_mps: 0.028192, loss_cps: 0.048078
[12:42:17.388] iteration 5403: total_loss: 0.227787, loss_sup: 0.107117, loss_mps: 0.041173, loss_cps: 0.079497
[12:42:17.533] iteration 5404: total_loss: 0.133626, loss_sup: 0.042488, loss_mps: 0.031668, loss_cps: 0.059470
[12:42:17.678] iteration 5405: total_loss: 0.316478, loss_sup: 0.243339, loss_mps: 0.027834, loss_cps: 0.045304
[12:42:17.824] iteration 5406: total_loss: 0.156900, loss_sup: 0.064953, loss_mps: 0.032652, loss_cps: 0.059296
[12:42:17.971] iteration 5407: total_loss: 0.305406, loss_sup: 0.164834, loss_mps: 0.047666, loss_cps: 0.092906
[12:42:18.118] iteration 5408: total_loss: 0.178742, loss_sup: 0.076173, loss_mps: 0.036371, loss_cps: 0.066198
[12:42:18.263] iteration 5409: total_loss: 0.152562, loss_sup: 0.026389, loss_mps: 0.042102, loss_cps: 0.084071
[12:42:18.410] iteration 5410: total_loss: 0.153140, loss_sup: 0.057905, loss_mps: 0.034947, loss_cps: 0.060288
[12:42:18.556] iteration 5411: total_loss: 0.405654, loss_sup: 0.319212, loss_mps: 0.032401, loss_cps: 0.054041
[12:42:18.704] iteration 5412: total_loss: 0.163453, loss_sup: 0.078619, loss_mps: 0.032240, loss_cps: 0.052595
[12:42:18.852] iteration 5413: total_loss: 0.484530, loss_sup: 0.337475, loss_mps: 0.048821, loss_cps: 0.098234
[12:42:18.998] iteration 5414: total_loss: 0.320655, loss_sup: 0.209778, loss_mps: 0.039844, loss_cps: 0.071033
[12:42:19.143] iteration 5415: total_loss: 0.444319, loss_sup: 0.339542, loss_mps: 0.037729, loss_cps: 0.067048
[12:42:19.288] iteration 5416: total_loss: 0.170456, loss_sup: 0.087974, loss_mps: 0.029729, loss_cps: 0.052753
[12:42:19.434] iteration 5417: total_loss: 0.137501, loss_sup: 0.058829, loss_mps: 0.028344, loss_cps: 0.050327
[12:42:19.582] iteration 5418: total_loss: 0.238427, loss_sup: 0.131041, loss_mps: 0.037403, loss_cps: 0.069982
[12:42:19.727] iteration 5419: total_loss: 0.267131, loss_sup: 0.160501, loss_mps: 0.037346, loss_cps: 0.069283
[12:42:19.872] iteration 5420: total_loss: 0.231519, loss_sup: 0.124051, loss_mps: 0.037661, loss_cps: 0.069806
[12:42:20.018] iteration 5421: total_loss: 0.169056, loss_sup: 0.074579, loss_mps: 0.035010, loss_cps: 0.059467
[12:42:20.163] iteration 5422: total_loss: 0.164667, loss_sup: 0.091451, loss_mps: 0.027366, loss_cps: 0.045850
[12:42:20.308] iteration 5423: total_loss: 0.234373, loss_sup: 0.134010, loss_mps: 0.035797, loss_cps: 0.064566
[12:42:20.453] iteration 5424: total_loss: 0.201802, loss_sup: 0.094204, loss_mps: 0.037667, loss_cps: 0.069931
[12:42:20.599] iteration 5425: total_loss: 0.494330, loss_sup: 0.339683, loss_mps: 0.052505, loss_cps: 0.102142
[12:42:20.744] iteration 5426: total_loss: 0.175904, loss_sup: 0.050932, loss_mps: 0.042497, loss_cps: 0.082476
[12:42:20.889] iteration 5427: total_loss: 0.324448, loss_sup: 0.190499, loss_mps: 0.045113, loss_cps: 0.088836
[12:42:21.034] iteration 5428: total_loss: 0.240862, loss_sup: 0.146480, loss_mps: 0.034749, loss_cps: 0.059633
[12:42:21.179] iteration 5429: total_loss: 0.273742, loss_sup: 0.182243, loss_mps: 0.032532, loss_cps: 0.058967
[12:42:21.324] iteration 5430: total_loss: 0.186304, loss_sup: 0.108922, loss_mps: 0.029490, loss_cps: 0.047893
[12:42:21.469] iteration 5431: total_loss: 0.299096, loss_sup: 0.167620, loss_mps: 0.045052, loss_cps: 0.086424
[12:42:21.614] iteration 5432: total_loss: 0.132706, loss_sup: 0.030416, loss_mps: 0.036963, loss_cps: 0.065326
[12:42:21.760] iteration 5433: total_loss: 0.227140, loss_sup: 0.119817, loss_mps: 0.038682, loss_cps: 0.068640
[12:42:21.820] iteration 5434: total_loss: 0.300101, loss_sup: 0.199406, loss_mps: 0.034283, loss_cps: 0.066412
[12:42:23.008] iteration 5435: total_loss: 0.237991, loss_sup: 0.155060, loss_mps: 0.029821, loss_cps: 0.053109
[12:42:23.158] iteration 5436: total_loss: 0.171670, loss_sup: 0.089612, loss_mps: 0.028913, loss_cps: 0.053145
[12:42:23.305] iteration 5437: total_loss: 0.310474, loss_sup: 0.199899, loss_mps: 0.037493, loss_cps: 0.073082
[12:42:23.454] iteration 5438: total_loss: 0.298961, loss_sup: 0.219550, loss_mps: 0.028888, loss_cps: 0.050523
[12:42:23.601] iteration 5439: total_loss: 0.128214, loss_sup: 0.041291, loss_mps: 0.032170, loss_cps: 0.054754
[12:42:23.747] iteration 5440: total_loss: 0.365527, loss_sup: 0.246111, loss_mps: 0.041411, loss_cps: 0.078005
[12:42:23.893] iteration 5441: total_loss: 0.177577, loss_sup: 0.083441, loss_mps: 0.034680, loss_cps: 0.059456
[12:42:24.044] iteration 5442: total_loss: 0.396119, loss_sup: 0.289043, loss_mps: 0.037687, loss_cps: 0.069389
[12:42:24.190] iteration 5443: total_loss: 0.121632, loss_sup: 0.041726, loss_mps: 0.030047, loss_cps: 0.049858
[12:42:24.337] iteration 5444: total_loss: 0.250791, loss_sup: 0.150309, loss_mps: 0.035092, loss_cps: 0.065390
[12:42:24.483] iteration 5445: total_loss: 0.195733, loss_sup: 0.066188, loss_mps: 0.042197, loss_cps: 0.087347
[12:42:24.630] iteration 5446: total_loss: 0.259276, loss_sup: 0.131388, loss_mps: 0.045154, loss_cps: 0.082734
[12:42:24.776] iteration 5447: total_loss: 0.219616, loss_sup: 0.104678, loss_mps: 0.040789, loss_cps: 0.074149
[12:42:24.923] iteration 5448: total_loss: 0.325954, loss_sup: 0.220533, loss_mps: 0.036350, loss_cps: 0.069071
[12:42:25.070] iteration 5449: total_loss: 0.241862, loss_sup: 0.129576, loss_mps: 0.038825, loss_cps: 0.073461
[12:42:25.216] iteration 5450: total_loss: 0.224819, loss_sup: 0.109150, loss_mps: 0.040497, loss_cps: 0.075172
[12:42:25.362] iteration 5451: total_loss: 0.194698, loss_sup: 0.091708, loss_mps: 0.037813, loss_cps: 0.065177
[12:42:25.509] iteration 5452: total_loss: 0.267730, loss_sup: 0.133970, loss_mps: 0.045951, loss_cps: 0.087809
[12:42:25.657] iteration 5453: total_loss: 0.278707, loss_sup: 0.182016, loss_mps: 0.034336, loss_cps: 0.062356
[12:42:25.805] iteration 5454: total_loss: 0.245373, loss_sup: 0.146070, loss_mps: 0.035514, loss_cps: 0.063788
[12:42:25.952] iteration 5455: total_loss: 0.131883, loss_sup: 0.059933, loss_mps: 0.025917, loss_cps: 0.046034
[12:42:26.098] iteration 5456: total_loss: 0.216133, loss_sup: 0.124787, loss_mps: 0.031735, loss_cps: 0.059611
[12:42:26.245] iteration 5457: total_loss: 0.184060, loss_sup: 0.079631, loss_mps: 0.036157, loss_cps: 0.068272
[12:42:26.393] iteration 5458: total_loss: 0.329671, loss_sup: 0.209634, loss_mps: 0.040979, loss_cps: 0.079058
[12:42:26.539] iteration 5459: total_loss: 0.296814, loss_sup: 0.171804, loss_mps: 0.041975, loss_cps: 0.083035
[12:42:26.685] iteration 5460: total_loss: 0.237754, loss_sup: 0.138255, loss_mps: 0.035622, loss_cps: 0.063877
[12:42:26.832] iteration 5461: total_loss: 0.147177, loss_sup: 0.070672, loss_mps: 0.028898, loss_cps: 0.047607
[12:42:26.979] iteration 5462: total_loss: 0.170192, loss_sup: 0.066822, loss_mps: 0.035966, loss_cps: 0.067404
[12:42:27.126] iteration 5463: total_loss: 0.167080, loss_sup: 0.054025, loss_mps: 0.039233, loss_cps: 0.073823
[12:42:27.273] iteration 5464: total_loss: 0.153694, loss_sup: 0.068120, loss_mps: 0.030627, loss_cps: 0.054948
[12:42:27.419] iteration 5465: total_loss: 0.263861, loss_sup: 0.176407, loss_mps: 0.031604, loss_cps: 0.055850
[12:42:27.567] iteration 5466: total_loss: 0.384124, loss_sup: 0.281609, loss_mps: 0.035341, loss_cps: 0.067174
[12:42:27.713] iteration 5467: total_loss: 0.273052, loss_sup: 0.185515, loss_mps: 0.031174, loss_cps: 0.056363
[12:42:27.860] iteration 5468: total_loss: 0.222532, loss_sup: 0.124542, loss_mps: 0.034004, loss_cps: 0.063986
[12:42:28.006] iteration 5469: total_loss: 0.500853, loss_sup: 0.408004, loss_mps: 0.033312, loss_cps: 0.059537
[12:42:28.154] iteration 5470: total_loss: 0.261638, loss_sup: 0.142849, loss_mps: 0.040054, loss_cps: 0.078735
[12:42:28.302] iteration 5471: total_loss: 0.618439, loss_sup: 0.508103, loss_mps: 0.039205, loss_cps: 0.071132
[12:42:28.450] iteration 5472: total_loss: 0.125233, loss_sup: 0.046990, loss_mps: 0.029676, loss_cps: 0.048567
[12:42:28.597] iteration 5473: total_loss: 0.200527, loss_sup: 0.120797, loss_mps: 0.030728, loss_cps: 0.049001
[12:42:28.744] iteration 5474: total_loss: 0.265773, loss_sup: 0.154587, loss_mps: 0.039648, loss_cps: 0.071538
[12:42:28.890] iteration 5475: total_loss: 0.155898, loss_sup: 0.070672, loss_mps: 0.031797, loss_cps: 0.053429
[12:42:29.037] iteration 5476: total_loss: 0.167346, loss_sup: 0.064940, loss_mps: 0.037114, loss_cps: 0.065292
[12:42:29.186] iteration 5477: total_loss: 0.164739, loss_sup: 0.077608, loss_mps: 0.032616, loss_cps: 0.054514
[12:42:29.333] iteration 5478: total_loss: 0.301855, loss_sup: 0.205661, loss_mps: 0.034767, loss_cps: 0.061427
[12:42:29.484] iteration 5479: total_loss: 0.417100, loss_sup: 0.312216, loss_mps: 0.038337, loss_cps: 0.066547
[12:42:29.631] iteration 5480: total_loss: 0.302205, loss_sup: 0.195965, loss_mps: 0.038179, loss_cps: 0.068061
[12:42:29.778] iteration 5481: total_loss: 0.144628, loss_sup: 0.063978, loss_mps: 0.029631, loss_cps: 0.051020
[12:42:29.924] iteration 5482: total_loss: 0.162954, loss_sup: 0.088124, loss_mps: 0.029119, loss_cps: 0.045710
[12:42:30.071] iteration 5483: total_loss: 0.158959, loss_sup: 0.058889, loss_mps: 0.035468, loss_cps: 0.064601
[12:42:30.219] iteration 5484: total_loss: 0.196298, loss_sup: 0.115695, loss_mps: 0.030236, loss_cps: 0.050367
[12:42:30.365] iteration 5485: total_loss: 0.294022, loss_sup: 0.216623, loss_mps: 0.029108, loss_cps: 0.048291
[12:42:30.511] iteration 5486: total_loss: 0.230394, loss_sup: 0.151684, loss_mps: 0.030228, loss_cps: 0.048481
[12:42:30.657] iteration 5487: total_loss: 0.222787, loss_sup: 0.106107, loss_mps: 0.040366, loss_cps: 0.076314
[12:42:30.804] iteration 5488: total_loss: 0.362849, loss_sup: 0.209968, loss_mps: 0.051374, loss_cps: 0.101507
[12:42:30.951] iteration 5489: total_loss: 0.188123, loss_sup: 0.085406, loss_mps: 0.036666, loss_cps: 0.066050
[12:42:31.099] iteration 5490: total_loss: 0.209115, loss_sup: 0.090089, loss_mps: 0.039607, loss_cps: 0.079419
[12:42:31.246] iteration 5491: total_loss: 0.166408, loss_sup: 0.090062, loss_mps: 0.028341, loss_cps: 0.048006
[12:42:31.392] iteration 5492: total_loss: 0.121862, loss_sup: 0.034067, loss_mps: 0.031902, loss_cps: 0.055893
[12:42:31.541] iteration 5493: total_loss: 0.252578, loss_sup: 0.086332, loss_mps: 0.054916, loss_cps: 0.111330
[12:42:31.687] iteration 5494: total_loss: 0.210814, loss_sup: 0.122227, loss_mps: 0.031816, loss_cps: 0.056771
[12:42:31.834] iteration 5495: total_loss: 0.210496, loss_sup: 0.147357, loss_mps: 0.024733, loss_cps: 0.038407
[12:42:31.983] iteration 5496: total_loss: 0.197913, loss_sup: 0.086710, loss_mps: 0.038852, loss_cps: 0.072351
[12:42:32.129] iteration 5497: total_loss: 0.140440, loss_sup: 0.038375, loss_mps: 0.035489, loss_cps: 0.066575
[12:42:32.277] iteration 5498: total_loss: 0.238784, loss_sup: 0.143272, loss_mps: 0.033999, loss_cps: 0.061513
[12:42:32.425] iteration 5499: total_loss: 0.191059, loss_sup: 0.105958, loss_mps: 0.030840, loss_cps: 0.054261
[12:42:32.571] iteration 5500: total_loss: 0.519677, loss_sup: 0.416603, loss_mps: 0.036797, loss_cps: 0.066276
[12:42:32.572] Evaluation Started ==>
[12:42:43.935] ==> valid iteration 5500: unet metrics: {'dc': 0.6142718791345543, 'jc': 0.4879963948471656, 'pre': 0.669426961000355, 'hd': 6.395845196560775}, ynet metrics: {'dc': 0.5142236997624047, 'jc': 0.39703844956217643, 'pre': 0.6256719137074759, 'hd': 6.688030029534212}.
[12:42:43.937] Evaluation Finished!⏹️
[12:42:44.090] iteration 5501: total_loss: 0.403704, loss_sup: 0.337906, loss_mps: 0.025191, loss_cps: 0.040607
[12:42:44.239] iteration 5502: total_loss: 0.161496, loss_sup: 0.077120, loss_mps: 0.029892, loss_cps: 0.054484
[12:42:44.387] iteration 5503: total_loss: 0.181478, loss_sup: 0.099459, loss_mps: 0.029517, loss_cps: 0.052503
[12:42:44.533] iteration 5504: total_loss: 0.256510, loss_sup: 0.134371, loss_mps: 0.042557, loss_cps: 0.079582
[12:42:44.678] iteration 5505: total_loss: 0.323934, loss_sup: 0.222636, loss_mps: 0.036265, loss_cps: 0.065033
[12:42:44.823] iteration 5506: total_loss: 0.302697, loss_sup: 0.218183, loss_mps: 0.031617, loss_cps: 0.052897
[12:42:44.971] iteration 5507: total_loss: 0.081076, loss_sup: 0.019797, loss_mps: 0.025118, loss_cps: 0.036161
[12:42:45.117] iteration 5508: total_loss: 0.195783, loss_sup: 0.067194, loss_mps: 0.045100, loss_cps: 0.083489
[12:42:45.263] iteration 5509: total_loss: 0.276001, loss_sup: 0.156142, loss_mps: 0.041572, loss_cps: 0.078288
[12:42:45.409] iteration 5510: total_loss: 0.299327, loss_sup: 0.180185, loss_mps: 0.042010, loss_cps: 0.077132
[12:42:45.555] iteration 5511: total_loss: 0.242035, loss_sup: 0.116140, loss_mps: 0.042463, loss_cps: 0.083432
[12:42:45.704] iteration 5512: total_loss: 0.226314, loss_sup: 0.133487, loss_mps: 0.032772, loss_cps: 0.060055
[12:42:45.849] iteration 5513: total_loss: 0.090022, loss_sup: 0.029927, loss_mps: 0.024405, loss_cps: 0.035691
[12:42:45.994] iteration 5514: total_loss: 0.178916, loss_sup: 0.083569, loss_mps: 0.034522, loss_cps: 0.060825
[12:42:46.140] iteration 5515: total_loss: 0.192556, loss_sup: 0.083603, loss_mps: 0.038357, loss_cps: 0.070595
[12:42:46.286] iteration 5516: total_loss: 0.136607, loss_sup: 0.040523, loss_mps: 0.035395, loss_cps: 0.060689
[12:42:46.432] iteration 5517: total_loss: 0.161334, loss_sup: 0.077883, loss_mps: 0.031262, loss_cps: 0.052189
[12:42:46.578] iteration 5518: total_loss: 0.295664, loss_sup: 0.184534, loss_mps: 0.037362, loss_cps: 0.073768
[12:42:46.723] iteration 5519: total_loss: 0.154975, loss_sup: 0.056635, loss_mps: 0.033611, loss_cps: 0.064730
[12:42:46.868] iteration 5520: total_loss: 0.279003, loss_sup: 0.201639, loss_mps: 0.029053, loss_cps: 0.048310
[12:42:47.013] iteration 5521: total_loss: 0.092997, loss_sup: 0.024942, loss_mps: 0.025863, loss_cps: 0.042192
[12:42:47.159] iteration 5522: total_loss: 0.333847, loss_sup: 0.195899, loss_mps: 0.046600, loss_cps: 0.091348
[12:42:47.304] iteration 5523: total_loss: 0.312726, loss_sup: 0.175483, loss_mps: 0.045485, loss_cps: 0.091758
[12:42:47.449] iteration 5524: total_loss: 0.374549, loss_sup: 0.286370, loss_mps: 0.032112, loss_cps: 0.056067
[12:42:47.597] iteration 5525: total_loss: 0.183121, loss_sup: 0.089020, loss_mps: 0.033642, loss_cps: 0.060460
[12:42:47.742] iteration 5526: total_loss: 0.208156, loss_sup: 0.097500, loss_mps: 0.038426, loss_cps: 0.072230
[12:42:47.887] iteration 5527: total_loss: 0.289062, loss_sup: 0.162136, loss_mps: 0.042383, loss_cps: 0.084543
[12:42:48.033] iteration 5528: total_loss: 0.254259, loss_sup: 0.162585, loss_mps: 0.031972, loss_cps: 0.059702
[12:42:48.178] iteration 5529: total_loss: 0.170260, loss_sup: 0.085255, loss_mps: 0.032916, loss_cps: 0.052090
[12:42:48.323] iteration 5530: total_loss: 0.560017, loss_sup: 0.453317, loss_mps: 0.036022, loss_cps: 0.070677
[12:42:48.469] iteration 5531: total_loss: 0.171534, loss_sup: 0.061885, loss_mps: 0.037761, loss_cps: 0.071888
[12:42:48.615] iteration 5532: total_loss: 0.251158, loss_sup: 0.154788, loss_mps: 0.033324, loss_cps: 0.063046
[12:42:48.761] iteration 5533: total_loss: 0.297180, loss_sup: 0.203711, loss_mps: 0.033714, loss_cps: 0.059755
[12:42:48.906] iteration 5534: total_loss: 0.184854, loss_sup: 0.106261, loss_mps: 0.028343, loss_cps: 0.050250
[12:42:49.053] iteration 5535: total_loss: 0.141962, loss_sup: 0.063799, loss_mps: 0.030290, loss_cps: 0.047873
[12:42:49.198] iteration 5536: total_loss: 0.279399, loss_sup: 0.168229, loss_mps: 0.039382, loss_cps: 0.071788
[12:42:49.343] iteration 5537: total_loss: 0.131632, loss_sup: 0.061010, loss_mps: 0.027175, loss_cps: 0.043447
[12:42:49.489] iteration 5538: total_loss: 0.267028, loss_sup: 0.178874, loss_mps: 0.031391, loss_cps: 0.056763
[12:42:49.637] iteration 5539: total_loss: 0.508915, loss_sup: 0.399508, loss_mps: 0.039792, loss_cps: 0.069616
[12:42:49.782] iteration 5540: total_loss: 0.172834, loss_sup: 0.088736, loss_mps: 0.031278, loss_cps: 0.052820
[12:42:49.927] iteration 5541: total_loss: 0.149283, loss_sup: 0.065532, loss_mps: 0.032239, loss_cps: 0.051512
[12:42:50.073] iteration 5542: total_loss: 0.330657, loss_sup: 0.195261, loss_mps: 0.046637, loss_cps: 0.088758
[12:42:50.218] iteration 5543: total_loss: 0.755274, loss_sup: 0.650000, loss_mps: 0.037797, loss_cps: 0.067477
[12:42:50.363] iteration 5544: total_loss: 0.217362, loss_sup: 0.112477, loss_mps: 0.037967, loss_cps: 0.066918
[12:42:50.510] iteration 5545: total_loss: 0.104536, loss_sup: 0.020808, loss_mps: 0.031253, loss_cps: 0.052475
[12:42:50.657] iteration 5546: total_loss: 0.346822, loss_sup: 0.219494, loss_mps: 0.043821, loss_cps: 0.083507
[12:42:50.802] iteration 5547: total_loss: 0.221702, loss_sup: 0.109975, loss_mps: 0.038355, loss_cps: 0.073373
[12:42:50.948] iteration 5548: total_loss: 0.206137, loss_sup: 0.125051, loss_mps: 0.030961, loss_cps: 0.050125
[12:42:51.093] iteration 5549: total_loss: 0.132068, loss_sup: 0.035587, loss_mps: 0.034087, loss_cps: 0.062395
[12:42:51.239] iteration 5550: total_loss: 0.454562, loss_sup: 0.300870, loss_mps: 0.053639, loss_cps: 0.100053
[12:42:51.384] iteration 5551: total_loss: 0.159080, loss_sup: 0.060230, loss_mps: 0.036090, loss_cps: 0.062760
[12:42:51.530] iteration 5552: total_loss: 0.259139, loss_sup: 0.154878, loss_mps: 0.036549, loss_cps: 0.067712
[12:42:51.676] iteration 5553: total_loss: 0.186708, loss_sup: 0.091132, loss_mps: 0.034951, loss_cps: 0.060625
[12:42:51.823] iteration 5554: total_loss: 0.282036, loss_sup: 0.169427, loss_mps: 0.041017, loss_cps: 0.071592
[12:42:51.969] iteration 5555: total_loss: 0.199039, loss_sup: 0.091688, loss_mps: 0.039075, loss_cps: 0.068276
[12:42:52.114] iteration 5556: total_loss: 0.209698, loss_sup: 0.116028, loss_mps: 0.035312, loss_cps: 0.058358
[12:42:52.259] iteration 5557: total_loss: 0.181134, loss_sup: 0.066340, loss_mps: 0.040828, loss_cps: 0.073966
[12:42:52.405] iteration 5558: total_loss: 0.238911, loss_sup: 0.106364, loss_mps: 0.046953, loss_cps: 0.085594
[12:42:52.550] iteration 5559: total_loss: 0.397644, loss_sup: 0.280679, loss_mps: 0.041452, loss_cps: 0.075512
[12:42:52.696] iteration 5560: total_loss: 0.331057, loss_sup: 0.228746, loss_mps: 0.035826, loss_cps: 0.066484
[12:42:52.841] iteration 5561: total_loss: 0.238230, loss_sup: 0.103992, loss_mps: 0.046130, loss_cps: 0.088108
[12:42:52.987] iteration 5562: total_loss: 0.144334, loss_sup: 0.072268, loss_mps: 0.027329, loss_cps: 0.044738
[12:42:53.132] iteration 5563: total_loss: 0.155696, loss_sup: 0.053350, loss_mps: 0.038575, loss_cps: 0.063771
[12:42:53.278] iteration 5564: total_loss: 0.275534, loss_sup: 0.198532, loss_mps: 0.028301, loss_cps: 0.048700
[12:42:53.423] iteration 5565: total_loss: 0.159300, loss_sup: 0.053192, loss_mps: 0.039101, loss_cps: 0.067008
[12:42:53.569] iteration 5566: total_loss: 0.181161, loss_sup: 0.101125, loss_mps: 0.029677, loss_cps: 0.050358
[12:42:53.715] iteration 5567: total_loss: 0.166829, loss_sup: 0.086128, loss_mps: 0.030321, loss_cps: 0.050381
[12:42:53.860] iteration 5568: total_loss: 0.202608, loss_sup: 0.143654, loss_mps: 0.022329, loss_cps: 0.036625
[12:42:54.006] iteration 5569: total_loss: 0.388557, loss_sup: 0.295401, loss_mps: 0.033497, loss_cps: 0.059659
[12:42:54.152] iteration 5570: total_loss: 0.128670, loss_sup: 0.052960, loss_mps: 0.027307, loss_cps: 0.048403
[12:42:54.298] iteration 5571: total_loss: 0.253582, loss_sup: 0.137268, loss_mps: 0.039762, loss_cps: 0.076553
[12:42:54.444] iteration 5572: total_loss: 0.172694, loss_sup: 0.083328, loss_mps: 0.032155, loss_cps: 0.057210
[12:42:54.590] iteration 5573: total_loss: 0.414085, loss_sup: 0.267160, loss_mps: 0.046463, loss_cps: 0.100462
[12:42:54.736] iteration 5574: total_loss: 0.324887, loss_sup: 0.185151, loss_mps: 0.046792, loss_cps: 0.092944
[12:42:54.881] iteration 5575: total_loss: 0.250494, loss_sup: 0.097875, loss_mps: 0.050853, loss_cps: 0.101766
[12:42:55.027] iteration 5576: total_loss: 0.170423, loss_sup: 0.076654, loss_mps: 0.033109, loss_cps: 0.060660
[12:42:55.172] iteration 5577: total_loss: 0.165423, loss_sup: 0.069572, loss_mps: 0.034761, loss_cps: 0.061089
[12:42:55.318] iteration 5578: total_loss: 0.147489, loss_sup: 0.055326, loss_mps: 0.034256, loss_cps: 0.057908
[12:42:55.463] iteration 5579: total_loss: 0.303628, loss_sup: 0.213414, loss_mps: 0.031005, loss_cps: 0.059209
[12:42:55.609] iteration 5580: total_loss: 0.328080, loss_sup: 0.214283, loss_mps: 0.040200, loss_cps: 0.073596
[12:42:55.754] iteration 5581: total_loss: 0.435949, loss_sup: 0.264264, loss_mps: 0.054917, loss_cps: 0.116767
[12:42:55.901] iteration 5582: total_loss: 0.212248, loss_sup: 0.077283, loss_mps: 0.045594, loss_cps: 0.089370
[12:42:56.047] iteration 5583: total_loss: 0.325160, loss_sup: 0.163060, loss_mps: 0.055655, loss_cps: 0.106445
[12:42:56.192] iteration 5584: total_loss: 0.216625, loss_sup: 0.101645, loss_mps: 0.040641, loss_cps: 0.074339
[12:42:56.338] iteration 5585: total_loss: 0.234340, loss_sup: 0.121711, loss_mps: 0.039085, loss_cps: 0.073543
[12:42:56.483] iteration 5586: total_loss: 0.282910, loss_sup: 0.182551, loss_mps: 0.036568, loss_cps: 0.063790
[12:42:56.630] iteration 5587: total_loss: 0.179390, loss_sup: 0.085958, loss_mps: 0.033598, loss_cps: 0.059833
[12:42:56.775] iteration 5588: total_loss: 0.334617, loss_sup: 0.232412, loss_mps: 0.034880, loss_cps: 0.067326
[12:42:56.921] iteration 5589: total_loss: 0.259927, loss_sup: 0.181299, loss_mps: 0.030206, loss_cps: 0.048422
[12:42:57.067] iteration 5590: total_loss: 0.209003, loss_sup: 0.059857, loss_mps: 0.047905, loss_cps: 0.101240
[12:42:57.213] iteration 5591: total_loss: 0.155535, loss_sup: 0.066160, loss_mps: 0.033057, loss_cps: 0.056318
[12:42:57.360] iteration 5592: total_loss: 0.163512, loss_sup: 0.054854, loss_mps: 0.037369, loss_cps: 0.071290
[12:42:57.506] iteration 5593: total_loss: 0.169131, loss_sup: 0.073427, loss_mps: 0.033488, loss_cps: 0.062216
[12:42:57.652] iteration 5594: total_loss: 0.190174, loss_sup: 0.098299, loss_mps: 0.034151, loss_cps: 0.057723
[12:42:57.797] iteration 5595: total_loss: 0.144642, loss_sup: 0.085473, loss_mps: 0.024120, loss_cps: 0.035049
[12:42:57.943] iteration 5596: total_loss: 0.408700, loss_sup: 0.311390, loss_mps: 0.034643, loss_cps: 0.062667
[12:42:58.089] iteration 5597: total_loss: 0.229996, loss_sup: 0.143737, loss_mps: 0.031897, loss_cps: 0.054362
[12:42:58.235] iteration 5598: total_loss: 0.216219, loss_sup: 0.115245, loss_mps: 0.035076, loss_cps: 0.065898
[12:42:58.382] iteration 5599: total_loss: 0.155010, loss_sup: 0.078432, loss_mps: 0.029554, loss_cps: 0.047024
[12:42:58.528] iteration 5600: total_loss: 0.146793, loss_sup: 0.081352, loss_mps: 0.025226, loss_cps: 0.040215
[12:42:58.528] Evaluation Started ==>
[12:43:09.917] ==> valid iteration 5600: unet metrics: {'dc': 0.6250172538015294, 'jc': 0.4955598312087167, 'pre': 0.6921900986699562, 'hd': 6.39800166028064}, ynet metrics: {'dc': 0.5420368624526185, 'jc': 0.42061978377426823, 'pre': 0.6519900865781734, 'hd': 6.586663376312369}.
[12:43:09.919] Evaluation Finished!⏹️
[12:43:10.072] iteration 5601: total_loss: 0.271009, loss_sup: 0.150752, loss_mps: 0.042076, loss_cps: 0.078181
[12:43:10.225] iteration 5602: total_loss: 0.285564, loss_sup: 0.178259, loss_mps: 0.037969, loss_cps: 0.069337
[12:43:10.372] iteration 5603: total_loss: 0.247161, loss_sup: 0.155937, loss_mps: 0.033067, loss_cps: 0.058157
[12:43:10.517] iteration 5604: total_loss: 0.273215, loss_sup: 0.133405, loss_mps: 0.046613, loss_cps: 0.093196
[12:43:10.662] iteration 5605: total_loss: 0.168265, loss_sup: 0.064806, loss_mps: 0.036742, loss_cps: 0.066717
[12:43:10.808] iteration 5606: total_loss: 0.266598, loss_sup: 0.171237, loss_mps: 0.033710, loss_cps: 0.061651
[12:43:10.953] iteration 5607: total_loss: 0.158278, loss_sup: 0.080701, loss_mps: 0.028571, loss_cps: 0.049006
[12:43:11.098] iteration 5608: total_loss: 0.430695, loss_sup: 0.267189, loss_mps: 0.052551, loss_cps: 0.110955
[12:43:11.246] iteration 5609: total_loss: 0.134917, loss_sup: 0.041742, loss_mps: 0.032379, loss_cps: 0.060796
[12:43:11.391] iteration 5610: total_loss: 0.303947, loss_sup: 0.225738, loss_mps: 0.027714, loss_cps: 0.050496
[12:43:11.536] iteration 5611: total_loss: 0.207601, loss_sup: 0.091134, loss_mps: 0.039392, loss_cps: 0.077075
[12:43:11.682] iteration 5612: total_loss: 0.138972, loss_sup: 0.050388, loss_mps: 0.031240, loss_cps: 0.057344
[12:43:11.827] iteration 5613: total_loss: 0.211426, loss_sup: 0.069489, loss_mps: 0.047902, loss_cps: 0.094035
[12:43:11.972] iteration 5614: total_loss: 0.159152, loss_sup: 0.067949, loss_mps: 0.031149, loss_cps: 0.060054
[12:43:12.117] iteration 5615: total_loss: 0.274530, loss_sup: 0.163818, loss_mps: 0.037425, loss_cps: 0.073287
[12:43:12.264] iteration 5616: total_loss: 0.155178, loss_sup: 0.046847, loss_mps: 0.036044, loss_cps: 0.072287
[12:43:12.410] iteration 5617: total_loss: 0.186036, loss_sup: 0.080521, loss_mps: 0.036447, loss_cps: 0.069068
[12:43:12.555] iteration 5618: total_loss: 0.279463, loss_sup: 0.188529, loss_mps: 0.032455, loss_cps: 0.058479
[12:43:12.702] iteration 5619: total_loss: 0.239357, loss_sup: 0.161188, loss_mps: 0.028337, loss_cps: 0.049831
[12:43:12.847] iteration 5620: total_loss: 0.202298, loss_sup: 0.118897, loss_mps: 0.030993, loss_cps: 0.052408
[12:43:12.993] iteration 5621: total_loss: 0.082563, loss_sup: 0.013161, loss_mps: 0.025582, loss_cps: 0.043820
[12:43:13.138] iteration 5622: total_loss: 0.309170, loss_sup: 0.212547, loss_mps: 0.033763, loss_cps: 0.062860
[12:43:13.285] iteration 5623: total_loss: 0.101837, loss_sup: 0.012687, loss_mps: 0.030928, loss_cps: 0.058222
[12:43:13.431] iteration 5624: total_loss: 0.280975, loss_sup: 0.176640, loss_mps: 0.035977, loss_cps: 0.068359
[12:43:13.577] iteration 5625: total_loss: 0.301719, loss_sup: 0.178862, loss_mps: 0.043291, loss_cps: 0.079566
[12:43:13.722] iteration 5626: total_loss: 0.247713, loss_sup: 0.156063, loss_mps: 0.033983, loss_cps: 0.057667
[12:43:13.870] iteration 5627: total_loss: 0.204624, loss_sup: 0.099446, loss_mps: 0.037791, loss_cps: 0.067386
[12:43:14.015] iteration 5628: total_loss: 0.304380, loss_sup: 0.185451, loss_mps: 0.040435, loss_cps: 0.078495
[12:43:14.160] iteration 5629: total_loss: 0.284896, loss_sup: 0.144017, loss_mps: 0.046240, loss_cps: 0.094640
[12:43:14.306] iteration 5630: total_loss: 0.214075, loss_sup: 0.126432, loss_mps: 0.031966, loss_cps: 0.055678
[12:43:14.452] iteration 5631: total_loss: 0.305996, loss_sup: 0.214153, loss_mps: 0.032828, loss_cps: 0.059015
[12:43:14.597] iteration 5632: total_loss: 0.159824, loss_sup: 0.073577, loss_mps: 0.029998, loss_cps: 0.056249
[12:43:14.743] iteration 5633: total_loss: 0.348256, loss_sup: 0.167740, loss_mps: 0.058468, loss_cps: 0.122048
[12:43:14.888] iteration 5634: total_loss: 0.202054, loss_sup: 0.110290, loss_mps: 0.032540, loss_cps: 0.059224
[12:43:15.035] iteration 5635: total_loss: 0.129783, loss_sup: 0.039384, loss_mps: 0.030935, loss_cps: 0.059464
[12:43:15.181] iteration 5636: total_loss: 0.079125, loss_sup: 0.028330, loss_mps: 0.020010, loss_cps: 0.030784
[12:43:15.326] iteration 5637: total_loss: 0.376641, loss_sup: 0.267286, loss_mps: 0.036203, loss_cps: 0.073152
[12:43:15.472] iteration 5638: total_loss: 0.193383, loss_sup: 0.104216, loss_mps: 0.033200, loss_cps: 0.055967
[12:43:15.618] iteration 5639: total_loss: 0.133652, loss_sup: 0.045233, loss_mps: 0.032516, loss_cps: 0.055902
[12:43:15.763] iteration 5640: total_loss: 0.259329, loss_sup: 0.140535, loss_mps: 0.040876, loss_cps: 0.077918
[12:43:15.909] iteration 5641: total_loss: 0.188763, loss_sup: 0.075679, loss_mps: 0.039166, loss_cps: 0.073918
[12:43:16.054] iteration 5642: total_loss: 0.219734, loss_sup: 0.104797, loss_mps: 0.039632, loss_cps: 0.075306
[12:43:16.200] iteration 5643: total_loss: 0.135367, loss_sup: 0.043268, loss_mps: 0.032793, loss_cps: 0.059306
[12:43:16.346] iteration 5644: total_loss: 0.283989, loss_sup: 0.207129, loss_mps: 0.027922, loss_cps: 0.048938
[12:43:16.493] iteration 5645: total_loss: 0.165781, loss_sup: 0.080477, loss_mps: 0.030728, loss_cps: 0.054576
[12:43:16.640] iteration 5646: total_loss: 0.250273, loss_sup: 0.145782, loss_mps: 0.035015, loss_cps: 0.069476
[12:43:16.787] iteration 5647: total_loss: 0.271328, loss_sup: 0.178767, loss_mps: 0.031873, loss_cps: 0.060687
[12:43:16.932] iteration 5648: total_loss: 0.183714, loss_sup: 0.113924, loss_mps: 0.025966, loss_cps: 0.043824
[12:43:17.078] iteration 5649: total_loss: 0.248452, loss_sup: 0.150294, loss_mps: 0.034304, loss_cps: 0.063854
[12:43:17.223] iteration 5650: total_loss: 0.241153, loss_sup: 0.131375, loss_mps: 0.039167, loss_cps: 0.070610
[12:43:17.370] iteration 5651: total_loss: 0.262851, loss_sup: 0.166962, loss_mps: 0.036125, loss_cps: 0.059765
[12:43:17.515] iteration 5652: total_loss: 0.673495, loss_sup: 0.544232, loss_mps: 0.044930, loss_cps: 0.084334
[12:43:17.660] iteration 5653: total_loss: 0.399708, loss_sup: 0.282497, loss_mps: 0.041334, loss_cps: 0.075878
[12:43:17.806] iteration 5654: total_loss: 0.137786, loss_sup: 0.064219, loss_mps: 0.027484, loss_cps: 0.046083
[12:43:17.952] iteration 5655: total_loss: 0.172707, loss_sup: 0.062378, loss_mps: 0.038045, loss_cps: 0.072283
[12:43:18.097] iteration 5656: total_loss: 0.398039, loss_sup: 0.302784, loss_mps: 0.034011, loss_cps: 0.061243
[12:43:18.245] iteration 5657: total_loss: 0.404262, loss_sup: 0.252728, loss_mps: 0.051239, loss_cps: 0.100295
[12:43:18.390] iteration 5658: total_loss: 0.148451, loss_sup: 0.065620, loss_mps: 0.031209, loss_cps: 0.051623
[12:43:18.538] iteration 5659: total_loss: 0.180581, loss_sup: 0.050197, loss_mps: 0.044889, loss_cps: 0.085495
[12:43:18.684] iteration 5660: total_loss: 0.251616, loss_sup: 0.125780, loss_mps: 0.044213, loss_cps: 0.081623
[12:43:18.830] iteration 5661: total_loss: 0.189644, loss_sup: 0.075210, loss_mps: 0.041809, loss_cps: 0.072625
[12:43:18.975] iteration 5662: total_loss: 0.106992, loss_sup: 0.033223, loss_mps: 0.028774, loss_cps: 0.044994
[12:43:19.121] iteration 5663: total_loss: 0.155777, loss_sup: 0.033682, loss_mps: 0.042832, loss_cps: 0.079262
[12:43:19.267] iteration 5664: total_loss: 0.265920, loss_sup: 0.141640, loss_mps: 0.043979, loss_cps: 0.080302
[12:43:19.412] iteration 5665: total_loss: 0.288810, loss_sup: 0.167915, loss_mps: 0.041959, loss_cps: 0.078936
[12:43:19.558] iteration 5666: total_loss: 0.195688, loss_sup: 0.086054, loss_mps: 0.038655, loss_cps: 0.070978
[12:43:19.703] iteration 5667: total_loss: 0.130381, loss_sup: 0.043170, loss_mps: 0.031602, loss_cps: 0.055609
[12:43:19.848] iteration 5668: total_loss: 0.191327, loss_sup: 0.110013, loss_mps: 0.030292, loss_cps: 0.051021
[12:43:19.994] iteration 5669: total_loss: 0.164443, loss_sup: 0.066515, loss_mps: 0.036462, loss_cps: 0.061466
[12:43:20.141] iteration 5670: total_loss: 0.190367, loss_sup: 0.100878, loss_mps: 0.032840, loss_cps: 0.056648
[12:43:20.290] iteration 5671: total_loss: 0.175200, loss_sup: 0.099012, loss_mps: 0.029596, loss_cps: 0.046592
[12:43:20.437] iteration 5672: total_loss: 0.237844, loss_sup: 0.146520, loss_mps: 0.033009, loss_cps: 0.058315
[12:43:20.583] iteration 5673: total_loss: 0.113094, loss_sup: 0.052908, loss_mps: 0.024296, loss_cps: 0.035891
[12:43:20.729] iteration 5674: total_loss: 0.211823, loss_sup: 0.138752, loss_mps: 0.027961, loss_cps: 0.045110
[12:43:20.876] iteration 5675: total_loss: 0.170232, loss_sup: 0.074342, loss_mps: 0.034893, loss_cps: 0.060997
[12:43:21.023] iteration 5676: total_loss: 0.183262, loss_sup: 0.113883, loss_mps: 0.026415, loss_cps: 0.042964
[12:43:21.169] iteration 5677: total_loss: 0.184237, loss_sup: 0.105336, loss_mps: 0.028317, loss_cps: 0.050584
[12:43:21.316] iteration 5678: total_loss: 0.192908, loss_sup: 0.094794, loss_mps: 0.034462, loss_cps: 0.063652
[12:43:21.464] iteration 5679: total_loss: 0.248253, loss_sup: 0.162945, loss_mps: 0.031093, loss_cps: 0.054215
[12:43:21.616] iteration 5680: total_loss: 0.287975, loss_sup: 0.194813, loss_mps: 0.032225, loss_cps: 0.060937
[12:43:21.763] iteration 5681: total_loss: 0.164286, loss_sup: 0.104067, loss_mps: 0.023512, loss_cps: 0.036707
[12:43:21.910] iteration 5682: total_loss: 0.201924, loss_sup: 0.116826, loss_mps: 0.030512, loss_cps: 0.054586
[12:43:22.057] iteration 5683: total_loss: 0.225254, loss_sup: 0.140026, loss_mps: 0.031642, loss_cps: 0.053586
[12:43:22.204] iteration 5684: total_loss: 0.097582, loss_sup: 0.012143, loss_mps: 0.030114, loss_cps: 0.055325
[12:43:22.351] iteration 5685: total_loss: 0.290156, loss_sup: 0.188594, loss_mps: 0.036173, loss_cps: 0.065389
[12:43:22.498] iteration 5686: total_loss: 0.153036, loss_sup: 0.053507, loss_mps: 0.034685, loss_cps: 0.064844
[12:43:22.646] iteration 5687: total_loss: 0.318178, loss_sup: 0.215218, loss_mps: 0.034925, loss_cps: 0.068035
[12:43:22.792] iteration 5688: total_loss: 0.167906, loss_sup: 0.067592, loss_mps: 0.035102, loss_cps: 0.065211
[12:43:22.941] iteration 5689: total_loss: 0.225184, loss_sup: 0.124792, loss_mps: 0.034555, loss_cps: 0.065836
[12:43:23.088] iteration 5690: total_loss: 0.357688, loss_sup: 0.273983, loss_mps: 0.030267, loss_cps: 0.053439
[12:43:23.234] iteration 5691: total_loss: 0.351932, loss_sup: 0.186493, loss_mps: 0.053098, loss_cps: 0.112342
[12:43:23.381] iteration 5692: total_loss: 0.209753, loss_sup: 0.113272, loss_mps: 0.035265, loss_cps: 0.061217
[12:43:23.526] iteration 5693: total_loss: 0.474304, loss_sup: 0.367332, loss_mps: 0.037135, loss_cps: 0.069836
[12:43:23.672] iteration 5694: total_loss: 0.194394, loss_sup: 0.083290, loss_mps: 0.039223, loss_cps: 0.071881
[12:43:23.818] iteration 5695: total_loss: 0.169882, loss_sup: 0.089537, loss_mps: 0.029276, loss_cps: 0.051069
[12:43:23.964] iteration 5696: total_loss: 0.126366, loss_sup: 0.013935, loss_mps: 0.038699, loss_cps: 0.073732
[12:43:24.111] iteration 5697: total_loss: 0.145144, loss_sup: 0.055200, loss_mps: 0.031981, loss_cps: 0.057964
[12:43:24.258] iteration 5698: total_loss: 0.398788, loss_sup: 0.244519, loss_mps: 0.051733, loss_cps: 0.102536
[12:43:24.405] iteration 5699: total_loss: 0.139720, loss_sup: 0.052823, loss_mps: 0.031596, loss_cps: 0.055302
[12:43:24.551] iteration 5700: total_loss: 0.134279, loss_sup: 0.035236, loss_mps: 0.035883, loss_cps: 0.063160
[12:43:24.551] Evaluation Started ==>
[12:43:35.878] ==> valid iteration 5700: unet metrics: {'dc': 0.610183887005413, 'jc': 0.47817928383139374, 'pre': 0.6731439256867859, 'hd': 6.40643747118104}, ynet metrics: {'dc': 0.5706354390531947, 'jc': 0.4403160413043751, 'pre': 0.634184887384438, 'hd': 6.719562339141177}.
[12:43:35.880] Evaluation Finished!⏹️
[12:43:36.031] iteration 5701: total_loss: 0.279886, loss_sup: 0.203693, loss_mps: 0.030117, loss_cps: 0.046077
[12:43:36.180] iteration 5702: total_loss: 0.213051, loss_sup: 0.091375, loss_mps: 0.041807, loss_cps: 0.079869
[12:43:36.326] iteration 5703: total_loss: 0.184567, loss_sup: 0.113291, loss_mps: 0.025997, loss_cps: 0.045279
[12:43:36.472] iteration 5704: total_loss: 0.108919, loss_sup: 0.047928, loss_mps: 0.024736, loss_cps: 0.036255
[12:43:36.619] iteration 5705: total_loss: 0.294292, loss_sup: 0.182158, loss_mps: 0.039527, loss_cps: 0.072607
[12:43:36.766] iteration 5706: total_loss: 0.274718, loss_sup: 0.193284, loss_mps: 0.029417, loss_cps: 0.052017
[12:43:36.912] iteration 5707: total_loss: 0.191355, loss_sup: 0.097701, loss_mps: 0.034073, loss_cps: 0.059581
[12:43:37.057] iteration 5708: total_loss: 0.156303, loss_sup: 0.087958, loss_mps: 0.027671, loss_cps: 0.040673
[12:43:37.203] iteration 5709: total_loss: 0.221582, loss_sup: 0.142482, loss_mps: 0.029581, loss_cps: 0.049519
[12:43:37.348] iteration 5710: total_loss: 0.275147, loss_sup: 0.197061, loss_mps: 0.028640, loss_cps: 0.049445
[12:43:37.494] iteration 5711: total_loss: 0.192535, loss_sup: 0.134395, loss_mps: 0.022375, loss_cps: 0.035765
[12:43:37.639] iteration 5712: total_loss: 0.133881, loss_sup: 0.058390, loss_mps: 0.028302, loss_cps: 0.047189
[12:43:37.786] iteration 5713: total_loss: 0.275751, loss_sup: 0.173413, loss_mps: 0.035763, loss_cps: 0.066574
[12:43:37.931] iteration 5714: total_loss: 0.242282, loss_sup: 0.146421, loss_mps: 0.032043, loss_cps: 0.063817
[12:43:38.076] iteration 5715: total_loss: 0.372948, loss_sup: 0.210893, loss_mps: 0.051156, loss_cps: 0.110899
[12:43:38.222] iteration 5716: total_loss: 0.343795, loss_sup: 0.179549, loss_mps: 0.053370, loss_cps: 0.110876
[12:43:38.370] iteration 5717: total_loss: 0.253827, loss_sup: 0.120894, loss_mps: 0.045776, loss_cps: 0.087156
[12:43:38.515] iteration 5718: total_loss: 0.293206, loss_sup: 0.188145, loss_mps: 0.035574, loss_cps: 0.069487
[12:43:38.661] iteration 5719: total_loss: 0.128297, loss_sup: 0.044902, loss_mps: 0.030084, loss_cps: 0.053312
[12:43:38.806] iteration 5720: total_loss: 0.278601, loss_sup: 0.153592, loss_mps: 0.042124, loss_cps: 0.082884
[12:43:38.954] iteration 5721: total_loss: 0.121621, loss_sup: 0.035808, loss_mps: 0.030290, loss_cps: 0.055523
[12:43:39.100] iteration 5722: total_loss: 0.205515, loss_sup: 0.100894, loss_mps: 0.036047, loss_cps: 0.068574
[12:43:39.246] iteration 5723: total_loss: 0.218758, loss_sup: 0.134016, loss_mps: 0.029393, loss_cps: 0.055348
[12:43:39.392] iteration 5724: total_loss: 0.247220, loss_sup: 0.147306, loss_mps: 0.034842, loss_cps: 0.065072
[12:43:39.539] iteration 5725: total_loss: 0.127339, loss_sup: 0.049908, loss_mps: 0.027592, loss_cps: 0.049838
[12:43:39.685] iteration 5726: total_loss: 0.235066, loss_sup: 0.156576, loss_mps: 0.029076, loss_cps: 0.049414
[12:43:39.832] iteration 5727: total_loss: 0.194285, loss_sup: 0.108082, loss_mps: 0.031539, loss_cps: 0.054664
[12:43:39.977] iteration 5728: total_loss: 0.158860, loss_sup: 0.058712, loss_mps: 0.035426, loss_cps: 0.064723
[12:43:40.122] iteration 5729: total_loss: 0.165667, loss_sup: 0.099352, loss_mps: 0.025457, loss_cps: 0.040858
[12:43:40.271] iteration 5730: total_loss: 0.234240, loss_sup: 0.144917, loss_mps: 0.032293, loss_cps: 0.057030
[12:43:40.418] iteration 5731: total_loss: 0.110075, loss_sup: 0.039149, loss_mps: 0.026868, loss_cps: 0.044058
[12:43:40.564] iteration 5732: total_loss: 0.209957, loss_sup: 0.155172, loss_mps: 0.020636, loss_cps: 0.034148
[12:43:40.710] iteration 5733: total_loss: 0.270578, loss_sup: 0.164946, loss_mps: 0.037041, loss_cps: 0.068591
[12:43:40.857] iteration 5734: total_loss: 0.157958, loss_sup: 0.094761, loss_mps: 0.024381, loss_cps: 0.038817
[12:43:41.002] iteration 5735: total_loss: 0.208007, loss_sup: 0.106067, loss_mps: 0.034502, loss_cps: 0.067438
[12:43:41.147] iteration 5736: total_loss: 0.361890, loss_sup: 0.257515, loss_mps: 0.035001, loss_cps: 0.069373
[12:43:41.293] iteration 5737: total_loss: 0.185249, loss_sup: 0.067064, loss_mps: 0.040369, loss_cps: 0.077816
[12:43:41.439] iteration 5738: total_loss: 0.189721, loss_sup: 0.061933, loss_mps: 0.042048, loss_cps: 0.085739
[12:43:41.584] iteration 5739: total_loss: 0.294684, loss_sup: 0.207296, loss_mps: 0.030580, loss_cps: 0.056808
[12:43:41.732] iteration 5740: total_loss: 0.139592, loss_sup: 0.040939, loss_mps: 0.035103, loss_cps: 0.063550
[12:43:41.879] iteration 5741: total_loss: 0.426076, loss_sup: 0.337731, loss_mps: 0.032072, loss_cps: 0.056274
[12:43:42.025] iteration 5742: total_loss: 0.199182, loss_sup: 0.065107, loss_mps: 0.045865, loss_cps: 0.088210
[12:43:42.171] iteration 5743: total_loss: 0.219923, loss_sup: 0.108142, loss_mps: 0.039188, loss_cps: 0.072593
[12:43:42.317] iteration 5744: total_loss: 0.177164, loss_sup: 0.077847, loss_mps: 0.033391, loss_cps: 0.065927
[12:43:42.467] iteration 5745: total_loss: 0.352060, loss_sup: 0.242877, loss_mps: 0.036885, loss_cps: 0.072299
[12:43:42.614] iteration 5746: total_loss: 0.298914, loss_sup: 0.225199, loss_mps: 0.026600, loss_cps: 0.047115
[12:43:42.761] iteration 5747: total_loss: 0.295151, loss_sup: 0.177865, loss_mps: 0.040394, loss_cps: 0.076892
[12:43:42.908] iteration 5748: total_loss: 0.268638, loss_sup: 0.102850, loss_mps: 0.053292, loss_cps: 0.112497
[12:43:43.054] iteration 5749: total_loss: 0.269130, loss_sup: 0.135628, loss_mps: 0.043166, loss_cps: 0.090335
[12:43:43.201] iteration 5750: total_loss: 0.248760, loss_sup: 0.156788, loss_mps: 0.032552, loss_cps: 0.059420
[12:43:43.347] iteration 5751: total_loss: 0.193295, loss_sup: 0.091607, loss_mps: 0.036148, loss_cps: 0.065539
[12:43:43.493] iteration 5752: total_loss: 0.248045, loss_sup: 0.155506, loss_mps: 0.031607, loss_cps: 0.060933
[12:43:43.640] iteration 5753: total_loss: 0.143226, loss_sup: 0.053031, loss_mps: 0.030835, loss_cps: 0.059360
[12:43:43.786] iteration 5754: total_loss: 0.142363, loss_sup: 0.060841, loss_mps: 0.029482, loss_cps: 0.052040
[12:43:43.937] iteration 5755: total_loss: 0.177452, loss_sup: 0.063499, loss_mps: 0.038542, loss_cps: 0.075411
[12:43:44.083] iteration 5756: total_loss: 0.241356, loss_sup: 0.139803, loss_mps: 0.034895, loss_cps: 0.066658
[12:43:44.228] iteration 5757: total_loss: 0.354035, loss_sup: 0.167899, loss_mps: 0.060196, loss_cps: 0.125940
[12:43:44.374] iteration 5758: total_loss: 0.414292, loss_sup: 0.262520, loss_mps: 0.050442, loss_cps: 0.101330
[12:43:44.521] iteration 5759: total_loss: 0.152067, loss_sup: 0.057945, loss_mps: 0.033644, loss_cps: 0.060478
[12:43:44.667] iteration 5760: total_loss: 0.197066, loss_sup: 0.076199, loss_mps: 0.042737, loss_cps: 0.078129
[12:43:44.813] iteration 5761: total_loss: 0.224101, loss_sup: 0.108592, loss_mps: 0.040145, loss_cps: 0.075363
[12:43:44.961] iteration 5762: total_loss: 0.155814, loss_sup: 0.065491, loss_mps: 0.033144, loss_cps: 0.057179
[12:43:45.107] iteration 5763: total_loss: 0.312108, loss_sup: 0.218159, loss_mps: 0.034367, loss_cps: 0.059582
[12:43:45.253] iteration 5764: total_loss: 0.227812, loss_sup: 0.156882, loss_mps: 0.026964, loss_cps: 0.043966
[12:43:45.399] iteration 5765: total_loss: 0.145308, loss_sup: 0.055070, loss_mps: 0.032849, loss_cps: 0.057389
[12:43:45.544] iteration 5766: total_loss: 0.499991, loss_sup: 0.397963, loss_mps: 0.038857, loss_cps: 0.063172
[12:43:45.691] iteration 5767: total_loss: 0.309791, loss_sup: 0.174593, loss_mps: 0.046367, loss_cps: 0.088831
[12:43:45.837] iteration 5768: total_loss: 0.236586, loss_sup: 0.100900, loss_mps: 0.045784, loss_cps: 0.089902
[12:43:45.983] iteration 5769: total_loss: 0.196759, loss_sup: 0.104972, loss_mps: 0.033597, loss_cps: 0.058190
[12:43:46.130] iteration 5770: total_loss: 0.106029, loss_sup: 0.033323, loss_mps: 0.028203, loss_cps: 0.044503
[12:43:46.277] iteration 5771: total_loss: 0.290620, loss_sup: 0.177387, loss_mps: 0.041094, loss_cps: 0.072140
[12:43:46.423] iteration 5772: total_loss: 0.615438, loss_sup: 0.484549, loss_mps: 0.044777, loss_cps: 0.086112
[12:43:46.570] iteration 5773: total_loss: 0.136102, loss_sup: 0.053092, loss_mps: 0.030601, loss_cps: 0.052409
[12:43:46.716] iteration 5774: total_loss: 0.204195, loss_sup: 0.116948, loss_mps: 0.033902, loss_cps: 0.053345
[12:43:46.862] iteration 5775: total_loss: 0.216099, loss_sup: 0.127473, loss_mps: 0.032119, loss_cps: 0.056506
[12:43:47.009] iteration 5776: total_loss: 0.107608, loss_sup: 0.025470, loss_mps: 0.030531, loss_cps: 0.051608
[12:43:47.155] iteration 5777: total_loss: 0.176803, loss_sup: 0.079484, loss_mps: 0.034606, loss_cps: 0.062713
[12:43:47.301] iteration 5778: total_loss: 0.250905, loss_sup: 0.171189, loss_mps: 0.030433, loss_cps: 0.049283
[12:43:47.447] iteration 5779: total_loss: 0.245206, loss_sup: 0.146236, loss_mps: 0.035332, loss_cps: 0.063638
[12:43:47.593] iteration 5780: total_loss: 0.250843, loss_sup: 0.129028, loss_mps: 0.043208, loss_cps: 0.078608
[12:43:47.738] iteration 5781: total_loss: 0.293507, loss_sup: 0.172578, loss_mps: 0.042165, loss_cps: 0.078764
[12:43:47.884] iteration 5782: total_loss: 0.142249, loss_sup: 0.026517, loss_mps: 0.039786, loss_cps: 0.075946
[12:43:48.031] iteration 5783: total_loss: 0.229026, loss_sup: 0.132409, loss_mps: 0.035624, loss_cps: 0.060993
[12:43:48.177] iteration 5784: total_loss: 0.198514, loss_sup: 0.114872, loss_mps: 0.030289, loss_cps: 0.053353
[12:43:48.322] iteration 5785: total_loss: 0.194959, loss_sup: 0.088522, loss_mps: 0.037689, loss_cps: 0.068749
[12:43:48.468] iteration 5786: total_loss: 0.314636, loss_sup: 0.151601, loss_mps: 0.054545, loss_cps: 0.108490
[12:43:48.614] iteration 5787: total_loss: 0.287176, loss_sup: 0.175489, loss_mps: 0.038009, loss_cps: 0.073679
[12:43:48.759] iteration 5788: total_loss: 0.301791, loss_sup: 0.123436, loss_mps: 0.057826, loss_cps: 0.120529
[12:43:48.905] iteration 5789: total_loss: 0.240838, loss_sup: 0.108837, loss_mps: 0.046338, loss_cps: 0.085664
[12:43:49.052] iteration 5790: total_loss: 0.207516, loss_sup: 0.121128, loss_mps: 0.032822, loss_cps: 0.053566
[12:43:49.198] iteration 5791: total_loss: 0.109890, loss_sup: 0.038653, loss_mps: 0.027421, loss_cps: 0.043816
[12:43:49.345] iteration 5792: total_loss: 0.328961, loss_sup: 0.234070, loss_mps: 0.034560, loss_cps: 0.060332
[12:43:49.491] iteration 5793: total_loss: 0.302210, loss_sup: 0.196005, loss_mps: 0.037763, loss_cps: 0.068442
[12:43:49.637] iteration 5794: total_loss: 0.442922, loss_sup: 0.325332, loss_mps: 0.042155, loss_cps: 0.075435
[12:43:49.783] iteration 5795: total_loss: 0.254650, loss_sup: 0.127265, loss_mps: 0.043016, loss_cps: 0.084369
[12:43:49.929] iteration 5796: total_loss: 0.183291, loss_sup: 0.084121, loss_mps: 0.035334, loss_cps: 0.063836
[12:43:50.075] iteration 5797: total_loss: 0.454398, loss_sup: 0.203179, loss_mps: 0.078848, loss_cps: 0.172371
[12:43:50.221] iteration 5798: total_loss: 0.117556, loss_sup: 0.043794, loss_mps: 0.028169, loss_cps: 0.045593
[12:43:50.367] iteration 5799: total_loss: 0.104792, loss_sup: 0.033775, loss_mps: 0.026686, loss_cps: 0.044330
[12:43:50.513] iteration 5800: total_loss: 0.205141, loss_sup: 0.070809, loss_mps: 0.046138, loss_cps: 0.088194
[12:43:50.514] Evaluation Started ==>
[12:44:01.871] ==> valid iteration 5800: unet metrics: {'dc': 0.6162159635655572, 'jc': 0.4921853412953922, 'pre': 0.7098524181042963, 'hd': 6.027819239244108}, ynet metrics: {'dc': 0.5221202489791271, 'jc': 0.4078359998518742, 'pre': 0.7110150948424085, 'hd': 6.460959855213476}.
[12:44:01.873] Evaluation Finished!⏹️
[12:44:02.024] iteration 5801: total_loss: 0.251863, loss_sup: 0.148999, loss_mps: 0.036068, loss_cps: 0.066795
[12:44:02.172] iteration 5802: total_loss: 0.303301, loss_sup: 0.174052, loss_mps: 0.043467, loss_cps: 0.085783
[12:44:02.318] iteration 5803: total_loss: 0.164695, loss_sup: 0.087298, loss_mps: 0.028404, loss_cps: 0.048994
[12:44:02.464] iteration 5804: total_loss: 0.244691, loss_sup: 0.139669, loss_mps: 0.037359, loss_cps: 0.067663
[12:44:02.610] iteration 5805: total_loss: 0.154186, loss_sup: 0.068671, loss_mps: 0.031654, loss_cps: 0.053861
[12:44:02.758] iteration 5806: total_loss: 0.174461, loss_sup: 0.094808, loss_mps: 0.029140, loss_cps: 0.050513
[12:44:02.904] iteration 5807: total_loss: 0.239825, loss_sup: 0.143640, loss_mps: 0.033333, loss_cps: 0.062852
[12:44:03.049] iteration 5808: total_loss: 0.292756, loss_sup: 0.142042, loss_mps: 0.050679, loss_cps: 0.100035
[12:44:03.195] iteration 5809: total_loss: 0.152527, loss_sup: 0.045229, loss_mps: 0.037326, loss_cps: 0.069972
[12:44:03.340] iteration 5810: total_loss: 0.401972, loss_sup: 0.293737, loss_mps: 0.037956, loss_cps: 0.070279
[12:44:03.486] iteration 5811: total_loss: 0.284912, loss_sup: 0.104356, loss_mps: 0.058354, loss_cps: 0.122203
[12:44:03.632] iteration 5812: total_loss: 0.126740, loss_sup: 0.030438, loss_mps: 0.033731, loss_cps: 0.062571
[12:44:03.778] iteration 5813: total_loss: 0.255491, loss_sup: 0.177909, loss_mps: 0.028654, loss_cps: 0.048928
[12:44:03.923] iteration 5814: total_loss: 0.207069, loss_sup: 0.090596, loss_mps: 0.040526, loss_cps: 0.075947
[12:44:04.069] iteration 5815: total_loss: 0.280105, loss_sup: 0.191911, loss_mps: 0.031381, loss_cps: 0.056813
[12:44:04.215] iteration 5816: total_loss: 0.265237, loss_sup: 0.151764, loss_mps: 0.039303, loss_cps: 0.074170
[12:44:04.363] iteration 5817: total_loss: 0.298520, loss_sup: 0.172271, loss_mps: 0.042773, loss_cps: 0.083476
[12:44:04.508] iteration 5818: total_loss: 0.156803, loss_sup: 0.058370, loss_mps: 0.034867, loss_cps: 0.063566
[12:44:04.655] iteration 5819: total_loss: 0.180997, loss_sup: 0.094003, loss_mps: 0.032522, loss_cps: 0.054472
[12:44:04.801] iteration 5820: total_loss: 0.262929, loss_sup: 0.146948, loss_mps: 0.039356, loss_cps: 0.076625
[12:44:04.946] iteration 5821: total_loss: 0.435147, loss_sup: 0.331321, loss_mps: 0.036206, loss_cps: 0.067620
[12:44:05.093] iteration 5822: total_loss: 0.162878, loss_sup: 0.090283, loss_mps: 0.027899, loss_cps: 0.044696
[12:44:05.239] iteration 5823: total_loss: 0.184948, loss_sup: 0.096962, loss_mps: 0.031809, loss_cps: 0.056177
[12:44:05.384] iteration 5824: total_loss: 0.188695, loss_sup: 0.097444, loss_mps: 0.034505, loss_cps: 0.056745
[12:44:05.530] iteration 5825: total_loss: 0.441951, loss_sup: 0.312043, loss_mps: 0.044555, loss_cps: 0.085353
[12:44:05.676] iteration 5826: total_loss: 0.178706, loss_sup: 0.103856, loss_mps: 0.028930, loss_cps: 0.045920
[12:44:05.821] iteration 5827: total_loss: 0.241775, loss_sup: 0.156698, loss_mps: 0.031213, loss_cps: 0.053863
[12:44:05.967] iteration 5828: total_loss: 0.153456, loss_sup: 0.085232, loss_mps: 0.026155, loss_cps: 0.042068
[12:44:06.113] iteration 5829: total_loss: 0.143372, loss_sup: 0.049399, loss_mps: 0.034420, loss_cps: 0.059553
[12:44:06.259] iteration 5830: total_loss: 0.377906, loss_sup: 0.278959, loss_mps: 0.034329, loss_cps: 0.064618
[12:44:06.405] iteration 5831: total_loss: 0.160316, loss_sup: 0.079725, loss_mps: 0.029858, loss_cps: 0.050733
[12:44:06.551] iteration 5832: total_loss: 0.292178, loss_sup: 0.209645, loss_mps: 0.029814, loss_cps: 0.052719
[12:44:06.696] iteration 5833: total_loss: 0.218913, loss_sup: 0.131404, loss_mps: 0.032584, loss_cps: 0.054925
[12:44:06.842] iteration 5834: total_loss: 0.323285, loss_sup: 0.193252, loss_mps: 0.046501, loss_cps: 0.083532
[12:44:06.988] iteration 5835: total_loss: 0.176076, loss_sup: 0.060580, loss_mps: 0.039394, loss_cps: 0.076102
[12:44:07.133] iteration 5836: total_loss: 0.196111, loss_sup: 0.090615, loss_mps: 0.037639, loss_cps: 0.067856
[12:44:07.279] iteration 5837: total_loss: 0.213971, loss_sup: 0.122882, loss_mps: 0.033859, loss_cps: 0.057230
[12:44:07.425] iteration 5838: total_loss: 0.116584, loss_sup: 0.042654, loss_mps: 0.027993, loss_cps: 0.045938
[12:44:07.570] iteration 5839: total_loss: 0.156865, loss_sup: 0.075934, loss_mps: 0.029291, loss_cps: 0.051639
[12:44:07.716] iteration 5840: total_loss: 0.151086, loss_sup: 0.053372, loss_mps: 0.035104, loss_cps: 0.062609
[12:44:07.861] iteration 5841: total_loss: 0.248972, loss_sup: 0.140552, loss_mps: 0.038482, loss_cps: 0.069938
[12:44:08.007] iteration 5842: total_loss: 0.430134, loss_sup: 0.292262, loss_mps: 0.047094, loss_cps: 0.090778
[12:44:08.154] iteration 5843: total_loss: 0.125593, loss_sup: 0.043557, loss_mps: 0.029259, loss_cps: 0.052777
[12:44:08.299] iteration 5844: total_loss: 0.756673, loss_sup: 0.580098, loss_mps: 0.059074, loss_cps: 0.117501
[12:44:08.446] iteration 5845: total_loss: 0.272896, loss_sup: 0.163866, loss_mps: 0.038234, loss_cps: 0.070796
[12:44:08.591] iteration 5846: total_loss: 0.154094, loss_sup: 0.084631, loss_mps: 0.025778, loss_cps: 0.043685
[12:44:08.736] iteration 5847: total_loss: 0.122483, loss_sup: 0.030279, loss_mps: 0.034054, loss_cps: 0.058150
[12:44:08.882] iteration 5848: total_loss: 0.251595, loss_sup: 0.128670, loss_mps: 0.044036, loss_cps: 0.078889
[12:44:09.027] iteration 5849: total_loss: 0.151443, loss_sup: 0.060165, loss_mps: 0.032603, loss_cps: 0.058675
[12:44:09.173] iteration 5850: total_loss: 0.236965, loss_sup: 0.160160, loss_mps: 0.028630, loss_cps: 0.048175
[12:44:09.319] iteration 5851: total_loss: 0.237507, loss_sup: 0.123850, loss_mps: 0.040342, loss_cps: 0.073315
[12:44:09.382] iteration 5852: total_loss: 0.081128, loss_sup: 0.015533, loss_mps: 0.024791, loss_cps: 0.040805
[12:44:10.597] iteration 5853: total_loss: 0.280103, loss_sup: 0.177474, loss_mps: 0.037818, loss_cps: 0.064811
[12:44:10.746] iteration 5854: total_loss: 0.110885, loss_sup: 0.031715, loss_mps: 0.029341, loss_cps: 0.049828
[12:44:10.894] iteration 5855: total_loss: 0.090840, loss_sup: 0.032945, loss_mps: 0.023596, loss_cps: 0.034299
[12:44:11.040] iteration 5856: total_loss: 0.433172, loss_sup: 0.291734, loss_mps: 0.047545, loss_cps: 0.093893
[12:44:11.187] iteration 5857: total_loss: 0.206383, loss_sup: 0.090490, loss_mps: 0.040368, loss_cps: 0.075524
[12:44:11.332] iteration 5858: total_loss: 0.258452, loss_sup: 0.149768, loss_mps: 0.040231, loss_cps: 0.068453
[12:44:11.479] iteration 5859: total_loss: 0.193002, loss_sup: 0.108026, loss_mps: 0.032524, loss_cps: 0.052451
[12:44:11.631] iteration 5860: total_loss: 0.199575, loss_sup: 0.101897, loss_mps: 0.036310, loss_cps: 0.061369
[12:44:11.778] iteration 5861: total_loss: 0.229207, loss_sup: 0.106260, loss_mps: 0.042321, loss_cps: 0.080625
[12:44:11.925] iteration 5862: total_loss: 0.323511, loss_sup: 0.141287, loss_mps: 0.060552, loss_cps: 0.121672
[12:44:12.070] iteration 5863: total_loss: 0.142183, loss_sup: 0.059009, loss_mps: 0.030659, loss_cps: 0.052515
[12:44:12.220] iteration 5864: total_loss: 0.275458, loss_sup: 0.163421, loss_mps: 0.040146, loss_cps: 0.071890
[12:44:12.366] iteration 5865: total_loss: 0.157460, loss_sup: 0.090999, loss_mps: 0.025586, loss_cps: 0.040876
[12:44:12.512] iteration 5866: total_loss: 0.158465, loss_sup: 0.085611, loss_mps: 0.027675, loss_cps: 0.045178
[12:44:12.658] iteration 5867: total_loss: 0.348853, loss_sup: 0.206555, loss_mps: 0.047925, loss_cps: 0.094374
[12:44:12.804] iteration 5868: total_loss: 0.331568, loss_sup: 0.246961, loss_mps: 0.032449, loss_cps: 0.052158
[12:44:12.952] iteration 5869: total_loss: 0.125637, loss_sup: 0.039383, loss_mps: 0.030804, loss_cps: 0.055451
[12:44:13.098] iteration 5870: total_loss: 0.290655, loss_sup: 0.165861, loss_mps: 0.042171, loss_cps: 0.082624
[12:44:13.243] iteration 5871: total_loss: 0.361221, loss_sup: 0.245789, loss_mps: 0.040371, loss_cps: 0.075060
[12:44:13.389] iteration 5872: total_loss: 0.283424, loss_sup: 0.162847, loss_mps: 0.041053, loss_cps: 0.079524
[12:44:13.535] iteration 5873: total_loss: 0.194164, loss_sup: 0.074951, loss_mps: 0.040811, loss_cps: 0.078402
[12:44:13.681] iteration 5874: total_loss: 0.214673, loss_sup: 0.082799, loss_mps: 0.044213, loss_cps: 0.087661
[12:44:13.827] iteration 5875: total_loss: 0.257523, loss_sup: 0.137218, loss_mps: 0.040398, loss_cps: 0.079907
[12:44:13.973] iteration 5876: total_loss: 0.372834, loss_sup: 0.245703, loss_mps: 0.042455, loss_cps: 0.084676
[12:44:14.119] iteration 5877: total_loss: 0.141672, loss_sup: 0.068398, loss_mps: 0.027722, loss_cps: 0.045552
[12:44:14.266] iteration 5878: total_loss: 0.334681, loss_sup: 0.242186, loss_mps: 0.032781, loss_cps: 0.059713
[12:44:14.411] iteration 5879: total_loss: 0.132992, loss_sup: 0.061000, loss_mps: 0.027191, loss_cps: 0.044800
[12:44:14.556] iteration 5880: total_loss: 0.299271, loss_sup: 0.226485, loss_mps: 0.028070, loss_cps: 0.044716
[12:44:14.702] iteration 5881: total_loss: 0.247318, loss_sup: 0.097259, loss_mps: 0.050555, loss_cps: 0.099504
[12:44:14.848] iteration 5882: total_loss: 0.323116, loss_sup: 0.219768, loss_mps: 0.036770, loss_cps: 0.066578
[12:44:14.994] iteration 5883: total_loss: 0.120814, loss_sup: 0.043437, loss_mps: 0.028379, loss_cps: 0.048998
[12:44:15.140] iteration 5884: total_loss: 0.513053, loss_sup: 0.370670, loss_mps: 0.047055, loss_cps: 0.095329
[12:44:15.286] iteration 5885: total_loss: 0.162795, loss_sup: 0.073060, loss_mps: 0.032760, loss_cps: 0.056976
[12:44:15.432] iteration 5886: total_loss: 0.124749, loss_sup: 0.052704, loss_mps: 0.027413, loss_cps: 0.044632
[12:44:15.578] iteration 5887: total_loss: 0.146702, loss_sup: 0.041546, loss_mps: 0.035846, loss_cps: 0.069311
[12:44:15.725] iteration 5888: total_loss: 0.238959, loss_sup: 0.090543, loss_mps: 0.051277, loss_cps: 0.097139
[12:44:15.871] iteration 5889: total_loss: 0.245959, loss_sup: 0.153095, loss_mps: 0.033531, loss_cps: 0.059333
[12:44:16.017] iteration 5890: total_loss: 0.264014, loss_sup: 0.161452, loss_mps: 0.036680, loss_cps: 0.065882
[12:44:16.165] iteration 5891: total_loss: 0.191000, loss_sup: 0.097566, loss_mps: 0.033423, loss_cps: 0.060011
[12:44:16.311] iteration 5892: total_loss: 0.252963, loss_sup: 0.128298, loss_mps: 0.043312, loss_cps: 0.081354
[12:44:16.457] iteration 5893: total_loss: 0.221418, loss_sup: 0.092551, loss_mps: 0.045570, loss_cps: 0.083297
[12:44:16.603] iteration 5894: total_loss: 0.127870, loss_sup: 0.056209, loss_mps: 0.026042, loss_cps: 0.045619
[12:44:16.750] iteration 5895: total_loss: 0.217333, loss_sup: 0.111006, loss_mps: 0.037171, loss_cps: 0.069156
[12:44:16.898] iteration 5896: total_loss: 0.186822, loss_sup: 0.053849, loss_mps: 0.043916, loss_cps: 0.089056
[12:44:17.044] iteration 5897: total_loss: 0.156943, loss_sup: 0.058808, loss_mps: 0.035694, loss_cps: 0.062441
[12:44:17.190] iteration 5898: total_loss: 0.218654, loss_sup: 0.098742, loss_mps: 0.040665, loss_cps: 0.079246
[12:44:17.335] iteration 5899: total_loss: 0.182273, loss_sup: 0.085500, loss_mps: 0.033634, loss_cps: 0.063139
[12:44:17.481] iteration 5900: total_loss: 0.172798, loss_sup: 0.089766, loss_mps: 0.029014, loss_cps: 0.054018
[12:44:17.481] Evaluation Started ==>
[12:44:28.875] ==> valid iteration 5900: unet metrics: {'dc': 0.6152713192591899, 'jc': 0.4867954661193916, 'pre': 0.7152113650218193, 'hd': 6.270239982857413}, ynet metrics: {'dc': 0.5751111743840424, 'jc': 0.452173967044254, 'pre': 0.6955666623912486, 'hd': 6.413259836308445}.
[12:44:29.036] ==> New best valid dice for ynet: 0.575111, at iteration 5900
[12:44:29.038] Evaluation Finished!⏹️
[12:44:29.191] iteration 5901: total_loss: 0.119352, loss_sup: 0.033990, loss_mps: 0.030074, loss_cps: 0.055288
[12:44:29.337] iteration 5902: total_loss: 0.247677, loss_sup: 0.133759, loss_mps: 0.040874, loss_cps: 0.073044
[12:44:29.483] iteration 5903: total_loss: 0.293884, loss_sup: 0.192130, loss_mps: 0.035493, loss_cps: 0.066261
[12:44:29.628] iteration 5904: total_loss: 0.149556, loss_sup: 0.068536, loss_mps: 0.028042, loss_cps: 0.052979
[12:44:29.774] iteration 5905: total_loss: 0.330641, loss_sup: 0.234977, loss_mps: 0.034351, loss_cps: 0.061313
[12:44:29.920] iteration 5906: total_loss: 0.176761, loss_sup: 0.082973, loss_mps: 0.032873, loss_cps: 0.060914
[12:44:30.066] iteration 5907: total_loss: 0.127092, loss_sup: 0.040217, loss_mps: 0.032193, loss_cps: 0.054681
[12:44:30.212] iteration 5908: total_loss: 0.188568, loss_sup: 0.114841, loss_mps: 0.026863, loss_cps: 0.046864
[12:44:30.358] iteration 5909: total_loss: 0.240102, loss_sup: 0.143711, loss_mps: 0.033541, loss_cps: 0.062850
[12:44:30.503] iteration 5910: total_loss: 0.381328, loss_sup: 0.289542, loss_mps: 0.032593, loss_cps: 0.059194
[12:44:30.651] iteration 5911: total_loss: 0.207038, loss_sup: 0.111507, loss_mps: 0.034782, loss_cps: 0.060749
[12:44:30.797] iteration 5912: total_loss: 0.243613, loss_sup: 0.156016, loss_mps: 0.030530, loss_cps: 0.057066
[12:44:30.945] iteration 5913: total_loss: 0.152762, loss_sup: 0.087061, loss_mps: 0.024430, loss_cps: 0.041271
[12:44:31.090] iteration 5914: total_loss: 0.138283, loss_sup: 0.042785, loss_mps: 0.033145, loss_cps: 0.062353
[12:44:31.236] iteration 5915: total_loss: 0.184170, loss_sup: 0.085832, loss_mps: 0.033825, loss_cps: 0.064514
[12:44:31.382] iteration 5916: total_loss: 0.249844, loss_sup: 0.106886, loss_mps: 0.047636, loss_cps: 0.095322
[12:44:31.527] iteration 5917: total_loss: 0.332833, loss_sup: 0.213871, loss_mps: 0.038951, loss_cps: 0.080011
[12:44:31.673] iteration 5918: total_loss: 0.307500, loss_sup: 0.196851, loss_mps: 0.039527, loss_cps: 0.071122
[12:44:31.818] iteration 5919: total_loss: 0.120298, loss_sup: 0.032337, loss_mps: 0.030542, loss_cps: 0.057419
[12:44:31.963] iteration 5920: total_loss: 0.283675, loss_sup: 0.192020, loss_mps: 0.032234, loss_cps: 0.059421
[12:44:32.109] iteration 5921: total_loss: 0.178211, loss_sup: 0.074045, loss_mps: 0.035178, loss_cps: 0.068988
[12:44:32.257] iteration 5922: total_loss: 0.142310, loss_sup: 0.027580, loss_mps: 0.038488, loss_cps: 0.076242
[12:44:32.403] iteration 5923: total_loss: 0.381843, loss_sup: 0.272250, loss_mps: 0.035462, loss_cps: 0.074131
[12:44:32.548] iteration 5924: total_loss: 0.448356, loss_sup: 0.321274, loss_mps: 0.040197, loss_cps: 0.086884
[12:44:32.695] iteration 5925: total_loss: 0.448731, loss_sup: 0.311751, loss_mps: 0.044939, loss_cps: 0.092041
[12:44:32.841] iteration 5926: total_loss: 0.319894, loss_sup: 0.177001, loss_mps: 0.047912, loss_cps: 0.094981
[12:44:32.986] iteration 5927: total_loss: 0.277634, loss_sup: 0.180818, loss_mps: 0.034743, loss_cps: 0.062073
[12:44:33.132] iteration 5928: total_loss: 0.171072, loss_sup: 0.088237, loss_mps: 0.030140, loss_cps: 0.052695
[12:44:33.280] iteration 5929: total_loss: 0.248867, loss_sup: 0.155901, loss_mps: 0.034799, loss_cps: 0.058166
[12:44:33.427] iteration 5930: total_loss: 0.211923, loss_sup: 0.089368, loss_mps: 0.041075, loss_cps: 0.081480
[12:44:33.576] iteration 5931: total_loss: 0.266891, loss_sup: 0.169632, loss_mps: 0.035845, loss_cps: 0.061415
[12:44:33.724] iteration 5932: total_loss: 0.366295, loss_sup: 0.278199, loss_mps: 0.033981, loss_cps: 0.054115
[12:44:33.870] iteration 5933: total_loss: 0.499575, loss_sup: 0.375477, loss_mps: 0.042364, loss_cps: 0.081734
[12:44:34.015] iteration 5934: total_loss: 0.189656, loss_sup: 0.097653, loss_mps: 0.033077, loss_cps: 0.058926
[12:44:34.161] iteration 5935: total_loss: 0.286988, loss_sup: 0.154090, loss_mps: 0.045444, loss_cps: 0.087454
[12:44:34.306] iteration 5936: total_loss: 0.218839, loss_sup: 0.090725, loss_mps: 0.045320, loss_cps: 0.082794
[12:44:34.452] iteration 5937: total_loss: 0.213309, loss_sup: 0.128603, loss_mps: 0.030860, loss_cps: 0.053846
[12:44:34.599] iteration 5938: total_loss: 0.240978, loss_sup: 0.085776, loss_mps: 0.052328, loss_cps: 0.102873
[12:44:34.744] iteration 5939: total_loss: 0.198876, loss_sup: 0.087012, loss_mps: 0.039142, loss_cps: 0.072721
[12:44:34.889] iteration 5940: total_loss: 0.343482, loss_sup: 0.157944, loss_mps: 0.060725, loss_cps: 0.124813
[12:44:35.035] iteration 5941: total_loss: 0.248964, loss_sup: 0.133859, loss_mps: 0.041744, loss_cps: 0.073361
[12:44:35.180] iteration 5942: total_loss: 0.226014, loss_sup: 0.118133, loss_mps: 0.040602, loss_cps: 0.067280
[12:44:35.326] iteration 5943: total_loss: 0.344562, loss_sup: 0.181592, loss_mps: 0.054964, loss_cps: 0.108005
[12:44:35.471] iteration 5944: total_loss: 0.156056, loss_sup: 0.046357, loss_mps: 0.039225, loss_cps: 0.070474
[12:44:35.617] iteration 5945: total_loss: 0.305622, loss_sup: 0.130366, loss_mps: 0.058461, loss_cps: 0.116795
[12:44:35.763] iteration 5946: total_loss: 0.172231, loss_sup: 0.047961, loss_mps: 0.043023, loss_cps: 0.081248
[12:44:35.909] iteration 5947: total_loss: 0.257607, loss_sup: 0.076369, loss_mps: 0.058891, loss_cps: 0.122347
[12:44:36.055] iteration 5948: total_loss: 0.110967, loss_sup: 0.016073, loss_mps: 0.034983, loss_cps: 0.059911
[12:44:36.201] iteration 5949: total_loss: 0.156244, loss_sup: 0.060620, loss_mps: 0.035410, loss_cps: 0.060215
[12:44:36.346] iteration 5950: total_loss: 0.226625, loss_sup: 0.101223, loss_mps: 0.043761, loss_cps: 0.081641
[12:44:36.493] iteration 5951: total_loss: 0.263831, loss_sup: 0.150656, loss_mps: 0.038735, loss_cps: 0.074441
[12:44:36.638] iteration 5952: total_loss: 0.229093, loss_sup: 0.117653, loss_mps: 0.038612, loss_cps: 0.072828
[12:44:36.786] iteration 5953: total_loss: 0.179701, loss_sup: 0.106029, loss_mps: 0.027397, loss_cps: 0.046275
[12:44:36.931] iteration 5954: total_loss: 0.376921, loss_sup: 0.217434, loss_mps: 0.053223, loss_cps: 0.106264
[12:44:37.079] iteration 5955: total_loss: 0.204508, loss_sup: 0.045936, loss_mps: 0.052226, loss_cps: 0.106345
[12:44:37.226] iteration 5956: total_loss: 0.204162, loss_sup: 0.094139, loss_mps: 0.038033, loss_cps: 0.071990
[12:44:37.373] iteration 5957: total_loss: 0.094926, loss_sup: 0.023308, loss_mps: 0.026663, loss_cps: 0.044955
[12:44:37.519] iteration 5958: total_loss: 0.292934, loss_sup: 0.215766, loss_mps: 0.028381, loss_cps: 0.048787
[12:44:37.665] iteration 5959: total_loss: 0.227825, loss_sup: 0.136749, loss_mps: 0.032901, loss_cps: 0.058175
[12:44:37.810] iteration 5960: total_loss: 0.177949, loss_sup: 0.094222, loss_mps: 0.030667, loss_cps: 0.053060
[12:44:37.958] iteration 5961: total_loss: 0.204956, loss_sup: 0.105889, loss_mps: 0.035127, loss_cps: 0.063940
[12:44:38.103] iteration 5962: total_loss: 0.271740, loss_sup: 0.168342, loss_mps: 0.036625, loss_cps: 0.066773
[12:44:38.249] iteration 5963: total_loss: 0.130609, loss_sup: 0.039515, loss_mps: 0.032349, loss_cps: 0.058745
[12:44:38.396] iteration 5964: total_loss: 0.147913, loss_sup: 0.068783, loss_mps: 0.029231, loss_cps: 0.049899
[12:44:38.542] iteration 5965: total_loss: 0.213095, loss_sup: 0.051517, loss_mps: 0.052497, loss_cps: 0.109081
[12:44:38.688] iteration 5966: total_loss: 0.296672, loss_sup: 0.184952, loss_mps: 0.039963, loss_cps: 0.071758
[12:44:38.834] iteration 5967: total_loss: 0.225309, loss_sup: 0.115243, loss_mps: 0.038108, loss_cps: 0.071958
[12:44:38.980] iteration 5968: total_loss: 0.435448, loss_sup: 0.319030, loss_mps: 0.041114, loss_cps: 0.075303
[12:44:39.127] iteration 5969: total_loss: 0.236749, loss_sup: 0.083891, loss_mps: 0.051297, loss_cps: 0.101561
[12:44:39.274] iteration 5970: total_loss: 0.263931, loss_sup: 0.110696, loss_mps: 0.050454, loss_cps: 0.102780
[12:44:39.420] iteration 5971: total_loss: 0.250052, loss_sup: 0.143413, loss_mps: 0.037879, loss_cps: 0.068760
[12:44:39.567] iteration 5972: total_loss: 0.172376, loss_sup: 0.080959, loss_mps: 0.032900, loss_cps: 0.058517
[12:44:39.713] iteration 5973: total_loss: 0.158235, loss_sup: 0.069286, loss_mps: 0.031888, loss_cps: 0.057061
[12:44:39.859] iteration 5974: total_loss: 0.203402, loss_sup: 0.094918, loss_mps: 0.037693, loss_cps: 0.070791
[12:44:40.005] iteration 5975: total_loss: 0.206347, loss_sup: 0.108507, loss_mps: 0.034395, loss_cps: 0.063445
[12:44:40.151] iteration 5976: total_loss: 0.437460, loss_sup: 0.340757, loss_mps: 0.034923, loss_cps: 0.061781
[12:44:40.298] iteration 5977: total_loss: 0.136248, loss_sup: 0.025170, loss_mps: 0.038052, loss_cps: 0.073027
[12:44:40.444] iteration 5978: total_loss: 0.310940, loss_sup: 0.195072, loss_mps: 0.042327, loss_cps: 0.073542
[12:44:40.589] iteration 5979: total_loss: 0.209223, loss_sup: 0.089779, loss_mps: 0.040087, loss_cps: 0.079357
[12:44:40.736] iteration 5980: total_loss: 0.147057, loss_sup: 0.055956, loss_mps: 0.033379, loss_cps: 0.057721
[12:44:40.882] iteration 5981: total_loss: 0.356625, loss_sup: 0.267567, loss_mps: 0.032924, loss_cps: 0.056134
[12:44:41.027] iteration 5982: total_loss: 0.111012, loss_sup: 0.022558, loss_mps: 0.031952, loss_cps: 0.056502
[12:44:41.173] iteration 5983: total_loss: 0.174936, loss_sup: 0.063141, loss_mps: 0.039204, loss_cps: 0.072590
[12:44:41.319] iteration 5984: total_loss: 0.214617, loss_sup: 0.111144, loss_mps: 0.035846, loss_cps: 0.067626
[12:44:41.465] iteration 5985: total_loss: 0.188086, loss_sup: 0.087022, loss_mps: 0.034126, loss_cps: 0.066938
[12:44:41.611] iteration 5986: total_loss: 0.261491, loss_sup: 0.127716, loss_mps: 0.043618, loss_cps: 0.090157
[12:44:41.757] iteration 5987: total_loss: 0.185957, loss_sup: 0.122072, loss_mps: 0.025221, loss_cps: 0.038664
[12:44:41.902] iteration 5988: total_loss: 0.283135, loss_sup: 0.190535, loss_mps: 0.033409, loss_cps: 0.059191
[12:44:42.048] iteration 5989: total_loss: 0.115928, loss_sup: 0.049815, loss_mps: 0.024786, loss_cps: 0.041327
[12:44:42.194] iteration 5990: total_loss: 0.127693, loss_sup: 0.056040, loss_mps: 0.025895, loss_cps: 0.045758
[12:44:42.339] iteration 5991: total_loss: 0.134430, loss_sup: 0.032808, loss_mps: 0.035467, loss_cps: 0.066154
[12:44:42.486] iteration 5992: total_loss: 0.317373, loss_sup: 0.225351, loss_mps: 0.032721, loss_cps: 0.059301
[12:44:42.633] iteration 5993: total_loss: 0.214928, loss_sup: 0.119907, loss_mps: 0.032702, loss_cps: 0.062319
[12:44:42.780] iteration 5994: total_loss: 0.351990, loss_sup: 0.256189, loss_mps: 0.033659, loss_cps: 0.062142
[12:44:42.926] iteration 5995: total_loss: 0.354306, loss_sup: 0.270301, loss_mps: 0.032109, loss_cps: 0.051896
[12:44:43.074] iteration 5996: total_loss: 0.212405, loss_sup: 0.112396, loss_mps: 0.036135, loss_cps: 0.063874
[12:44:43.220] iteration 5997: total_loss: 0.267430, loss_sup: 0.177511, loss_mps: 0.033647, loss_cps: 0.056272
[12:44:43.366] iteration 5998: total_loss: 0.273834, loss_sup: 0.134769, loss_mps: 0.046721, loss_cps: 0.092344
[12:44:43.512] iteration 5999: total_loss: 0.111034, loss_sup: 0.034990, loss_mps: 0.027462, loss_cps: 0.048583
[12:44:43.658] iteration 6000: total_loss: 0.177155, loss_sup: 0.084849, loss_mps: 0.033979, loss_cps: 0.058327
[12:44:43.658] Evaluation Started ==>
[12:44:55.046] ==> valid iteration 6000: unet metrics: {'dc': 0.5831960247240996, 'jc': 0.4636673983713584, 'pre': 0.6975386780915829, 'hd': 5.90497392944275}, ynet metrics: {'dc': 0.5165762278429079, 'jc': 0.40515235201004035, 'pre': 0.6654391626867537, 'hd': 6.274364312190189}.
[12:44:55.048] Evaluation Finished!⏹️
[12:44:55.198] iteration 6001: total_loss: 0.177454, loss_sup: 0.092274, loss_mps: 0.031107, loss_cps: 0.054074
[12:44:55.347] iteration 6002: total_loss: 0.194274, loss_sup: 0.094928, loss_mps: 0.035080, loss_cps: 0.064266
[12:44:55.493] iteration 6003: total_loss: 0.266059, loss_sup: 0.141548, loss_mps: 0.042150, loss_cps: 0.082361
[12:44:55.640] iteration 6004: total_loss: 0.227900, loss_sup: 0.102202, loss_mps: 0.043631, loss_cps: 0.082067
[12:44:55.787] iteration 6005: total_loss: 0.416501, loss_sup: 0.316846, loss_mps: 0.034608, loss_cps: 0.065047
[12:44:55.935] iteration 6006: total_loss: 0.227967, loss_sup: 0.099371, loss_mps: 0.042405, loss_cps: 0.086191
[12:44:56.080] iteration 6007: total_loss: 0.243388, loss_sup: 0.115991, loss_mps: 0.043085, loss_cps: 0.084312
[12:44:56.225] iteration 6008: total_loss: 0.179682, loss_sup: 0.073247, loss_mps: 0.037372, loss_cps: 0.069063
[12:44:56.371] iteration 6009: total_loss: 0.226232, loss_sup: 0.072168, loss_mps: 0.050535, loss_cps: 0.103528
[12:44:56.516] iteration 6010: total_loss: 0.423937, loss_sup: 0.249863, loss_mps: 0.056658, loss_cps: 0.117416
[12:44:56.664] iteration 6011: total_loss: 0.200128, loss_sup: 0.049357, loss_mps: 0.049684, loss_cps: 0.101087
[12:44:56.810] iteration 6012: total_loss: 0.246328, loss_sup: 0.161601, loss_mps: 0.031715, loss_cps: 0.053011
[12:44:56.960] iteration 6013: total_loss: 0.206272, loss_sup: 0.146379, loss_mps: 0.023533, loss_cps: 0.036360
[12:44:57.106] iteration 6014: total_loss: 0.197701, loss_sup: 0.075754, loss_mps: 0.043636, loss_cps: 0.078311
[12:44:57.251] iteration 6015: total_loss: 0.220956, loss_sup: 0.061763, loss_mps: 0.050461, loss_cps: 0.108733
[12:44:57.397] iteration 6016: total_loss: 0.194070, loss_sup: 0.054394, loss_mps: 0.046894, loss_cps: 0.092782
[12:44:57.542] iteration 6017: total_loss: 0.262853, loss_sup: 0.105595, loss_mps: 0.049842, loss_cps: 0.107417
[12:44:57.689] iteration 6018: total_loss: 0.262944, loss_sup: 0.142006, loss_mps: 0.040418, loss_cps: 0.080520
[12:44:57.834] iteration 6019: total_loss: 0.130210, loss_sup: 0.069519, loss_mps: 0.023272, loss_cps: 0.037419
[12:44:57.980] iteration 6020: total_loss: 0.163787, loss_sup: 0.078503, loss_mps: 0.030275, loss_cps: 0.055009
[12:44:58.126] iteration 6021: total_loss: 0.087981, loss_sup: 0.016731, loss_mps: 0.026213, loss_cps: 0.045036
[12:44:58.272] iteration 6022: total_loss: 0.261262, loss_sup: 0.109211, loss_mps: 0.050610, loss_cps: 0.101441
[12:44:58.417] iteration 6023: total_loss: 0.156164, loss_sup: 0.066023, loss_mps: 0.030387, loss_cps: 0.059754
[12:44:58.563] iteration 6024: total_loss: 0.135805, loss_sup: 0.067170, loss_mps: 0.024914, loss_cps: 0.043721
[12:44:58.710] iteration 6025: total_loss: 0.120269, loss_sup: 0.040195, loss_mps: 0.028342, loss_cps: 0.051732
[12:44:58.856] iteration 6026: total_loss: 0.423151, loss_sup: 0.339367, loss_mps: 0.029526, loss_cps: 0.054258
[12:44:59.002] iteration 6027: total_loss: 0.202214, loss_sup: 0.087080, loss_mps: 0.040033, loss_cps: 0.075102
[12:44:59.148] iteration 6028: total_loss: 0.371982, loss_sup: 0.178367, loss_mps: 0.062024, loss_cps: 0.131591
[12:44:59.293] iteration 6029: total_loss: 0.270174, loss_sup: 0.167478, loss_mps: 0.036342, loss_cps: 0.066354
[12:44:59.438] iteration 6030: total_loss: 0.205127, loss_sup: 0.087347, loss_mps: 0.040273, loss_cps: 0.077507
[12:44:59.584] iteration 6031: total_loss: 0.260391, loss_sup: 0.167465, loss_mps: 0.033190, loss_cps: 0.059735
[12:44:59.730] iteration 6032: total_loss: 0.210899, loss_sup: 0.069060, loss_mps: 0.045765, loss_cps: 0.096074
[12:44:59.876] iteration 6033: total_loss: 0.117768, loss_sup: 0.025893, loss_mps: 0.031880, loss_cps: 0.059994
[12:45:00.023] iteration 6034: total_loss: 0.143911, loss_sup: 0.048844, loss_mps: 0.032984, loss_cps: 0.062083
[12:45:00.168] iteration 6035: total_loss: 0.085231, loss_sup: 0.027135, loss_mps: 0.021510, loss_cps: 0.036586
[12:45:00.314] iteration 6036: total_loss: 0.118479, loss_sup: 0.030015, loss_mps: 0.031514, loss_cps: 0.056950
[12:45:00.460] iteration 6037: total_loss: 0.162571, loss_sup: 0.097741, loss_mps: 0.024603, loss_cps: 0.040228
[12:45:00.605] iteration 6038: total_loss: 0.205903, loss_sup: 0.135126, loss_mps: 0.027538, loss_cps: 0.043239
[12:45:00.752] iteration 6039: total_loss: 0.308873, loss_sup: 0.213295, loss_mps: 0.032631, loss_cps: 0.062947
[12:45:00.898] iteration 6040: total_loss: 0.250525, loss_sup: 0.131483, loss_mps: 0.039457, loss_cps: 0.079586
[12:45:01.045] iteration 6041: total_loss: 0.235237, loss_sup: 0.147316, loss_mps: 0.030976, loss_cps: 0.056946
[12:45:01.191] iteration 6042: total_loss: 0.135030, loss_sup: 0.074326, loss_mps: 0.022604, loss_cps: 0.038101
[12:45:01.336] iteration 6043: total_loss: 0.166381, loss_sup: 0.091836, loss_mps: 0.026666, loss_cps: 0.047879
[12:45:01.482] iteration 6044: total_loss: 0.165270, loss_sup: 0.087245, loss_mps: 0.027453, loss_cps: 0.050571
[12:45:01.629] iteration 6045: total_loss: 0.299084, loss_sup: 0.157456, loss_mps: 0.044520, loss_cps: 0.097108
[12:45:01.774] iteration 6046: total_loss: 0.093897, loss_sup: 0.028559, loss_mps: 0.023953, loss_cps: 0.041386
[12:45:01.922] iteration 6047: total_loss: 0.373478, loss_sup: 0.242436, loss_mps: 0.044275, loss_cps: 0.086766
[12:45:02.067] iteration 6048: total_loss: 0.124872, loss_sup: 0.055794, loss_mps: 0.025557, loss_cps: 0.043521
[12:45:02.213] iteration 6049: total_loss: 0.309882, loss_sup: 0.172271, loss_mps: 0.043751, loss_cps: 0.093861
[12:45:02.359] iteration 6050: total_loss: 0.251143, loss_sup: 0.158353, loss_mps: 0.031803, loss_cps: 0.060987
[12:45:02.505] iteration 6051: total_loss: 0.273909, loss_sup: 0.186961, loss_mps: 0.031912, loss_cps: 0.055035
[12:45:02.652] iteration 6052: total_loss: 0.109030, loss_sup: 0.026624, loss_mps: 0.028027, loss_cps: 0.054379
[12:45:02.799] iteration 6053: total_loss: 0.204742, loss_sup: 0.081103, loss_mps: 0.041646, loss_cps: 0.081992
[12:45:02.944] iteration 6054: total_loss: 0.207426, loss_sup: 0.143505, loss_mps: 0.022858, loss_cps: 0.041063
[12:45:03.092] iteration 6055: total_loss: 0.226198, loss_sup: 0.103067, loss_mps: 0.041072, loss_cps: 0.082059
[12:45:03.238] iteration 6056: total_loss: 0.121675, loss_sup: 0.030456, loss_mps: 0.032475, loss_cps: 0.058744
[12:45:03.384] iteration 6057: total_loss: 0.470409, loss_sup: 0.324921, loss_mps: 0.047990, loss_cps: 0.097497
[12:45:03.529] iteration 6058: total_loss: 0.175621, loss_sup: 0.078087, loss_mps: 0.032792, loss_cps: 0.064743
[12:45:03.675] iteration 6059: total_loss: 0.133749, loss_sup: 0.064963, loss_mps: 0.026132, loss_cps: 0.042654
[12:45:03.820] iteration 6060: total_loss: 0.327529, loss_sup: 0.197801, loss_mps: 0.042053, loss_cps: 0.087676
[12:45:03.966] iteration 6061: total_loss: 0.233221, loss_sup: 0.143934, loss_mps: 0.032428, loss_cps: 0.056859
[12:45:04.112] iteration 6062: total_loss: 0.221175, loss_sup: 0.097227, loss_mps: 0.041428, loss_cps: 0.082520
[12:45:04.258] iteration 6063: total_loss: 0.283008, loss_sup: 0.124870, loss_mps: 0.052188, loss_cps: 0.105950
[12:45:04.404] iteration 6064: total_loss: 0.271879, loss_sup: 0.097476, loss_mps: 0.056564, loss_cps: 0.117838
[12:45:04.550] iteration 6065: total_loss: 0.362697, loss_sup: 0.204231, loss_mps: 0.052289, loss_cps: 0.106177
[12:45:04.696] iteration 6066: total_loss: 0.212741, loss_sup: 0.108988, loss_mps: 0.035828, loss_cps: 0.067924
[12:45:04.842] iteration 6067: total_loss: 0.126936, loss_sup: 0.051125, loss_mps: 0.028917, loss_cps: 0.046894
[12:45:04.988] iteration 6068: total_loss: 0.174222, loss_sup: 0.034248, loss_mps: 0.046883, loss_cps: 0.093091
[12:45:05.134] iteration 6069: total_loss: 0.274935, loss_sup: 0.169874, loss_mps: 0.037009, loss_cps: 0.068053
[12:45:05.279] iteration 6070: total_loss: 0.139892, loss_sup: 0.042724, loss_mps: 0.033445, loss_cps: 0.063723
[12:45:05.425] iteration 6071: total_loss: 0.333879, loss_sup: 0.206238, loss_mps: 0.043762, loss_cps: 0.083878
[12:45:05.571] iteration 6072: total_loss: 0.297094, loss_sup: 0.172836, loss_mps: 0.043273, loss_cps: 0.080985
[12:45:05.717] iteration 6073: total_loss: 0.193731, loss_sup: 0.063819, loss_mps: 0.044504, loss_cps: 0.085407
[12:45:05.863] iteration 6074: total_loss: 0.313590, loss_sup: 0.174089, loss_mps: 0.045561, loss_cps: 0.093940
[12:45:06.010] iteration 6075: total_loss: 0.289657, loss_sup: 0.181660, loss_mps: 0.037761, loss_cps: 0.070236
[12:45:06.156] iteration 6076: total_loss: 0.166925, loss_sup: 0.079141, loss_mps: 0.032182, loss_cps: 0.055602
[12:45:06.302] iteration 6077: total_loss: 0.217037, loss_sup: 0.121555, loss_mps: 0.034521, loss_cps: 0.060961
[12:45:06.448] iteration 6078: total_loss: 0.134379, loss_sup: 0.051038, loss_mps: 0.030083, loss_cps: 0.053259
[12:45:06.594] iteration 6079: total_loss: 0.270246, loss_sup: 0.132979, loss_mps: 0.045650, loss_cps: 0.091617
[12:45:06.739] iteration 6080: total_loss: 0.220708, loss_sup: 0.104033, loss_mps: 0.042099, loss_cps: 0.074576
[12:45:06.885] iteration 6081: total_loss: 0.163837, loss_sup: 0.082940, loss_mps: 0.029814, loss_cps: 0.051083
[12:45:07.031] iteration 6082: total_loss: 0.170676, loss_sup: 0.064277, loss_mps: 0.036826, loss_cps: 0.069573
[12:45:07.176] iteration 6083: total_loss: 0.236127, loss_sup: 0.112785, loss_mps: 0.042741, loss_cps: 0.080601
[12:45:07.322] iteration 6084: total_loss: 0.442890, loss_sup: 0.291801, loss_mps: 0.050972, loss_cps: 0.100117
[12:45:07.468] iteration 6085: total_loss: 0.117536, loss_sup: 0.050183, loss_mps: 0.025255, loss_cps: 0.042098
[12:45:07.613] iteration 6086: total_loss: 0.255125, loss_sup: 0.124915, loss_mps: 0.044089, loss_cps: 0.086121
[12:45:07.760] iteration 6087: total_loss: 0.116166, loss_sup: 0.047092, loss_mps: 0.026564, loss_cps: 0.042509
[12:45:07.906] iteration 6088: total_loss: 0.161072, loss_sup: 0.035264, loss_mps: 0.042690, loss_cps: 0.083118
[12:45:08.052] iteration 6089: total_loss: 0.165383, loss_sup: 0.062327, loss_mps: 0.036897, loss_cps: 0.066159
[12:45:08.197] iteration 6090: total_loss: 0.169400, loss_sup: 0.102711, loss_mps: 0.025556, loss_cps: 0.041133
[12:45:08.343] iteration 6091: total_loss: 0.150458, loss_sup: 0.061499, loss_mps: 0.031322, loss_cps: 0.057637
[12:45:08.489] iteration 6092: total_loss: 0.226972, loss_sup: 0.138208, loss_mps: 0.031444, loss_cps: 0.057320
[12:45:08.634] iteration 6093: total_loss: 0.371543, loss_sup: 0.304830, loss_mps: 0.024142, loss_cps: 0.042571
[12:45:08.780] iteration 6094: total_loss: 0.245448, loss_sup: 0.152908, loss_mps: 0.033408, loss_cps: 0.059133
[12:45:08.926] iteration 6095: total_loss: 0.117256, loss_sup: 0.038738, loss_mps: 0.028864, loss_cps: 0.049655
[12:45:09.071] iteration 6096: total_loss: 0.204213, loss_sup: 0.090105, loss_mps: 0.039582, loss_cps: 0.074526
[12:45:09.217] iteration 6097: total_loss: 0.131433, loss_sup: 0.073010, loss_mps: 0.022631, loss_cps: 0.035792
[12:45:09.363] iteration 6098: total_loss: 0.293636, loss_sup: 0.225438, loss_mps: 0.025526, loss_cps: 0.042672
[12:45:09.508] iteration 6099: total_loss: 0.165915, loss_sup: 0.055388, loss_mps: 0.037171, loss_cps: 0.073356
[12:45:09.654] iteration 6100: total_loss: 0.208922, loss_sup: 0.069390, loss_mps: 0.047513, loss_cps: 0.092019
[12:45:09.654] Evaluation Started ==>
[12:45:21.067] ==> valid iteration 6100: unet metrics: {'dc': 0.6245697447288688, 'jc': 0.49652410985853207, 'pre': 0.6575743900264863, 'hd': 6.46113545332344}, ynet metrics: {'dc': 0.574254167088509, 'jc': 0.45031115898660345, 'pre': 0.6377637611867132, 'hd': 6.563496564661824}.
[12:45:21.069] Evaluation Finished!⏹️
[12:45:21.222] iteration 6101: total_loss: 0.174611, loss_sup: 0.058068, loss_mps: 0.040812, loss_cps: 0.075732
[12:45:21.370] iteration 6102: total_loss: 0.229410, loss_sup: 0.135130, loss_mps: 0.033616, loss_cps: 0.060664
[12:45:21.516] iteration 6103: total_loss: 0.186455, loss_sup: 0.089876, loss_mps: 0.034923, loss_cps: 0.061655
[12:45:21.664] iteration 6104: total_loss: 0.323266, loss_sup: 0.152661, loss_mps: 0.057341, loss_cps: 0.113264
[12:45:21.810] iteration 6105: total_loss: 0.202423, loss_sup: 0.135401, loss_mps: 0.026730, loss_cps: 0.040292
[12:45:21.956] iteration 6106: total_loss: 0.175255, loss_sup: 0.076959, loss_mps: 0.034937, loss_cps: 0.063359
[12:45:22.102] iteration 6107: total_loss: 0.222681, loss_sup: 0.125984, loss_mps: 0.035736, loss_cps: 0.060961
[12:45:22.249] iteration 6108: total_loss: 0.394355, loss_sup: 0.197163, loss_mps: 0.063106, loss_cps: 0.134085
[12:45:22.396] iteration 6109: total_loss: 0.126482, loss_sup: 0.033629, loss_mps: 0.033319, loss_cps: 0.059534
[12:45:22.547] iteration 6110: total_loss: 0.142736, loss_sup: 0.024239, loss_mps: 0.041166, loss_cps: 0.077332
[12:45:22.693] iteration 6111: total_loss: 0.273852, loss_sup: 0.120656, loss_mps: 0.051219, loss_cps: 0.101977
[12:45:22.839] iteration 6112: total_loss: 0.385696, loss_sup: 0.244098, loss_mps: 0.050329, loss_cps: 0.091269
[12:45:22.985] iteration 6113: total_loss: 0.373413, loss_sup: 0.272234, loss_mps: 0.036656, loss_cps: 0.064524
[12:45:23.131] iteration 6114: total_loss: 0.210914, loss_sup: 0.054417, loss_mps: 0.051850, loss_cps: 0.104648
[12:45:23.276] iteration 6115: total_loss: 0.425237, loss_sup: 0.285365, loss_mps: 0.047654, loss_cps: 0.092219
[12:45:23.423] iteration 6116: total_loss: 0.184507, loss_sup: 0.079500, loss_mps: 0.036935, loss_cps: 0.068072
[12:45:23.569] iteration 6117: total_loss: 0.137152, loss_sup: 0.029181, loss_mps: 0.037768, loss_cps: 0.070203
[12:45:23.714] iteration 6118: total_loss: 0.209065, loss_sup: 0.074016, loss_mps: 0.044316, loss_cps: 0.090733
[12:45:23.860] iteration 6119: total_loss: 0.250197, loss_sup: 0.130683, loss_mps: 0.040675, loss_cps: 0.078838
[12:45:24.007] iteration 6120: total_loss: 0.137885, loss_sup: 0.058782, loss_mps: 0.029343, loss_cps: 0.049760
[12:45:24.153] iteration 6121: total_loss: 0.181031, loss_sup: 0.043582, loss_mps: 0.046198, loss_cps: 0.091251
[12:45:24.300] iteration 6122: total_loss: 0.142762, loss_sup: 0.062403, loss_mps: 0.029137, loss_cps: 0.051222
[12:45:24.447] iteration 6123: total_loss: 0.170102, loss_sup: 0.064437, loss_mps: 0.037110, loss_cps: 0.068554
[12:45:24.593] iteration 6124: total_loss: 0.286233, loss_sup: 0.174451, loss_mps: 0.038792, loss_cps: 0.072990
[12:45:24.739] iteration 6125: total_loss: 0.221876, loss_sup: 0.108094, loss_mps: 0.039385, loss_cps: 0.074398
[12:45:24.885] iteration 6126: total_loss: 0.271526, loss_sup: 0.163575, loss_mps: 0.038717, loss_cps: 0.069234
[12:45:25.032] iteration 6127: total_loss: 0.174536, loss_sup: 0.065406, loss_mps: 0.037427, loss_cps: 0.071703
[12:45:25.181] iteration 6128: total_loss: 0.259573, loss_sup: 0.175315, loss_mps: 0.031350, loss_cps: 0.052908
[12:45:25.327] iteration 6129: total_loss: 0.200590, loss_sup: 0.087715, loss_mps: 0.039474, loss_cps: 0.073401
[12:45:25.476] iteration 6130: total_loss: 0.227468, loss_sup: 0.137820, loss_mps: 0.032043, loss_cps: 0.057605
[12:45:25.622] iteration 6131: total_loss: 0.210301, loss_sup: 0.092829, loss_mps: 0.042136, loss_cps: 0.075336
[12:45:25.768] iteration 6132: total_loss: 0.225045, loss_sup: 0.133426, loss_mps: 0.033433, loss_cps: 0.058186
[12:45:25.915] iteration 6133: total_loss: 0.324515, loss_sup: 0.223056, loss_mps: 0.037216, loss_cps: 0.064242
[12:45:26.064] iteration 6134: total_loss: 0.294162, loss_sup: 0.222135, loss_mps: 0.027123, loss_cps: 0.044905
[12:45:26.210] iteration 6135: total_loss: 0.378887, loss_sup: 0.243144, loss_mps: 0.046729, loss_cps: 0.089015
[12:45:26.358] iteration 6136: total_loss: 0.204957, loss_sup: 0.096580, loss_mps: 0.038090, loss_cps: 0.070286
[12:45:26.505] iteration 6137: total_loss: 0.316335, loss_sup: 0.175237, loss_mps: 0.047587, loss_cps: 0.093511
[12:45:26.652] iteration 6138: total_loss: 0.244589, loss_sup: 0.106465, loss_mps: 0.048006, loss_cps: 0.090118
[12:45:26.799] iteration 6139: total_loss: 0.183467, loss_sup: 0.096517, loss_mps: 0.031886, loss_cps: 0.055064
[12:45:26.950] iteration 6140: total_loss: 0.226379, loss_sup: 0.089252, loss_mps: 0.048381, loss_cps: 0.088746
[12:45:27.096] iteration 6141: total_loss: 0.245416, loss_sup: 0.124135, loss_mps: 0.042739, loss_cps: 0.078542
[12:45:27.242] iteration 6142: total_loss: 0.158399, loss_sup: 0.043864, loss_mps: 0.039680, loss_cps: 0.074855
[12:45:27.388] iteration 6143: total_loss: 0.190577, loss_sup: 0.120136, loss_mps: 0.026358, loss_cps: 0.044082
[12:45:27.534] iteration 6144: total_loss: 0.341854, loss_sup: 0.186917, loss_mps: 0.050709, loss_cps: 0.104228
[12:45:27.680] iteration 6145: total_loss: 0.201425, loss_sup: 0.096179, loss_mps: 0.037087, loss_cps: 0.068159
[12:45:27.829] iteration 6146: total_loss: 0.289529, loss_sup: 0.182315, loss_mps: 0.037977, loss_cps: 0.069238
[12:45:27.975] iteration 6147: total_loss: 0.091846, loss_sup: 0.027201, loss_mps: 0.024272, loss_cps: 0.040372
[12:45:28.121] iteration 6148: total_loss: 0.240556, loss_sup: 0.131081, loss_mps: 0.038653, loss_cps: 0.070823
[12:45:28.267] iteration 6149: total_loss: 0.192734, loss_sup: 0.102107, loss_mps: 0.033689, loss_cps: 0.056938
[12:45:28.416] iteration 6150: total_loss: 0.198140, loss_sup: 0.107978, loss_mps: 0.032034, loss_cps: 0.058129
[12:45:28.563] iteration 6151: total_loss: 0.326294, loss_sup: 0.237330, loss_mps: 0.032687, loss_cps: 0.056276
[12:45:28.714] iteration 6152: total_loss: 0.173022, loss_sup: 0.078348, loss_mps: 0.033157, loss_cps: 0.061517
[12:45:28.861] iteration 6153: total_loss: 0.150389, loss_sup: 0.064366, loss_mps: 0.031005, loss_cps: 0.055018
[12:45:29.008] iteration 6154: total_loss: 0.413760, loss_sup: 0.286811, loss_mps: 0.043206, loss_cps: 0.083743
[12:45:29.155] iteration 6155: total_loss: 0.301370, loss_sup: 0.213385, loss_mps: 0.032751, loss_cps: 0.055235
[12:45:29.302] iteration 6156: total_loss: 0.164162, loss_sup: 0.059031, loss_mps: 0.037495, loss_cps: 0.067636
[12:45:29.450] iteration 6157: total_loss: 0.328547, loss_sup: 0.162943, loss_mps: 0.055272, loss_cps: 0.110333
[12:45:29.597] iteration 6158: total_loss: 0.481368, loss_sup: 0.319191, loss_mps: 0.054895, loss_cps: 0.107281
[12:45:29.746] iteration 6159: total_loss: 0.240374, loss_sup: 0.094922, loss_mps: 0.049353, loss_cps: 0.096099
[12:45:29.892] iteration 6160: total_loss: 0.363992, loss_sup: 0.161626, loss_mps: 0.064826, loss_cps: 0.137539
[12:45:30.038] iteration 6161: total_loss: 0.123498, loss_sup: 0.046105, loss_mps: 0.028126, loss_cps: 0.049267
[12:45:30.185] iteration 6162: total_loss: 0.196827, loss_sup: 0.054503, loss_mps: 0.047108, loss_cps: 0.095216
[12:45:30.331] iteration 6163: total_loss: 0.201279, loss_sup: 0.090092, loss_mps: 0.041430, loss_cps: 0.069757
[12:45:30.478] iteration 6164: total_loss: 0.411632, loss_sup: 0.230778, loss_mps: 0.062129, loss_cps: 0.118726
[12:45:30.627] iteration 6165: total_loss: 0.179698, loss_sup: 0.074134, loss_mps: 0.039519, loss_cps: 0.066045
[12:45:30.773] iteration 6166: total_loss: 0.214031, loss_sup: 0.097995, loss_mps: 0.041684, loss_cps: 0.074352
[12:45:30.925] iteration 6167: total_loss: 0.268952, loss_sup: 0.082728, loss_mps: 0.063115, loss_cps: 0.123109
[12:45:31.071] iteration 6168: total_loss: 0.263682, loss_sup: 0.101813, loss_mps: 0.055361, loss_cps: 0.106509
[12:45:31.217] iteration 6169: total_loss: 0.491485, loss_sup: 0.333486, loss_mps: 0.054380, loss_cps: 0.103619
[12:45:31.364] iteration 6170: total_loss: 0.110235, loss_sup: 0.008958, loss_mps: 0.036702, loss_cps: 0.064575
[12:45:31.511] iteration 6171: total_loss: 0.172703, loss_sup: 0.032649, loss_mps: 0.049002, loss_cps: 0.091052
[12:45:31.658] iteration 6172: total_loss: 0.162919, loss_sup: 0.050536, loss_mps: 0.039625, loss_cps: 0.072757
[12:45:31.805] iteration 6173: total_loss: 0.234633, loss_sup: 0.144233, loss_mps: 0.034807, loss_cps: 0.055593
[12:45:31.954] iteration 6174: total_loss: 0.129903, loss_sup: 0.031169, loss_mps: 0.036411, loss_cps: 0.062324
[12:45:32.101] iteration 6175: total_loss: 0.188944, loss_sup: 0.085671, loss_mps: 0.037261, loss_cps: 0.066013
[12:45:32.247] iteration 6176: total_loss: 0.146169, loss_sup: 0.051931, loss_mps: 0.033691, loss_cps: 0.060548
[12:45:32.393] iteration 6177: total_loss: 0.205614, loss_sup: 0.083663, loss_mps: 0.044390, loss_cps: 0.077561
[12:45:32.541] iteration 6178: total_loss: 0.216066, loss_sup: 0.103532, loss_mps: 0.040885, loss_cps: 0.071649
[12:45:32.687] iteration 6179: total_loss: 0.258886, loss_sup: 0.151129, loss_mps: 0.038838, loss_cps: 0.068920
[12:45:32.833] iteration 6180: total_loss: 0.258489, loss_sup: 0.131322, loss_mps: 0.044067, loss_cps: 0.083100
[12:45:32.980] iteration 6181: total_loss: 0.233961, loss_sup: 0.104952, loss_mps: 0.045588, loss_cps: 0.083422
[12:45:33.126] iteration 6182: total_loss: 0.153588, loss_sup: 0.076995, loss_mps: 0.029456, loss_cps: 0.047137
[12:45:33.271] iteration 6183: total_loss: 0.136984, loss_sup: 0.055157, loss_mps: 0.030586, loss_cps: 0.051241
[12:45:33.418] iteration 6184: total_loss: 0.176526, loss_sup: 0.079363, loss_mps: 0.035283, loss_cps: 0.061880
[12:45:33.566] iteration 6185: total_loss: 0.188186, loss_sup: 0.100696, loss_mps: 0.031079, loss_cps: 0.056411
[12:45:33.712] iteration 6186: total_loss: 0.284953, loss_sup: 0.215396, loss_mps: 0.026553, loss_cps: 0.043004
[12:45:33.860] iteration 6187: total_loss: 0.199038, loss_sup: 0.107473, loss_mps: 0.032293, loss_cps: 0.059272
[12:45:34.007] iteration 6188: total_loss: 0.179351, loss_sup: 0.047367, loss_mps: 0.043855, loss_cps: 0.088129
[12:45:34.152] iteration 6189: total_loss: 0.184449, loss_sup: 0.101555, loss_mps: 0.030159, loss_cps: 0.052735
[12:45:34.299] iteration 6190: total_loss: 0.622154, loss_sup: 0.517334, loss_mps: 0.036555, loss_cps: 0.068265
[12:45:34.445] iteration 6191: total_loss: 0.151653, loss_sup: 0.068860, loss_mps: 0.030833, loss_cps: 0.051960
[12:45:34.592] iteration 6192: total_loss: 0.090299, loss_sup: 0.018313, loss_mps: 0.026324, loss_cps: 0.045662
[12:45:34.738] iteration 6193: total_loss: 0.158173, loss_sup: 0.043428, loss_mps: 0.039533, loss_cps: 0.075212
[12:45:34.884] iteration 6194: total_loss: 0.240084, loss_sup: 0.144777, loss_mps: 0.033600, loss_cps: 0.061706
[12:45:35.031] iteration 6195: total_loss: 0.265727, loss_sup: 0.166810, loss_mps: 0.036463, loss_cps: 0.062454
[12:45:35.177] iteration 6196: total_loss: 0.709559, loss_sup: 0.552921, loss_mps: 0.052992, loss_cps: 0.103647
[12:45:35.324] iteration 6197: total_loss: 0.120511, loss_sup: 0.044460, loss_mps: 0.028476, loss_cps: 0.047575
[12:45:35.472] iteration 6198: total_loss: 0.378546, loss_sup: 0.248295, loss_mps: 0.047352, loss_cps: 0.082898
[12:45:35.619] iteration 6199: total_loss: 0.220497, loss_sup: 0.087996, loss_mps: 0.044781, loss_cps: 0.087721
[12:45:35.765] iteration 6200: total_loss: 0.141748, loss_sup: 0.050232, loss_mps: 0.034721, loss_cps: 0.056795
[12:45:35.765] Evaluation Started ==>
[12:45:47.122] ==> valid iteration 6200: unet metrics: {'dc': 0.6054667813416706, 'jc': 0.48184254724981573, 'pre': 0.7129395434384842, 'hd': 5.94226827332721}, ynet metrics: {'dc': 0.5692628380546861, 'jc': 0.4416012934354219, 'pre': 0.6595647927657907, 'hd': 6.519793136734567}.
[12:45:47.124] Evaluation Finished!⏹️
[12:45:47.275] iteration 6201: total_loss: 0.162707, loss_sup: 0.068325, loss_mps: 0.034140, loss_cps: 0.060241
[12:45:47.422] iteration 6202: total_loss: 0.383114, loss_sup: 0.248835, loss_mps: 0.046196, loss_cps: 0.088084
[12:45:47.568] iteration 6203: total_loss: 0.214594, loss_sup: 0.074360, loss_mps: 0.049104, loss_cps: 0.091130
[12:45:47.713] iteration 6204: total_loss: 0.173119, loss_sup: 0.038867, loss_mps: 0.044870, loss_cps: 0.089383
[12:45:47.858] iteration 6205: total_loss: 0.250583, loss_sup: 0.141992, loss_mps: 0.038782, loss_cps: 0.069809
[12:45:48.005] iteration 6206: total_loss: 0.258018, loss_sup: 0.118417, loss_mps: 0.046903, loss_cps: 0.092698
[12:45:48.151] iteration 6207: total_loss: 0.344906, loss_sup: 0.190638, loss_mps: 0.052644, loss_cps: 0.101624
[12:45:48.297] iteration 6208: total_loss: 0.204357, loss_sup: 0.072323, loss_mps: 0.046427, loss_cps: 0.085608
[12:45:48.443] iteration 6209: total_loss: 0.287375, loss_sup: 0.087381, loss_mps: 0.064007, loss_cps: 0.135988
[12:45:48.589] iteration 6210: total_loss: 0.142095, loss_sup: 0.058513, loss_mps: 0.031980, loss_cps: 0.051602
[12:45:48.734] iteration 6211: total_loss: 0.205863, loss_sup: 0.118268, loss_mps: 0.034066, loss_cps: 0.053529
[12:45:48.880] iteration 6212: total_loss: 0.238239, loss_sup: 0.115077, loss_mps: 0.043805, loss_cps: 0.079357
[12:45:49.025] iteration 6213: total_loss: 0.190676, loss_sup: 0.054321, loss_mps: 0.045808, loss_cps: 0.090548
[12:45:49.173] iteration 6214: total_loss: 0.389378, loss_sup: 0.278447, loss_mps: 0.040939, loss_cps: 0.069992
[12:45:49.320] iteration 6215: total_loss: 0.153067, loss_sup: 0.076912, loss_mps: 0.027876, loss_cps: 0.048280
[12:45:49.466] iteration 6216: total_loss: 0.257129, loss_sup: 0.124959, loss_mps: 0.045637, loss_cps: 0.086532
[12:45:49.611] iteration 6217: total_loss: 0.147626, loss_sup: 0.036306, loss_mps: 0.039298, loss_cps: 0.072022
[12:45:49.757] iteration 6218: total_loss: 0.277062, loss_sup: 0.172826, loss_mps: 0.036667, loss_cps: 0.067570
[12:45:49.902] iteration 6219: total_loss: 0.169126, loss_sup: 0.069708, loss_mps: 0.035047, loss_cps: 0.064372
[12:45:50.047] iteration 6220: total_loss: 0.103261, loss_sup: 0.036823, loss_mps: 0.025878, loss_cps: 0.040560
[12:45:50.193] iteration 6221: total_loss: 0.361276, loss_sup: 0.255830, loss_mps: 0.037070, loss_cps: 0.068376
[12:45:50.340] iteration 6222: total_loss: 0.172189, loss_sup: 0.091490, loss_mps: 0.029258, loss_cps: 0.051441
[12:45:50.486] iteration 6223: total_loss: 0.231022, loss_sup: 0.133452, loss_mps: 0.036309, loss_cps: 0.061261
[12:45:50.635] iteration 6224: total_loss: 0.271249, loss_sup: 0.159030, loss_mps: 0.038741, loss_cps: 0.073478
[12:45:50.780] iteration 6225: total_loss: 0.272199, loss_sup: 0.181435, loss_mps: 0.031808, loss_cps: 0.058956
[12:45:50.926] iteration 6226: total_loss: 0.137682, loss_sup: 0.034416, loss_mps: 0.036058, loss_cps: 0.067208
[12:45:51.073] iteration 6227: total_loss: 0.186255, loss_sup: 0.072653, loss_mps: 0.039710, loss_cps: 0.073892
[12:45:51.218] iteration 6228: total_loss: 0.150019, loss_sup: 0.025400, loss_mps: 0.042406, loss_cps: 0.082213
[12:45:51.364] iteration 6229: total_loss: 0.366323, loss_sup: 0.232386, loss_mps: 0.044345, loss_cps: 0.089592
[12:45:51.510] iteration 6230: total_loss: 0.254097, loss_sup: 0.139439, loss_mps: 0.039412, loss_cps: 0.075246
[12:45:51.655] iteration 6231: total_loss: 0.160443, loss_sup: 0.057528, loss_mps: 0.035365, loss_cps: 0.067550
[12:45:51.801] iteration 6232: total_loss: 0.298105, loss_sup: 0.180041, loss_mps: 0.042270, loss_cps: 0.075794
[12:45:51.946] iteration 6233: total_loss: 0.412961, loss_sup: 0.230318, loss_mps: 0.059078, loss_cps: 0.123565
[12:45:52.092] iteration 6234: total_loss: 0.317592, loss_sup: 0.164209, loss_mps: 0.053783, loss_cps: 0.099600
[12:45:52.238] iteration 6235: total_loss: 0.135750, loss_sup: 0.037245, loss_mps: 0.036057, loss_cps: 0.062448
[12:45:52.383] iteration 6236: total_loss: 0.289552, loss_sup: 0.174727, loss_mps: 0.040794, loss_cps: 0.074031
[12:45:52.529] iteration 6237: total_loss: 0.141306, loss_sup: 0.051581, loss_mps: 0.031153, loss_cps: 0.058572
[12:45:52.676] iteration 6238: total_loss: 0.269372, loss_sup: 0.169694, loss_mps: 0.034404, loss_cps: 0.065274
[12:45:52.821] iteration 6239: total_loss: 0.157859, loss_sup: 0.068554, loss_mps: 0.030846, loss_cps: 0.058458
[12:45:52.967] iteration 6240: total_loss: 0.246509, loss_sup: 0.128840, loss_mps: 0.040916, loss_cps: 0.076753
[12:45:53.113] iteration 6241: total_loss: 0.167257, loss_sup: 0.085139, loss_mps: 0.028688, loss_cps: 0.053429
[12:45:53.259] iteration 6242: total_loss: 0.215214, loss_sup: 0.132209, loss_mps: 0.029638, loss_cps: 0.053367
[12:45:53.407] iteration 6243: total_loss: 0.249681, loss_sup: 0.133009, loss_mps: 0.039663, loss_cps: 0.077008
[12:45:53.552] iteration 6244: total_loss: 0.425923, loss_sup: 0.318159, loss_mps: 0.037373, loss_cps: 0.070392
[12:45:53.698] iteration 6245: total_loss: 0.147746, loss_sup: 0.061856, loss_mps: 0.030481, loss_cps: 0.055410
[12:45:53.844] iteration 6246: total_loss: 0.230516, loss_sup: 0.116594, loss_mps: 0.039485, loss_cps: 0.074437
[12:45:53.990] iteration 6247: total_loss: 0.233859, loss_sup: 0.104557, loss_mps: 0.044561, loss_cps: 0.084741
[12:45:54.136] iteration 6248: total_loss: 0.158382, loss_sup: 0.077142, loss_mps: 0.029119, loss_cps: 0.052121
[12:45:54.281] iteration 6249: total_loss: 0.199993, loss_sup: 0.109042, loss_mps: 0.031536, loss_cps: 0.059415
[12:45:54.427] iteration 6250: total_loss: 0.256632, loss_sup: 0.099766, loss_mps: 0.052088, loss_cps: 0.104778
[12:45:54.574] iteration 6251: total_loss: 0.248911, loss_sup: 0.074443, loss_mps: 0.058064, loss_cps: 0.116404
[12:45:54.719] iteration 6252: total_loss: 0.308861, loss_sup: 0.179336, loss_mps: 0.044997, loss_cps: 0.084529
[12:45:54.865] iteration 6253: total_loss: 0.260365, loss_sup: 0.105877, loss_mps: 0.051543, loss_cps: 0.102945
[12:45:55.011] iteration 6254: total_loss: 0.252489, loss_sup: 0.159885, loss_mps: 0.034660, loss_cps: 0.057944
[12:45:55.158] iteration 6255: total_loss: 0.383855, loss_sup: 0.244075, loss_mps: 0.047026, loss_cps: 0.092754
[12:45:55.303] iteration 6256: total_loss: 0.544789, loss_sup: 0.408192, loss_mps: 0.046532, loss_cps: 0.090064
[12:45:55.448] iteration 6257: total_loss: 0.298154, loss_sup: 0.187476, loss_mps: 0.040363, loss_cps: 0.070314
[12:45:55.594] iteration 6258: total_loss: 0.177148, loss_sup: 0.088841, loss_mps: 0.032431, loss_cps: 0.055876
[12:45:55.740] iteration 6259: total_loss: 0.233254, loss_sup: 0.124671, loss_mps: 0.038406, loss_cps: 0.070176
[12:45:55.888] iteration 6260: total_loss: 0.316427, loss_sup: 0.184666, loss_mps: 0.045896, loss_cps: 0.085866
[12:45:56.034] iteration 6261: total_loss: 0.146022, loss_sup: 0.063335, loss_mps: 0.030804, loss_cps: 0.051883
[12:45:56.180] iteration 6262: total_loss: 0.249324, loss_sup: 0.130050, loss_mps: 0.040697, loss_cps: 0.078578
[12:45:56.326] iteration 6263: total_loss: 0.166427, loss_sup: 0.081997, loss_mps: 0.029697, loss_cps: 0.054732
[12:45:56.472] iteration 6264: total_loss: 0.169778, loss_sup: 0.077048, loss_mps: 0.033732, loss_cps: 0.058999
[12:45:56.617] iteration 6265: total_loss: 0.267205, loss_sup: 0.158997, loss_mps: 0.037723, loss_cps: 0.070485
[12:45:56.763] iteration 6266: total_loss: 0.535378, loss_sup: 0.387518, loss_mps: 0.048606, loss_cps: 0.099254
[12:45:56.909] iteration 6267: total_loss: 0.314787, loss_sup: 0.185736, loss_mps: 0.044977, loss_cps: 0.084074
[12:45:57.054] iteration 6268: total_loss: 0.163032, loss_sup: 0.062632, loss_mps: 0.036369, loss_cps: 0.064030
[12:45:57.199] iteration 6269: total_loss: 0.191782, loss_sup: 0.048560, loss_mps: 0.047223, loss_cps: 0.095998
[12:45:57.262] iteration 6270: total_loss: 0.158968, loss_sup: 0.086409, loss_mps: 0.026516, loss_cps: 0.046044
[12:45:58.481] iteration 6271: total_loss: 0.170104, loss_sup: 0.092633, loss_mps: 0.029037, loss_cps: 0.048435
[12:45:58.631] iteration 6272: total_loss: 0.242866, loss_sup: 0.087404, loss_mps: 0.053395, loss_cps: 0.102068
[12:45:58.779] iteration 6273: total_loss: 0.172764, loss_sup: 0.069066, loss_mps: 0.038179, loss_cps: 0.065520
[12:45:58.926] iteration 6274: total_loss: 0.339842, loss_sup: 0.225541, loss_mps: 0.039558, loss_cps: 0.074743
[12:45:59.073] iteration 6275: total_loss: 0.211745, loss_sup: 0.107590, loss_mps: 0.037593, loss_cps: 0.066563
[12:45:59.221] iteration 6276: total_loss: 0.170865, loss_sup: 0.075579, loss_mps: 0.036511, loss_cps: 0.058774
[12:45:59.369] iteration 6277: total_loss: 0.273004, loss_sup: 0.163587, loss_mps: 0.038680, loss_cps: 0.070737
[12:45:59.517] iteration 6278: total_loss: 0.339996, loss_sup: 0.203002, loss_mps: 0.046132, loss_cps: 0.090863
[12:45:59.666] iteration 6279: total_loss: 0.234676, loss_sup: 0.123701, loss_mps: 0.040394, loss_cps: 0.070581
[12:45:59.813] iteration 6280: total_loss: 0.259429, loss_sup: 0.126804, loss_mps: 0.046188, loss_cps: 0.086438
[12:45:59.959] iteration 6281: total_loss: 0.299586, loss_sup: 0.184354, loss_mps: 0.039996, loss_cps: 0.075236
[12:46:00.107] iteration 6282: total_loss: 0.176544, loss_sup: 0.099532, loss_mps: 0.029986, loss_cps: 0.047026
[12:46:00.255] iteration 6283: total_loss: 0.185857, loss_sup: 0.103469, loss_mps: 0.029956, loss_cps: 0.052432
[12:46:00.402] iteration 6284: total_loss: 0.158646, loss_sup: 0.055842, loss_mps: 0.036347, loss_cps: 0.066456
[12:46:00.550] iteration 6285: total_loss: 0.291964, loss_sup: 0.189318, loss_mps: 0.036820, loss_cps: 0.065826
[12:46:00.697] iteration 6286: total_loss: 0.162877, loss_sup: 0.075959, loss_mps: 0.030474, loss_cps: 0.056443
[12:46:00.844] iteration 6287: total_loss: 0.240783, loss_sup: 0.091162, loss_mps: 0.052827, loss_cps: 0.096793
[12:46:00.990] iteration 6288: total_loss: 0.144957, loss_sup: 0.016431, loss_mps: 0.042980, loss_cps: 0.085547
[12:46:01.137] iteration 6289: total_loss: 0.432418, loss_sup: 0.258475, loss_mps: 0.057262, loss_cps: 0.116682
[12:46:01.286] iteration 6290: total_loss: 0.147335, loss_sup: 0.054166, loss_mps: 0.033956, loss_cps: 0.059214
[12:46:01.435] iteration 6291: total_loss: 0.155924, loss_sup: 0.058174, loss_mps: 0.034798, loss_cps: 0.062952
[12:46:01.583] iteration 6292: total_loss: 0.156698, loss_sup: 0.056093, loss_mps: 0.035648, loss_cps: 0.064956
[12:46:01.730] iteration 6293: total_loss: 0.330840, loss_sup: 0.150751, loss_mps: 0.060701, loss_cps: 0.119388
[12:46:01.878] iteration 6294: total_loss: 0.295773, loss_sup: 0.164961, loss_mps: 0.044055, loss_cps: 0.086757
[12:46:02.025] iteration 6295: total_loss: 0.301860, loss_sup: 0.167775, loss_mps: 0.044899, loss_cps: 0.089186
[12:46:02.173] iteration 6296: total_loss: 0.149393, loss_sup: 0.059202, loss_mps: 0.032004, loss_cps: 0.058187
[12:46:02.320] iteration 6297: total_loss: 0.206822, loss_sup: 0.098015, loss_mps: 0.036980, loss_cps: 0.071827
[12:46:02.470] iteration 6298: total_loss: 0.195094, loss_sup: 0.072746, loss_mps: 0.041878, loss_cps: 0.080470
[12:46:02.618] iteration 6299: total_loss: 0.121329, loss_sup: 0.028516, loss_mps: 0.032822, loss_cps: 0.059991
[12:46:02.764] iteration 6300: total_loss: 0.146946, loss_sup: 0.025540, loss_mps: 0.041721, loss_cps: 0.079685
[12:46:02.764] Evaluation Started ==>
[12:46:14.218] ==> valid iteration 6300: unet metrics: {'dc': 0.6168456635098903, 'jc': 0.48700621179188897, 'pre': 0.6964544843281884, 'hd': 6.197867079329019}, ynet metrics: {'dc': 0.5895692868489188, 'jc': 0.4620035938896464, 'pre': 0.6927760218489138, 'hd': 6.423340093252828}.
[12:46:14.372] ==> New best valid dice for ynet: 0.589569, at iteration 6300
[12:46:14.373] Evaluation Finished!⏹️
[12:46:14.525] iteration 6301: total_loss: 0.186353, loss_sup: 0.067766, loss_mps: 0.041882, loss_cps: 0.076706
[12:46:14.671] iteration 6302: total_loss: 0.220863, loss_sup: 0.066682, loss_mps: 0.049866, loss_cps: 0.104316
[12:46:14.816] iteration 6303: total_loss: 0.442815, loss_sup: 0.221727, loss_mps: 0.067625, loss_cps: 0.153463
[12:46:14.961] iteration 6304: total_loss: 0.249492, loss_sup: 0.124280, loss_mps: 0.041478, loss_cps: 0.083734
[12:46:15.106] iteration 6305: total_loss: 0.233300, loss_sup: 0.146227, loss_mps: 0.031592, loss_cps: 0.055481
[12:46:15.251] iteration 6306: total_loss: 0.194717, loss_sup: 0.080163, loss_mps: 0.037575, loss_cps: 0.076978
[12:46:15.396] iteration 6307: total_loss: 0.193277, loss_sup: 0.091841, loss_mps: 0.034532, loss_cps: 0.066904
[12:46:15.541] iteration 6308: total_loss: 0.202729, loss_sup: 0.136580, loss_mps: 0.025410, loss_cps: 0.040739
[12:46:15.686] iteration 6309: total_loss: 0.188227, loss_sup: 0.083104, loss_mps: 0.037886, loss_cps: 0.067237
[12:46:15.835] iteration 6310: total_loss: 0.168389, loss_sup: 0.110762, loss_mps: 0.022116, loss_cps: 0.035511
[12:46:15.980] iteration 6311: total_loss: 0.142101, loss_sup: 0.050461, loss_mps: 0.032819, loss_cps: 0.058821
[12:46:16.125] iteration 6312: total_loss: 0.479149, loss_sup: 0.373555, loss_mps: 0.034740, loss_cps: 0.070853
[12:46:16.270] iteration 6313: total_loss: 0.176107, loss_sup: 0.088543, loss_mps: 0.030945, loss_cps: 0.056618
[12:46:16.418] iteration 6314: total_loss: 0.246591, loss_sup: 0.136127, loss_mps: 0.038258, loss_cps: 0.072205
[12:46:16.563] iteration 6315: total_loss: 0.216817, loss_sup: 0.082125, loss_mps: 0.044504, loss_cps: 0.090188
[12:46:16.708] iteration 6316: total_loss: 0.192901, loss_sup: 0.051671, loss_mps: 0.045966, loss_cps: 0.095264
[12:46:16.853] iteration 6317: total_loss: 0.441271, loss_sup: 0.314603, loss_mps: 0.042756, loss_cps: 0.083913
[12:46:16.999] iteration 6318: total_loss: 0.243658, loss_sup: 0.159345, loss_mps: 0.030899, loss_cps: 0.053414
[12:46:17.144] iteration 6319: total_loss: 0.277519, loss_sup: 0.076320, loss_mps: 0.063864, loss_cps: 0.137334
[12:46:17.289] iteration 6320: total_loss: 0.177767, loss_sup: 0.052059, loss_mps: 0.043161, loss_cps: 0.082547
[12:46:17.434] iteration 6321: total_loss: 0.204254, loss_sup: 0.085608, loss_mps: 0.040180, loss_cps: 0.078466
[12:46:17.581] iteration 6322: total_loss: 0.228973, loss_sup: 0.111736, loss_mps: 0.040222, loss_cps: 0.077016
[12:46:17.726] iteration 6323: total_loss: 0.119779, loss_sup: 0.024794, loss_mps: 0.034285, loss_cps: 0.060700
[12:46:17.871] iteration 6324: total_loss: 0.122832, loss_sup: 0.042390, loss_mps: 0.030265, loss_cps: 0.050177
[12:46:18.017] iteration 6325: total_loss: 0.458366, loss_sup: 0.334898, loss_mps: 0.041686, loss_cps: 0.081783
[12:46:18.162] iteration 6326: total_loss: 0.234566, loss_sup: 0.087511, loss_mps: 0.050549, loss_cps: 0.096505
[12:46:18.307] iteration 6327: total_loss: 0.129594, loss_sup: 0.066524, loss_mps: 0.024702, loss_cps: 0.038367
[12:46:18.453] iteration 6328: total_loss: 0.232855, loss_sup: 0.133182, loss_mps: 0.035005, loss_cps: 0.064669
[12:46:18.598] iteration 6329: total_loss: 0.147074, loss_sup: 0.057029, loss_mps: 0.032239, loss_cps: 0.057806
[12:46:18.744] iteration 6330: total_loss: 0.140113, loss_sup: 0.045984, loss_mps: 0.034316, loss_cps: 0.059813
[12:46:18.889] iteration 6331: total_loss: 0.319895, loss_sup: 0.168739, loss_mps: 0.051816, loss_cps: 0.099341
[12:46:19.034] iteration 6332: total_loss: 0.198829, loss_sup: 0.101065, loss_mps: 0.036117, loss_cps: 0.061648
[12:46:19.181] iteration 6333: total_loss: 0.186862, loss_sup: 0.121613, loss_mps: 0.026069, loss_cps: 0.039180
[12:46:19.327] iteration 6334: total_loss: 0.276201, loss_sup: 0.129150, loss_mps: 0.048612, loss_cps: 0.098439
[12:46:19.473] iteration 6335: total_loss: 0.373733, loss_sup: 0.199846, loss_mps: 0.056795, loss_cps: 0.117091
[12:46:19.618] iteration 6336: total_loss: 0.159010, loss_sup: 0.070843, loss_mps: 0.032962, loss_cps: 0.055205
[12:46:19.764] iteration 6337: total_loss: 0.207686, loss_sup: 0.114898, loss_mps: 0.033797, loss_cps: 0.058992
[12:46:19.910] iteration 6338: total_loss: 0.112207, loss_sup: 0.022093, loss_mps: 0.033182, loss_cps: 0.056933
[12:46:20.055] iteration 6339: total_loss: 0.108934, loss_sup: 0.037538, loss_mps: 0.027208, loss_cps: 0.044188
[12:46:20.200] iteration 6340: total_loss: 0.545521, loss_sup: 0.373725, loss_mps: 0.057572, loss_cps: 0.114224
[12:46:20.345] iteration 6341: total_loss: 0.095518, loss_sup: 0.032146, loss_mps: 0.025831, loss_cps: 0.037541
[12:46:20.491] iteration 6342: total_loss: 0.284744, loss_sup: 0.146194, loss_mps: 0.047845, loss_cps: 0.090705
[12:46:20.635] iteration 6343: total_loss: 0.303718, loss_sup: 0.190758, loss_mps: 0.039402, loss_cps: 0.073557
[12:46:20.781] iteration 6344: total_loss: 0.140504, loss_sup: 0.052360, loss_mps: 0.032907, loss_cps: 0.055237
[12:46:20.928] iteration 6345: total_loss: 0.178316, loss_sup: 0.094180, loss_mps: 0.032055, loss_cps: 0.052081
[12:46:21.074] iteration 6346: total_loss: 0.291690, loss_sup: 0.149682, loss_mps: 0.048507, loss_cps: 0.093500
[12:46:21.219] iteration 6347: total_loss: 0.226424, loss_sup: 0.075471, loss_mps: 0.052325, loss_cps: 0.098628
[12:46:21.364] iteration 6348: total_loss: 0.276964, loss_sup: 0.136739, loss_mps: 0.049095, loss_cps: 0.091129
[12:46:21.509] iteration 6349: total_loss: 0.336401, loss_sup: 0.226709, loss_mps: 0.038233, loss_cps: 0.071459
[12:46:21.654] iteration 6350: total_loss: 0.127473, loss_sup: 0.046091, loss_mps: 0.031305, loss_cps: 0.050077
[12:46:21.800] iteration 6351: total_loss: 0.174344, loss_sup: 0.086027, loss_mps: 0.032570, loss_cps: 0.055747
[12:46:21.945] iteration 6352: total_loss: 0.214151, loss_sup: 0.075082, loss_mps: 0.048634, loss_cps: 0.090436
[12:46:22.090] iteration 6353: total_loss: 0.252411, loss_sup: 0.148796, loss_mps: 0.038482, loss_cps: 0.065132
[12:46:22.239] iteration 6354: total_loss: 0.445495, loss_sup: 0.310058, loss_mps: 0.044661, loss_cps: 0.090776
[12:46:22.386] iteration 6355: total_loss: 0.187588, loss_sup: 0.078982, loss_mps: 0.039210, loss_cps: 0.069397
[12:46:22.533] iteration 6356: total_loss: 0.194699, loss_sup: 0.084974, loss_mps: 0.038737, loss_cps: 0.070988
[12:46:22.680] iteration 6357: total_loss: 0.169182, loss_sup: 0.074488, loss_mps: 0.034712, loss_cps: 0.059983
[12:46:22.826] iteration 6358: total_loss: 0.141788, loss_sup: 0.062379, loss_mps: 0.029439, loss_cps: 0.049970
[12:46:22.971] iteration 6359: total_loss: 0.236409, loss_sup: 0.098455, loss_mps: 0.048852, loss_cps: 0.089102
[12:46:23.122] iteration 6360: total_loss: 0.219703, loss_sup: 0.085047, loss_mps: 0.044164, loss_cps: 0.090492
[12:46:23.269] iteration 6361: total_loss: 0.325297, loss_sup: 0.210686, loss_mps: 0.040971, loss_cps: 0.073641
[12:46:23.416] iteration 6362: total_loss: 0.407781, loss_sup: 0.268241, loss_mps: 0.047763, loss_cps: 0.091776
[12:46:23.562] iteration 6363: total_loss: 0.268881, loss_sup: 0.152282, loss_mps: 0.040682, loss_cps: 0.075917
[12:46:23.712] iteration 6364: total_loss: 0.219931, loss_sup: 0.135413, loss_mps: 0.032257, loss_cps: 0.052261
[12:46:23.857] iteration 6365: total_loss: 0.124384, loss_sup: 0.039834, loss_mps: 0.031494, loss_cps: 0.053056
[12:46:24.005] iteration 6366: total_loss: 0.259337, loss_sup: 0.146664, loss_mps: 0.039377, loss_cps: 0.073296
[12:46:24.151] iteration 6367: total_loss: 0.220100, loss_sup: 0.073649, loss_mps: 0.049452, loss_cps: 0.096999
[12:46:24.298] iteration 6368: total_loss: 0.275291, loss_sup: 0.143866, loss_mps: 0.046087, loss_cps: 0.085338
[12:46:24.444] iteration 6369: total_loss: 0.180295, loss_sup: 0.048225, loss_mps: 0.044636, loss_cps: 0.087434
[12:46:24.590] iteration 6370: total_loss: 0.213422, loss_sup: 0.090330, loss_mps: 0.042011, loss_cps: 0.081081
[12:46:24.736] iteration 6371: total_loss: 0.210608, loss_sup: 0.104396, loss_mps: 0.038447, loss_cps: 0.067764
[12:46:24.881] iteration 6372: total_loss: 0.281160, loss_sup: 0.153051, loss_mps: 0.044907, loss_cps: 0.083203
[12:46:25.027] iteration 6373: total_loss: 0.178602, loss_sup: 0.062767, loss_mps: 0.041741, loss_cps: 0.074094
[12:46:25.173] iteration 6374: total_loss: 0.135210, loss_sup: 0.041555, loss_mps: 0.033321, loss_cps: 0.060334
[12:46:25.320] iteration 6375: total_loss: 0.202518, loss_sup: 0.099522, loss_mps: 0.037823, loss_cps: 0.065173
[12:46:25.466] iteration 6376: total_loss: 0.645534, loss_sup: 0.488672, loss_mps: 0.054506, loss_cps: 0.102356
[12:46:25.611] iteration 6377: total_loss: 0.305820, loss_sup: 0.175395, loss_mps: 0.044892, loss_cps: 0.085533
[12:46:25.757] iteration 6378: total_loss: 0.228124, loss_sup: 0.114260, loss_mps: 0.040474, loss_cps: 0.073390
[12:46:25.903] iteration 6379: total_loss: 0.232331, loss_sup: 0.090596, loss_mps: 0.047942, loss_cps: 0.093792
[12:46:26.049] iteration 6380: total_loss: 0.258494, loss_sup: 0.129599, loss_mps: 0.044097, loss_cps: 0.084798
[12:46:26.197] iteration 6381: total_loss: 0.139156, loss_sup: 0.082696, loss_mps: 0.022422, loss_cps: 0.034038
[12:46:26.346] iteration 6382: total_loss: 0.154234, loss_sup: 0.026220, loss_mps: 0.044792, loss_cps: 0.083221
[12:46:26.493] iteration 6383: total_loss: 0.104669, loss_sup: 0.028974, loss_mps: 0.028575, loss_cps: 0.047119
[12:46:26.639] iteration 6384: total_loss: 0.170252, loss_sup: 0.073712, loss_mps: 0.034862, loss_cps: 0.061678
[12:46:26.787] iteration 6385: total_loss: 0.218222, loss_sup: 0.144915, loss_mps: 0.027227, loss_cps: 0.046080
[12:46:26.936] iteration 6386: total_loss: 0.158916, loss_sup: 0.062815, loss_mps: 0.033337, loss_cps: 0.062764
[12:46:27.082] iteration 6387: total_loss: 0.253781, loss_sup: 0.151652, loss_mps: 0.037073, loss_cps: 0.065056
[12:46:27.232] iteration 6388: total_loss: 0.129027, loss_sup: 0.037567, loss_mps: 0.033935, loss_cps: 0.057525
[12:46:27.379] iteration 6389: total_loss: 0.207967, loss_sup: 0.075486, loss_mps: 0.044464, loss_cps: 0.088017
[12:46:27.525] iteration 6390: total_loss: 0.256132, loss_sup: 0.133704, loss_mps: 0.043972, loss_cps: 0.078456
[12:46:27.671] iteration 6391: total_loss: 0.112009, loss_sup: 0.039587, loss_mps: 0.026948, loss_cps: 0.045475
[12:46:27.817] iteration 6392: total_loss: 0.289288, loss_sup: 0.197734, loss_mps: 0.032788, loss_cps: 0.058765
[12:46:27.963] iteration 6393: total_loss: 0.150252, loss_sup: 0.082974, loss_mps: 0.025022, loss_cps: 0.042256
[12:46:28.112] iteration 6394: total_loss: 0.244340, loss_sup: 0.167983, loss_mps: 0.030185, loss_cps: 0.046172
[12:46:28.262] iteration 6395: total_loss: 0.517327, loss_sup: 0.360969, loss_mps: 0.050173, loss_cps: 0.106186
[12:46:28.408] iteration 6396: total_loss: 0.252236, loss_sup: 0.118034, loss_mps: 0.046286, loss_cps: 0.087917
[12:46:28.554] iteration 6397: total_loss: 0.111764, loss_sup: 0.029411, loss_mps: 0.030583, loss_cps: 0.051770
[12:46:28.701] iteration 6398: total_loss: 0.371050, loss_sup: 0.228801, loss_mps: 0.048244, loss_cps: 0.094004
[12:46:28.847] iteration 6399: total_loss: 0.190663, loss_sup: 0.109191, loss_mps: 0.030130, loss_cps: 0.051341
[12:46:28.993] iteration 6400: total_loss: 0.159486, loss_sup: 0.019691, loss_mps: 0.046663, loss_cps: 0.093132
[12:46:28.993] Evaluation Started ==>
[12:46:40.385] ==> valid iteration 6400: unet metrics: {'dc': 0.5868840878195306, 'jc': 0.46234751375870714, 'pre': 0.6724619860032338, 'hd': 6.46078701207489}, ynet metrics: {'dc': 0.5511722495080323, 'jc': 0.43087343985749654, 'pre': 0.676026692956708, 'hd': 6.300239417719651}.
[12:46:40.386] Evaluation Finished!⏹️
[12:46:40.539] iteration 6401: total_loss: 0.215544, loss_sup: 0.081471, loss_mps: 0.045895, loss_cps: 0.088177
[12:46:40.686] iteration 6402: total_loss: 0.191950, loss_sup: 0.046288, loss_mps: 0.049279, loss_cps: 0.096384
[12:46:40.831] iteration 6403: total_loss: 0.272210, loss_sup: 0.141553, loss_mps: 0.043858, loss_cps: 0.086799
[12:46:40.977] iteration 6404: total_loss: 0.154669, loss_sup: 0.052785, loss_mps: 0.037167, loss_cps: 0.064716
[12:46:41.122] iteration 6405: total_loss: 0.261606, loss_sup: 0.158937, loss_mps: 0.036888, loss_cps: 0.065781
[12:46:41.267] iteration 6406: total_loss: 0.186182, loss_sup: 0.082563, loss_mps: 0.036528, loss_cps: 0.067091
[12:46:41.413] iteration 6407: total_loss: 0.151917, loss_sup: 0.064591, loss_mps: 0.031695, loss_cps: 0.055631
[12:46:41.559] iteration 6408: total_loss: 0.174206, loss_sup: 0.087762, loss_mps: 0.032303, loss_cps: 0.054141
[12:46:41.708] iteration 6409: total_loss: 0.281072, loss_sup: 0.133134, loss_mps: 0.050240, loss_cps: 0.097697
[12:46:41.854] iteration 6410: total_loss: 0.101162, loss_sup: 0.027611, loss_mps: 0.026895, loss_cps: 0.046656
[12:46:42.000] iteration 6411: total_loss: 0.295515, loss_sup: 0.136588, loss_mps: 0.051398, loss_cps: 0.107529
[12:46:42.147] iteration 6412: total_loss: 0.232530, loss_sup: 0.116969, loss_mps: 0.039484, loss_cps: 0.076078
[12:46:42.296] iteration 6413: total_loss: 0.247832, loss_sup: 0.144489, loss_mps: 0.035113, loss_cps: 0.068230
[12:46:42.441] iteration 6414: total_loss: 0.195459, loss_sup: 0.122062, loss_mps: 0.027665, loss_cps: 0.045732
[12:46:42.587] iteration 6415: total_loss: 0.208727, loss_sup: 0.082065, loss_mps: 0.044169, loss_cps: 0.082493
[12:46:42.733] iteration 6416: total_loss: 0.116953, loss_sup: 0.043631, loss_mps: 0.027591, loss_cps: 0.045731
[12:46:42.878] iteration 6417: total_loss: 0.260173, loss_sup: 0.169154, loss_mps: 0.031197, loss_cps: 0.059822
[12:46:43.025] iteration 6418: total_loss: 0.233002, loss_sup: 0.127160, loss_mps: 0.036528, loss_cps: 0.069315
[12:46:43.171] iteration 6419: total_loss: 0.242898, loss_sup: 0.076626, loss_mps: 0.054843, loss_cps: 0.111429
[12:46:43.321] iteration 6420: total_loss: 0.308236, loss_sup: 0.172592, loss_mps: 0.045738, loss_cps: 0.089907
[12:46:43.467] iteration 6421: total_loss: 0.159689, loss_sup: 0.038488, loss_mps: 0.042483, loss_cps: 0.078717
[12:46:43.613] iteration 6422: total_loss: 0.103949, loss_sup: 0.031116, loss_mps: 0.026623, loss_cps: 0.046209
[12:46:43.759] iteration 6423: total_loss: 0.178826, loss_sup: 0.066833, loss_mps: 0.039745, loss_cps: 0.072248
[12:46:43.905] iteration 6424: total_loss: 0.146267, loss_sup: 0.046308, loss_mps: 0.035119, loss_cps: 0.064841
[12:46:44.051] iteration 6425: total_loss: 0.168724, loss_sup: 0.072885, loss_mps: 0.035252, loss_cps: 0.060587
[12:46:44.195] iteration 6426: total_loss: 0.158833, loss_sup: 0.077093, loss_mps: 0.031055, loss_cps: 0.050685
[12:46:44.341] iteration 6427: total_loss: 0.257536, loss_sup: 0.142960, loss_mps: 0.038964, loss_cps: 0.075612
[12:46:44.486] iteration 6428: total_loss: 0.126338, loss_sup: 0.037588, loss_mps: 0.032711, loss_cps: 0.056039
[12:46:44.633] iteration 6429: total_loss: 0.140224, loss_sup: 0.053425, loss_mps: 0.032296, loss_cps: 0.054503
[12:46:44.778] iteration 6430: total_loss: 0.144544, loss_sup: 0.031314, loss_mps: 0.038662, loss_cps: 0.074569
[12:46:44.925] iteration 6431: total_loss: 0.145788, loss_sup: 0.074733, loss_mps: 0.026729, loss_cps: 0.044326
[12:46:45.070] iteration 6432: total_loss: 0.125498, loss_sup: 0.041289, loss_mps: 0.030575, loss_cps: 0.053633
[12:46:45.216] iteration 6433: total_loss: 0.208441, loss_sup: 0.081723, loss_mps: 0.043246, loss_cps: 0.083473
[12:46:45.361] iteration 6434: total_loss: 0.187278, loss_sup: 0.093116, loss_mps: 0.034818, loss_cps: 0.059345
[12:46:45.506] iteration 6435: total_loss: 0.165859, loss_sup: 0.041840, loss_mps: 0.040151, loss_cps: 0.083869
[12:46:45.651] iteration 6436: total_loss: 0.255562, loss_sup: 0.127599, loss_mps: 0.042696, loss_cps: 0.085267
[12:46:45.797] iteration 6437: total_loss: 0.205927, loss_sup: 0.082921, loss_mps: 0.041858, loss_cps: 0.081148
[12:46:45.942] iteration 6438: total_loss: 0.322265, loss_sup: 0.251320, loss_mps: 0.026566, loss_cps: 0.044378
[12:46:46.089] iteration 6439: total_loss: 0.253269, loss_sup: 0.145145, loss_mps: 0.038141, loss_cps: 0.069983
[12:46:46.236] iteration 6440: total_loss: 0.121503, loss_sup: 0.043956, loss_mps: 0.028541, loss_cps: 0.049006
[12:46:46.381] iteration 6441: total_loss: 0.198645, loss_sup: 0.116827, loss_mps: 0.028969, loss_cps: 0.052850
[12:46:46.527] iteration 6442: total_loss: 0.455125, loss_sup: 0.312669, loss_mps: 0.047124, loss_cps: 0.095332
[12:46:46.672] iteration 6443: total_loss: 0.179662, loss_sup: 0.110802, loss_mps: 0.026387, loss_cps: 0.042472
[12:46:46.817] iteration 6444: total_loss: 0.177519, loss_sup: 0.042278, loss_mps: 0.044738, loss_cps: 0.090503
[12:46:46.965] iteration 6445: total_loss: 0.222884, loss_sup: 0.161397, loss_mps: 0.022813, loss_cps: 0.038675
[12:46:47.115] iteration 6446: total_loss: 0.153677, loss_sup: 0.035228, loss_mps: 0.040140, loss_cps: 0.078309
[12:46:47.265] iteration 6447: total_loss: 0.507845, loss_sup: 0.363639, loss_mps: 0.046578, loss_cps: 0.097628
[12:46:47.415] iteration 6448: total_loss: 0.201486, loss_sup: 0.088178, loss_mps: 0.039610, loss_cps: 0.073698
[12:46:47.561] iteration 6449: total_loss: 0.141108, loss_sup: 0.043714, loss_mps: 0.034099, loss_cps: 0.063296
[12:46:47.706] iteration 6450: total_loss: 0.284686, loss_sup: 0.153658, loss_mps: 0.043960, loss_cps: 0.087068
[12:46:47.851] iteration 6451: total_loss: 0.272887, loss_sup: 0.166907, loss_mps: 0.037878, loss_cps: 0.068102
[12:46:47.997] iteration 6452: total_loss: 0.333344, loss_sup: 0.211885, loss_mps: 0.040956, loss_cps: 0.080503
[12:46:48.142] iteration 6453: total_loss: 0.186676, loss_sup: 0.062819, loss_mps: 0.041468, loss_cps: 0.082389
[12:46:48.288] iteration 6454: total_loss: 0.192428, loss_sup: 0.081440, loss_mps: 0.037611, loss_cps: 0.073377
[12:46:48.434] iteration 6455: total_loss: 0.189893, loss_sup: 0.084565, loss_mps: 0.037734, loss_cps: 0.067593
[12:46:48.580] iteration 6456: total_loss: 0.132947, loss_sup: 0.042320, loss_mps: 0.032272, loss_cps: 0.058356
[12:46:48.726] iteration 6457: total_loss: 0.229882, loss_sup: 0.100228, loss_mps: 0.044320, loss_cps: 0.085334
[12:46:48.872] iteration 6458: total_loss: 0.144688, loss_sup: 0.041124, loss_mps: 0.036407, loss_cps: 0.067157
[12:46:49.017] iteration 6459: total_loss: 0.180577, loss_sup: 0.062046, loss_mps: 0.040200, loss_cps: 0.078332
[12:46:49.163] iteration 6460: total_loss: 0.148055, loss_sup: 0.058698, loss_mps: 0.033162, loss_cps: 0.056195
[12:46:49.308] iteration 6461: total_loss: 0.134081, loss_sup: 0.050653, loss_mps: 0.030736, loss_cps: 0.052692
[12:46:49.455] iteration 6462: total_loss: 0.208889, loss_sup: 0.069036, loss_mps: 0.046751, loss_cps: 0.093102
[12:46:49.601] iteration 6463: total_loss: 0.317592, loss_sup: 0.214430, loss_mps: 0.037098, loss_cps: 0.066064
[12:46:49.746] iteration 6464: total_loss: 0.161655, loss_sup: 0.092482, loss_mps: 0.026547, loss_cps: 0.042627
[12:46:49.892] iteration 6465: total_loss: 0.165015, loss_sup: 0.063425, loss_mps: 0.036174, loss_cps: 0.065417
[12:46:50.038] iteration 6466: total_loss: 0.301391, loss_sup: 0.198631, loss_mps: 0.034873, loss_cps: 0.067887
[12:46:50.183] iteration 6467: total_loss: 0.245942, loss_sup: 0.149965, loss_mps: 0.036095, loss_cps: 0.059883
[12:46:50.330] iteration 6468: total_loss: 0.166614, loss_sup: 0.102068, loss_mps: 0.025515, loss_cps: 0.039031
[12:46:50.476] iteration 6469: total_loss: 0.402843, loss_sup: 0.289557, loss_mps: 0.040708, loss_cps: 0.072578
[12:46:50.621] iteration 6470: total_loss: 0.278248, loss_sup: 0.189689, loss_mps: 0.030845, loss_cps: 0.057714
[12:46:50.768] iteration 6471: total_loss: 0.304785, loss_sup: 0.141812, loss_mps: 0.052345, loss_cps: 0.110629
[12:46:50.914] iteration 6472: total_loss: 0.151075, loss_sup: 0.054098, loss_mps: 0.035438, loss_cps: 0.061539
[12:46:51.061] iteration 6473: total_loss: 0.184081, loss_sup: 0.073283, loss_mps: 0.039187, loss_cps: 0.071612
[12:46:51.207] iteration 6474: total_loss: 0.157033, loss_sup: 0.027628, loss_mps: 0.042974, loss_cps: 0.086431
[12:46:51.355] iteration 6475: total_loss: 0.110795, loss_sup: 0.019645, loss_mps: 0.032616, loss_cps: 0.058534
[12:46:51.500] iteration 6476: total_loss: 0.302477, loss_sup: 0.190072, loss_mps: 0.037707, loss_cps: 0.074698
[12:46:51.646] iteration 6477: total_loss: 0.181490, loss_sup: 0.061943, loss_mps: 0.041446, loss_cps: 0.078101
[12:46:51.792] iteration 6478: total_loss: 0.305544, loss_sup: 0.179007, loss_mps: 0.042919, loss_cps: 0.083618
[12:46:51.938] iteration 6479: total_loss: 0.167626, loss_sup: 0.038302, loss_mps: 0.043947, loss_cps: 0.085376
[12:46:52.083] iteration 6480: total_loss: 0.216766, loss_sup: 0.060922, loss_mps: 0.050088, loss_cps: 0.105757
[12:46:52.229] iteration 6481: total_loss: 0.172099, loss_sup: 0.050946, loss_mps: 0.040189, loss_cps: 0.080963
[12:46:52.376] iteration 6482: total_loss: 0.296086, loss_sup: 0.141369, loss_mps: 0.051105, loss_cps: 0.103613
[12:46:52.522] iteration 6483: total_loss: 0.216669, loss_sup: 0.054204, loss_mps: 0.054109, loss_cps: 0.108356
[12:46:52.668] iteration 6484: total_loss: 0.720500, loss_sup: 0.503610, loss_mps: 0.067148, loss_cps: 0.149741
[12:46:52.816] iteration 6485: total_loss: 0.242549, loss_sup: 0.139549, loss_mps: 0.035410, loss_cps: 0.067590
[12:46:52.964] iteration 6486: total_loss: 0.249083, loss_sup: 0.110035, loss_mps: 0.046258, loss_cps: 0.092790
[12:46:53.114] iteration 6487: total_loss: 0.258544, loss_sup: 0.153901, loss_mps: 0.037003, loss_cps: 0.067641
[12:46:53.260] iteration 6488: total_loss: 0.308054, loss_sup: 0.168441, loss_mps: 0.048128, loss_cps: 0.091485
[12:46:53.406] iteration 6489: total_loss: 0.298711, loss_sup: 0.163797, loss_mps: 0.046617, loss_cps: 0.088297
[12:46:53.551] iteration 6490: total_loss: 0.231345, loss_sup: 0.083076, loss_mps: 0.052109, loss_cps: 0.096159
[12:46:53.697] iteration 6491: total_loss: 0.296501, loss_sup: 0.166142, loss_mps: 0.046533, loss_cps: 0.083826
[12:46:53.843] iteration 6492: total_loss: 0.208602, loss_sup: 0.125765, loss_mps: 0.032392, loss_cps: 0.050445
[12:46:53.990] iteration 6493: total_loss: 0.215704, loss_sup: 0.128093, loss_mps: 0.032389, loss_cps: 0.055221
[12:46:54.136] iteration 6494: total_loss: 0.164866, loss_sup: 0.065495, loss_mps: 0.036194, loss_cps: 0.063177
[12:46:54.281] iteration 6495: total_loss: 0.137959, loss_sup: 0.053256, loss_mps: 0.031941, loss_cps: 0.052762
[12:46:54.427] iteration 6496: total_loss: 0.254413, loss_sup: 0.146285, loss_mps: 0.038399, loss_cps: 0.069729
[12:46:54.572] iteration 6497: total_loss: 0.177692, loss_sup: 0.070642, loss_mps: 0.038443, loss_cps: 0.068607
[12:46:54.718] iteration 6498: total_loss: 0.292700, loss_sup: 0.165788, loss_mps: 0.044606, loss_cps: 0.082306
[12:46:54.864] iteration 6499: total_loss: 0.190745, loss_sup: 0.092710, loss_mps: 0.037213, loss_cps: 0.060822
[12:46:55.012] iteration 6500: total_loss: 0.275809, loss_sup: 0.168157, loss_mps: 0.039948, loss_cps: 0.067705
[12:46:55.012] Evaluation Started ==>
[12:47:06.448] ==> valid iteration 6500: unet metrics: {'dc': 0.6285658464275989, 'jc': 0.5005167746746185, 'pre': 0.6977570908986437, 'hd': 6.154768835124485}, ynet metrics: {'dc': 0.5861320842256434, 'jc': 0.4581647179926455, 'pre': 0.6870437881938674, 'hd': 6.37982306739573}.
[12:47:06.450] Evaluation Finished!⏹️
[12:47:06.603] iteration 6501: total_loss: 0.151774, loss_sup: 0.075136, loss_mps: 0.029643, loss_cps: 0.046995
[12:47:06.753] iteration 6502: total_loss: 0.137892, loss_sup: 0.044764, loss_mps: 0.034846, loss_cps: 0.058282
[12:47:06.899] iteration 6503: total_loss: 0.119934, loss_sup: 0.036271, loss_mps: 0.030898, loss_cps: 0.052765
[12:47:07.044] iteration 6504: total_loss: 0.172967, loss_sup: 0.052373, loss_mps: 0.042873, loss_cps: 0.077722
[12:47:07.190] iteration 6505: total_loss: 0.199223, loss_sup: 0.112796, loss_mps: 0.030986, loss_cps: 0.055441
[12:47:07.335] iteration 6506: total_loss: 0.145845, loss_sup: 0.053347, loss_mps: 0.034229, loss_cps: 0.058269
[12:47:07.481] iteration 6507: total_loss: 0.224436, loss_sup: 0.080581, loss_mps: 0.052002, loss_cps: 0.091854
[12:47:07.626] iteration 6508: total_loss: 0.151591, loss_sup: 0.035379, loss_mps: 0.042180, loss_cps: 0.074032
[12:47:07.771] iteration 6509: total_loss: 0.192677, loss_sup: 0.104251, loss_mps: 0.034254, loss_cps: 0.054172
[12:47:07.917] iteration 6510: total_loss: 0.207236, loss_sup: 0.070932, loss_mps: 0.046189, loss_cps: 0.090115
[12:47:08.062] iteration 6511: total_loss: 0.215652, loss_sup: 0.108037, loss_mps: 0.040898, loss_cps: 0.066717
[12:47:08.207] iteration 6512: total_loss: 0.284145, loss_sup: 0.164085, loss_mps: 0.042508, loss_cps: 0.077553
[12:47:08.353] iteration 6513: total_loss: 0.207868, loss_sup: 0.083176, loss_mps: 0.044297, loss_cps: 0.080396
[12:47:08.499] iteration 6514: total_loss: 0.575710, loss_sup: 0.436250, loss_mps: 0.047157, loss_cps: 0.092303
[12:47:08.644] iteration 6515: total_loss: 0.114284, loss_sup: 0.044428, loss_mps: 0.027445, loss_cps: 0.042411
[12:47:08.790] iteration 6516: total_loss: 0.149749, loss_sup: 0.040078, loss_mps: 0.038050, loss_cps: 0.071621
[12:47:08.936] iteration 6517: total_loss: 0.172591, loss_sup: 0.095947, loss_mps: 0.028194, loss_cps: 0.048450
[12:47:09.081] iteration 6518: total_loss: 0.142287, loss_sup: 0.056035, loss_mps: 0.032186, loss_cps: 0.054065
[12:47:09.227] iteration 6519: total_loss: 0.202871, loss_sup: 0.143233, loss_mps: 0.022636, loss_cps: 0.037003
[12:47:09.373] iteration 6520: total_loss: 0.272036, loss_sup: 0.096879, loss_mps: 0.056235, loss_cps: 0.118921
[12:47:09.518] iteration 6521: total_loss: 0.326955, loss_sup: 0.133740, loss_mps: 0.059270, loss_cps: 0.133945
[12:47:09.664] iteration 6522: total_loss: 0.200695, loss_sup: 0.086727, loss_mps: 0.038449, loss_cps: 0.075519
[12:47:09.810] iteration 6523: total_loss: 0.221773, loss_sup: 0.084872, loss_mps: 0.044820, loss_cps: 0.092081
[12:47:09.956] iteration 6524: total_loss: 0.316976, loss_sup: 0.128068, loss_mps: 0.059029, loss_cps: 0.129880
[12:47:10.103] iteration 6525: total_loss: 0.388150, loss_sup: 0.217366, loss_mps: 0.055198, loss_cps: 0.115586
[12:47:10.249] iteration 6526: total_loss: 0.164259, loss_sup: 0.088438, loss_mps: 0.026315, loss_cps: 0.049506
[12:47:10.394] iteration 6527: total_loss: 0.216226, loss_sup: 0.122484, loss_mps: 0.032114, loss_cps: 0.061627
[12:47:10.539] iteration 6528: total_loss: 0.211877, loss_sup: 0.115619, loss_mps: 0.034153, loss_cps: 0.062105
[12:47:10.684] iteration 6529: total_loss: 0.181752, loss_sup: 0.055043, loss_mps: 0.042608, loss_cps: 0.084101
[12:47:10.831] iteration 6530: total_loss: 0.296185, loss_sup: 0.143450, loss_mps: 0.050827, loss_cps: 0.101908
[12:47:10.976] iteration 6531: total_loss: 0.226401, loss_sup: 0.116277, loss_mps: 0.038541, loss_cps: 0.071582
[12:47:11.123] iteration 6532: total_loss: 0.175970, loss_sup: 0.077315, loss_mps: 0.032657, loss_cps: 0.065998
[12:47:11.268] iteration 6533: total_loss: 0.208651, loss_sup: 0.106180, loss_mps: 0.034927, loss_cps: 0.067544
[12:47:11.413] iteration 6534: total_loss: 0.238357, loss_sup: 0.061141, loss_mps: 0.056941, loss_cps: 0.120275
[12:47:11.558] iteration 6535: total_loss: 0.365717, loss_sup: 0.225059, loss_mps: 0.046643, loss_cps: 0.094015
[12:47:11.707] iteration 6536: total_loss: 0.386293, loss_sup: 0.257400, loss_mps: 0.043328, loss_cps: 0.085565
[12:47:11.854] iteration 6537: total_loss: 0.294480, loss_sup: 0.182772, loss_mps: 0.039611, loss_cps: 0.072097
[12:47:11.999] iteration 6538: total_loss: 0.385872, loss_sup: 0.279057, loss_mps: 0.037367, loss_cps: 0.069448
[12:47:12.146] iteration 6539: total_loss: 0.158673, loss_sup: 0.068978, loss_mps: 0.032388, loss_cps: 0.057307
[12:47:12.293] iteration 6540: total_loss: 0.204240, loss_sup: 0.089733, loss_mps: 0.039533, loss_cps: 0.074974
[12:47:12.439] iteration 6541: total_loss: 0.207523, loss_sup: 0.101249, loss_mps: 0.037382, loss_cps: 0.068892
[12:47:12.584] iteration 6542: total_loss: 0.244326, loss_sup: 0.073693, loss_mps: 0.056600, loss_cps: 0.114034
[12:47:12.730] iteration 6543: total_loss: 0.480017, loss_sup: 0.326095, loss_mps: 0.053301, loss_cps: 0.100621
[12:47:12.876] iteration 6544: total_loss: 0.229805, loss_sup: 0.142910, loss_mps: 0.032018, loss_cps: 0.054876
[12:47:13.022] iteration 6545: total_loss: 0.282042, loss_sup: 0.166736, loss_mps: 0.040149, loss_cps: 0.075157
[12:47:13.169] iteration 6546: total_loss: 0.365282, loss_sup: 0.227601, loss_mps: 0.046879, loss_cps: 0.090802
[12:47:13.315] iteration 6547: total_loss: 0.237197, loss_sup: 0.099775, loss_mps: 0.047423, loss_cps: 0.089999
[12:47:13.464] iteration 6548: total_loss: 0.166204, loss_sup: 0.054355, loss_mps: 0.042110, loss_cps: 0.069739
[12:47:13.611] iteration 6549: total_loss: 0.121117, loss_sup: 0.042894, loss_mps: 0.029519, loss_cps: 0.048704
[12:47:13.760] iteration 6550: total_loss: 0.254888, loss_sup: 0.156196, loss_mps: 0.038216, loss_cps: 0.060476
[12:47:13.906] iteration 6551: total_loss: 0.189031, loss_sup: 0.051102, loss_mps: 0.049726, loss_cps: 0.088203
[12:47:14.051] iteration 6552: total_loss: 0.142761, loss_sup: 0.030150, loss_mps: 0.041268, loss_cps: 0.071343
[12:47:14.197] iteration 6553: total_loss: 0.342415, loss_sup: 0.179681, loss_mps: 0.057265, loss_cps: 0.105469
[12:47:14.343] iteration 6554: total_loss: 0.585320, loss_sup: 0.345250, loss_mps: 0.077740, loss_cps: 0.162330
[12:47:14.490] iteration 6555: total_loss: 0.192928, loss_sup: 0.102386, loss_mps: 0.033524, loss_cps: 0.057017
[12:47:14.637] iteration 6556: total_loss: 0.103154, loss_sup: 0.012202, loss_mps: 0.034260, loss_cps: 0.056692
[12:47:14.783] iteration 6557: total_loss: 0.161763, loss_sup: 0.042461, loss_mps: 0.040865, loss_cps: 0.078438
[12:47:14.928] iteration 6558: total_loss: 0.168323, loss_sup: 0.050010, loss_mps: 0.041604, loss_cps: 0.076709
[12:47:15.074] iteration 6559: total_loss: 0.248297, loss_sup: 0.133976, loss_mps: 0.040680, loss_cps: 0.073640
[12:47:15.220] iteration 6560: total_loss: 0.151174, loss_sup: 0.051871, loss_mps: 0.035112, loss_cps: 0.064191
[12:47:15.366] iteration 6561: total_loss: 0.232743, loss_sup: 0.105980, loss_mps: 0.042132, loss_cps: 0.084631
[12:47:15.512] iteration 6562: total_loss: 0.253990, loss_sup: 0.148860, loss_mps: 0.037383, loss_cps: 0.067747
[12:47:15.657] iteration 6563: total_loss: 0.267094, loss_sup: 0.162572, loss_mps: 0.036650, loss_cps: 0.067872
[12:47:15.803] iteration 6564: total_loss: 0.158426, loss_sup: 0.071084, loss_mps: 0.030857, loss_cps: 0.056484
[12:47:15.948] iteration 6565: total_loss: 0.127410, loss_sup: 0.059510, loss_mps: 0.025381, loss_cps: 0.042519
[12:47:16.094] iteration 6566: total_loss: 0.220108, loss_sup: 0.128040, loss_mps: 0.033261, loss_cps: 0.058807
[12:47:16.242] iteration 6567: total_loss: 0.122801, loss_sup: 0.028476, loss_mps: 0.034187, loss_cps: 0.060137
[12:47:16.388] iteration 6568: total_loss: 0.314676, loss_sup: 0.171209, loss_mps: 0.048345, loss_cps: 0.095122
[12:47:16.534] iteration 6569: total_loss: 0.164251, loss_sup: 0.052417, loss_mps: 0.038634, loss_cps: 0.073200
[12:47:16.679] iteration 6570: total_loss: 0.151835, loss_sup: 0.058213, loss_mps: 0.035067, loss_cps: 0.058555
[12:47:16.825] iteration 6571: total_loss: 0.189341, loss_sup: 0.088734, loss_mps: 0.036530, loss_cps: 0.064077
[12:47:16.972] iteration 6572: total_loss: 0.151324, loss_sup: 0.068228, loss_mps: 0.032111, loss_cps: 0.050985
[12:47:17.118] iteration 6573: total_loss: 0.434295, loss_sup: 0.239944, loss_mps: 0.062883, loss_cps: 0.131468
[12:47:17.264] iteration 6574: total_loss: 0.132940, loss_sup: 0.035952, loss_mps: 0.035034, loss_cps: 0.061953
[12:47:17.409] iteration 6575: total_loss: 0.174162, loss_sup: 0.036334, loss_mps: 0.046040, loss_cps: 0.091788
[12:47:17.555] iteration 6576: total_loss: 0.227469, loss_sup: 0.118754, loss_mps: 0.037843, loss_cps: 0.070872
[12:47:17.701] iteration 6577: total_loss: 0.147836, loss_sup: 0.042024, loss_mps: 0.037001, loss_cps: 0.068811
[12:47:17.847] iteration 6578: total_loss: 0.283316, loss_sup: 0.154720, loss_mps: 0.044959, loss_cps: 0.083637
[12:47:17.994] iteration 6579: total_loss: 0.199303, loss_sup: 0.082268, loss_mps: 0.039911, loss_cps: 0.077125
[12:47:18.140] iteration 6580: total_loss: 0.265591, loss_sup: 0.114972, loss_mps: 0.049793, loss_cps: 0.100826
[12:47:18.287] iteration 6581: total_loss: 0.173581, loss_sup: 0.076748, loss_mps: 0.035614, loss_cps: 0.061220
[12:47:18.434] iteration 6582: total_loss: 0.183534, loss_sup: 0.110128, loss_mps: 0.028328, loss_cps: 0.045078
[12:47:18.580] iteration 6583: total_loss: 0.233868, loss_sup: 0.133017, loss_mps: 0.035358, loss_cps: 0.065493
[12:47:18.727] iteration 6584: total_loss: 0.546407, loss_sup: 0.353978, loss_mps: 0.062711, loss_cps: 0.129718
[12:47:18.874] iteration 6585: total_loss: 0.172376, loss_sup: 0.048366, loss_mps: 0.043869, loss_cps: 0.080141
[12:47:19.020] iteration 6586: total_loss: 0.295686, loss_sup: 0.152248, loss_mps: 0.048983, loss_cps: 0.094454
[12:47:19.165] iteration 6587: total_loss: 0.076699, loss_sup: 0.012695, loss_mps: 0.024079, loss_cps: 0.039926
[12:47:19.312] iteration 6588: total_loss: 0.212909, loss_sup: 0.114192, loss_mps: 0.034191, loss_cps: 0.064526
[12:47:19.459] iteration 6589: total_loss: 0.197556, loss_sup: 0.091031, loss_mps: 0.036605, loss_cps: 0.069920
[12:47:19.606] iteration 6590: total_loss: 0.282174, loss_sup: 0.200491, loss_mps: 0.030048, loss_cps: 0.051635
[12:47:19.752] iteration 6591: total_loss: 0.219212, loss_sup: 0.121796, loss_mps: 0.034239, loss_cps: 0.063176
[12:47:19.898] iteration 6592: total_loss: 0.472062, loss_sup: 0.321263, loss_mps: 0.050367, loss_cps: 0.100432
[12:47:20.044] iteration 6593: total_loss: 0.255254, loss_sup: 0.147855, loss_mps: 0.038386, loss_cps: 0.069014
[12:47:20.190] iteration 6594: total_loss: 0.213585, loss_sup: 0.111127, loss_mps: 0.037048, loss_cps: 0.065410
[12:47:20.338] iteration 6595: total_loss: 0.191066, loss_sup: 0.090789, loss_mps: 0.034884, loss_cps: 0.065394
[12:47:20.484] iteration 6596: total_loss: 0.386203, loss_sup: 0.286266, loss_mps: 0.036008, loss_cps: 0.063928
[12:47:20.631] iteration 6597: total_loss: 0.183981, loss_sup: 0.064574, loss_mps: 0.041573, loss_cps: 0.077833
[12:47:20.779] iteration 6598: total_loss: 0.503645, loss_sup: 0.371703, loss_mps: 0.047218, loss_cps: 0.084724
[12:47:20.925] iteration 6599: total_loss: 0.256020, loss_sup: 0.166320, loss_mps: 0.032859, loss_cps: 0.056841
[12:47:21.072] iteration 6600: total_loss: 0.183987, loss_sup: 0.087026, loss_mps: 0.035150, loss_cps: 0.061811
[12:47:21.072] Evaluation Started ==>
[12:47:32.453] ==> valid iteration 6600: unet metrics: {'dc': 0.6226478220625259, 'jc': 0.5008139416537747, 'pre': 0.7259370298197967, 'hd': 5.896298999425214}, ynet metrics: {'dc': 0.5481254106792098, 'jc': 0.42772595740072944, 'pre': 0.705385991733686, 'hd': 6.328366295194349}.
[12:47:32.454] Evaluation Finished!⏹️
[12:47:32.609] iteration 6601: total_loss: 0.443352, loss_sup: 0.320516, loss_mps: 0.042704, loss_cps: 0.080132
[12:47:32.759] iteration 6602: total_loss: 0.210326, loss_sup: 0.049244, loss_mps: 0.055714, loss_cps: 0.105368
[12:47:32.904] iteration 6603: total_loss: 0.355402, loss_sup: 0.251329, loss_mps: 0.037038, loss_cps: 0.067035
[12:47:33.049] iteration 6604: total_loss: 0.168758, loss_sup: 0.065902, loss_mps: 0.035086, loss_cps: 0.067769
[12:47:33.195] iteration 6605: total_loss: 0.183589, loss_sup: 0.066544, loss_mps: 0.041481, loss_cps: 0.075564
[12:47:33.341] iteration 6606: total_loss: 0.708947, loss_sup: 0.526489, loss_mps: 0.060146, loss_cps: 0.122312
[12:47:33.488] iteration 6607: total_loss: 0.170004, loss_sup: 0.033036, loss_mps: 0.050603, loss_cps: 0.086366
[12:47:33.635] iteration 6608: total_loss: 0.440413, loss_sup: 0.281544, loss_mps: 0.054567, loss_cps: 0.104302
[12:47:33.781] iteration 6609: total_loss: 0.275006, loss_sup: 0.150364, loss_mps: 0.043650, loss_cps: 0.080991
[12:47:33.926] iteration 6610: total_loss: 0.282776, loss_sup: 0.140501, loss_mps: 0.049093, loss_cps: 0.093183
[12:47:34.071] iteration 6611: total_loss: 0.188217, loss_sup: 0.077214, loss_mps: 0.041478, loss_cps: 0.069525
[12:47:34.217] iteration 6612: total_loss: 0.325119, loss_sup: 0.139284, loss_mps: 0.066165, loss_cps: 0.119669
[12:47:34.363] iteration 6613: total_loss: 0.151539, loss_sup: 0.026346, loss_mps: 0.047535, loss_cps: 0.077657
[12:47:34.509] iteration 6614: total_loss: 0.161275, loss_sup: 0.064540, loss_mps: 0.036372, loss_cps: 0.060363
[12:47:34.658] iteration 6615: total_loss: 0.371122, loss_sup: 0.207484, loss_mps: 0.056263, loss_cps: 0.107374
[12:47:34.803] iteration 6616: total_loss: 0.219363, loss_sup: 0.104525, loss_mps: 0.041063, loss_cps: 0.073775
[12:47:34.948] iteration 6617: total_loss: 0.166329, loss_sup: 0.039479, loss_mps: 0.046063, loss_cps: 0.080788
[12:47:35.094] iteration 6618: total_loss: 0.387374, loss_sup: 0.194277, loss_mps: 0.066149, loss_cps: 0.126948
[12:47:35.241] iteration 6619: total_loss: 0.294878, loss_sup: 0.144271, loss_mps: 0.051512, loss_cps: 0.099095
[12:47:35.387] iteration 6620: total_loss: 0.343892, loss_sup: 0.208374, loss_mps: 0.047255, loss_cps: 0.088263
[12:47:35.534] iteration 6621: total_loss: 0.167252, loss_sup: 0.068247, loss_mps: 0.036149, loss_cps: 0.062856
[12:47:35.679] iteration 6622: total_loss: 0.321314, loss_sup: 0.100218, loss_mps: 0.072882, loss_cps: 0.148213
[12:47:35.826] iteration 6623: total_loss: 0.282858, loss_sup: 0.157885, loss_mps: 0.044992, loss_cps: 0.079980
[12:47:35.972] iteration 6624: total_loss: 0.249318, loss_sup: 0.128310, loss_mps: 0.043262, loss_cps: 0.077746
[12:47:36.118] iteration 6625: total_loss: 0.115491, loss_sup: 0.044081, loss_mps: 0.028374, loss_cps: 0.043035
[12:47:36.264] iteration 6626: total_loss: 0.226530, loss_sup: 0.093628, loss_mps: 0.047066, loss_cps: 0.085837
[12:47:36.409] iteration 6627: total_loss: 0.153307, loss_sup: 0.032833, loss_mps: 0.044946, loss_cps: 0.075529
[12:47:36.555] iteration 6628: total_loss: 0.240223, loss_sup: 0.142728, loss_mps: 0.038875, loss_cps: 0.058620
[12:47:36.703] iteration 6629: total_loss: 0.500436, loss_sup: 0.380813, loss_mps: 0.041988, loss_cps: 0.077635
[12:47:36.852] iteration 6630: total_loss: 0.108811, loss_sup: 0.028160, loss_mps: 0.031563, loss_cps: 0.049088
[12:47:36.997] iteration 6631: total_loss: 0.138966, loss_sup: 0.054053, loss_mps: 0.033255, loss_cps: 0.051658
[12:47:37.143] iteration 6632: total_loss: 0.185156, loss_sup: 0.052783, loss_mps: 0.046905, loss_cps: 0.085468
[12:47:37.289] iteration 6633: total_loss: 0.357964, loss_sup: 0.241444, loss_mps: 0.041951, loss_cps: 0.074569
[12:47:37.434] iteration 6634: total_loss: 0.125322, loss_sup: 0.041976, loss_mps: 0.032766, loss_cps: 0.050580
[12:47:37.580] iteration 6635: total_loss: 0.248167, loss_sup: 0.147255, loss_mps: 0.036511, loss_cps: 0.064402
[12:47:37.729] iteration 6636: total_loss: 0.304961, loss_sup: 0.174376, loss_mps: 0.046696, loss_cps: 0.083888
[12:47:37.875] iteration 6637: total_loss: 0.216793, loss_sup: 0.087258, loss_mps: 0.043921, loss_cps: 0.085614
[12:47:38.020] iteration 6638: total_loss: 0.106854, loss_sup: 0.019166, loss_mps: 0.032304, loss_cps: 0.055384
[12:47:38.165] iteration 6639: total_loss: 0.459099, loss_sup: 0.280351, loss_mps: 0.060243, loss_cps: 0.118505
[12:47:38.311] iteration 6640: total_loss: 0.266566, loss_sup: 0.104524, loss_mps: 0.054848, loss_cps: 0.107193
[12:47:38.457] iteration 6641: total_loss: 0.213256, loss_sup: 0.083183, loss_mps: 0.045602, loss_cps: 0.084470
[12:47:38.607] iteration 6642: total_loss: 0.136418, loss_sup: 0.046369, loss_mps: 0.033027, loss_cps: 0.057021
[12:47:38.752] iteration 6643: total_loss: 0.318070, loss_sup: 0.182343, loss_mps: 0.047339, loss_cps: 0.088389
[12:47:38.898] iteration 6644: total_loss: 0.342650, loss_sup: 0.229479, loss_mps: 0.039043, loss_cps: 0.074128
[12:47:39.043] iteration 6645: total_loss: 0.230129, loss_sup: 0.128848, loss_mps: 0.036721, loss_cps: 0.064560
[12:47:39.189] iteration 6646: total_loss: 0.215287, loss_sup: 0.112034, loss_mps: 0.035906, loss_cps: 0.067347
[12:47:39.335] iteration 6647: total_loss: 0.369081, loss_sup: 0.203056, loss_mps: 0.056974, loss_cps: 0.109051
[12:47:39.481] iteration 6648: total_loss: 0.196215, loss_sup: 0.068933, loss_mps: 0.044383, loss_cps: 0.082899
[12:47:39.627] iteration 6649: total_loss: 0.198355, loss_sup: 0.107074, loss_mps: 0.032149, loss_cps: 0.059133
[12:47:39.772] iteration 6650: total_loss: 0.230178, loss_sup: 0.076399, loss_mps: 0.052626, loss_cps: 0.101154
[12:47:39.917] iteration 6651: total_loss: 0.219635, loss_sup: 0.129673, loss_mps: 0.033694, loss_cps: 0.056268
[12:47:40.064] iteration 6652: total_loss: 0.201733, loss_sup: 0.062883, loss_mps: 0.046697, loss_cps: 0.092152
[12:47:40.213] iteration 6653: total_loss: 0.311348, loss_sup: 0.161112, loss_mps: 0.050233, loss_cps: 0.100003
[12:47:40.359] iteration 6654: total_loss: 0.244454, loss_sup: 0.106457, loss_mps: 0.045634, loss_cps: 0.092363
[12:47:40.504] iteration 6655: total_loss: 0.165082, loss_sup: 0.103637, loss_mps: 0.023605, loss_cps: 0.037839
[12:47:40.650] iteration 6656: total_loss: 0.269031, loss_sup: 0.158573, loss_mps: 0.037548, loss_cps: 0.072909
[12:47:40.797] iteration 6657: total_loss: 0.152692, loss_sup: 0.045693, loss_mps: 0.037942, loss_cps: 0.069057
[12:47:40.943] iteration 6658: total_loss: 0.191234, loss_sup: 0.072251, loss_mps: 0.040721, loss_cps: 0.078262
[12:47:41.089] iteration 6659: total_loss: 0.152059, loss_sup: 0.071000, loss_mps: 0.030626, loss_cps: 0.050433
[12:47:41.235] iteration 6660: total_loss: 0.173543, loss_sup: 0.096888, loss_mps: 0.028759, loss_cps: 0.047896
[12:47:41.381] iteration 6661: total_loss: 0.107582, loss_sup: 0.034106, loss_mps: 0.029369, loss_cps: 0.044106
[12:47:41.527] iteration 6662: total_loss: 0.229189, loss_sup: 0.128316, loss_mps: 0.035562, loss_cps: 0.065311
[12:47:41.673] iteration 6663: total_loss: 0.208563, loss_sup: 0.085386, loss_mps: 0.042957, loss_cps: 0.080221
[12:47:41.819] iteration 6664: total_loss: 0.267028, loss_sup: 0.115166, loss_mps: 0.050314, loss_cps: 0.101548
[12:47:41.964] iteration 6665: total_loss: 0.302030, loss_sup: 0.177777, loss_mps: 0.044387, loss_cps: 0.079867
[12:47:42.110] iteration 6666: total_loss: 0.377637, loss_sup: 0.191564, loss_mps: 0.059231, loss_cps: 0.126842
[12:47:42.255] iteration 6667: total_loss: 0.265642, loss_sup: 0.178658, loss_mps: 0.029696, loss_cps: 0.057288
[12:47:42.401] iteration 6668: total_loss: 0.662881, loss_sup: 0.482896, loss_mps: 0.057377, loss_cps: 0.122607
[12:47:42.547] iteration 6669: total_loss: 0.184037, loss_sup: 0.059810, loss_mps: 0.043273, loss_cps: 0.080954
[12:47:42.694] iteration 6670: total_loss: 0.213918, loss_sup: 0.060402, loss_mps: 0.054026, loss_cps: 0.099491
[12:47:42.841] iteration 6671: total_loss: 0.209798, loss_sup: 0.099762, loss_mps: 0.040844, loss_cps: 0.069192
[12:47:42.987] iteration 6672: total_loss: 0.258022, loss_sup: 0.148993, loss_mps: 0.039177, loss_cps: 0.069852
[12:47:43.133] iteration 6673: total_loss: 0.152988, loss_sup: 0.048040, loss_mps: 0.037961, loss_cps: 0.066987
[12:47:43.278] iteration 6674: total_loss: 0.183931, loss_sup: 0.087617, loss_mps: 0.037020, loss_cps: 0.059294
[12:47:43.423] iteration 6675: total_loss: 0.097528, loss_sup: 0.027689, loss_mps: 0.026865, loss_cps: 0.042975
[12:47:43.568] iteration 6676: total_loss: 0.164275, loss_sup: 0.081986, loss_mps: 0.031597, loss_cps: 0.050693
[12:47:43.715] iteration 6677: total_loss: 0.114577, loss_sup: 0.023713, loss_mps: 0.033513, loss_cps: 0.057351
[12:47:43.860] iteration 6678: total_loss: 0.273427, loss_sup: 0.105159, loss_mps: 0.057056, loss_cps: 0.111212
[12:47:44.006] iteration 6679: total_loss: 0.226177, loss_sup: 0.086931, loss_mps: 0.049121, loss_cps: 0.090125
[12:47:44.151] iteration 6680: total_loss: 0.151287, loss_sup: 0.058258, loss_mps: 0.033935, loss_cps: 0.059093
[12:47:44.296] iteration 6681: total_loss: 0.179926, loss_sup: 0.045028, loss_mps: 0.045376, loss_cps: 0.089521
[12:47:44.442] iteration 6682: total_loss: 0.207704, loss_sup: 0.112133, loss_mps: 0.034341, loss_cps: 0.061231
[12:47:44.588] iteration 6683: total_loss: 0.251041, loss_sup: 0.108601, loss_mps: 0.050336, loss_cps: 0.092104
[12:47:44.733] iteration 6684: total_loss: 0.241832, loss_sup: 0.104581, loss_mps: 0.046994, loss_cps: 0.090257
[12:47:44.879] iteration 6685: total_loss: 0.198626, loss_sup: 0.097429, loss_mps: 0.036406, loss_cps: 0.064791
[12:47:45.024] iteration 6686: total_loss: 0.163807, loss_sup: 0.069248, loss_mps: 0.033957, loss_cps: 0.060601
[12:47:45.170] iteration 6687: total_loss: 0.100870, loss_sup: 0.019650, loss_mps: 0.030632, loss_cps: 0.050589
[12:47:45.233] iteration 6688: total_loss: 0.215767, loss_sup: 0.081947, loss_mps: 0.045247, loss_cps: 0.088573
[12:47:46.424] iteration 6689: total_loss: 0.355309, loss_sup: 0.245164, loss_mps: 0.040092, loss_cps: 0.070053
[12:47:46.573] iteration 6690: total_loss: 0.107310, loss_sup: 0.025767, loss_mps: 0.028766, loss_cps: 0.052776
[12:47:46.720] iteration 6691: total_loss: 0.223034, loss_sup: 0.082642, loss_mps: 0.047494, loss_cps: 0.092899
[12:47:46.868] iteration 6692: total_loss: 0.144943, loss_sup: 0.028608, loss_mps: 0.040564, loss_cps: 0.075771
[12:47:47.015] iteration 6693: total_loss: 0.418006, loss_sup: 0.229236, loss_mps: 0.061567, loss_cps: 0.127203
[12:47:47.161] iteration 6694: total_loss: 0.257912, loss_sup: 0.143296, loss_mps: 0.038464, loss_cps: 0.076152
[12:47:47.307] iteration 6695: total_loss: 0.105982, loss_sup: 0.033653, loss_mps: 0.028290, loss_cps: 0.044039
[12:47:47.455] iteration 6696: total_loss: 0.143315, loss_sup: 0.060059, loss_mps: 0.030669, loss_cps: 0.052587
[12:47:47.601] iteration 6697: total_loss: 0.138210, loss_sup: 0.046972, loss_mps: 0.032073, loss_cps: 0.059166
[12:47:47.747] iteration 6698: total_loss: 0.176344, loss_sup: 0.089151, loss_mps: 0.030963, loss_cps: 0.056230
[12:47:47.894] iteration 6699: total_loss: 0.249411, loss_sup: 0.114589, loss_mps: 0.044560, loss_cps: 0.090262
[12:47:48.042] iteration 6700: total_loss: 0.178357, loss_sup: 0.085941, loss_mps: 0.033566, loss_cps: 0.058849
[12:47:48.042] Evaluation Started ==>
[12:47:59.425] ==> valid iteration 6700: unet metrics: {'dc': 0.6288973838991282, 'jc': 0.5042573923588771, 'pre': 0.6934169463769639, 'hd': 6.049726106606679}, ynet metrics: {'dc': 0.5169986415316096, 'jc': 0.4070956215393681, 'pre': 0.6781242326502606, 'hd': 6.250282105672816}.
[12:47:59.427] Evaluation Finished!⏹️
[12:47:59.579] iteration 6701: total_loss: 0.163908, loss_sup: 0.076498, loss_mps: 0.030962, loss_cps: 0.056448
[12:47:59.726] iteration 6702: total_loss: 0.356586, loss_sup: 0.224809, loss_mps: 0.045133, loss_cps: 0.086644
[12:47:59.872] iteration 6703: total_loss: 0.312088, loss_sup: 0.200439, loss_mps: 0.039624, loss_cps: 0.072025
[12:48:00.018] iteration 6704: total_loss: 0.167167, loss_sup: 0.042893, loss_mps: 0.042111, loss_cps: 0.082163
[12:48:00.163] iteration 6705: total_loss: 0.268776, loss_sup: 0.151383, loss_mps: 0.041829, loss_cps: 0.075564
[12:48:00.309] iteration 6706: total_loss: 0.153030, loss_sup: 0.083352, loss_mps: 0.027045, loss_cps: 0.042632
[12:48:00.454] iteration 6707: total_loss: 0.174153, loss_sup: 0.047031, loss_mps: 0.042670, loss_cps: 0.084453
[12:48:00.599] iteration 6708: total_loss: 0.115163, loss_sup: 0.040031, loss_mps: 0.026803, loss_cps: 0.048329
[12:48:00.744] iteration 6709: total_loss: 0.219307, loss_sup: 0.099350, loss_mps: 0.040149, loss_cps: 0.079808
[12:48:00.889] iteration 6710: total_loss: 0.360227, loss_sup: 0.237670, loss_mps: 0.042097, loss_cps: 0.080460
[12:48:01.034] iteration 6711: total_loss: 0.297608, loss_sup: 0.103520, loss_mps: 0.064149, loss_cps: 0.129939
[12:48:01.179] iteration 6712: total_loss: 0.149695, loss_sup: 0.029453, loss_mps: 0.042014, loss_cps: 0.078229
[12:48:01.328] iteration 6713: total_loss: 0.171967, loss_sup: 0.073357, loss_mps: 0.034142, loss_cps: 0.064467
[12:48:01.475] iteration 6714: total_loss: 0.197084, loss_sup: 0.051059, loss_mps: 0.049209, loss_cps: 0.096816
[12:48:01.621] iteration 6715: total_loss: 0.360056, loss_sup: 0.204708, loss_mps: 0.053407, loss_cps: 0.101941
[12:48:01.767] iteration 6716: total_loss: 0.162108, loss_sup: 0.098861, loss_mps: 0.024820, loss_cps: 0.038427
[12:48:01.915] iteration 6717: total_loss: 0.219274, loss_sup: 0.104169, loss_mps: 0.038712, loss_cps: 0.076393
[12:48:02.061] iteration 6718: total_loss: 0.131386, loss_sup: 0.044372, loss_mps: 0.031826, loss_cps: 0.055188
[12:48:02.207] iteration 6719: total_loss: 0.191347, loss_sup: 0.075950, loss_mps: 0.040749, loss_cps: 0.074648
[12:48:02.355] iteration 6720: total_loss: 0.206108, loss_sup: 0.108055, loss_mps: 0.034874, loss_cps: 0.063180
[12:48:02.501] iteration 6721: total_loss: 0.261485, loss_sup: 0.068611, loss_mps: 0.064577, loss_cps: 0.128297
[12:48:02.648] iteration 6722: total_loss: 0.180895, loss_sup: 0.066423, loss_mps: 0.039304, loss_cps: 0.075168
[12:48:02.794] iteration 6723: total_loss: 0.196985, loss_sup: 0.099770, loss_mps: 0.033274, loss_cps: 0.063941
[12:48:02.939] iteration 6724: total_loss: 0.333729, loss_sup: 0.170360, loss_mps: 0.053636, loss_cps: 0.109733
[12:48:03.085] iteration 6725: total_loss: 0.273713, loss_sup: 0.151738, loss_mps: 0.042417, loss_cps: 0.079559
[12:48:03.230] iteration 6726: total_loss: 0.217350, loss_sup: 0.124701, loss_mps: 0.032875, loss_cps: 0.059775
[12:48:03.377] iteration 6727: total_loss: 0.248534, loss_sup: 0.139687, loss_mps: 0.037543, loss_cps: 0.071304
[12:48:03.523] iteration 6728: total_loss: 0.265646, loss_sup: 0.170539, loss_mps: 0.032845, loss_cps: 0.062262
[12:48:03.671] iteration 6729: total_loss: 0.231321, loss_sup: 0.136469, loss_mps: 0.033124, loss_cps: 0.061728
[12:48:03.817] iteration 6730: total_loss: 0.210679, loss_sup: 0.107556, loss_mps: 0.036058, loss_cps: 0.067065
[12:48:03.964] iteration 6731: total_loss: 0.214974, loss_sup: 0.098023, loss_mps: 0.039800, loss_cps: 0.077150
[12:48:04.109] iteration 6732: total_loss: 0.430531, loss_sup: 0.277231, loss_mps: 0.050624, loss_cps: 0.102676
[12:48:04.256] iteration 6733: total_loss: 0.137372, loss_sup: 0.051552, loss_mps: 0.030320, loss_cps: 0.055501
[12:48:04.402] iteration 6734: total_loss: 0.255222, loss_sup: 0.113813, loss_mps: 0.045659, loss_cps: 0.095751
[12:48:04.551] iteration 6735: total_loss: 0.428440, loss_sup: 0.237266, loss_mps: 0.061942, loss_cps: 0.129232
[12:48:04.696] iteration 6736: total_loss: 0.263793, loss_sup: 0.137860, loss_mps: 0.044039, loss_cps: 0.081894
[12:48:04.841] iteration 6737: total_loss: 0.221933, loss_sup: 0.075238, loss_mps: 0.049793, loss_cps: 0.096902
[12:48:04.989] iteration 6738: total_loss: 0.232237, loss_sup: 0.110034, loss_mps: 0.042544, loss_cps: 0.079658
[12:48:05.134] iteration 6739: total_loss: 0.271467, loss_sup: 0.114362, loss_mps: 0.052229, loss_cps: 0.104875
[12:48:05.280] iteration 6740: total_loss: 0.162445, loss_sup: 0.085468, loss_mps: 0.028202, loss_cps: 0.048775
[12:48:05.426] iteration 6741: total_loss: 0.133455, loss_sup: 0.039995, loss_mps: 0.033236, loss_cps: 0.060224
[12:48:05.573] iteration 6742: total_loss: 0.416246, loss_sup: 0.318492, loss_mps: 0.035513, loss_cps: 0.062241
[12:48:05.719] iteration 6743: total_loss: 0.152421, loss_sup: 0.037706, loss_mps: 0.040740, loss_cps: 0.073975
[12:48:05.866] iteration 6744: total_loss: 0.145243, loss_sup: 0.041040, loss_mps: 0.038517, loss_cps: 0.065686
[12:48:06.014] iteration 6745: total_loss: 0.322385, loss_sup: 0.144401, loss_mps: 0.058851, loss_cps: 0.119133
[12:48:06.160] iteration 6746: total_loss: 0.186989, loss_sup: 0.080599, loss_mps: 0.038083, loss_cps: 0.068307
[12:48:06.306] iteration 6747: total_loss: 0.352959, loss_sup: 0.212541, loss_mps: 0.049385, loss_cps: 0.091034
[12:48:06.451] iteration 6748: total_loss: 0.158709, loss_sup: 0.077288, loss_mps: 0.030556, loss_cps: 0.050865
[12:48:06.597] iteration 6749: total_loss: 0.137493, loss_sup: 0.048298, loss_mps: 0.033578, loss_cps: 0.055618
[12:48:06.743] iteration 6750: total_loss: 0.570425, loss_sup: 0.390948, loss_mps: 0.060094, loss_cps: 0.119383
[12:48:06.889] iteration 6751: total_loss: 0.218692, loss_sup: 0.084764, loss_mps: 0.045858, loss_cps: 0.088070
[12:48:07.034] iteration 6752: total_loss: 0.139220, loss_sup: 0.039654, loss_mps: 0.036480, loss_cps: 0.063085
[12:48:07.180] iteration 6753: total_loss: 0.214874, loss_sup: 0.130054, loss_mps: 0.032115, loss_cps: 0.052705
[12:48:07.328] iteration 6754: total_loss: 0.142248, loss_sup: 0.060136, loss_mps: 0.030929, loss_cps: 0.051183
[12:48:07.473] iteration 6755: total_loss: 0.168830, loss_sup: 0.064108, loss_mps: 0.038369, loss_cps: 0.066352
[12:48:07.618] iteration 6756: total_loss: 0.491894, loss_sup: 0.387689, loss_mps: 0.037862, loss_cps: 0.066344
[12:48:07.765] iteration 6757: total_loss: 0.177296, loss_sup: 0.063741, loss_mps: 0.041025, loss_cps: 0.072530
[12:48:07.911] iteration 6758: total_loss: 0.162674, loss_sup: 0.070266, loss_mps: 0.034740, loss_cps: 0.057668
[12:48:08.057] iteration 6759: total_loss: 0.273088, loss_sup: 0.175220, loss_mps: 0.035419, loss_cps: 0.062449
[12:48:08.202] iteration 6760: total_loss: 0.282372, loss_sup: 0.144588, loss_mps: 0.044444, loss_cps: 0.093340
[12:48:08.348] iteration 6761: total_loss: 0.179935, loss_sup: 0.118057, loss_mps: 0.025497, loss_cps: 0.036380
[12:48:08.493] iteration 6762: total_loss: 0.102290, loss_sup: 0.023228, loss_mps: 0.030150, loss_cps: 0.048912
[12:48:08.639] iteration 6763: total_loss: 0.253822, loss_sup: 0.116536, loss_mps: 0.046247, loss_cps: 0.091039
[12:48:08.786] iteration 6764: total_loss: 0.406206, loss_sup: 0.294838, loss_mps: 0.040205, loss_cps: 0.071164
[12:48:08.932] iteration 6765: total_loss: 0.111964, loss_sup: 0.032876, loss_mps: 0.030488, loss_cps: 0.048600
[12:48:09.082] iteration 6766: total_loss: 0.261212, loss_sup: 0.122302, loss_mps: 0.047616, loss_cps: 0.091294
[12:48:09.228] iteration 6767: total_loss: 0.181863, loss_sup: 0.073020, loss_mps: 0.039851, loss_cps: 0.068993
[12:48:09.377] iteration 6768: total_loss: 0.264231, loss_sup: 0.159557, loss_mps: 0.036868, loss_cps: 0.067805
[12:48:09.522] iteration 6769: total_loss: 0.231612, loss_sup: 0.114304, loss_mps: 0.042195, loss_cps: 0.075112
[12:48:09.670] iteration 6770: total_loss: 0.341391, loss_sup: 0.227963, loss_mps: 0.040226, loss_cps: 0.073202
[12:48:09.817] iteration 6771: total_loss: 0.360210, loss_sup: 0.212337, loss_mps: 0.050805, loss_cps: 0.097068
[12:48:09.963] iteration 6772: total_loss: 0.283201, loss_sup: 0.161270, loss_mps: 0.043360, loss_cps: 0.078572
[12:48:10.109] iteration 6773: total_loss: 0.125682, loss_sup: 0.041453, loss_mps: 0.032976, loss_cps: 0.051254
[12:48:10.255] iteration 6774: total_loss: 0.115163, loss_sup: 0.048176, loss_mps: 0.027144, loss_cps: 0.039843
[12:48:10.400] iteration 6775: total_loss: 0.197815, loss_sup: 0.111931, loss_mps: 0.032833, loss_cps: 0.053051
[12:48:10.546] iteration 6776: total_loss: 0.160495, loss_sup: 0.066072, loss_mps: 0.035419, loss_cps: 0.059004
[12:48:10.692] iteration 6777: total_loss: 0.164064, loss_sup: 0.049440, loss_mps: 0.041908, loss_cps: 0.072717
[12:48:10.842] iteration 6778: total_loss: 0.197374, loss_sup: 0.078922, loss_mps: 0.040808, loss_cps: 0.077643
[12:48:10.989] iteration 6779: total_loss: 0.170126, loss_sup: 0.048798, loss_mps: 0.043680, loss_cps: 0.077649
[12:48:11.134] iteration 6780: total_loss: 0.433910, loss_sup: 0.258614, loss_mps: 0.058640, loss_cps: 0.116657
[12:48:11.282] iteration 6781: total_loss: 0.261805, loss_sup: 0.102341, loss_mps: 0.051191, loss_cps: 0.108273
[12:48:11.432] iteration 6782: total_loss: 0.206555, loss_sup: 0.113142, loss_mps: 0.033454, loss_cps: 0.059959
[12:48:11.578] iteration 6783: total_loss: 0.248759, loss_sup: 0.103869, loss_mps: 0.049496, loss_cps: 0.095393
[12:48:11.724] iteration 6784: total_loss: 0.246178, loss_sup: 0.091348, loss_mps: 0.052268, loss_cps: 0.102561
[12:48:11.870] iteration 6785: total_loss: 0.181893, loss_sup: 0.085325, loss_mps: 0.035856, loss_cps: 0.060713
[12:48:12.016] iteration 6786: total_loss: 0.179239, loss_sup: 0.064784, loss_mps: 0.042604, loss_cps: 0.071851
[12:48:12.161] iteration 6787: total_loss: 0.181083, loss_sup: 0.073427, loss_mps: 0.037819, loss_cps: 0.069837
[12:48:12.307] iteration 6788: total_loss: 0.174126, loss_sup: 0.086078, loss_mps: 0.033750, loss_cps: 0.054298
[12:48:12.455] iteration 6789: total_loss: 0.236517, loss_sup: 0.146456, loss_mps: 0.033138, loss_cps: 0.056923
[12:48:12.602] iteration 6790: total_loss: 0.135301, loss_sup: 0.030412, loss_mps: 0.037560, loss_cps: 0.067328
[12:48:12.750] iteration 6791: total_loss: 0.184844, loss_sup: 0.095855, loss_mps: 0.033000, loss_cps: 0.055990
[12:48:12.899] iteration 6792: total_loss: 0.127810, loss_sup: 0.026235, loss_mps: 0.035936, loss_cps: 0.065639
[12:48:13.045] iteration 6793: total_loss: 0.142474, loss_sup: 0.050388, loss_mps: 0.033253, loss_cps: 0.058833
[12:48:13.192] iteration 6794: total_loss: 0.131482, loss_sup: 0.033414, loss_mps: 0.033802, loss_cps: 0.064267
[12:48:13.338] iteration 6795: total_loss: 0.090171, loss_sup: 0.007928, loss_mps: 0.029847, loss_cps: 0.052396
[12:48:13.483] iteration 6796: total_loss: 0.572631, loss_sup: 0.268523, loss_mps: 0.094412, loss_cps: 0.209696
[12:48:13.632] iteration 6797: total_loss: 0.103106, loss_sup: 0.033400, loss_mps: 0.026430, loss_cps: 0.043276
[12:48:13.780] iteration 6798: total_loss: 0.105030, loss_sup: 0.026377, loss_mps: 0.029227, loss_cps: 0.049427
[12:48:13.925] iteration 6799: total_loss: 0.274771, loss_sup: 0.193030, loss_mps: 0.030234, loss_cps: 0.051507
[12:48:14.071] iteration 6800: total_loss: 0.155334, loss_sup: 0.039791, loss_mps: 0.038170, loss_cps: 0.077373
[12:48:14.071] Evaluation Started ==>
[12:48:25.427] ==> valid iteration 6800: unet metrics: {'dc': 0.6443519416961976, 'jc': 0.5177853235790089, 'pre': 0.6874814255661961, 'hd': 6.391946797416542}, ynet metrics: {'dc': 0.5908021863390959, 'jc': 0.4672501091860259, 'pre': 0.7433282700668816, 'hd': 6.049437874270968}.
[12:48:25.492] ==> New best valid dice for unet: 0.644352, at iteration 6800
[12:48:25.650] ==> New best valid dice for ynet: 0.590802, at iteration 6800
[12:48:25.652] Evaluation Finished!⏹️
[12:48:25.807] iteration 6801: total_loss: 0.349289, loss_sup: 0.262390, loss_mps: 0.031132, loss_cps: 0.055767
[12:48:25.956] iteration 6802: total_loss: 0.128794, loss_sup: 0.020308, loss_mps: 0.036604, loss_cps: 0.071881
[12:48:26.102] iteration 6803: total_loss: 0.638124, loss_sup: 0.536457, loss_mps: 0.035085, loss_cps: 0.066583
[12:48:26.246] iteration 6804: total_loss: 0.199457, loss_sup: 0.071880, loss_mps: 0.042903, loss_cps: 0.084674
[12:48:26.393] iteration 6805: total_loss: 0.384017, loss_sup: 0.213922, loss_mps: 0.055478, loss_cps: 0.114617
[12:48:26.540] iteration 6806: total_loss: 0.131432, loss_sup: 0.034126, loss_mps: 0.034344, loss_cps: 0.062962
[12:48:26.685] iteration 6807: total_loss: 0.244060, loss_sup: 0.095229, loss_mps: 0.049941, loss_cps: 0.098890
[12:48:26.830] iteration 6808: total_loss: 0.217721, loss_sup: 0.133165, loss_mps: 0.032030, loss_cps: 0.052526
[12:48:26.975] iteration 6809: total_loss: 0.271562, loss_sup: 0.111722, loss_mps: 0.050914, loss_cps: 0.108926
[12:48:27.123] iteration 6810: total_loss: 0.228929, loss_sup: 0.115626, loss_mps: 0.041036, loss_cps: 0.072268
[12:48:27.269] iteration 6811: total_loss: 0.213920, loss_sup: 0.119145, loss_mps: 0.035607, loss_cps: 0.059168
[12:48:27.414] iteration 6812: total_loss: 0.277934, loss_sup: 0.152565, loss_mps: 0.043391, loss_cps: 0.081978
[12:48:27.561] iteration 6813: total_loss: 0.169003, loss_sup: 0.059506, loss_mps: 0.038823, loss_cps: 0.070674
[12:48:27.707] iteration 6814: total_loss: 0.153610, loss_sup: 0.028298, loss_mps: 0.044244, loss_cps: 0.081068
[12:48:27.855] iteration 6815: total_loss: 0.143958, loss_sup: 0.062717, loss_mps: 0.029844, loss_cps: 0.051397
[12:48:28.000] iteration 6816: total_loss: 0.236968, loss_sup: 0.130615, loss_mps: 0.038533, loss_cps: 0.067819
[12:48:28.147] iteration 6817: total_loss: 0.208060, loss_sup: 0.099735, loss_mps: 0.039433, loss_cps: 0.068892
[12:48:28.295] iteration 6818: total_loss: 0.282940, loss_sup: 0.102666, loss_mps: 0.058806, loss_cps: 0.121468
[12:48:28.441] iteration 6819: total_loss: 0.383229, loss_sup: 0.185460, loss_mps: 0.062867, loss_cps: 0.134902
[12:48:28.586] iteration 6820: total_loss: 0.194412, loss_sup: 0.115821, loss_mps: 0.029927, loss_cps: 0.048665
[12:48:28.732] iteration 6821: total_loss: 0.143616, loss_sup: 0.059756, loss_mps: 0.032692, loss_cps: 0.051167
[12:48:28.877] iteration 6822: total_loss: 0.176742, loss_sup: 0.069578, loss_mps: 0.037534, loss_cps: 0.069630
[12:48:29.025] iteration 6823: total_loss: 0.235720, loss_sup: 0.069983, loss_mps: 0.055506, loss_cps: 0.110231
[12:48:29.172] iteration 6824: total_loss: 0.180703, loss_sup: 0.059281, loss_mps: 0.042649, loss_cps: 0.078773
[12:48:29.318] iteration 6825: total_loss: 0.138341, loss_sup: 0.048023, loss_mps: 0.032914, loss_cps: 0.057404
[12:48:29.464] iteration 6826: total_loss: 0.224974, loss_sup: 0.114157, loss_mps: 0.039764, loss_cps: 0.071052
[12:48:29.609] iteration 6827: total_loss: 0.134110, loss_sup: 0.057296, loss_mps: 0.029485, loss_cps: 0.047328
[12:48:29.754] iteration 6828: total_loss: 0.167366, loss_sup: 0.071982, loss_mps: 0.034860, loss_cps: 0.060524
[12:48:29.901] iteration 6829: total_loss: 0.144502, loss_sup: 0.036033, loss_mps: 0.037787, loss_cps: 0.070683
[12:48:30.048] iteration 6830: total_loss: 0.159645, loss_sup: 0.057823, loss_mps: 0.035522, loss_cps: 0.066300
[12:48:30.193] iteration 6831: total_loss: 0.189646, loss_sup: 0.091512, loss_mps: 0.034958, loss_cps: 0.063176
[12:48:30.338] iteration 6832: total_loss: 0.112962, loss_sup: 0.023465, loss_mps: 0.031486, loss_cps: 0.058011
[12:48:30.484] iteration 6833: total_loss: 0.188659, loss_sup: 0.078247, loss_mps: 0.037515, loss_cps: 0.072898
[12:48:30.630] iteration 6834: total_loss: 0.257008, loss_sup: 0.082319, loss_mps: 0.054864, loss_cps: 0.119824
[12:48:30.776] iteration 6835: total_loss: 0.226060, loss_sup: 0.139997, loss_mps: 0.031745, loss_cps: 0.054317
[12:48:30.921] iteration 6836: total_loss: 0.205983, loss_sup: 0.096671, loss_mps: 0.037918, loss_cps: 0.071394
[12:48:31.068] iteration 6837: total_loss: 0.428587, loss_sup: 0.251419, loss_mps: 0.058695, loss_cps: 0.118472
[12:48:31.214] iteration 6838: total_loss: 0.157566, loss_sup: 0.044201, loss_mps: 0.039326, loss_cps: 0.074039
[12:48:31.360] iteration 6839: total_loss: 0.252861, loss_sup: 0.122634, loss_mps: 0.044638, loss_cps: 0.085589
[12:48:31.505] iteration 6840: total_loss: 0.210263, loss_sup: 0.123566, loss_mps: 0.030245, loss_cps: 0.056452
[12:48:31.651] iteration 6841: total_loss: 0.164361, loss_sup: 0.069456, loss_mps: 0.033831, loss_cps: 0.061074
[12:48:31.797] iteration 6842: total_loss: 0.140282, loss_sup: 0.021925, loss_mps: 0.040389, loss_cps: 0.077969
[12:48:31.943] iteration 6843: total_loss: 0.214929, loss_sup: 0.093008, loss_mps: 0.043378, loss_cps: 0.078543
[12:48:32.089] iteration 6844: total_loss: 0.172676, loss_sup: 0.033092, loss_mps: 0.046684, loss_cps: 0.092900
[12:48:32.235] iteration 6845: total_loss: 0.103075, loss_sup: 0.024200, loss_mps: 0.028143, loss_cps: 0.050732
[12:48:32.381] iteration 6846: total_loss: 0.145070, loss_sup: 0.060674, loss_mps: 0.030648, loss_cps: 0.053748
[12:48:32.527] iteration 6847: total_loss: 0.527575, loss_sup: 0.352398, loss_mps: 0.058480, loss_cps: 0.116697
[12:48:32.674] iteration 6848: total_loss: 0.167904, loss_sup: 0.047675, loss_mps: 0.041098, loss_cps: 0.079131
[12:48:32.820] iteration 6849: total_loss: 0.254991, loss_sup: 0.134914, loss_mps: 0.041455, loss_cps: 0.078621
[12:48:32.966] iteration 6850: total_loss: 0.214316, loss_sup: 0.090902, loss_mps: 0.042106, loss_cps: 0.081309
[12:48:33.111] iteration 6851: total_loss: 0.330380, loss_sup: 0.199750, loss_mps: 0.043146, loss_cps: 0.087485
[12:48:33.257] iteration 6852: total_loss: 0.330760, loss_sup: 0.182838, loss_mps: 0.050576, loss_cps: 0.097346
[12:48:33.404] iteration 6853: total_loss: 0.185589, loss_sup: 0.099609, loss_mps: 0.030392, loss_cps: 0.055587
[12:48:33.550] iteration 6854: total_loss: 0.242019, loss_sup: 0.127570, loss_mps: 0.038828, loss_cps: 0.075622
[12:48:33.696] iteration 6855: total_loss: 0.194980, loss_sup: 0.085195, loss_mps: 0.038573, loss_cps: 0.071212
[12:48:33.842] iteration 6856: total_loss: 0.110989, loss_sup: 0.028967, loss_mps: 0.030450, loss_cps: 0.051572
[12:48:33.987] iteration 6857: total_loss: 0.119312, loss_sup: 0.029249, loss_mps: 0.033070, loss_cps: 0.056993
[12:48:34.134] iteration 6858: total_loss: 0.198604, loss_sup: 0.072745, loss_mps: 0.044536, loss_cps: 0.081323
[12:48:34.280] iteration 6859: total_loss: 0.232368, loss_sup: 0.072188, loss_mps: 0.053196, loss_cps: 0.106983
[12:48:34.426] iteration 6860: total_loss: 0.273993, loss_sup: 0.186357, loss_mps: 0.032798, loss_cps: 0.054838
[12:48:34.573] iteration 6861: total_loss: 0.111163, loss_sup: 0.031529, loss_mps: 0.028462, loss_cps: 0.051172
[12:48:34.720] iteration 6862: total_loss: 0.351670, loss_sup: 0.178376, loss_mps: 0.056140, loss_cps: 0.117154
[12:48:34.868] iteration 6863: total_loss: 0.172122, loss_sup: 0.062354, loss_mps: 0.037682, loss_cps: 0.072086
[12:48:35.013] iteration 6864: total_loss: 0.191104, loss_sup: 0.102708, loss_mps: 0.031854, loss_cps: 0.056542
[12:48:35.159] iteration 6865: total_loss: 0.121387, loss_sup: 0.031905, loss_mps: 0.031628, loss_cps: 0.057854
[12:48:35.306] iteration 6866: total_loss: 0.224190, loss_sup: 0.138131, loss_mps: 0.030201, loss_cps: 0.055858
[12:48:35.451] iteration 6867: total_loss: 0.214284, loss_sup: 0.123324, loss_mps: 0.032990, loss_cps: 0.057969
[12:48:35.600] iteration 6868: total_loss: 0.286144, loss_sup: 0.189916, loss_mps: 0.034325, loss_cps: 0.061903
[12:48:35.749] iteration 6869: total_loss: 0.264156, loss_sup: 0.142048, loss_mps: 0.041552, loss_cps: 0.080555
[12:48:35.898] iteration 6870: total_loss: 0.305403, loss_sup: 0.154675, loss_mps: 0.050000, loss_cps: 0.100728
[12:48:36.043] iteration 6871: total_loss: 0.157370, loss_sup: 0.048327, loss_mps: 0.037291, loss_cps: 0.071751
[12:48:36.190] iteration 6872: total_loss: 0.333037, loss_sup: 0.172884, loss_mps: 0.052211, loss_cps: 0.107942
[12:48:36.336] iteration 6873: total_loss: 0.285400, loss_sup: 0.107426, loss_mps: 0.056437, loss_cps: 0.121537
[12:48:36.486] iteration 6874: total_loss: 0.233873, loss_sup: 0.118290, loss_mps: 0.041683, loss_cps: 0.073900
[12:48:36.631] iteration 6875: total_loss: 0.295400, loss_sup: 0.178292, loss_mps: 0.042024, loss_cps: 0.075084
[12:48:36.779] iteration 6876: total_loss: 0.330151, loss_sup: 0.197167, loss_mps: 0.046068, loss_cps: 0.086915
[12:48:36.925] iteration 6877: total_loss: 0.190391, loss_sup: 0.077976, loss_mps: 0.039998, loss_cps: 0.072417
[12:48:37.071] iteration 6878: total_loss: 0.179173, loss_sup: 0.067677, loss_mps: 0.038883, loss_cps: 0.072612
[12:48:37.217] iteration 6879: total_loss: 0.182558, loss_sup: 0.099181, loss_mps: 0.030109, loss_cps: 0.053269
[12:48:37.363] iteration 6880: total_loss: 0.355787, loss_sup: 0.214360, loss_mps: 0.050128, loss_cps: 0.091299
[12:48:37.509] iteration 6881: total_loss: 0.141460, loss_sup: 0.047806, loss_mps: 0.035260, loss_cps: 0.058393
[12:48:37.655] iteration 6882: total_loss: 0.315317, loss_sup: 0.195252, loss_mps: 0.042363, loss_cps: 0.077702
[12:48:37.801] iteration 6883: total_loss: 0.255323, loss_sup: 0.170661, loss_mps: 0.030868, loss_cps: 0.053793
[12:48:37.947] iteration 6884: total_loss: 0.149220, loss_sup: 0.021587, loss_mps: 0.044604, loss_cps: 0.083029
[12:48:38.092] iteration 6885: total_loss: 0.252740, loss_sup: 0.113531, loss_mps: 0.047087, loss_cps: 0.092122
[12:48:38.238] iteration 6886: total_loss: 0.389216, loss_sup: 0.222030, loss_mps: 0.056334, loss_cps: 0.110853
[12:48:38.384] iteration 6887: total_loss: 0.228314, loss_sup: 0.120578, loss_mps: 0.039356, loss_cps: 0.068380
[12:48:38.529] iteration 6888: total_loss: 0.163865, loss_sup: 0.041277, loss_mps: 0.044913, loss_cps: 0.077674
[12:48:38.674] iteration 6889: total_loss: 0.231460, loss_sup: 0.069917, loss_mps: 0.054632, loss_cps: 0.106911
[12:48:38.820] iteration 6890: total_loss: 0.212960, loss_sup: 0.096600, loss_mps: 0.041725, loss_cps: 0.074635
[12:48:38.965] iteration 6891: total_loss: 0.145028, loss_sup: 0.039229, loss_mps: 0.038443, loss_cps: 0.067356
[12:48:39.111] iteration 6892: total_loss: 0.290021, loss_sup: 0.150138, loss_mps: 0.048166, loss_cps: 0.091718
[12:48:39.257] iteration 6893: total_loss: 0.184133, loss_sup: 0.046614, loss_mps: 0.047757, loss_cps: 0.089763
[12:48:39.403] iteration 6894: total_loss: 0.168214, loss_sup: 0.039788, loss_mps: 0.045466, loss_cps: 0.082959
[12:48:39.550] iteration 6895: total_loss: 0.278097, loss_sup: 0.168190, loss_mps: 0.040544, loss_cps: 0.069363
[12:48:39.696] iteration 6896: total_loss: 0.156655, loss_sup: 0.048189, loss_mps: 0.038595, loss_cps: 0.069871
[12:48:39.841] iteration 6897: total_loss: 0.222588, loss_sup: 0.118139, loss_mps: 0.037756, loss_cps: 0.066693
[12:48:39.986] iteration 6898: total_loss: 0.172706, loss_sup: 0.063974, loss_mps: 0.039484, loss_cps: 0.069247
[12:48:40.132] iteration 6899: total_loss: 0.469087, loss_sup: 0.224994, loss_mps: 0.077120, loss_cps: 0.166973
[12:48:40.279] iteration 6900: total_loss: 0.278233, loss_sup: 0.118303, loss_mps: 0.053424, loss_cps: 0.106506
[12:48:40.279] Evaluation Started ==>
[12:48:51.761] ==> valid iteration 6900: unet metrics: {'dc': 0.6072062415319488, 'jc': 0.4783880378923448, 'pre': 0.6544278618812548, 'hd': 6.418994068886585}, ynet metrics: {'dc': 0.5518820976422272, 'jc': 0.42575907473996566, 'pre': 0.6774336134965157, 'hd': 6.407411070622921}.
[12:48:51.763] Evaluation Finished!⏹️
[12:48:51.913] iteration 6901: total_loss: 0.251946, loss_sup: 0.160053, loss_mps: 0.033235, loss_cps: 0.058659
[12:48:52.060] iteration 6902: total_loss: 0.301349, loss_sup: 0.138651, loss_mps: 0.053140, loss_cps: 0.109558
[12:48:52.206] iteration 6903: total_loss: 0.257125, loss_sup: 0.134721, loss_mps: 0.044222, loss_cps: 0.078182
[12:48:52.355] iteration 6904: total_loss: 0.224618, loss_sup: 0.089794, loss_mps: 0.048357, loss_cps: 0.086467
[12:48:52.501] iteration 6905: total_loss: 0.362322, loss_sup: 0.214926, loss_mps: 0.050546, loss_cps: 0.096851
[12:48:52.646] iteration 6906: total_loss: 0.230769, loss_sup: 0.111540, loss_mps: 0.040475, loss_cps: 0.078753
[12:48:52.791] iteration 6907: total_loss: 0.200265, loss_sup: 0.094057, loss_mps: 0.038374, loss_cps: 0.067835
[12:48:52.936] iteration 6908: total_loss: 0.143182, loss_sup: 0.043099, loss_mps: 0.037027, loss_cps: 0.063056
[12:48:53.083] iteration 6909: total_loss: 0.199062, loss_sup: 0.118186, loss_mps: 0.030226, loss_cps: 0.050649
[12:48:53.231] iteration 6910: total_loss: 0.490593, loss_sup: 0.379814, loss_mps: 0.039710, loss_cps: 0.071069
[12:48:53.379] iteration 6911: total_loss: 0.127159, loss_sup: 0.045808, loss_mps: 0.030313, loss_cps: 0.051038
[12:48:53.525] iteration 6912: total_loss: 0.288257, loss_sup: 0.148086, loss_mps: 0.048822, loss_cps: 0.091349
[12:48:53.672] iteration 6913: total_loss: 0.148065, loss_sup: 0.056793, loss_mps: 0.032718, loss_cps: 0.058554
[12:48:53.818] iteration 6914: total_loss: 0.172500, loss_sup: 0.065252, loss_mps: 0.038416, loss_cps: 0.068832
[12:48:53.963] iteration 6915: total_loss: 0.129857, loss_sup: 0.035527, loss_mps: 0.033992, loss_cps: 0.060338
[12:48:54.111] iteration 6916: total_loss: 0.177885, loss_sup: 0.081328, loss_mps: 0.035244, loss_cps: 0.061313
[12:48:54.256] iteration 6917: total_loss: 0.265936, loss_sup: 0.099319, loss_mps: 0.053998, loss_cps: 0.112619
[12:48:54.403] iteration 6918: total_loss: 0.181583, loss_sup: 0.085705, loss_mps: 0.034563, loss_cps: 0.061314
[12:48:54.549] iteration 6919: total_loss: 0.194622, loss_sup: 0.079836, loss_mps: 0.041145, loss_cps: 0.073641
[12:48:54.695] iteration 6920: total_loss: 0.295792, loss_sup: 0.198755, loss_mps: 0.034520, loss_cps: 0.062516
[12:48:54.840] iteration 6921: total_loss: 0.237131, loss_sup: 0.093859, loss_mps: 0.048319, loss_cps: 0.094952
[12:48:54.985] iteration 6922: total_loss: 0.743777, loss_sup: 0.554608, loss_mps: 0.062454, loss_cps: 0.126714
[12:48:55.130] iteration 6923: total_loss: 0.165669, loss_sup: 0.051390, loss_mps: 0.041666, loss_cps: 0.072614
[12:48:55.278] iteration 6924: total_loss: 0.121258, loss_sup: 0.032670, loss_mps: 0.032300, loss_cps: 0.056287
[12:48:55.424] iteration 6925: total_loss: 0.210695, loss_sup: 0.091144, loss_mps: 0.042437, loss_cps: 0.077114
[12:48:55.570] iteration 6926: total_loss: 0.169231, loss_sup: 0.060099, loss_mps: 0.039873, loss_cps: 0.069259
[12:48:55.715] iteration 6927: total_loss: 0.148839, loss_sup: 0.039880, loss_mps: 0.039576, loss_cps: 0.069383
[12:48:55.862] iteration 6928: total_loss: 0.398661, loss_sup: 0.249925, loss_mps: 0.051362, loss_cps: 0.097375
[12:48:56.007] iteration 6929: total_loss: 0.122363, loss_sup: 0.015855, loss_mps: 0.038426, loss_cps: 0.068082
[12:48:56.153] iteration 6930: total_loss: 0.254669, loss_sup: 0.120357, loss_mps: 0.045371, loss_cps: 0.088941
[12:48:56.298] iteration 6931: total_loss: 0.271529, loss_sup: 0.159511, loss_mps: 0.041051, loss_cps: 0.070967
[12:48:56.445] iteration 6932: total_loss: 0.245874, loss_sup: 0.130032, loss_mps: 0.040968, loss_cps: 0.074874
[12:48:56.595] iteration 6933: total_loss: 0.238950, loss_sup: 0.071081, loss_mps: 0.056312, loss_cps: 0.111557
[12:48:56.740] iteration 6934: total_loss: 0.221932, loss_sup: 0.082909, loss_mps: 0.051241, loss_cps: 0.087782
[12:48:56.887] iteration 6935: total_loss: 0.277663, loss_sup: 0.130268, loss_mps: 0.051618, loss_cps: 0.095776
[12:48:57.033] iteration 6936: total_loss: 0.173111, loss_sup: 0.088888, loss_mps: 0.032113, loss_cps: 0.052110
[12:48:57.179] iteration 6937: total_loss: 0.349217, loss_sup: 0.239177, loss_mps: 0.041350, loss_cps: 0.068691
[12:48:57.324] iteration 6938: total_loss: 0.179502, loss_sup: 0.088258, loss_mps: 0.034971, loss_cps: 0.056272
[12:48:57.471] iteration 6939: total_loss: 0.194212, loss_sup: 0.095256, loss_mps: 0.035040, loss_cps: 0.063916
[12:48:57.617] iteration 6940: total_loss: 0.184387, loss_sup: 0.077928, loss_mps: 0.039529, loss_cps: 0.066930
[12:48:57.762] iteration 6941: total_loss: 0.144787, loss_sup: 0.064703, loss_mps: 0.029546, loss_cps: 0.050538
[12:48:57.908] iteration 6942: total_loss: 0.247935, loss_sup: 0.164417, loss_mps: 0.031247, loss_cps: 0.052271
[12:48:58.056] iteration 6943: total_loss: 0.311799, loss_sup: 0.174196, loss_mps: 0.046657, loss_cps: 0.090946
[12:48:58.202] iteration 6944: total_loss: 0.160522, loss_sup: 0.062875, loss_mps: 0.034956, loss_cps: 0.062691
[12:48:58.347] iteration 6945: total_loss: 0.172530, loss_sup: 0.077195, loss_mps: 0.034352, loss_cps: 0.060984
[12:48:58.494] iteration 6946: total_loss: 0.133440, loss_sup: 0.035841, loss_mps: 0.035574, loss_cps: 0.062025
[12:48:58.640] iteration 6947: total_loss: 0.133871, loss_sup: 0.037075, loss_mps: 0.034278, loss_cps: 0.062518
[12:48:58.786] iteration 6948: total_loss: 0.171679, loss_sup: 0.087873, loss_mps: 0.032059, loss_cps: 0.051747
[12:48:58.931] iteration 6949: total_loss: 0.272299, loss_sup: 0.150627, loss_mps: 0.040916, loss_cps: 0.080757
[12:48:59.077] iteration 6950: total_loss: 0.125460, loss_sup: 0.034036, loss_mps: 0.033833, loss_cps: 0.057591
[12:48:59.222] iteration 6951: total_loss: 0.175183, loss_sup: 0.084558, loss_mps: 0.031896, loss_cps: 0.058729
[12:48:59.368] iteration 6952: total_loss: 0.204892, loss_sup: 0.071607, loss_mps: 0.045864, loss_cps: 0.087420
[12:48:59.514] iteration 6953: total_loss: 0.234255, loss_sup: 0.119466, loss_mps: 0.041783, loss_cps: 0.073007
[12:48:59.660] iteration 6954: total_loss: 0.295597, loss_sup: 0.176537, loss_mps: 0.043325, loss_cps: 0.075735
[12:48:59.806] iteration 6955: total_loss: 0.263163, loss_sup: 0.137963, loss_mps: 0.042908, loss_cps: 0.082292
[12:48:59.952] iteration 6956: total_loss: 0.311857, loss_sup: 0.210999, loss_mps: 0.036600, loss_cps: 0.064257
[12:49:00.099] iteration 6957: total_loss: 0.548049, loss_sup: 0.400007, loss_mps: 0.049543, loss_cps: 0.098499
[12:49:00.245] iteration 6958: total_loss: 0.406492, loss_sup: 0.232514, loss_mps: 0.057611, loss_cps: 0.116368
[12:49:00.390] iteration 6959: total_loss: 0.230810, loss_sup: 0.118347, loss_mps: 0.039982, loss_cps: 0.072481
[12:49:00.537] iteration 6960: total_loss: 0.224669, loss_sup: 0.120637, loss_mps: 0.036930, loss_cps: 0.067101
[12:49:00.684] iteration 6961: total_loss: 0.216264, loss_sup: 0.105974, loss_mps: 0.040644, loss_cps: 0.069646
[12:49:00.829] iteration 6962: total_loss: 0.388666, loss_sup: 0.214664, loss_mps: 0.059093, loss_cps: 0.114910
[12:49:00.975] iteration 6963: total_loss: 0.163822, loss_sup: 0.016971, loss_mps: 0.050253, loss_cps: 0.096598
[12:49:01.120] iteration 6964: total_loss: 0.176814, loss_sup: 0.087689, loss_mps: 0.035252, loss_cps: 0.053873
[12:49:01.271] iteration 6965: total_loss: 0.175053, loss_sup: 0.057542, loss_mps: 0.041534, loss_cps: 0.075977
[12:49:01.417] iteration 6966: total_loss: 0.172456, loss_sup: 0.042488, loss_mps: 0.048059, loss_cps: 0.081908
[12:49:01.566] iteration 6967: total_loss: 0.312744, loss_sup: 0.104842, loss_mps: 0.069364, loss_cps: 0.138538
[12:49:01.713] iteration 6968: total_loss: 0.206899, loss_sup: 0.062379, loss_mps: 0.047753, loss_cps: 0.096767
[12:49:01.859] iteration 6969: total_loss: 0.203052, loss_sup: 0.083173, loss_mps: 0.043051, loss_cps: 0.076828
[12:49:02.005] iteration 6970: total_loss: 0.236828, loss_sup: 0.104123, loss_mps: 0.045322, loss_cps: 0.087383
[12:49:02.151] iteration 6971: total_loss: 0.135077, loss_sup: 0.020850, loss_mps: 0.040417, loss_cps: 0.073810
[12:49:02.299] iteration 6972: total_loss: 0.275846, loss_sup: 0.140700, loss_mps: 0.044861, loss_cps: 0.090285
[12:49:02.445] iteration 6973: total_loss: 0.329914, loss_sup: 0.221432, loss_mps: 0.038228, loss_cps: 0.070254
[12:49:02.592] iteration 6974: total_loss: 0.268292, loss_sup: 0.140816, loss_mps: 0.043452, loss_cps: 0.084024
[12:49:02.738] iteration 6975: total_loss: 0.221354, loss_sup: 0.120568, loss_mps: 0.036828, loss_cps: 0.063958
[12:49:02.883] iteration 6976: total_loss: 0.435342, loss_sup: 0.312935, loss_mps: 0.043375, loss_cps: 0.079031
[12:49:03.029] iteration 6977: total_loss: 0.336424, loss_sup: 0.197490, loss_mps: 0.047120, loss_cps: 0.091814
[12:49:03.175] iteration 6978: total_loss: 0.290459, loss_sup: 0.185362, loss_mps: 0.037375, loss_cps: 0.067721
[12:49:03.322] iteration 6979: total_loss: 0.174811, loss_sup: 0.098254, loss_mps: 0.029114, loss_cps: 0.047442
[12:49:03.469] iteration 6980: total_loss: 0.209360, loss_sup: 0.073332, loss_mps: 0.049288, loss_cps: 0.086740
[12:49:03.616] iteration 6981: total_loss: 0.156788, loss_sup: 0.044862, loss_mps: 0.039145, loss_cps: 0.072780
[12:49:03.762] iteration 6982: total_loss: 0.210691, loss_sup: 0.061947, loss_mps: 0.052194, loss_cps: 0.096549
[12:49:03.909] iteration 6983: total_loss: 0.243273, loss_sup: 0.129577, loss_mps: 0.039719, loss_cps: 0.073976
[12:49:04.057] iteration 6984: total_loss: 0.233138, loss_sup: 0.114123, loss_mps: 0.042710, loss_cps: 0.076305
[12:49:04.203] iteration 6985: total_loss: 0.241989, loss_sup: 0.146809, loss_mps: 0.037423, loss_cps: 0.057757
[12:49:04.349] iteration 6986: total_loss: 0.124244, loss_sup: 0.035563, loss_mps: 0.033164, loss_cps: 0.055517
[12:49:04.495] iteration 6987: total_loss: 0.300349, loss_sup: 0.110701, loss_mps: 0.060524, loss_cps: 0.129123
[12:49:04.642] iteration 6988: total_loss: 0.261493, loss_sup: 0.106625, loss_mps: 0.055669, loss_cps: 0.099199
[12:49:04.792] iteration 6989: total_loss: 0.152311, loss_sup: 0.057963, loss_mps: 0.035043, loss_cps: 0.059306
[12:49:04.937] iteration 6990: total_loss: 0.252958, loss_sup: 0.079560, loss_mps: 0.057312, loss_cps: 0.116086
[12:49:05.083] iteration 6991: total_loss: 0.217469, loss_sup: 0.090532, loss_mps: 0.047258, loss_cps: 0.079679
[12:49:05.233] iteration 6992: total_loss: 0.176920, loss_sup: 0.060787, loss_mps: 0.044172, loss_cps: 0.071961
[12:49:05.379] iteration 6993: total_loss: 0.366598, loss_sup: 0.185217, loss_mps: 0.061660, loss_cps: 0.119721
[12:49:05.525] iteration 6994: total_loss: 0.275766, loss_sup: 0.169090, loss_mps: 0.039042, loss_cps: 0.067634
[12:49:05.671] iteration 6995: total_loss: 0.157983, loss_sup: 0.076618, loss_mps: 0.032149, loss_cps: 0.049216
[12:49:05.818] iteration 6996: total_loss: 0.212815, loss_sup: 0.063460, loss_mps: 0.051793, loss_cps: 0.097562
[12:49:05.964] iteration 6997: total_loss: 0.125806, loss_sup: 0.034036, loss_mps: 0.033396, loss_cps: 0.058373
[12:49:06.111] iteration 6998: total_loss: 0.349703, loss_sup: 0.181734, loss_mps: 0.054782, loss_cps: 0.113188
[12:49:06.257] iteration 6999: total_loss: 0.178407, loss_sup: 0.048764, loss_mps: 0.045508, loss_cps: 0.084134
[12:49:06.404] iteration 7000: total_loss: 0.140120, loss_sup: 0.070104, loss_mps: 0.026613, loss_cps: 0.043403
[12:49:06.405] Evaluation Started ==>
[12:49:17.751] ==> valid iteration 7000: unet metrics: {'dc': 0.6171418444905032, 'jc': 0.48653037236717683, 'pre': 0.6318046551752047, 'hd': 6.741800254146692}, ynet metrics: {'dc': 0.586314983006671, 'jc': 0.46369128172358104, 'pre': 0.6960040830489417, 'hd': 6.243726249075238}.
[12:49:17.754] Evaluation Finished!⏹️
[12:49:17.904] iteration 7001: total_loss: 0.370207, loss_sup: 0.232755, loss_mps: 0.046279, loss_cps: 0.091173
[12:49:18.052] iteration 7002: total_loss: 0.176546, loss_sup: 0.053358, loss_mps: 0.043868, loss_cps: 0.079320
[12:49:18.197] iteration 7003: total_loss: 0.228429, loss_sup: 0.121304, loss_mps: 0.039235, loss_cps: 0.067890
[12:49:18.342] iteration 7004: total_loss: 0.132840, loss_sup: 0.036225, loss_mps: 0.036058, loss_cps: 0.060556
[12:49:18.487] iteration 7005: total_loss: 0.253969, loss_sup: 0.147082, loss_mps: 0.037328, loss_cps: 0.069560
[12:49:18.632] iteration 7006: total_loss: 0.140454, loss_sup: 0.043368, loss_mps: 0.035165, loss_cps: 0.061921
[12:49:18.777] iteration 7007: total_loss: 0.144157, loss_sup: 0.042200, loss_mps: 0.037032, loss_cps: 0.064925
[12:49:18.923] iteration 7008: total_loss: 0.406232, loss_sup: 0.163024, loss_mps: 0.076113, loss_cps: 0.167095
[12:49:19.069] iteration 7009: total_loss: 0.216889, loss_sup: 0.113109, loss_mps: 0.038504, loss_cps: 0.065276
[12:49:19.214] iteration 7010: total_loss: 0.209449, loss_sup: 0.073810, loss_mps: 0.046977, loss_cps: 0.088661
[12:49:19.359] iteration 7011: total_loss: 0.325995, loss_sup: 0.168634, loss_mps: 0.051907, loss_cps: 0.105454
[12:49:19.504] iteration 7012: total_loss: 0.365078, loss_sup: 0.208014, loss_mps: 0.052653, loss_cps: 0.104410
[12:49:19.649] iteration 7013: total_loss: 0.259641, loss_sup: 0.144664, loss_mps: 0.039960, loss_cps: 0.075017
[12:49:19.795] iteration 7014: total_loss: 0.152011, loss_sup: 0.050791, loss_mps: 0.036268, loss_cps: 0.064953
[12:49:19.940] iteration 7015: total_loss: 0.212189, loss_sup: 0.081197, loss_mps: 0.043318, loss_cps: 0.087675
[12:49:20.086] iteration 7016: total_loss: 0.167583, loss_sup: 0.046863, loss_mps: 0.042266, loss_cps: 0.078454
[12:49:20.231] iteration 7017: total_loss: 0.370064, loss_sup: 0.210691, loss_mps: 0.052982, loss_cps: 0.106390
[12:49:20.376] iteration 7018: total_loss: 0.228230, loss_sup: 0.113896, loss_mps: 0.039954, loss_cps: 0.074380
[12:49:20.521] iteration 7019: total_loss: 0.317772, loss_sup: 0.110266, loss_mps: 0.065893, loss_cps: 0.141613
[12:49:20.667] iteration 7020: total_loss: 0.218619, loss_sup: 0.092878, loss_mps: 0.042529, loss_cps: 0.083212
[12:49:20.812] iteration 7021: total_loss: 0.221648, loss_sup: 0.114352, loss_mps: 0.039260, loss_cps: 0.068035
[12:49:20.957] iteration 7022: total_loss: 0.169528, loss_sup: 0.055083, loss_mps: 0.040108, loss_cps: 0.074336
[12:49:21.103] iteration 7023: total_loss: 0.206208, loss_sup: 0.116445, loss_mps: 0.032112, loss_cps: 0.057651
[12:49:21.250] iteration 7024: total_loss: 0.478561, loss_sup: 0.269449, loss_mps: 0.068305, loss_cps: 0.140807
[12:49:21.395] iteration 7025: total_loss: 0.178155, loss_sup: 0.055646, loss_mps: 0.044013, loss_cps: 0.078495
[12:49:21.541] iteration 7026: total_loss: 0.355630, loss_sup: 0.198934, loss_mps: 0.053997, loss_cps: 0.102698
[12:49:21.687] iteration 7027: total_loss: 0.245034, loss_sup: 0.135827, loss_mps: 0.039313, loss_cps: 0.069893
[12:49:21.833] iteration 7028: total_loss: 0.483675, loss_sup: 0.324349, loss_mps: 0.054922, loss_cps: 0.104404
[12:49:21.978] iteration 7029: total_loss: 0.284052, loss_sup: 0.193906, loss_mps: 0.033077, loss_cps: 0.057069
[12:49:22.124] iteration 7030: total_loss: 0.292347, loss_sup: 0.189538, loss_mps: 0.037789, loss_cps: 0.065020
[12:49:22.270] iteration 7031: total_loss: 0.227164, loss_sup: 0.059286, loss_mps: 0.055336, loss_cps: 0.112543
[12:49:22.416] iteration 7032: total_loss: 0.233660, loss_sup: 0.092801, loss_mps: 0.048028, loss_cps: 0.092830
[12:49:22.562] iteration 7033: total_loss: 0.240666, loss_sup: 0.110107, loss_mps: 0.045321, loss_cps: 0.085237
[12:49:22.708] iteration 7034: total_loss: 0.653135, loss_sup: 0.505038, loss_mps: 0.050123, loss_cps: 0.097974
[12:49:22.853] iteration 7035: total_loss: 0.425209, loss_sup: 0.278422, loss_mps: 0.050977, loss_cps: 0.095811
[12:49:22.999] iteration 7036: total_loss: 0.273459, loss_sup: 0.107547, loss_mps: 0.055203, loss_cps: 0.110709
[12:49:23.145] iteration 7037: total_loss: 0.200617, loss_sup: 0.064308, loss_mps: 0.048323, loss_cps: 0.087985
[12:49:23.291] iteration 7038: total_loss: 0.135733, loss_sup: 0.043102, loss_mps: 0.034463, loss_cps: 0.058167
[12:49:23.436] iteration 7039: total_loss: 0.251850, loss_sup: 0.069614, loss_mps: 0.060756, loss_cps: 0.121480
[12:49:23.584] iteration 7040: total_loss: 0.305159, loss_sup: 0.169094, loss_mps: 0.048456, loss_cps: 0.087609
[12:49:23.730] iteration 7041: total_loss: 0.186160, loss_sup: 0.029411, loss_mps: 0.053372, loss_cps: 0.103377
[12:49:23.875] iteration 7042: total_loss: 0.248721, loss_sup: 0.122550, loss_mps: 0.043958, loss_cps: 0.082213
[12:49:24.021] iteration 7043: total_loss: 0.255839, loss_sup: 0.106005, loss_mps: 0.050676, loss_cps: 0.099158
[12:49:24.167] iteration 7044: total_loss: 0.206797, loss_sup: 0.071605, loss_mps: 0.047825, loss_cps: 0.087367
[12:49:24.312] iteration 7045: total_loss: 0.158284, loss_sup: 0.073359, loss_mps: 0.033928, loss_cps: 0.050997
[12:49:24.458] iteration 7046: total_loss: 0.343763, loss_sup: 0.225524, loss_mps: 0.041718, loss_cps: 0.076521
[12:49:24.604] iteration 7047: total_loss: 0.147097, loss_sup: 0.041096, loss_mps: 0.039903, loss_cps: 0.066098
[12:49:24.749] iteration 7048: total_loss: 0.262174, loss_sup: 0.122201, loss_mps: 0.048881, loss_cps: 0.091091
[12:49:24.895] iteration 7049: total_loss: 0.198590, loss_sup: 0.073291, loss_mps: 0.043779, loss_cps: 0.081521
[12:49:25.040] iteration 7050: total_loss: 0.098290, loss_sup: 0.027079, loss_mps: 0.027712, loss_cps: 0.043499
[12:49:25.188] iteration 7051: total_loss: 0.322693, loss_sup: 0.190570, loss_mps: 0.046945, loss_cps: 0.085178
[12:49:25.334] iteration 7052: total_loss: 0.144998, loss_sup: 0.049816, loss_mps: 0.036391, loss_cps: 0.058791
[12:49:25.479] iteration 7053: total_loss: 0.164880, loss_sup: 0.072727, loss_mps: 0.034348, loss_cps: 0.057805
[12:49:25.625] iteration 7054: total_loss: 0.295736, loss_sup: 0.162601, loss_mps: 0.047002, loss_cps: 0.086133
[12:49:25.770] iteration 7055: total_loss: 0.193587, loss_sup: 0.066919, loss_mps: 0.043515, loss_cps: 0.083153
[12:49:25.916] iteration 7056: total_loss: 0.208929, loss_sup: 0.126279, loss_mps: 0.031285, loss_cps: 0.051365
[12:49:26.062] iteration 7057: total_loss: 0.290481, loss_sup: 0.161488, loss_mps: 0.046151, loss_cps: 0.082842
[12:49:26.207] iteration 7058: total_loss: 0.229630, loss_sup: 0.106832, loss_mps: 0.044254, loss_cps: 0.078544
[12:49:26.356] iteration 7059: total_loss: 0.102372, loss_sup: 0.030844, loss_mps: 0.027432, loss_cps: 0.044096
[12:49:26.502] iteration 7060: total_loss: 0.204199, loss_sup: 0.116712, loss_mps: 0.033515, loss_cps: 0.053972
[12:49:26.647] iteration 7061: total_loss: 0.208302, loss_sup: 0.096306, loss_mps: 0.040572, loss_cps: 0.071424
[12:49:26.793] iteration 7062: total_loss: 0.104520, loss_sup: 0.019595, loss_mps: 0.031698, loss_cps: 0.053227
[12:49:26.940] iteration 7063: total_loss: 0.205192, loss_sup: 0.083326, loss_mps: 0.042534, loss_cps: 0.079331
[12:49:27.087] iteration 7064: total_loss: 0.248338, loss_sup: 0.128150, loss_mps: 0.043886, loss_cps: 0.076301
[12:49:27.233] iteration 7065: total_loss: 0.248952, loss_sup: 0.096956, loss_mps: 0.051817, loss_cps: 0.100179
[12:49:27.379] iteration 7066: total_loss: 0.150140, loss_sup: 0.038908, loss_mps: 0.040254, loss_cps: 0.070979
[12:49:27.526] iteration 7067: total_loss: 0.214398, loss_sup: 0.099551, loss_mps: 0.041501, loss_cps: 0.073345
[12:49:27.672] iteration 7068: total_loss: 0.276186, loss_sup: 0.120871, loss_mps: 0.052691, loss_cps: 0.102624
[12:49:27.817] iteration 7069: total_loss: 0.381162, loss_sup: 0.210816, loss_mps: 0.058601, loss_cps: 0.111745
[12:49:27.963] iteration 7070: total_loss: 0.181671, loss_sup: 0.065811, loss_mps: 0.040544, loss_cps: 0.075317
[12:49:28.109] iteration 7071: total_loss: 0.191061, loss_sup: 0.103982, loss_mps: 0.032838, loss_cps: 0.054241
[12:49:28.254] iteration 7072: total_loss: 0.227938, loss_sup: 0.106539, loss_mps: 0.040759, loss_cps: 0.080639
[12:49:28.400] iteration 7073: total_loss: 0.210187, loss_sup: 0.116283, loss_mps: 0.033463, loss_cps: 0.060441
[12:49:28.546] iteration 7074: total_loss: 0.242013, loss_sup: 0.155895, loss_mps: 0.032911, loss_cps: 0.053207
[12:49:28.699] iteration 7075: total_loss: 0.116433, loss_sup: 0.036135, loss_mps: 0.029123, loss_cps: 0.051175
[12:49:28.845] iteration 7076: total_loss: 0.139340, loss_sup: 0.032599, loss_mps: 0.037905, loss_cps: 0.068835
[12:49:28.991] iteration 7077: total_loss: 0.094048, loss_sup: 0.006938, loss_mps: 0.030051, loss_cps: 0.057059
[12:49:29.137] iteration 7078: total_loss: 0.276685, loss_sup: 0.148831, loss_mps: 0.043421, loss_cps: 0.084434
[12:49:29.284] iteration 7079: total_loss: 0.208482, loss_sup: 0.144597, loss_mps: 0.024156, loss_cps: 0.039729
[12:49:29.429] iteration 7080: total_loss: 0.164742, loss_sup: 0.029509, loss_mps: 0.044682, loss_cps: 0.090551
[12:49:29.578] iteration 7081: total_loss: 0.165243, loss_sup: 0.043082, loss_mps: 0.041331, loss_cps: 0.080830
[12:49:29.726] iteration 7082: total_loss: 0.350091, loss_sup: 0.225078, loss_mps: 0.043701, loss_cps: 0.081312
[12:49:29.871] iteration 7083: total_loss: 0.294896, loss_sup: 0.150889, loss_mps: 0.051051, loss_cps: 0.092956
[12:49:30.016] iteration 7084: total_loss: 0.330195, loss_sup: 0.159536, loss_mps: 0.055998, loss_cps: 0.114660
[12:49:30.165] iteration 7085: total_loss: 0.241191, loss_sup: 0.086071, loss_mps: 0.050292, loss_cps: 0.104828
[12:49:30.311] iteration 7086: total_loss: 0.164625, loss_sup: 0.088872, loss_mps: 0.027964, loss_cps: 0.047788
[12:49:30.457] iteration 7087: total_loss: 0.138225, loss_sup: 0.022310, loss_mps: 0.040961, loss_cps: 0.074955
[12:49:30.605] iteration 7088: total_loss: 0.204089, loss_sup: 0.037914, loss_mps: 0.055790, loss_cps: 0.110385
[12:49:30.754] iteration 7089: total_loss: 0.095655, loss_sup: 0.010405, loss_mps: 0.032560, loss_cps: 0.052689
[12:49:30.901] iteration 7090: total_loss: 0.141192, loss_sup: 0.025566, loss_mps: 0.039223, loss_cps: 0.076403
[12:49:31.049] iteration 7091: total_loss: 0.235888, loss_sup: 0.140046, loss_mps: 0.034894, loss_cps: 0.060948
[12:49:31.195] iteration 7092: total_loss: 0.367700, loss_sup: 0.176099, loss_mps: 0.062219, loss_cps: 0.129382
[12:49:31.341] iteration 7093: total_loss: 0.265677, loss_sup: 0.173133, loss_mps: 0.033216, loss_cps: 0.059328
[12:49:31.487] iteration 7094: total_loss: 0.321413, loss_sup: 0.149283, loss_mps: 0.056757, loss_cps: 0.115373
[12:49:31.632] iteration 7095: total_loss: 0.225543, loss_sup: 0.051871, loss_mps: 0.057117, loss_cps: 0.116554
[12:49:31.778] iteration 7096: total_loss: 0.141293, loss_sup: 0.061967, loss_mps: 0.028183, loss_cps: 0.051143
[12:49:31.924] iteration 7097: total_loss: 0.249843, loss_sup: 0.119101, loss_mps: 0.043795, loss_cps: 0.086948
[12:49:32.069] iteration 7098: total_loss: 0.263904, loss_sup: 0.151780, loss_mps: 0.038661, loss_cps: 0.073463
[12:49:32.215] iteration 7099: total_loss: 0.169127, loss_sup: 0.081876, loss_mps: 0.030830, loss_cps: 0.056421
[12:49:32.360] iteration 7100: total_loss: 0.417052, loss_sup: 0.193695, loss_mps: 0.073170, loss_cps: 0.150187
[12:49:32.360] Evaluation Started ==>
[12:49:43.726] ==> valid iteration 7100: unet metrics: {'dc': 0.6435500055513855, 'jc': 0.5126179506987318, 'pre': 0.6872473504655583, 'hd': 6.376843618391172}, ynet metrics: {'dc': 0.5596705056360228, 'jc': 0.43699846315721647, 'pre': 0.7614024048811006, 'hd': 5.92451284154549}.
[12:49:43.728] Evaluation Finished!⏹️
[12:49:43.881] iteration 7101: total_loss: 0.114547, loss_sup: 0.011692, loss_mps: 0.035163, loss_cps: 0.067692
[12:49:44.028] iteration 7102: total_loss: 0.163832, loss_sup: 0.029225, loss_mps: 0.045021, loss_cps: 0.089586
[12:49:44.173] iteration 7103: total_loss: 0.156817, loss_sup: 0.045353, loss_mps: 0.039540, loss_cps: 0.071924
[12:49:44.319] iteration 7104: total_loss: 0.222396, loss_sup: 0.083424, loss_mps: 0.047030, loss_cps: 0.091941
[12:49:44.464] iteration 7105: total_loss: 0.140231, loss_sup: 0.049084, loss_mps: 0.031336, loss_cps: 0.059810
[12:49:44.525] iteration 7106: total_loss: 0.097038, loss_sup: 0.018837, loss_mps: 0.027589, loss_cps: 0.050613
[12:49:45.764] iteration 7107: total_loss: 0.133601, loss_sup: 0.033155, loss_mps: 0.035648, loss_cps: 0.064798
[12:49:45.912] iteration 7108: total_loss: 0.444793, loss_sup: 0.258113, loss_mps: 0.060865, loss_cps: 0.125815
[12:49:46.059] iteration 7109: total_loss: 0.274026, loss_sup: 0.139549, loss_mps: 0.045440, loss_cps: 0.089037
[12:49:46.205] iteration 7110: total_loss: 0.181298, loss_sup: 0.039191, loss_mps: 0.048709, loss_cps: 0.093398
[12:49:46.351] iteration 7111: total_loss: 0.185564, loss_sup: 0.086695, loss_mps: 0.035807, loss_cps: 0.063062
[12:49:46.499] iteration 7112: total_loss: 0.196390, loss_sup: 0.102098, loss_mps: 0.034762, loss_cps: 0.059530
[12:49:46.645] iteration 7113: total_loss: 0.282355, loss_sup: 0.190011, loss_mps: 0.032312, loss_cps: 0.060032
[12:49:46.795] iteration 7114: total_loss: 0.260300, loss_sup: 0.114807, loss_mps: 0.049027, loss_cps: 0.096466
[12:49:46.942] iteration 7115: total_loss: 0.179940, loss_sup: 0.044523, loss_mps: 0.046504, loss_cps: 0.088913
[12:49:47.088] iteration 7116: total_loss: 0.182321, loss_sup: 0.038806, loss_mps: 0.049635, loss_cps: 0.093880
[12:49:47.234] iteration 7117: total_loss: 0.141509, loss_sup: 0.059645, loss_mps: 0.030463, loss_cps: 0.051401
[12:49:47.380] iteration 7118: total_loss: 0.165942, loss_sup: 0.048821, loss_mps: 0.042846, loss_cps: 0.074275
[12:49:47.527] iteration 7119: total_loss: 0.133853, loss_sup: 0.026388, loss_mps: 0.038039, loss_cps: 0.069426
[12:49:47.674] iteration 7120: total_loss: 0.109090, loss_sup: 0.012726, loss_mps: 0.034385, loss_cps: 0.061979
[12:49:47.821] iteration 7121: total_loss: 0.159460, loss_sup: 0.031490, loss_mps: 0.045589, loss_cps: 0.082381
[12:49:47.968] iteration 7122: total_loss: 0.198780, loss_sup: 0.101974, loss_mps: 0.033197, loss_cps: 0.063609
[12:49:48.114] iteration 7123: total_loss: 0.162037, loss_sup: 0.051523, loss_mps: 0.039523, loss_cps: 0.070991
[12:49:48.261] iteration 7124: total_loss: 0.165542, loss_sup: 0.080643, loss_mps: 0.031346, loss_cps: 0.053553
[12:49:48.407] iteration 7125: total_loss: 0.115778, loss_sup: 0.041533, loss_mps: 0.027741, loss_cps: 0.046504
[12:49:48.553] iteration 7126: total_loss: 0.267386, loss_sup: 0.137514, loss_mps: 0.044937, loss_cps: 0.084935
[12:49:48.700] iteration 7127: total_loss: 0.173304, loss_sup: 0.074269, loss_mps: 0.035565, loss_cps: 0.063470
[12:49:48.846] iteration 7128: total_loss: 0.199592, loss_sup: 0.105798, loss_mps: 0.033339, loss_cps: 0.060454
[12:49:48.992] iteration 7129: total_loss: 0.491676, loss_sup: 0.348404, loss_mps: 0.049548, loss_cps: 0.093724
[12:49:49.137] iteration 7130: total_loss: 0.262223, loss_sup: 0.111092, loss_mps: 0.050573, loss_cps: 0.100558
[12:49:49.284] iteration 7131: total_loss: 0.170433, loss_sup: 0.060152, loss_mps: 0.037852, loss_cps: 0.072430
[12:49:49.430] iteration 7132: total_loss: 0.193449, loss_sup: 0.077766, loss_mps: 0.039469, loss_cps: 0.076214
[12:49:49.576] iteration 7133: total_loss: 0.100546, loss_sup: 0.023340, loss_mps: 0.026816, loss_cps: 0.050389
[12:49:49.722] iteration 7134: total_loss: 0.340940, loss_sup: 0.267519, loss_mps: 0.026885, loss_cps: 0.046536
[12:49:49.870] iteration 7135: total_loss: 0.344519, loss_sup: 0.180170, loss_mps: 0.053036, loss_cps: 0.111313
[12:49:50.017] iteration 7136: total_loss: 0.157764, loss_sup: 0.048089, loss_mps: 0.038572, loss_cps: 0.071103
[12:49:50.163] iteration 7137: total_loss: 0.230951, loss_sup: 0.048773, loss_mps: 0.059209, loss_cps: 0.122969
[12:49:50.309] iteration 7138: total_loss: 0.253846, loss_sup: 0.117406, loss_mps: 0.044323, loss_cps: 0.092117
[12:49:50.456] iteration 7139: total_loss: 0.224918, loss_sup: 0.095379, loss_mps: 0.043821, loss_cps: 0.085718
[12:49:50.602] iteration 7140: total_loss: 0.141231, loss_sup: 0.054319, loss_mps: 0.032038, loss_cps: 0.054874
[12:49:50.748] iteration 7141: total_loss: 0.268248, loss_sup: 0.077938, loss_mps: 0.060454, loss_cps: 0.129856
[12:49:50.895] iteration 7142: total_loss: 0.319783, loss_sup: 0.094231, loss_mps: 0.072344, loss_cps: 0.153208
[12:49:51.043] iteration 7143: total_loss: 0.264738, loss_sup: 0.146883, loss_mps: 0.040796, loss_cps: 0.077059
[12:49:51.190] iteration 7144: total_loss: 0.155597, loss_sup: 0.030135, loss_mps: 0.043733, loss_cps: 0.081728
[12:49:51.337] iteration 7145: total_loss: 0.103495, loss_sup: 0.045228, loss_mps: 0.022639, loss_cps: 0.035628
[12:49:51.483] iteration 7146: total_loss: 0.309832, loss_sup: 0.181581, loss_mps: 0.045155, loss_cps: 0.083096
[12:49:51.629] iteration 7147: total_loss: 0.161191, loss_sup: 0.074893, loss_mps: 0.032798, loss_cps: 0.053500
[12:49:51.775] iteration 7148: total_loss: 0.268985, loss_sup: 0.077715, loss_mps: 0.059680, loss_cps: 0.131590
[12:49:51.921] iteration 7149: total_loss: 0.221175, loss_sup: 0.094602, loss_mps: 0.043762, loss_cps: 0.082811
[12:49:52.068] iteration 7150: total_loss: 0.365491, loss_sup: 0.225593, loss_mps: 0.046999, loss_cps: 0.092900
[12:49:52.216] iteration 7151: total_loss: 0.157917, loss_sup: 0.041834, loss_mps: 0.040751, loss_cps: 0.075331
[12:49:52.362] iteration 7152: total_loss: 0.367119, loss_sup: 0.234148, loss_mps: 0.043933, loss_cps: 0.089038
[12:49:52.508] iteration 7153: total_loss: 0.256845, loss_sup: 0.085640, loss_mps: 0.055172, loss_cps: 0.116033
[12:49:52.654] iteration 7154: total_loss: 0.448253, loss_sup: 0.320622, loss_mps: 0.044155, loss_cps: 0.083476
[12:49:52.801] iteration 7155: total_loss: 0.162955, loss_sup: 0.071547, loss_mps: 0.033745, loss_cps: 0.057664
[12:49:52.947] iteration 7156: total_loss: 0.220125, loss_sup: 0.092539, loss_mps: 0.045132, loss_cps: 0.082454
[12:49:53.094] iteration 7157: total_loss: 0.244323, loss_sup: 0.106251, loss_mps: 0.047105, loss_cps: 0.090967
[12:49:53.240] iteration 7158: total_loss: 0.127919, loss_sup: 0.042704, loss_mps: 0.032316, loss_cps: 0.052899
[12:49:53.387] iteration 7159: total_loss: 0.205146, loss_sup: 0.124372, loss_mps: 0.030807, loss_cps: 0.049968
[12:49:53.534] iteration 7160: total_loss: 0.211922, loss_sup: 0.073844, loss_mps: 0.046909, loss_cps: 0.091170
[12:49:53.680] iteration 7161: total_loss: 0.171011, loss_sup: 0.047264, loss_mps: 0.043860, loss_cps: 0.079887
[12:49:53.826] iteration 7162: total_loss: 0.208284, loss_sup: 0.065294, loss_mps: 0.050397, loss_cps: 0.092593
[12:49:53.974] iteration 7163: total_loss: 0.283764, loss_sup: 0.136854, loss_mps: 0.049421, loss_cps: 0.097488
[12:49:54.120] iteration 7164: total_loss: 0.173087, loss_sup: 0.033148, loss_mps: 0.047641, loss_cps: 0.092299
[12:49:54.267] iteration 7165: total_loss: 0.135343, loss_sup: 0.025717, loss_mps: 0.038949, loss_cps: 0.070677
[12:49:54.414] iteration 7166: total_loss: 0.128611, loss_sup: 0.033766, loss_mps: 0.033878, loss_cps: 0.060967
[12:49:54.561] iteration 7167: total_loss: 0.291671, loss_sup: 0.114527, loss_mps: 0.059280, loss_cps: 0.117863
[12:49:54.707] iteration 7168: total_loss: 0.177562, loss_sup: 0.081802, loss_mps: 0.034480, loss_cps: 0.061280
[12:49:54.854] iteration 7169: total_loss: 0.319201, loss_sup: 0.142468, loss_mps: 0.057314, loss_cps: 0.119419
[12:49:55.000] iteration 7170: total_loss: 0.266931, loss_sup: 0.145936, loss_mps: 0.041256, loss_cps: 0.079739
[12:49:55.147] iteration 7171: total_loss: 0.191141, loss_sup: 0.077649, loss_mps: 0.041132, loss_cps: 0.072360
[12:49:55.293] iteration 7172: total_loss: 0.103895, loss_sup: 0.021263, loss_mps: 0.030505, loss_cps: 0.052128
[12:49:55.439] iteration 7173: total_loss: 0.289170, loss_sup: 0.109606, loss_mps: 0.059201, loss_cps: 0.120363
[12:49:55.585] iteration 7174: total_loss: 0.311463, loss_sup: 0.182497, loss_mps: 0.045483, loss_cps: 0.083483
[12:49:55.732] iteration 7175: total_loss: 0.280213, loss_sup: 0.154780, loss_mps: 0.043962, loss_cps: 0.081470
[12:49:55.879] iteration 7176: total_loss: 0.169097, loss_sup: 0.076730, loss_mps: 0.034131, loss_cps: 0.058236
[12:49:56.027] iteration 7177: total_loss: 0.309604, loss_sup: 0.201966, loss_mps: 0.036084, loss_cps: 0.071555
[12:49:56.174] iteration 7178: total_loss: 0.200828, loss_sup: 0.070468, loss_mps: 0.043526, loss_cps: 0.086834
[12:49:56.321] iteration 7179: total_loss: 0.238283, loss_sup: 0.076712, loss_mps: 0.051827, loss_cps: 0.109743
[12:49:56.468] iteration 7180: total_loss: 0.187651, loss_sup: 0.064245, loss_mps: 0.042743, loss_cps: 0.080663
[12:49:56.614] iteration 7181: total_loss: 0.292997, loss_sup: 0.171129, loss_mps: 0.041767, loss_cps: 0.080100
[12:49:56.760] iteration 7182: total_loss: 0.125856, loss_sup: 0.032587, loss_mps: 0.033771, loss_cps: 0.059498
[12:49:56.907] iteration 7183: total_loss: 0.183812, loss_sup: 0.061225, loss_mps: 0.040957, loss_cps: 0.081630
[12:49:57.054] iteration 7184: total_loss: 0.353566, loss_sup: 0.147786, loss_mps: 0.065698, loss_cps: 0.140083
[12:49:57.201] iteration 7185: total_loss: 0.320843, loss_sup: 0.158481, loss_mps: 0.052196, loss_cps: 0.110166
[12:49:57.348] iteration 7186: total_loss: 0.158242, loss_sup: 0.086128, loss_mps: 0.027971, loss_cps: 0.044143
[12:49:57.494] iteration 7187: total_loss: 0.453405, loss_sup: 0.236245, loss_mps: 0.067957, loss_cps: 0.149202
[12:49:57.640] iteration 7188: total_loss: 0.175483, loss_sup: 0.096327, loss_mps: 0.029822, loss_cps: 0.049333
[12:49:57.787] iteration 7189: total_loss: 0.160322, loss_sup: 0.060247, loss_mps: 0.035663, loss_cps: 0.064412
[12:49:57.935] iteration 7190: total_loss: 0.204416, loss_sup: 0.097860, loss_mps: 0.038141, loss_cps: 0.068415
[12:49:58.081] iteration 7191: total_loss: 0.274815, loss_sup: 0.105268, loss_mps: 0.055843, loss_cps: 0.113704
[12:49:58.228] iteration 7192: total_loss: 0.148019, loss_sup: 0.043942, loss_mps: 0.036967, loss_cps: 0.067109
[12:49:58.374] iteration 7193: total_loss: 0.185638, loss_sup: 0.046064, loss_mps: 0.048152, loss_cps: 0.091422
[12:49:58.521] iteration 7194: total_loss: 0.173622, loss_sup: 0.057330, loss_mps: 0.040325, loss_cps: 0.075966
[12:49:58.668] iteration 7195: total_loss: 0.354602, loss_sup: 0.143497, loss_mps: 0.066415, loss_cps: 0.144690
[12:49:58.815] iteration 7196: total_loss: 0.149796, loss_sup: 0.061207, loss_mps: 0.033845, loss_cps: 0.054744
[12:49:58.963] iteration 7197: total_loss: 0.141358, loss_sup: 0.021254, loss_mps: 0.042087, loss_cps: 0.078017
[12:49:59.110] iteration 7198: total_loss: 0.369895, loss_sup: 0.208143, loss_mps: 0.055086, loss_cps: 0.106666
[12:49:59.259] iteration 7199: total_loss: 0.238883, loss_sup: 0.144107, loss_mps: 0.032666, loss_cps: 0.062111
[12:49:59.406] iteration 7200: total_loss: 0.184712, loss_sup: 0.072330, loss_mps: 0.041383, loss_cps: 0.070999
[12:49:59.406] Evaluation Started ==>
[12:50:10.798] ==> valid iteration 7200: unet metrics: {'dc': 0.627344477944767, 'jc': 0.50283520127595, 'pre': 0.7462737195708925, 'hd': 5.8519643214211055}, ynet metrics: {'dc': 0.5625885198395538, 'jc': 0.4477189344971742, 'pre': 0.7582713768734555, 'hd': 6.023951145767384}.
[12:50:10.800] Evaluation Finished!⏹️
[12:50:10.955] iteration 7201: total_loss: 0.228851, loss_sup: 0.108260, loss_mps: 0.041852, loss_cps: 0.078738
[12:50:11.105] iteration 7202: total_loss: 0.238164, loss_sup: 0.049787, loss_mps: 0.064191, loss_cps: 0.124186
[12:50:11.250] iteration 7203: total_loss: 0.242625, loss_sup: 0.121812, loss_mps: 0.041374, loss_cps: 0.079439
[12:50:11.394] iteration 7204: total_loss: 0.206901, loss_sup: 0.066760, loss_mps: 0.047140, loss_cps: 0.093001
[12:50:11.540] iteration 7205: total_loss: 0.324623, loss_sup: 0.192295, loss_mps: 0.044101, loss_cps: 0.088226
[12:50:11.685] iteration 7206: total_loss: 0.194240, loss_sup: 0.069730, loss_mps: 0.042105, loss_cps: 0.082405
[12:50:11.831] iteration 7207: total_loss: 0.372018, loss_sup: 0.172769, loss_mps: 0.064570, loss_cps: 0.134679
[12:50:11.977] iteration 7208: total_loss: 0.263630, loss_sup: 0.062302, loss_mps: 0.065233, loss_cps: 0.136095
[12:50:12.122] iteration 7209: total_loss: 0.271489, loss_sup: 0.143099, loss_mps: 0.042972, loss_cps: 0.085417
[12:50:12.266] iteration 7210: total_loss: 0.112052, loss_sup: 0.035559, loss_mps: 0.027785, loss_cps: 0.048708
[12:50:12.412] iteration 7211: total_loss: 0.320093, loss_sup: 0.177797, loss_mps: 0.047249, loss_cps: 0.095047
[12:50:12.558] iteration 7212: total_loss: 0.134106, loss_sup: 0.032879, loss_mps: 0.035039, loss_cps: 0.066188
[12:50:12.705] iteration 7213: total_loss: 0.201892, loss_sup: 0.076103, loss_mps: 0.043327, loss_cps: 0.082461
[12:50:12.851] iteration 7214: total_loss: 0.238472, loss_sup: 0.077194, loss_mps: 0.054503, loss_cps: 0.106774
[12:50:12.996] iteration 7215: total_loss: 0.139037, loss_sup: 0.033821, loss_mps: 0.036104, loss_cps: 0.069112
[12:50:13.142] iteration 7216: total_loss: 0.170379, loss_sup: 0.063086, loss_mps: 0.038270, loss_cps: 0.069024
[12:50:13.287] iteration 7217: total_loss: 0.195332, loss_sup: 0.075861, loss_mps: 0.042568, loss_cps: 0.076903
[12:50:13.432] iteration 7218: total_loss: 0.405848, loss_sup: 0.223377, loss_mps: 0.058885, loss_cps: 0.123586
[12:50:13.579] iteration 7219: total_loss: 0.158785, loss_sup: 0.052569, loss_mps: 0.039829, loss_cps: 0.066387
[12:50:13.726] iteration 7220: total_loss: 0.263375, loss_sup: 0.109265, loss_mps: 0.052407, loss_cps: 0.101703
[12:50:13.871] iteration 7221: total_loss: 0.253067, loss_sup: 0.140709, loss_mps: 0.040564, loss_cps: 0.071795
[12:50:14.017] iteration 7222: total_loss: 0.247460, loss_sup: 0.150916, loss_mps: 0.034369, loss_cps: 0.062175
[12:50:14.162] iteration 7223: total_loss: 0.220862, loss_sup: 0.130909, loss_mps: 0.032783, loss_cps: 0.057170
[12:50:14.307] iteration 7224: total_loss: 0.320565, loss_sup: 0.124886, loss_mps: 0.064390, loss_cps: 0.131289
[12:50:14.453] iteration 7225: total_loss: 0.281960, loss_sup: 0.138396, loss_mps: 0.052502, loss_cps: 0.091062
[12:50:14.599] iteration 7226: total_loss: 0.116707, loss_sup: 0.047237, loss_mps: 0.026181, loss_cps: 0.043289
[12:50:14.745] iteration 7227: total_loss: 0.193758, loss_sup: 0.071258, loss_mps: 0.042071, loss_cps: 0.080429
[12:50:14.891] iteration 7228: total_loss: 0.279771, loss_sup: 0.104130, loss_mps: 0.058124, loss_cps: 0.117517
[12:50:15.036] iteration 7229: total_loss: 0.279614, loss_sup: 0.097384, loss_mps: 0.058489, loss_cps: 0.123741
[12:50:15.182] iteration 7230: total_loss: 0.736923, loss_sup: 0.570892, loss_mps: 0.055535, loss_cps: 0.110496
[12:50:15.327] iteration 7231: total_loss: 0.264230, loss_sup: 0.162805, loss_mps: 0.036365, loss_cps: 0.065060
[12:50:15.473] iteration 7232: total_loss: 0.218554, loss_sup: 0.069602, loss_mps: 0.052024, loss_cps: 0.096927
[12:50:15.620] iteration 7233: total_loss: 0.317930, loss_sup: 0.234725, loss_mps: 0.030573, loss_cps: 0.052632
[12:50:15.769] iteration 7234: total_loss: 0.132953, loss_sup: 0.038615, loss_mps: 0.034009, loss_cps: 0.060329
[12:50:15.914] iteration 7235: total_loss: 0.264799, loss_sup: 0.144839, loss_mps: 0.042392, loss_cps: 0.077567
[12:50:16.059] iteration 7236: total_loss: 0.137220, loss_sup: 0.052665, loss_mps: 0.032976, loss_cps: 0.051579
[12:50:16.205] iteration 7237: total_loss: 0.189850, loss_sup: 0.072459, loss_mps: 0.044429, loss_cps: 0.072962
[12:50:16.350] iteration 7238: total_loss: 0.264471, loss_sup: 0.116922, loss_mps: 0.050113, loss_cps: 0.097437
[12:50:16.496] iteration 7239: total_loss: 0.144974, loss_sup: 0.049545, loss_mps: 0.035886, loss_cps: 0.059544
[12:50:16.642] iteration 7240: total_loss: 0.293852, loss_sup: 0.157385, loss_mps: 0.048347, loss_cps: 0.088120
[12:50:16.787] iteration 7241: total_loss: 0.302291, loss_sup: 0.178791, loss_mps: 0.044145, loss_cps: 0.079355
[12:50:16.933] iteration 7242: total_loss: 0.176452, loss_sup: 0.055472, loss_mps: 0.042440, loss_cps: 0.078540
[12:50:17.079] iteration 7243: total_loss: 0.162233, loss_sup: 0.060017, loss_mps: 0.037892, loss_cps: 0.064324
[12:50:17.224] iteration 7244: total_loss: 0.274568, loss_sup: 0.130588, loss_mps: 0.051390, loss_cps: 0.092590
[12:50:17.370] iteration 7245: total_loss: 0.299105, loss_sup: 0.202802, loss_mps: 0.035036, loss_cps: 0.061268
[12:50:17.515] iteration 7246: total_loss: 0.261308, loss_sup: 0.103468, loss_mps: 0.053927, loss_cps: 0.103913
[12:50:17.662] iteration 7247: total_loss: 0.266800, loss_sup: 0.126838, loss_mps: 0.047072, loss_cps: 0.092890
[12:50:17.807] iteration 7248: total_loss: 0.191441, loss_sup: 0.062567, loss_mps: 0.046408, loss_cps: 0.082466
[12:50:17.952] iteration 7249: total_loss: 0.116449, loss_sup: 0.025054, loss_mps: 0.035486, loss_cps: 0.055909
[12:50:18.098] iteration 7250: total_loss: 0.166352, loss_sup: 0.025143, loss_mps: 0.050859, loss_cps: 0.090349
[12:50:18.244] iteration 7251: total_loss: 0.173731, loss_sup: 0.076549, loss_mps: 0.036309, loss_cps: 0.060873
[12:50:18.390] iteration 7252: total_loss: 0.133714, loss_sup: 0.050205, loss_mps: 0.032923, loss_cps: 0.050586
[12:50:18.535] iteration 7253: total_loss: 0.217273, loss_sup: 0.057083, loss_mps: 0.053455, loss_cps: 0.106735
[12:50:18.681] iteration 7254: total_loss: 0.189305, loss_sup: 0.067956, loss_mps: 0.044014, loss_cps: 0.077336
[12:50:18.826] iteration 7255: total_loss: 0.202330, loss_sup: 0.081844, loss_mps: 0.043051, loss_cps: 0.077435
[12:50:18.971] iteration 7256: total_loss: 0.151002, loss_sup: 0.046886, loss_mps: 0.038796, loss_cps: 0.065320
[12:50:19.117] iteration 7257: total_loss: 0.225078, loss_sup: 0.090033, loss_mps: 0.046999, loss_cps: 0.088045
[12:50:19.263] iteration 7258: total_loss: 0.258315, loss_sup: 0.142306, loss_mps: 0.042420, loss_cps: 0.073589
[12:50:19.409] iteration 7259: total_loss: 0.345997, loss_sup: 0.165373, loss_mps: 0.060423, loss_cps: 0.120201
[12:50:19.557] iteration 7260: total_loss: 0.225457, loss_sup: 0.101238, loss_mps: 0.043374, loss_cps: 0.080845
[12:50:19.702] iteration 7261: total_loss: 0.174340, loss_sup: 0.034605, loss_mps: 0.046845, loss_cps: 0.092890
[12:50:19.848] iteration 7262: total_loss: 0.171131, loss_sup: 0.084318, loss_mps: 0.031323, loss_cps: 0.055490
[12:50:19.993] iteration 7263: total_loss: 0.228790, loss_sup: 0.061803, loss_mps: 0.055918, loss_cps: 0.111070
[12:50:20.138] iteration 7264: total_loss: 0.127403, loss_sup: 0.028702, loss_mps: 0.035060, loss_cps: 0.063641
[12:50:20.284] iteration 7265: total_loss: 0.127388, loss_sup: 0.022493, loss_mps: 0.037814, loss_cps: 0.067080
[12:50:20.430] iteration 7266: total_loss: 0.274943, loss_sup: 0.121488, loss_mps: 0.051129, loss_cps: 0.102326
[12:50:20.576] iteration 7267: total_loss: 0.172936, loss_sup: 0.074664, loss_mps: 0.034481, loss_cps: 0.063792
[12:50:20.721] iteration 7268: total_loss: 0.214063, loss_sup: 0.061625, loss_mps: 0.052425, loss_cps: 0.100013
[12:50:20.867] iteration 7269: total_loss: 0.340908, loss_sup: 0.252038, loss_mps: 0.032016, loss_cps: 0.056853
[12:50:21.012] iteration 7270: total_loss: 0.459262, loss_sup: 0.298670, loss_mps: 0.052400, loss_cps: 0.108192
[12:50:21.162] iteration 7271: total_loss: 0.338499, loss_sup: 0.223836, loss_mps: 0.038116, loss_cps: 0.076547
[12:50:21.310] iteration 7272: total_loss: 0.250136, loss_sup: 0.132166, loss_mps: 0.040125, loss_cps: 0.077845
[12:50:21.456] iteration 7273: total_loss: 0.205288, loss_sup: 0.077561, loss_mps: 0.042971, loss_cps: 0.084756
[12:50:21.602] iteration 7274: total_loss: 0.278915, loss_sup: 0.122007, loss_mps: 0.054149, loss_cps: 0.102759
[12:50:21.748] iteration 7275: total_loss: 0.394350, loss_sup: 0.237569, loss_mps: 0.051113, loss_cps: 0.105668
[12:50:21.894] iteration 7276: total_loss: 0.141315, loss_sup: 0.026264, loss_mps: 0.041534, loss_cps: 0.073518
[12:50:22.040] iteration 7277: total_loss: 0.254254, loss_sup: 0.088693, loss_mps: 0.055712, loss_cps: 0.109849
[12:50:22.187] iteration 7278: total_loss: 0.260686, loss_sup: 0.130881, loss_mps: 0.043925, loss_cps: 0.085880
[12:50:22.335] iteration 7279: total_loss: 0.100232, loss_sup: 0.015631, loss_mps: 0.033091, loss_cps: 0.051510
[12:50:22.481] iteration 7280: total_loss: 0.209648, loss_sup: 0.085669, loss_mps: 0.043599, loss_cps: 0.080380
[12:50:22.630] iteration 7281: total_loss: 0.159643, loss_sup: 0.037520, loss_mps: 0.041533, loss_cps: 0.080591
[12:50:22.776] iteration 7282: total_loss: 0.338886, loss_sup: 0.203396, loss_mps: 0.047602, loss_cps: 0.087888
[12:50:22.922] iteration 7283: total_loss: 0.183960, loss_sup: 0.053994, loss_mps: 0.046753, loss_cps: 0.083213
[12:50:23.067] iteration 7284: total_loss: 0.183978, loss_sup: 0.093750, loss_mps: 0.034683, loss_cps: 0.055545
[12:50:23.213] iteration 7285: total_loss: 0.190821, loss_sup: 0.038194, loss_mps: 0.051492, loss_cps: 0.101134
[12:50:23.358] iteration 7286: total_loss: 0.610372, loss_sup: 0.338919, loss_mps: 0.084996, loss_cps: 0.186456
[12:50:23.505] iteration 7287: total_loss: 0.331196, loss_sup: 0.134478, loss_mps: 0.065837, loss_cps: 0.130881
[12:50:23.650] iteration 7288: total_loss: 0.294376, loss_sup: 0.129502, loss_mps: 0.056989, loss_cps: 0.107884
[12:50:23.796] iteration 7289: total_loss: 0.155722, loss_sup: 0.043678, loss_mps: 0.038794, loss_cps: 0.073249
[12:50:23.942] iteration 7290: total_loss: 0.358249, loss_sup: 0.219185, loss_mps: 0.047400, loss_cps: 0.091664
[12:50:24.087] iteration 7291: total_loss: 0.177061, loss_sup: 0.043184, loss_mps: 0.048938, loss_cps: 0.084939
[12:50:24.233] iteration 7292: total_loss: 0.239387, loss_sup: 0.135035, loss_mps: 0.036800, loss_cps: 0.067552
[12:50:24.378] iteration 7293: total_loss: 0.224098, loss_sup: 0.021378, loss_mps: 0.066841, loss_cps: 0.135879
[12:50:24.524] iteration 7294: total_loss: 0.236300, loss_sup: 0.065756, loss_mps: 0.058780, loss_cps: 0.111764
[12:50:24.670] iteration 7295: total_loss: 0.279422, loss_sup: 0.169605, loss_mps: 0.038991, loss_cps: 0.070827
[12:50:24.815] iteration 7296: total_loss: 0.254386, loss_sup: 0.072942, loss_mps: 0.059963, loss_cps: 0.121480
[12:50:24.961] iteration 7297: total_loss: 0.207949, loss_sup: 0.060491, loss_mps: 0.050133, loss_cps: 0.097325
[12:50:25.107] iteration 7298: total_loss: 0.145374, loss_sup: 0.057705, loss_mps: 0.034110, loss_cps: 0.053558
[12:50:25.252] iteration 7299: total_loss: 0.153438, loss_sup: 0.036971, loss_mps: 0.041161, loss_cps: 0.075306
[12:50:25.397] iteration 7300: total_loss: 0.229717, loss_sup: 0.070326, loss_mps: 0.055052, loss_cps: 0.104339
[12:50:25.397] Evaluation Started ==>
[12:50:36.780] ==> valid iteration 7300: unet metrics: {'dc': 0.6209587202457095, 'jc': 0.49520992077673553, 'pre': 0.6959977232356113, 'hd': 6.020802208107696}, ynet metrics: {'dc': 0.5757209728173905, 'jc': 0.454209051771292, 'pre': 0.7073157380449125, 'hd': 6.1458177057318135}.
[12:50:36.781] Evaluation Finished!⏹️
[12:50:36.934] iteration 7301: total_loss: 0.341400, loss_sup: 0.161718, loss_mps: 0.062233, loss_cps: 0.117449
[12:50:37.084] iteration 7302: total_loss: 0.215994, loss_sup: 0.085520, loss_mps: 0.047536, loss_cps: 0.082938
[12:50:37.230] iteration 7303: total_loss: 0.118982, loss_sup: 0.029681, loss_mps: 0.035171, loss_cps: 0.054130
[12:50:37.374] iteration 7304: total_loss: 0.157200, loss_sup: 0.030114, loss_mps: 0.044958, loss_cps: 0.082128
[12:50:37.521] iteration 7305: total_loss: 0.181583, loss_sup: 0.064327, loss_mps: 0.043180, loss_cps: 0.074076
[12:50:37.667] iteration 7306: total_loss: 0.154787, loss_sup: 0.046614, loss_mps: 0.038464, loss_cps: 0.069710
[12:50:37.812] iteration 7307: total_loss: 0.154993, loss_sup: 0.038021, loss_mps: 0.042329, loss_cps: 0.074643
[12:50:37.957] iteration 7308: total_loss: 0.246762, loss_sup: 0.118272, loss_mps: 0.045286, loss_cps: 0.083204
[12:50:38.105] iteration 7309: total_loss: 0.140312, loss_sup: 0.030477, loss_mps: 0.040072, loss_cps: 0.069762
[12:50:38.252] iteration 7310: total_loss: 0.303331, loss_sup: 0.182948, loss_mps: 0.042272, loss_cps: 0.078111
[12:50:38.399] iteration 7311: total_loss: 0.139261, loss_sup: 0.039328, loss_mps: 0.036447, loss_cps: 0.063485
[12:50:38.545] iteration 7312: total_loss: 0.180353, loss_sup: 0.068358, loss_mps: 0.039373, loss_cps: 0.072623
[12:50:38.690] iteration 7313: total_loss: 0.128712, loss_sup: 0.033088, loss_mps: 0.033430, loss_cps: 0.062194
[12:50:38.837] iteration 7314: total_loss: 0.121499, loss_sup: 0.041779, loss_mps: 0.029847, loss_cps: 0.049873
[12:50:38.982] iteration 7315: total_loss: 0.098987, loss_sup: 0.023058, loss_mps: 0.028797, loss_cps: 0.047132
[12:50:39.128] iteration 7316: total_loss: 0.215094, loss_sup: 0.135971, loss_mps: 0.029282, loss_cps: 0.049842
[12:50:39.274] iteration 7317: total_loss: 0.141947, loss_sup: 0.051963, loss_mps: 0.032453, loss_cps: 0.057531
[12:50:39.419] iteration 7318: total_loss: 0.317460, loss_sup: 0.159916, loss_mps: 0.049816, loss_cps: 0.107729
[12:50:39.565] iteration 7319: total_loss: 0.147178, loss_sup: 0.028281, loss_mps: 0.041216, loss_cps: 0.077681
[12:50:39.710] iteration 7320: total_loss: 0.253177, loss_sup: 0.103827, loss_mps: 0.048461, loss_cps: 0.100889
[12:50:39.856] iteration 7321: total_loss: 0.121430, loss_sup: 0.029598, loss_mps: 0.031526, loss_cps: 0.060306
[12:50:40.003] iteration 7322: total_loss: 0.255512, loss_sup: 0.139022, loss_mps: 0.040045, loss_cps: 0.076445
[12:50:40.148] iteration 7323: total_loss: 0.141295, loss_sup: 0.053314, loss_mps: 0.030829, loss_cps: 0.057152
[12:50:40.294] iteration 7324: total_loss: 0.309694, loss_sup: 0.153243, loss_mps: 0.050596, loss_cps: 0.105855
[12:50:40.439] iteration 7325: total_loss: 0.144049, loss_sup: 0.053568, loss_mps: 0.032559, loss_cps: 0.057923
[12:50:40.585] iteration 7326: total_loss: 0.333484, loss_sup: 0.171707, loss_mps: 0.053097, loss_cps: 0.108680
[12:50:40.730] iteration 7327: total_loss: 0.125477, loss_sup: 0.050047, loss_mps: 0.027447, loss_cps: 0.047983
[12:50:40.876] iteration 7328: total_loss: 0.329211, loss_sup: 0.178548, loss_mps: 0.051048, loss_cps: 0.099615
[12:50:41.022] iteration 7329: total_loss: 0.273966, loss_sup: 0.126375, loss_mps: 0.049015, loss_cps: 0.098575
[12:50:41.167] iteration 7330: total_loss: 0.122284, loss_sup: 0.018976, loss_mps: 0.037382, loss_cps: 0.065925
[12:50:41.313] iteration 7331: total_loss: 0.261818, loss_sup: 0.101441, loss_mps: 0.052786, loss_cps: 0.107592
[12:50:41.458] iteration 7332: total_loss: 0.112012, loss_sup: 0.026866, loss_mps: 0.032089, loss_cps: 0.053057
[12:50:41.604] iteration 7333: total_loss: 0.411323, loss_sup: 0.222029, loss_mps: 0.060037, loss_cps: 0.129256
[12:50:41.752] iteration 7334: total_loss: 0.205791, loss_sup: 0.077617, loss_mps: 0.044347, loss_cps: 0.083827
[12:50:41.898] iteration 7335: total_loss: 0.127708, loss_sup: 0.046721, loss_mps: 0.029567, loss_cps: 0.051420
[12:50:42.044] iteration 7336: total_loss: 0.266141, loss_sup: 0.122720, loss_mps: 0.049040, loss_cps: 0.094382
[12:50:42.190] iteration 7337: total_loss: 0.229760, loss_sup: 0.081093, loss_mps: 0.050867, loss_cps: 0.097799
[12:50:42.336] iteration 7338: total_loss: 0.217639, loss_sup: 0.107700, loss_mps: 0.036972, loss_cps: 0.072966
[12:50:42.483] iteration 7339: total_loss: 0.272287, loss_sup: 0.146832, loss_mps: 0.042553, loss_cps: 0.082903
[12:50:42.629] iteration 7340: total_loss: 0.263589, loss_sup: 0.164809, loss_mps: 0.035505, loss_cps: 0.063275
[12:50:42.774] iteration 7341: total_loss: 0.128512, loss_sup: 0.032655, loss_mps: 0.032895, loss_cps: 0.062963
[12:50:42.920] iteration 7342: total_loss: 0.096712, loss_sup: 0.011088, loss_mps: 0.031473, loss_cps: 0.054150
[12:50:43.066] iteration 7343: total_loss: 0.321796, loss_sup: 0.213441, loss_mps: 0.037297, loss_cps: 0.071057
[12:50:43.211] iteration 7344: total_loss: 0.257854, loss_sup: 0.088517, loss_mps: 0.056752, loss_cps: 0.112585
[12:50:43.357] iteration 7345: total_loss: 0.213716, loss_sup: 0.111650, loss_mps: 0.035446, loss_cps: 0.066620
[12:50:43.502] iteration 7346: total_loss: 0.266076, loss_sup: 0.125628, loss_mps: 0.045132, loss_cps: 0.095317
[12:50:43.648] iteration 7347: total_loss: 0.208768, loss_sup: 0.115385, loss_mps: 0.033759, loss_cps: 0.059624
[12:50:43.793] iteration 7348: total_loss: 0.145806, loss_sup: 0.036617, loss_mps: 0.037950, loss_cps: 0.071239
[12:50:43.939] iteration 7349: total_loss: 0.146528, loss_sup: 0.054737, loss_mps: 0.033711, loss_cps: 0.058080
[12:50:44.084] iteration 7350: total_loss: 0.155968, loss_sup: 0.060088, loss_mps: 0.033079, loss_cps: 0.062801
[12:50:44.231] iteration 7351: total_loss: 0.189870, loss_sup: 0.068268, loss_mps: 0.041837, loss_cps: 0.079764
[12:50:44.376] iteration 7352: total_loss: 0.255792, loss_sup: 0.129945, loss_mps: 0.043708, loss_cps: 0.082139
[12:50:44.522] iteration 7353: total_loss: 0.211937, loss_sup: 0.052697, loss_mps: 0.052420, loss_cps: 0.106820
[12:50:44.669] iteration 7354: total_loss: 0.330697, loss_sup: 0.209870, loss_mps: 0.042725, loss_cps: 0.078102
[12:50:44.814] iteration 7355: total_loss: 0.329871, loss_sup: 0.176848, loss_mps: 0.052004, loss_cps: 0.101020
[12:50:44.960] iteration 7356: total_loss: 0.158523, loss_sup: 0.033985, loss_mps: 0.041834, loss_cps: 0.082705
[12:50:45.105] iteration 7357: total_loss: 0.245314, loss_sup: 0.122090, loss_mps: 0.041175, loss_cps: 0.082049
[12:50:45.251] iteration 7358: total_loss: 0.114239, loss_sup: 0.042852, loss_mps: 0.026693, loss_cps: 0.044694
[12:50:45.396] iteration 7359: total_loss: 0.137482, loss_sup: 0.030337, loss_mps: 0.037702, loss_cps: 0.069443
[12:50:45.542] iteration 7360: total_loss: 0.222987, loss_sup: 0.090854, loss_mps: 0.045782, loss_cps: 0.086351
[12:50:45.687] iteration 7361: total_loss: 0.368005, loss_sup: 0.231797, loss_mps: 0.045848, loss_cps: 0.090360
[12:50:45.833] iteration 7362: total_loss: 0.502385, loss_sup: 0.273671, loss_mps: 0.071830, loss_cps: 0.156885
[12:50:45.978] iteration 7363: total_loss: 0.379420, loss_sup: 0.226596, loss_mps: 0.050189, loss_cps: 0.102636
[12:50:46.124] iteration 7364: total_loss: 0.221165, loss_sup: 0.096405, loss_mps: 0.042487, loss_cps: 0.082273
[12:50:46.271] iteration 7365: total_loss: 0.231910, loss_sup: 0.061526, loss_mps: 0.056605, loss_cps: 0.113778
[12:50:46.417] iteration 7366: total_loss: 0.404760, loss_sup: 0.294288, loss_mps: 0.039774, loss_cps: 0.070698
[12:50:46.564] iteration 7367: total_loss: 0.190994, loss_sup: 0.068014, loss_mps: 0.043424, loss_cps: 0.079556
[12:50:46.709] iteration 7368: total_loss: 0.217605, loss_sup: 0.096095, loss_mps: 0.041810, loss_cps: 0.079700
[12:50:46.855] iteration 7369: total_loss: 0.351311, loss_sup: 0.148038, loss_mps: 0.066635, loss_cps: 0.136638
[12:50:47.001] iteration 7370: total_loss: 0.124596, loss_sup: 0.045119, loss_mps: 0.031062, loss_cps: 0.048414
[12:50:47.147] iteration 7371: total_loss: 0.106338, loss_sup: 0.019821, loss_mps: 0.031380, loss_cps: 0.055137
[12:50:47.292] iteration 7372: total_loss: 0.105983, loss_sup: 0.034466, loss_mps: 0.028035, loss_cps: 0.043482
[12:50:47.437] iteration 7373: total_loss: 0.229635, loss_sup: 0.072407, loss_mps: 0.055237, loss_cps: 0.101991
[12:50:47.583] iteration 7374: total_loss: 0.369956, loss_sup: 0.161692, loss_mps: 0.070754, loss_cps: 0.137510
[12:50:47.728] iteration 7375: total_loss: 0.213627, loss_sup: 0.129387, loss_mps: 0.031671, loss_cps: 0.052568
[12:50:47.874] iteration 7376: total_loss: 0.232297, loss_sup: 0.100389, loss_mps: 0.047157, loss_cps: 0.084751
[12:50:48.019] iteration 7377: total_loss: 0.200311, loss_sup: 0.094587, loss_mps: 0.040178, loss_cps: 0.065546
[12:50:48.165] iteration 7378: total_loss: 0.204819, loss_sup: 0.083775, loss_mps: 0.042314, loss_cps: 0.078730
[12:50:48.314] iteration 7379: total_loss: 0.122889, loss_sup: 0.030565, loss_mps: 0.035198, loss_cps: 0.057126
[12:50:48.460] iteration 7380: total_loss: 0.299710, loss_sup: 0.153607, loss_mps: 0.052274, loss_cps: 0.093829
[12:50:48.606] iteration 7381: total_loss: 0.303221, loss_sup: 0.150155, loss_mps: 0.052984, loss_cps: 0.100082
[12:50:48.755] iteration 7382: total_loss: 0.198497, loss_sup: 0.118144, loss_mps: 0.032347, loss_cps: 0.048006
[12:50:48.900] iteration 7383: total_loss: 0.136846, loss_sup: 0.054693, loss_mps: 0.031712, loss_cps: 0.050441
[12:50:49.046] iteration 7384: total_loss: 0.289275, loss_sup: 0.159879, loss_mps: 0.046605, loss_cps: 0.082791
[12:50:49.192] iteration 7385: total_loss: 0.216017, loss_sup: 0.015410, loss_mps: 0.062973, loss_cps: 0.137633
[12:50:49.338] iteration 7386: total_loss: 0.337577, loss_sup: 0.216586, loss_mps: 0.040902, loss_cps: 0.080089
[12:50:49.484] iteration 7387: total_loss: 0.164253, loss_sup: 0.067372, loss_mps: 0.034705, loss_cps: 0.062176
[12:50:49.629] iteration 7388: total_loss: 0.575031, loss_sup: 0.377829, loss_mps: 0.061597, loss_cps: 0.135605
[12:50:49.776] iteration 7389: total_loss: 0.105079, loss_sup: 0.024844, loss_mps: 0.029954, loss_cps: 0.050281
[12:50:49.921] iteration 7390: total_loss: 0.210225, loss_sup: 0.069741, loss_mps: 0.049288, loss_cps: 0.091196
[12:50:50.067] iteration 7391: total_loss: 0.194716, loss_sup: 0.105229, loss_mps: 0.034182, loss_cps: 0.055306
[12:50:50.214] iteration 7392: total_loss: 0.157651, loss_sup: 0.072488, loss_mps: 0.031867, loss_cps: 0.053296
[12:50:50.359] iteration 7393: total_loss: 0.447446, loss_sup: 0.255067, loss_mps: 0.062042, loss_cps: 0.130337
[12:50:50.505] iteration 7394: total_loss: 0.218044, loss_sup: 0.090311, loss_mps: 0.042860, loss_cps: 0.084873
[12:50:50.651] iteration 7395: total_loss: 0.276027, loss_sup: 0.075068, loss_mps: 0.065901, loss_cps: 0.135058
[12:50:50.796] iteration 7396: total_loss: 0.281598, loss_sup: 0.129801, loss_mps: 0.051936, loss_cps: 0.099861
[12:50:50.942] iteration 7397: total_loss: 0.207598, loss_sup: 0.072064, loss_mps: 0.045477, loss_cps: 0.090057
[12:50:51.087] iteration 7398: total_loss: 0.185485, loss_sup: 0.054320, loss_mps: 0.045767, loss_cps: 0.085398
[12:50:51.234] iteration 7399: total_loss: 0.205369, loss_sup: 0.109701, loss_mps: 0.035206, loss_cps: 0.060462
[12:50:51.380] iteration 7400: total_loss: 0.221337, loss_sup: 0.120877, loss_mps: 0.035152, loss_cps: 0.065308
[12:50:51.380] Evaluation Started ==>
[12:51:02.751] ==> valid iteration 7400: unet metrics: {'dc': 0.6413655367541061, 'jc': 0.5133898645787951, 'pre': 0.712926818245824, 'hd': 6.018109882986065}, ynet metrics: {'dc': 0.5769528252697506, 'jc': 0.4516509859048992, 'pre': 0.7195933890055061, 'hd': 6.00510503599713}.
[12:51:02.753] Evaluation Finished!⏹️
[12:51:02.908] iteration 7401: total_loss: 0.092779, loss_sup: 0.026136, loss_mps: 0.025423, loss_cps: 0.041220
[12:51:03.055] iteration 7402: total_loss: 0.144494, loss_sup: 0.029039, loss_mps: 0.041489, loss_cps: 0.073965
[12:51:03.199] iteration 7403: total_loss: 0.271311, loss_sup: 0.074070, loss_mps: 0.065808, loss_cps: 0.131433
[12:51:03.347] iteration 7404: total_loss: 0.339495, loss_sup: 0.186563, loss_mps: 0.052189, loss_cps: 0.100743
[12:51:03.494] iteration 7405: total_loss: 0.217740, loss_sup: 0.064548, loss_mps: 0.052435, loss_cps: 0.100757
[12:51:03.639] iteration 7406: total_loss: 0.228542, loss_sup: 0.111350, loss_mps: 0.043106, loss_cps: 0.074087
[12:51:03.784] iteration 7407: total_loss: 0.220696, loss_sup: 0.122598, loss_mps: 0.035727, loss_cps: 0.062370
[12:51:03.930] iteration 7408: total_loss: 0.266077, loss_sup: 0.070783, loss_mps: 0.064493, loss_cps: 0.130802
[12:51:04.075] iteration 7409: total_loss: 0.221911, loss_sup: 0.071243, loss_mps: 0.052654, loss_cps: 0.098014
[12:51:04.220] iteration 7410: total_loss: 0.178271, loss_sup: 0.081276, loss_mps: 0.035483, loss_cps: 0.061513
[12:51:04.365] iteration 7411: total_loss: 0.246373, loss_sup: 0.089595, loss_mps: 0.052002, loss_cps: 0.104776
[12:51:04.510] iteration 7412: total_loss: 0.152621, loss_sup: 0.048253, loss_mps: 0.037821, loss_cps: 0.066546
[12:51:04.656] iteration 7413: total_loss: 0.221000, loss_sup: 0.073733, loss_mps: 0.049607, loss_cps: 0.097660
[12:51:04.801] iteration 7414: total_loss: 0.244323, loss_sup: 0.101861, loss_mps: 0.048534, loss_cps: 0.093929
[12:51:04.946] iteration 7415: total_loss: 0.149300, loss_sup: 0.070054, loss_mps: 0.030958, loss_cps: 0.048289
[12:51:05.092] iteration 7416: total_loss: 0.208311, loss_sup: 0.059885, loss_mps: 0.049769, loss_cps: 0.098657
[12:51:05.237] iteration 7417: total_loss: 0.126163, loss_sup: 0.031740, loss_mps: 0.034576, loss_cps: 0.059847
[12:51:05.382] iteration 7418: total_loss: 0.308886, loss_sup: 0.155227, loss_mps: 0.052353, loss_cps: 0.101306
[12:51:05.529] iteration 7419: total_loss: 0.254443, loss_sup: 0.162197, loss_mps: 0.034414, loss_cps: 0.057832
[12:51:05.674] iteration 7420: total_loss: 0.130483, loss_sup: 0.060272, loss_mps: 0.026278, loss_cps: 0.043932
[12:51:05.819] iteration 7421: total_loss: 0.229193, loss_sup: 0.137712, loss_mps: 0.033601, loss_cps: 0.057879
[12:51:05.969] iteration 7422: total_loss: 0.127239, loss_sup: 0.065339, loss_mps: 0.023304, loss_cps: 0.038596
[12:51:06.119] iteration 7423: total_loss: 0.114467, loss_sup: 0.023188, loss_mps: 0.032608, loss_cps: 0.058671
[12:51:06.265] iteration 7424: total_loss: 0.149105, loss_sup: 0.086657, loss_mps: 0.023464, loss_cps: 0.038984
[12:51:06.411] iteration 7425: total_loss: 0.449792, loss_sup: 0.246733, loss_mps: 0.066794, loss_cps: 0.136265
[12:51:06.557] iteration 7426: total_loss: 0.169123, loss_sup: 0.059246, loss_mps: 0.038339, loss_cps: 0.071539
[12:51:06.703] iteration 7427: total_loss: 0.171052, loss_sup: 0.075178, loss_mps: 0.033317, loss_cps: 0.062557
[12:51:06.849] iteration 7428: total_loss: 0.205087, loss_sup: 0.055712, loss_mps: 0.051384, loss_cps: 0.097992
[12:51:06.996] iteration 7429: total_loss: 0.201919, loss_sup: 0.066067, loss_mps: 0.047497, loss_cps: 0.088355
[12:51:07.142] iteration 7430: total_loss: 0.277599, loss_sup: 0.081605, loss_mps: 0.066359, loss_cps: 0.129635
[12:51:07.287] iteration 7431: total_loss: 0.221471, loss_sup: 0.088139, loss_mps: 0.043945, loss_cps: 0.089387
[12:51:07.433] iteration 7432: total_loss: 0.333174, loss_sup: 0.225219, loss_mps: 0.038227, loss_cps: 0.069728
[12:51:07.578] iteration 7433: total_loss: 0.169408, loss_sup: 0.069378, loss_mps: 0.035971, loss_cps: 0.064059
[12:51:07.723] iteration 7434: total_loss: 0.233874, loss_sup: 0.093137, loss_mps: 0.048347, loss_cps: 0.092390
[12:51:07.869] iteration 7435: total_loss: 0.204100, loss_sup: 0.095977, loss_mps: 0.039493, loss_cps: 0.068629
[12:51:08.014] iteration 7436: total_loss: 0.290431, loss_sup: 0.108964, loss_mps: 0.059317, loss_cps: 0.122149
[12:51:08.159] iteration 7437: total_loss: 0.230392, loss_sup: 0.096766, loss_mps: 0.045993, loss_cps: 0.087634
[12:51:08.305] iteration 7438: total_loss: 0.239940, loss_sup: 0.073030, loss_mps: 0.056451, loss_cps: 0.110459
[12:51:08.450] iteration 7439: total_loss: 0.452637, loss_sup: 0.334588, loss_mps: 0.043254, loss_cps: 0.074795
[12:51:08.596] iteration 7440: total_loss: 0.152480, loss_sup: 0.074027, loss_mps: 0.029356, loss_cps: 0.049096
[12:51:08.741] iteration 7441: total_loss: 0.600996, loss_sup: 0.425899, loss_mps: 0.057207, loss_cps: 0.117890
[12:51:08.887] iteration 7442: total_loss: 0.155033, loss_sup: 0.049115, loss_mps: 0.039234, loss_cps: 0.066685
[12:51:09.032] iteration 7443: total_loss: 0.263214, loss_sup: 0.101049, loss_mps: 0.056231, loss_cps: 0.105934
[12:51:09.177] iteration 7444: total_loss: 0.187423, loss_sup: 0.064837, loss_mps: 0.042566, loss_cps: 0.080021
[12:51:09.322] iteration 7445: total_loss: 0.311103, loss_sup: 0.135785, loss_mps: 0.058755, loss_cps: 0.116563
[12:51:09.468] iteration 7446: total_loss: 0.137268, loss_sup: 0.046836, loss_mps: 0.034477, loss_cps: 0.055955
[12:51:09.613] iteration 7447: total_loss: 0.239975, loss_sup: 0.131557, loss_mps: 0.038885, loss_cps: 0.069534
[12:51:09.759] iteration 7448: total_loss: 0.247871, loss_sup: 0.077658, loss_mps: 0.057033, loss_cps: 0.113181
[12:51:09.904] iteration 7449: total_loss: 0.238488, loss_sup: 0.134551, loss_mps: 0.036881, loss_cps: 0.067056
[12:51:10.050] iteration 7450: total_loss: 0.223487, loss_sup: 0.099753, loss_mps: 0.042285, loss_cps: 0.081448
[12:51:10.195] iteration 7451: total_loss: 0.297937, loss_sup: 0.109299, loss_mps: 0.059365, loss_cps: 0.129273
[12:51:10.341] iteration 7452: total_loss: 0.190365, loss_sup: 0.063077, loss_mps: 0.045658, loss_cps: 0.081630
[12:51:10.487] iteration 7453: total_loss: 0.152980, loss_sup: 0.026989, loss_mps: 0.042691, loss_cps: 0.083301
[12:51:10.632] iteration 7454: total_loss: 0.163309, loss_sup: 0.028194, loss_mps: 0.046031, loss_cps: 0.089083
[12:51:10.781] iteration 7455: total_loss: 0.237949, loss_sup: 0.058599, loss_mps: 0.056366, loss_cps: 0.122984
[12:51:10.926] iteration 7456: total_loss: 0.205753, loss_sup: 0.108550, loss_mps: 0.035229, loss_cps: 0.061974
[12:51:11.073] iteration 7457: total_loss: 0.362419, loss_sup: 0.194437, loss_mps: 0.057386, loss_cps: 0.110596
[12:51:11.218] iteration 7458: total_loss: 0.297263, loss_sup: 0.195020, loss_mps: 0.037888, loss_cps: 0.064355
[12:51:11.363] iteration 7459: total_loss: 0.199437, loss_sup: 0.111124, loss_mps: 0.032426, loss_cps: 0.055886
[12:51:11.508] iteration 7460: total_loss: 0.124984, loss_sup: 0.022805, loss_mps: 0.035827, loss_cps: 0.066351
[12:51:11.653] iteration 7461: total_loss: 0.286784, loss_sup: 0.084609, loss_mps: 0.065652, loss_cps: 0.136522
[12:51:11.799] iteration 7462: total_loss: 0.214047, loss_sup: 0.118428, loss_mps: 0.032887, loss_cps: 0.062733
[12:51:11.944] iteration 7463: total_loss: 0.133659, loss_sup: 0.037156, loss_mps: 0.035454, loss_cps: 0.061049
[12:51:12.089] iteration 7464: total_loss: 0.238638, loss_sup: 0.135144, loss_mps: 0.035803, loss_cps: 0.067692
[12:51:12.235] iteration 7465: total_loss: 0.216335, loss_sup: 0.119431, loss_mps: 0.034349, loss_cps: 0.062555
[12:51:12.380] iteration 7466: total_loss: 0.205791, loss_sup: 0.103317, loss_mps: 0.036826, loss_cps: 0.065648
[12:51:12.526] iteration 7467: total_loss: 0.149186, loss_sup: 0.041389, loss_mps: 0.039718, loss_cps: 0.068078
[12:51:12.671] iteration 7468: total_loss: 0.231460, loss_sup: 0.052413, loss_mps: 0.059232, loss_cps: 0.119815
[12:51:12.817] iteration 7469: total_loss: 0.354255, loss_sup: 0.137781, loss_mps: 0.068610, loss_cps: 0.147863
[12:51:12.962] iteration 7470: total_loss: 0.575663, loss_sup: 0.405213, loss_mps: 0.054736, loss_cps: 0.115714
[12:51:13.109] iteration 7471: total_loss: 0.266609, loss_sup: 0.095206, loss_mps: 0.059561, loss_cps: 0.111842
[12:51:13.255] iteration 7472: total_loss: 0.176743, loss_sup: 0.064846, loss_mps: 0.040293, loss_cps: 0.071604
[12:51:13.400] iteration 7473: total_loss: 0.182854, loss_sup: 0.098459, loss_mps: 0.032248, loss_cps: 0.052148
[12:51:13.548] iteration 7474: total_loss: 0.144876, loss_sup: 0.030657, loss_mps: 0.040667, loss_cps: 0.073552
[12:51:13.694] iteration 7475: total_loss: 0.249335, loss_sup: 0.116117, loss_mps: 0.045615, loss_cps: 0.087603
[12:51:13.840] iteration 7476: total_loss: 0.221277, loss_sup: 0.101723, loss_mps: 0.041966, loss_cps: 0.077588
[12:51:13.986] iteration 7477: total_loss: 0.176731, loss_sup: 0.038606, loss_mps: 0.047624, loss_cps: 0.090501
[12:51:14.133] iteration 7478: total_loss: 0.250671, loss_sup: 0.070473, loss_mps: 0.059039, loss_cps: 0.121159
[12:51:14.279] iteration 7479: total_loss: 0.167911, loss_sup: 0.036785, loss_mps: 0.045410, loss_cps: 0.085716
[12:51:14.426] iteration 7480: total_loss: 0.243545, loss_sup: 0.105701, loss_mps: 0.047731, loss_cps: 0.090113
[12:51:14.571] iteration 7481: total_loss: 0.208777, loss_sup: 0.064427, loss_mps: 0.050721, loss_cps: 0.093628
[12:51:14.717] iteration 7482: total_loss: 0.239165, loss_sup: 0.120705, loss_mps: 0.043483, loss_cps: 0.074977
[12:51:14.863] iteration 7483: total_loss: 0.119644, loss_sup: 0.024759, loss_mps: 0.034236, loss_cps: 0.060649
[12:51:15.008] iteration 7484: total_loss: 0.296971, loss_sup: 0.177142, loss_mps: 0.041871, loss_cps: 0.077959
[12:51:15.154] iteration 7485: total_loss: 0.163374, loss_sup: 0.050017, loss_mps: 0.040308, loss_cps: 0.073049
[12:51:15.300] iteration 7486: total_loss: 0.268046, loss_sup: 0.104064, loss_mps: 0.053726, loss_cps: 0.110257
[12:51:15.445] iteration 7487: total_loss: 0.202843, loss_sup: 0.081915, loss_mps: 0.042213, loss_cps: 0.078715
[12:51:15.591] iteration 7488: total_loss: 0.339164, loss_sup: 0.204621, loss_mps: 0.047189, loss_cps: 0.087354
[12:51:15.736] iteration 7489: total_loss: 0.197437, loss_sup: 0.046888, loss_mps: 0.050330, loss_cps: 0.100218
[12:51:15.883] iteration 7490: total_loss: 0.155467, loss_sup: 0.049430, loss_mps: 0.037725, loss_cps: 0.068312
[12:51:16.029] iteration 7491: total_loss: 0.173517, loss_sup: 0.067137, loss_mps: 0.038825, loss_cps: 0.067554
[12:51:16.174] iteration 7492: total_loss: 0.248118, loss_sup: 0.115228, loss_mps: 0.045346, loss_cps: 0.087544
[12:51:16.320] iteration 7493: total_loss: 0.390329, loss_sup: 0.183330, loss_mps: 0.067509, loss_cps: 0.139491
[12:51:16.466] iteration 7494: total_loss: 0.206920, loss_sup: 0.073254, loss_mps: 0.046243, loss_cps: 0.087423
[12:51:16.612] iteration 7495: total_loss: 0.278105, loss_sup: 0.153108, loss_mps: 0.044161, loss_cps: 0.080836
[12:51:16.758] iteration 7496: total_loss: 0.243800, loss_sup: 0.107355, loss_mps: 0.046081, loss_cps: 0.090364
[12:51:16.904] iteration 7497: total_loss: 0.207638, loss_sup: 0.087020, loss_mps: 0.041844, loss_cps: 0.078774
[12:51:17.050] iteration 7498: total_loss: 0.181252, loss_sup: 0.077156, loss_mps: 0.038159, loss_cps: 0.065937
[12:51:17.196] iteration 7499: total_loss: 0.151051, loss_sup: 0.044753, loss_mps: 0.036693, loss_cps: 0.069605
[12:51:17.341] iteration 7500: total_loss: 0.176052, loss_sup: 0.069042, loss_mps: 0.038327, loss_cps: 0.068683
[12:51:17.342] Evaluation Started ==>
[12:51:28.719] ==> valid iteration 7500: unet metrics: {'dc': 0.6219962993966665, 'jc': 0.4903829177507328, 'pre': 0.6883793269649185, 'hd': 6.176908457722242}, ynet metrics: {'dc': 0.5682693310722519, 'jc': 0.4472663328282333, 'pre': 0.7152340361796673, 'hd': 6.026976883238712}.
[12:51:28.721] Evaluation Finished!⏹️
[12:51:28.871] iteration 7501: total_loss: 0.250968, loss_sup: 0.112436, loss_mps: 0.046326, loss_cps: 0.092206
[12:51:29.018] iteration 7502: total_loss: 0.184187, loss_sup: 0.074047, loss_mps: 0.038294, loss_cps: 0.071846
[12:51:29.166] iteration 7503: total_loss: 0.132811, loss_sup: 0.057179, loss_mps: 0.028123, loss_cps: 0.047509
[12:51:29.311] iteration 7504: total_loss: 0.149482, loss_sup: 0.069313, loss_mps: 0.030192, loss_cps: 0.049977
[12:51:29.456] iteration 7505: total_loss: 0.222219, loss_sup: 0.124160, loss_mps: 0.035036, loss_cps: 0.063024
[12:51:29.605] iteration 7506: total_loss: 0.296891, loss_sup: 0.152043, loss_mps: 0.047401, loss_cps: 0.097447
[12:51:29.750] iteration 7507: total_loss: 0.224230, loss_sup: 0.098192, loss_mps: 0.043897, loss_cps: 0.082142
[12:51:29.895] iteration 7508: total_loss: 0.281157, loss_sup: 0.121201, loss_mps: 0.052545, loss_cps: 0.107411
[12:51:30.041] iteration 7509: total_loss: 0.140193, loss_sup: 0.029962, loss_mps: 0.038779, loss_cps: 0.071451
[12:51:30.187] iteration 7510: total_loss: 0.223083, loss_sup: 0.071717, loss_mps: 0.049093, loss_cps: 0.102274
[12:51:30.332] iteration 7511: total_loss: 0.207967, loss_sup: 0.057910, loss_mps: 0.050150, loss_cps: 0.099908
[12:51:30.478] iteration 7512: total_loss: 0.180336, loss_sup: 0.073061, loss_mps: 0.035461, loss_cps: 0.071813
[12:51:30.623] iteration 7513: total_loss: 0.168487, loss_sup: 0.045893, loss_mps: 0.042851, loss_cps: 0.079744
[12:51:30.769] iteration 7514: total_loss: 0.227375, loss_sup: 0.099586, loss_mps: 0.042988, loss_cps: 0.084801
[12:51:30.914] iteration 7515: total_loss: 0.307282, loss_sup: 0.165488, loss_mps: 0.044812, loss_cps: 0.096983
[12:51:31.059] iteration 7516: total_loss: 0.320457, loss_sup: 0.199867, loss_mps: 0.041086, loss_cps: 0.079504
[12:51:31.204] iteration 7517: total_loss: 0.237049, loss_sup: 0.093841, loss_mps: 0.047739, loss_cps: 0.095470
[12:51:31.349] iteration 7518: total_loss: 0.183570, loss_sup: 0.061182, loss_mps: 0.042225, loss_cps: 0.080162
[12:51:31.494] iteration 7519: total_loss: 0.221149, loss_sup: 0.058331, loss_mps: 0.051533, loss_cps: 0.111286
[12:51:31.639] iteration 7520: total_loss: 0.260494, loss_sup: 0.140671, loss_mps: 0.041629, loss_cps: 0.078193
[12:51:31.784] iteration 7521: total_loss: 0.158982, loss_sup: 0.107261, loss_mps: 0.021543, loss_cps: 0.030178
[12:51:31.929] iteration 7522: total_loss: 0.122857, loss_sup: 0.026233, loss_mps: 0.034368, loss_cps: 0.062255
[12:51:32.074] iteration 7523: total_loss: 0.341088, loss_sup: 0.232775, loss_mps: 0.040126, loss_cps: 0.068188
[12:51:32.136] iteration 7524: total_loss: 0.193044, loss_sup: 0.042795, loss_mps: 0.049304, loss_cps: 0.100945
[12:51:33.340] iteration 7525: total_loss: 0.142516, loss_sup: 0.028951, loss_mps: 0.039625, loss_cps: 0.073940
[12:51:33.489] iteration 7526: total_loss: 0.192391, loss_sup: 0.039873, loss_mps: 0.051804, loss_cps: 0.100714
[12:51:33.636] iteration 7527: total_loss: 0.243810, loss_sup: 0.131406, loss_mps: 0.040085, loss_cps: 0.072319
[12:51:33.783] iteration 7528: total_loss: 0.185922, loss_sup: 0.104787, loss_mps: 0.030333, loss_cps: 0.050802
[12:51:33.930] iteration 7529: total_loss: 0.238373, loss_sup: 0.093137, loss_mps: 0.049621, loss_cps: 0.095615
[12:51:34.075] iteration 7530: total_loss: 0.129927, loss_sup: 0.047903, loss_mps: 0.030511, loss_cps: 0.051514
[12:51:34.221] iteration 7531: total_loss: 0.288224, loss_sup: 0.132785, loss_mps: 0.052650, loss_cps: 0.102789
[12:51:34.372] iteration 7532: total_loss: 0.429623, loss_sup: 0.252658, loss_mps: 0.057806, loss_cps: 0.119160
[12:51:34.519] iteration 7533: total_loss: 0.221037, loss_sup: 0.078031, loss_mps: 0.047596, loss_cps: 0.095411
[12:51:34.666] iteration 7534: total_loss: 0.206300, loss_sup: 0.056618, loss_mps: 0.049626, loss_cps: 0.100055
[12:51:34.812] iteration 7535: total_loss: 0.169957, loss_sup: 0.034457, loss_mps: 0.046584, loss_cps: 0.088917
[12:51:34.957] iteration 7536: total_loss: 0.307801, loss_sup: 0.177089, loss_mps: 0.045172, loss_cps: 0.085541
[12:51:35.104] iteration 7537: total_loss: 0.131820, loss_sup: 0.051562, loss_mps: 0.029020, loss_cps: 0.051239
[12:51:35.250] iteration 7538: total_loss: 0.284644, loss_sup: 0.089124, loss_mps: 0.062780, loss_cps: 0.132741
[12:51:35.396] iteration 7539: total_loss: 0.271827, loss_sup: 0.117186, loss_mps: 0.051033, loss_cps: 0.103608
[12:51:35.541] iteration 7540: total_loss: 0.184874, loss_sup: 0.101527, loss_mps: 0.029750, loss_cps: 0.053597
[12:51:35.688] iteration 7541: total_loss: 0.173773, loss_sup: 0.087304, loss_mps: 0.031987, loss_cps: 0.054481
[12:51:35.833] iteration 7542: total_loss: 0.147535, loss_sup: 0.049077, loss_mps: 0.035276, loss_cps: 0.063182
[12:51:35.981] iteration 7543: total_loss: 0.127629, loss_sup: 0.027185, loss_mps: 0.034656, loss_cps: 0.065787
[12:51:36.127] iteration 7544: total_loss: 0.262734, loss_sup: 0.110109, loss_mps: 0.050434, loss_cps: 0.102191
[12:51:36.273] iteration 7545: total_loss: 0.202615, loss_sup: 0.085366, loss_mps: 0.040829, loss_cps: 0.076420
[12:51:36.420] iteration 7546: total_loss: 0.351274, loss_sup: 0.162922, loss_mps: 0.062382, loss_cps: 0.125970
[12:51:36.566] iteration 7547: total_loss: 0.294809, loss_sup: 0.125266, loss_mps: 0.054889, loss_cps: 0.114653
[12:51:36.713] iteration 7548: total_loss: 0.300015, loss_sup: 0.159210, loss_mps: 0.045459, loss_cps: 0.095345
[12:51:36.859] iteration 7549: total_loss: 0.268007, loss_sup: 0.128306, loss_mps: 0.047291, loss_cps: 0.092409
[12:51:37.005] iteration 7550: total_loss: 0.190286, loss_sup: 0.072663, loss_mps: 0.043864, loss_cps: 0.073758
[12:51:37.150] iteration 7551: total_loss: 0.291727, loss_sup: 0.195485, loss_mps: 0.035521, loss_cps: 0.060721
[12:51:37.296] iteration 7552: total_loss: 0.209053, loss_sup: 0.087346, loss_mps: 0.043780, loss_cps: 0.077927
[12:51:37.442] iteration 7553: total_loss: 0.084090, loss_sup: 0.022645, loss_mps: 0.023813, loss_cps: 0.037631
[12:51:37.588] iteration 7554: total_loss: 0.281289, loss_sup: 0.107971, loss_mps: 0.059018, loss_cps: 0.114300
[12:51:37.735] iteration 7555: total_loss: 0.223613, loss_sup: 0.078984, loss_mps: 0.049747, loss_cps: 0.094881
[12:51:37.881] iteration 7556: total_loss: 0.154744, loss_sup: 0.040639, loss_mps: 0.039980, loss_cps: 0.074125
[12:51:38.028] iteration 7557: total_loss: 0.149584, loss_sup: 0.016366, loss_mps: 0.045453, loss_cps: 0.087764
[12:51:38.174] iteration 7558: total_loss: 0.230622, loss_sup: 0.099578, loss_mps: 0.046677, loss_cps: 0.084367
[12:51:38.320] iteration 7559: total_loss: 0.278978, loss_sup: 0.111022, loss_mps: 0.056441, loss_cps: 0.111516
[12:51:38.467] iteration 7560: total_loss: 0.271540, loss_sup: 0.175851, loss_mps: 0.036901, loss_cps: 0.058788
[12:51:38.614] iteration 7561: total_loss: 0.239602, loss_sup: 0.101118, loss_mps: 0.048746, loss_cps: 0.089738
[12:51:38.760] iteration 7562: total_loss: 0.256808, loss_sup: 0.063924, loss_mps: 0.065178, loss_cps: 0.127707
[12:51:38.908] iteration 7563: total_loss: 0.216543, loss_sup: 0.085203, loss_mps: 0.043830, loss_cps: 0.087510
[12:51:39.054] iteration 7564: total_loss: 0.296281, loss_sup: 0.113445, loss_mps: 0.062013, loss_cps: 0.120824
[12:51:39.200] iteration 7565: total_loss: 0.212255, loss_sup: 0.110921, loss_mps: 0.034091, loss_cps: 0.067242
[12:51:39.345] iteration 7566: total_loss: 0.197778, loss_sup: 0.084253, loss_mps: 0.039245, loss_cps: 0.074280
[12:51:39.491] iteration 7567: total_loss: 0.309766, loss_sup: 0.159747, loss_mps: 0.051930, loss_cps: 0.098089
[12:51:39.637] iteration 7568: total_loss: 0.250239, loss_sup: 0.157866, loss_mps: 0.034927, loss_cps: 0.057446
[12:51:39.783] iteration 7569: total_loss: 0.214281, loss_sup: 0.099162, loss_mps: 0.040991, loss_cps: 0.074127
[12:51:39.929] iteration 7570: total_loss: 0.197862, loss_sup: 0.059957, loss_mps: 0.047243, loss_cps: 0.090662
[12:51:40.075] iteration 7571: total_loss: 0.175035, loss_sup: 0.046494, loss_mps: 0.046975, loss_cps: 0.081565
[12:51:40.220] iteration 7572: total_loss: 0.194991, loss_sup: 0.079444, loss_mps: 0.040986, loss_cps: 0.074561
[12:51:40.366] iteration 7573: total_loss: 0.240791, loss_sup: 0.137193, loss_mps: 0.036732, loss_cps: 0.066866
[12:51:40.513] iteration 7574: total_loss: 0.209567, loss_sup: 0.092808, loss_mps: 0.040323, loss_cps: 0.076436
[12:51:40.659] iteration 7575: total_loss: 0.223614, loss_sup: 0.106721, loss_mps: 0.040523, loss_cps: 0.076370
[12:51:40.807] iteration 7576: total_loss: 0.190421, loss_sup: 0.047166, loss_mps: 0.048616, loss_cps: 0.094638
[12:51:40.953] iteration 7577: total_loss: 0.131002, loss_sup: 0.038038, loss_mps: 0.036456, loss_cps: 0.056508
[12:51:41.099] iteration 7578: total_loss: 0.334856, loss_sup: 0.236902, loss_mps: 0.035580, loss_cps: 0.062375
[12:51:41.245] iteration 7579: total_loss: 0.371338, loss_sup: 0.192086, loss_mps: 0.058929, loss_cps: 0.120323
[12:51:41.391] iteration 7580: total_loss: 0.423415, loss_sup: 0.200453, loss_mps: 0.071384, loss_cps: 0.151577
[12:51:41.541] iteration 7581: total_loss: 0.309920, loss_sup: 0.134040, loss_mps: 0.058306, loss_cps: 0.117574
[12:51:41.686] iteration 7582: total_loss: 0.255310, loss_sup: 0.125154, loss_mps: 0.045935, loss_cps: 0.084221
[12:51:41.833] iteration 7583: total_loss: 0.242421, loss_sup: 0.143572, loss_mps: 0.037603, loss_cps: 0.061246
[12:51:41.980] iteration 7584: total_loss: 0.210557, loss_sup: 0.023679, loss_mps: 0.061079, loss_cps: 0.125799
[12:51:42.126] iteration 7585: total_loss: 0.149559, loss_sup: 0.038854, loss_mps: 0.040637, loss_cps: 0.070069
[12:51:42.272] iteration 7586: total_loss: 0.294051, loss_sup: 0.125769, loss_mps: 0.055330, loss_cps: 0.112952
[12:51:42.419] iteration 7587: total_loss: 0.492144, loss_sup: 0.350652, loss_mps: 0.048085, loss_cps: 0.093406
[12:51:42.565] iteration 7588: total_loss: 0.193854, loss_sup: 0.087378, loss_mps: 0.038503, loss_cps: 0.067974
[12:51:42.711] iteration 7589: total_loss: 0.229225, loss_sup: 0.099562, loss_mps: 0.047575, loss_cps: 0.082088
[12:51:42.858] iteration 7590: total_loss: 0.108377, loss_sup: 0.025882, loss_mps: 0.032568, loss_cps: 0.049928
[12:51:43.004] iteration 7591: total_loss: 0.226358, loss_sup: 0.124777, loss_mps: 0.036732, loss_cps: 0.064849
[12:51:43.150] iteration 7592: total_loss: 0.159351, loss_sup: 0.048906, loss_mps: 0.041903, loss_cps: 0.068542
[12:51:43.297] iteration 7593: total_loss: 0.252175, loss_sup: 0.099466, loss_mps: 0.052785, loss_cps: 0.099924
[12:51:43.443] iteration 7594: total_loss: 0.200827, loss_sup: 0.077261, loss_mps: 0.044914, loss_cps: 0.078652
[12:51:43.590] iteration 7595: total_loss: 0.214238, loss_sup: 0.070592, loss_mps: 0.049003, loss_cps: 0.094643
[12:51:43.740] iteration 7596: total_loss: 0.187351, loss_sup: 0.028256, loss_mps: 0.057339, loss_cps: 0.101756
[12:51:43.888] iteration 7597: total_loss: 0.327403, loss_sup: 0.163228, loss_mps: 0.055194, loss_cps: 0.108981
[12:51:44.035] iteration 7598: total_loss: 0.117321, loss_sup: 0.019804, loss_mps: 0.035189, loss_cps: 0.062328
[12:51:44.181] iteration 7599: total_loss: 0.252288, loss_sup: 0.122178, loss_mps: 0.047348, loss_cps: 0.082762
[12:51:44.328] iteration 7600: total_loss: 0.271650, loss_sup: 0.096092, loss_mps: 0.062116, loss_cps: 0.113442
[12:51:44.328] Evaluation Started ==>
[12:51:55.703] ==> valid iteration 7600: unet metrics: {'dc': 0.6457237752337351, 'jc': 0.5191675980268272, 'pre': 0.717713367750776, 'hd': 6.045354561866108}, ynet metrics: {'dc': 0.589149041419603, 'jc': 0.4671083502189449, 'pre': 0.7162753722987628, 'hd': 6.23799702981388}.
[12:51:55.768] ==> New best valid dice for unet: 0.645724, at iteration 7600
[12:51:55.770] Evaluation Finished!⏹️
[12:51:55.922] iteration 7601: total_loss: 0.222193, loss_sup: 0.140081, loss_mps: 0.032913, loss_cps: 0.049198
[12:51:56.070] iteration 7602: total_loss: 0.355981, loss_sup: 0.137517, loss_mps: 0.072684, loss_cps: 0.145780
[12:51:56.215] iteration 7603: total_loss: 0.195704, loss_sup: 0.044472, loss_mps: 0.052229, loss_cps: 0.099003
[12:51:56.360] iteration 7604: total_loss: 0.226804, loss_sup: 0.122557, loss_mps: 0.038395, loss_cps: 0.065852
[12:51:56.505] iteration 7605: total_loss: 0.222909, loss_sup: 0.110483, loss_mps: 0.039223, loss_cps: 0.073203
[12:51:56.649] iteration 7606: total_loss: 0.205822, loss_sup: 0.050392, loss_mps: 0.054853, loss_cps: 0.100577
[12:51:56.794] iteration 7607: total_loss: 0.184184, loss_sup: 0.100246, loss_mps: 0.032813, loss_cps: 0.051125
[12:51:56.939] iteration 7608: total_loss: 0.142867, loss_sup: 0.029331, loss_mps: 0.041749, loss_cps: 0.071787
[12:51:57.084] iteration 7609: total_loss: 0.326641, loss_sup: 0.197964, loss_mps: 0.044663, loss_cps: 0.084015
[12:51:57.229] iteration 7610: total_loss: 0.459263, loss_sup: 0.293666, loss_mps: 0.055187, loss_cps: 0.110410
[12:51:57.374] iteration 7611: total_loss: 0.178500, loss_sup: 0.053799, loss_mps: 0.044850, loss_cps: 0.079852
[12:51:57.519] iteration 7612: total_loss: 0.352255, loss_sup: 0.222062, loss_mps: 0.045050, loss_cps: 0.085143
[12:51:57.665] iteration 7613: total_loss: 0.179792, loss_sup: 0.088079, loss_mps: 0.033589, loss_cps: 0.058124
[12:51:57.810] iteration 7614: total_loss: 0.248552, loss_sup: 0.131151, loss_mps: 0.043114, loss_cps: 0.074286
[12:51:57.955] iteration 7615: total_loss: 0.190351, loss_sup: 0.090343, loss_mps: 0.035185, loss_cps: 0.064824
[12:51:58.102] iteration 7616: total_loss: 0.201389, loss_sup: 0.092466, loss_mps: 0.038351, loss_cps: 0.070572
[12:51:58.247] iteration 7617: total_loss: 0.225628, loss_sup: 0.058045, loss_mps: 0.056923, loss_cps: 0.110660
[12:51:58.393] iteration 7618: total_loss: 0.227371, loss_sup: 0.089928, loss_mps: 0.047520, loss_cps: 0.089924
[12:51:58.539] iteration 7619: total_loss: 0.137632, loss_sup: 0.025622, loss_mps: 0.042250, loss_cps: 0.069760
[12:51:58.684] iteration 7620: total_loss: 0.343858, loss_sup: 0.165924, loss_mps: 0.059253, loss_cps: 0.118681
[12:51:58.830] iteration 7621: total_loss: 0.327451, loss_sup: 0.111277, loss_mps: 0.070549, loss_cps: 0.145626
[12:51:58.975] iteration 7622: total_loss: 0.236589, loss_sup: 0.084690, loss_mps: 0.053466, loss_cps: 0.098433
[12:51:59.120] iteration 7623: total_loss: 0.300848, loss_sup: 0.145724, loss_mps: 0.052207, loss_cps: 0.102917
[12:51:59.265] iteration 7624: total_loss: 0.158751, loss_sup: 0.050177, loss_mps: 0.038769, loss_cps: 0.069805
[12:51:59.414] iteration 7625: total_loss: 0.380186, loss_sup: 0.174598, loss_mps: 0.069023, loss_cps: 0.136565
[12:51:59.562] iteration 7626: total_loss: 0.313011, loss_sup: 0.173761, loss_mps: 0.047313, loss_cps: 0.091937
[12:51:59.707] iteration 7627: total_loss: 0.165273, loss_sup: 0.029386, loss_mps: 0.047364, loss_cps: 0.088522
[12:51:59.853] iteration 7628: total_loss: 0.219150, loss_sup: 0.079875, loss_mps: 0.047308, loss_cps: 0.091967
[12:51:59.999] iteration 7629: total_loss: 0.275840, loss_sup: 0.123784, loss_mps: 0.051007, loss_cps: 0.101049
[12:52:00.147] iteration 7630: total_loss: 0.403053, loss_sup: 0.239161, loss_mps: 0.056485, loss_cps: 0.107407
[12:52:00.293] iteration 7631: total_loss: 0.199033, loss_sup: 0.057540, loss_mps: 0.050668, loss_cps: 0.090824
[12:52:00.438] iteration 7632: total_loss: 0.153907, loss_sup: 0.037795, loss_mps: 0.042111, loss_cps: 0.074002
[12:52:00.585] iteration 7633: total_loss: 0.189424, loss_sup: 0.084320, loss_mps: 0.037804, loss_cps: 0.067299
[12:52:00.732] iteration 7634: total_loss: 0.336452, loss_sup: 0.195911, loss_mps: 0.048331, loss_cps: 0.092209
[12:52:00.878] iteration 7635: total_loss: 0.357374, loss_sup: 0.195355, loss_mps: 0.054550, loss_cps: 0.107469
[12:52:01.025] iteration 7636: total_loss: 0.450775, loss_sup: 0.294340, loss_mps: 0.050679, loss_cps: 0.105756
[12:52:01.171] iteration 7637: total_loss: 0.198353, loss_sup: 0.044996, loss_mps: 0.055129, loss_cps: 0.098228
[12:52:01.319] iteration 7638: total_loss: 0.277798, loss_sup: 0.127865, loss_mps: 0.051234, loss_cps: 0.098699
[12:52:01.465] iteration 7639: total_loss: 0.348640, loss_sup: 0.246793, loss_mps: 0.037370, loss_cps: 0.064476
[12:52:01.611] iteration 7640: total_loss: 0.141647, loss_sup: 0.055825, loss_mps: 0.032920, loss_cps: 0.052901
[12:52:01.757] iteration 7641: total_loss: 0.127208, loss_sup: 0.011873, loss_mps: 0.040709, loss_cps: 0.074626
[12:52:01.902] iteration 7642: total_loss: 0.217783, loss_sup: 0.058496, loss_mps: 0.054920, loss_cps: 0.104367
[12:52:02.048] iteration 7643: total_loss: 0.238097, loss_sup: 0.058155, loss_mps: 0.063392, loss_cps: 0.116550
[12:52:02.196] iteration 7644: total_loss: 0.335777, loss_sup: 0.123454, loss_mps: 0.069903, loss_cps: 0.142419
[12:52:02.342] iteration 7645: total_loss: 0.495580, loss_sup: 0.209168, loss_mps: 0.090062, loss_cps: 0.196350
[12:52:02.488] iteration 7646: total_loss: 0.158375, loss_sup: 0.031907, loss_mps: 0.045309, loss_cps: 0.081158
[12:52:02.634] iteration 7647: total_loss: 0.156188, loss_sup: 0.048338, loss_mps: 0.041940, loss_cps: 0.065909
[12:52:02.783] iteration 7648: total_loss: 0.254701, loss_sup: 0.130298, loss_mps: 0.045773, loss_cps: 0.078630
[12:52:02.929] iteration 7649: total_loss: 0.193680, loss_sup: 0.068380, loss_mps: 0.046370, loss_cps: 0.078930
[12:52:03.076] iteration 7650: total_loss: 0.461044, loss_sup: 0.251281, loss_mps: 0.070967, loss_cps: 0.138796
[12:52:03.224] iteration 7651: total_loss: 0.240644, loss_sup: 0.036812, loss_mps: 0.069628, loss_cps: 0.134205
[12:52:03.369] iteration 7652: total_loss: 0.212834, loss_sup: 0.106884, loss_mps: 0.042252, loss_cps: 0.063697
[12:52:03.515] iteration 7653: total_loss: 0.190344, loss_sup: 0.062630, loss_mps: 0.046031, loss_cps: 0.081682
[12:52:03.660] iteration 7654: total_loss: 0.310020, loss_sup: 0.129672, loss_mps: 0.062919, loss_cps: 0.117429
[12:52:03.806] iteration 7655: total_loss: 0.208252, loss_sup: 0.073144, loss_mps: 0.049811, loss_cps: 0.085298
[12:52:03.952] iteration 7656: total_loss: 0.167418, loss_sup: 0.042237, loss_mps: 0.045396, loss_cps: 0.079785
[12:52:04.098] iteration 7657: total_loss: 0.374528, loss_sup: 0.205219, loss_mps: 0.056001, loss_cps: 0.113309
[12:52:04.251] iteration 7658: total_loss: 0.137239, loss_sup: 0.030983, loss_mps: 0.038479, loss_cps: 0.067777
[12:52:04.398] iteration 7659: total_loss: 0.165209, loss_sup: 0.042628, loss_mps: 0.045580, loss_cps: 0.077001
[12:52:04.546] iteration 7660: total_loss: 0.256287, loss_sup: 0.142547, loss_mps: 0.040870, loss_cps: 0.072869
[12:52:04.692] iteration 7661: total_loss: 0.255235, loss_sup: 0.107163, loss_mps: 0.051494, loss_cps: 0.096579
[12:52:04.844] iteration 7662: total_loss: 0.175694, loss_sup: 0.033778, loss_mps: 0.048489, loss_cps: 0.093427
[12:52:04.993] iteration 7663: total_loss: 0.342169, loss_sup: 0.176515, loss_mps: 0.057227, loss_cps: 0.108427
[12:52:05.141] iteration 7664: total_loss: 0.238177, loss_sup: 0.105426, loss_mps: 0.046379, loss_cps: 0.086371
[12:52:05.287] iteration 7665: total_loss: 0.132430, loss_sup: 0.037842, loss_mps: 0.034764, loss_cps: 0.059824
[12:52:05.432] iteration 7666: total_loss: 0.303229, loss_sup: 0.166397, loss_mps: 0.048029, loss_cps: 0.088803
[12:52:05.579] iteration 7667: total_loss: 0.326675, loss_sup: 0.201164, loss_mps: 0.044358, loss_cps: 0.081153
[12:52:05.725] iteration 7668: total_loss: 0.171773, loss_sup: 0.049798, loss_mps: 0.042358, loss_cps: 0.079616
[12:52:05.871] iteration 7669: total_loss: 0.230108, loss_sup: 0.106419, loss_mps: 0.045341, loss_cps: 0.078348
[12:52:06.017] iteration 7670: total_loss: 0.205395, loss_sup: 0.056250, loss_mps: 0.051435, loss_cps: 0.097709
[12:52:06.163] iteration 7671: total_loss: 0.214229, loss_sup: 0.106096, loss_mps: 0.040177, loss_cps: 0.067956
[12:52:06.310] iteration 7672: total_loss: 0.174372, loss_sup: 0.058800, loss_mps: 0.043557, loss_cps: 0.072014
[12:52:06.455] iteration 7673: total_loss: 0.174438, loss_sup: 0.091442, loss_mps: 0.032069, loss_cps: 0.050927
[12:52:06.601] iteration 7674: total_loss: 0.203218, loss_sup: 0.083740, loss_mps: 0.044952, loss_cps: 0.074526
[12:52:06.748] iteration 7675: total_loss: 0.129620, loss_sup: 0.015222, loss_mps: 0.040877, loss_cps: 0.073522
[12:52:06.894] iteration 7676: total_loss: 0.273056, loss_sup: 0.186472, loss_mps: 0.031793, loss_cps: 0.054791
[12:52:07.043] iteration 7677: total_loss: 0.253275, loss_sup: 0.074341, loss_mps: 0.060826, loss_cps: 0.118107
[12:52:07.189] iteration 7678: total_loss: 0.148542, loss_sup: 0.040887, loss_mps: 0.039552, loss_cps: 0.068104
[12:52:07.335] iteration 7679: total_loss: 0.148594, loss_sup: 0.015700, loss_mps: 0.045372, loss_cps: 0.087522
[12:52:07.481] iteration 7680: total_loss: 0.163997, loss_sup: 0.062570, loss_mps: 0.038081, loss_cps: 0.063346
[12:52:07.626] iteration 7681: total_loss: 0.300272, loss_sup: 0.159113, loss_mps: 0.050367, loss_cps: 0.090792
[12:52:07.772] iteration 7682: total_loss: 0.125673, loss_sup: 0.045496, loss_mps: 0.029865, loss_cps: 0.050312
[12:52:07.919] iteration 7683: total_loss: 0.205637, loss_sup: 0.074180, loss_mps: 0.045916, loss_cps: 0.085541
[12:52:08.066] iteration 7684: total_loss: 0.230028, loss_sup: 0.125868, loss_mps: 0.037913, loss_cps: 0.066247
[12:52:08.211] iteration 7685: total_loss: 0.350031, loss_sup: 0.244682, loss_mps: 0.038727, loss_cps: 0.066621
[12:52:08.358] iteration 7686: total_loss: 0.353945, loss_sup: 0.170178, loss_mps: 0.064807, loss_cps: 0.118959
[12:52:08.504] iteration 7687: total_loss: 0.222706, loss_sup: 0.115548, loss_mps: 0.040853, loss_cps: 0.066304
[12:52:08.650] iteration 7688: total_loss: 0.294198, loss_sup: 0.128255, loss_mps: 0.057057, loss_cps: 0.108886
[12:52:08.796] iteration 7689: total_loss: 0.105961, loss_sup: 0.020985, loss_mps: 0.033549, loss_cps: 0.051427
[12:52:08.942] iteration 7690: total_loss: 0.130926, loss_sup: 0.024906, loss_mps: 0.037982, loss_cps: 0.068037
[12:52:09.088] iteration 7691: total_loss: 0.239473, loss_sup: 0.050894, loss_mps: 0.062917, loss_cps: 0.125661
[12:52:09.235] iteration 7692: total_loss: 0.236752, loss_sup: 0.133489, loss_mps: 0.036965, loss_cps: 0.066297
[12:52:09.380] iteration 7693: total_loss: 0.396630, loss_sup: 0.298003, loss_mps: 0.035075, loss_cps: 0.063552
[12:52:09.526] iteration 7694: total_loss: 0.145192, loss_sup: 0.038556, loss_mps: 0.038249, loss_cps: 0.068386
[12:52:09.672] iteration 7695: total_loss: 0.490924, loss_sup: 0.223022, loss_mps: 0.084208, loss_cps: 0.183694
[12:52:09.817] iteration 7696: total_loss: 0.175418, loss_sup: 0.047628, loss_mps: 0.044500, loss_cps: 0.083289
[12:52:09.963] iteration 7697: total_loss: 0.182291, loss_sup: 0.069065, loss_mps: 0.041070, loss_cps: 0.072157
[12:52:10.109] iteration 7698: total_loss: 0.195021, loss_sup: 0.087594, loss_mps: 0.037887, loss_cps: 0.069541
[12:52:10.255] iteration 7699: total_loss: 0.232466, loss_sup: 0.065613, loss_mps: 0.054444, loss_cps: 0.112409
[12:52:10.402] iteration 7700: total_loss: 0.257160, loss_sup: 0.083586, loss_mps: 0.056702, loss_cps: 0.116872
[12:52:10.402] Evaluation Started ==>
[12:52:21.797] ==> valid iteration 7700: unet metrics: {'dc': 0.6281725729190549, 'jc': 0.5006443408221276, 'pre': 0.7278792093886574, 'hd': 6.071013011751224}, ynet metrics: {'dc': 0.5470580545892622, 'jc': 0.42392850318288655, 'pre': 0.725603853173164, 'hd': 6.287098558997346}.
[12:52:21.799] Evaluation Finished!⏹️
[12:52:21.951] iteration 7701: total_loss: 0.230002, loss_sup: 0.082273, loss_mps: 0.051145, loss_cps: 0.096585
[12:52:22.099] iteration 7702: total_loss: 0.210446, loss_sup: 0.126214, loss_mps: 0.031780, loss_cps: 0.052452
[12:52:22.245] iteration 7703: total_loss: 0.079693, loss_sup: 0.023775, loss_mps: 0.022974, loss_cps: 0.032944
[12:52:22.390] iteration 7704: total_loss: 0.194009, loss_sup: 0.093748, loss_mps: 0.036239, loss_cps: 0.064023
[12:52:22.535] iteration 7705: total_loss: 0.350116, loss_sup: 0.149784, loss_mps: 0.064560, loss_cps: 0.135772
[12:52:22.680] iteration 7706: total_loss: 0.160888, loss_sup: 0.056490, loss_mps: 0.038719, loss_cps: 0.065678
[12:52:22.825] iteration 7707: total_loss: 0.557515, loss_sup: 0.299401, loss_mps: 0.084299, loss_cps: 0.173815
[12:52:22.971] iteration 7708: total_loss: 0.175960, loss_sup: 0.024970, loss_mps: 0.050671, loss_cps: 0.100318
[12:52:23.116] iteration 7709: total_loss: 0.512491, loss_sup: 0.179582, loss_mps: 0.106675, loss_cps: 0.226234
[12:52:23.262] iteration 7710: total_loss: 0.167781, loss_sup: 0.034222, loss_mps: 0.048114, loss_cps: 0.085445
[12:52:23.407] iteration 7711: total_loss: 0.242489, loss_sup: 0.100461, loss_mps: 0.050126, loss_cps: 0.091903
[12:52:23.553] iteration 7712: total_loss: 0.171261, loss_sup: 0.026362, loss_mps: 0.049541, loss_cps: 0.095358
[12:52:23.698] iteration 7713: total_loss: 0.333544, loss_sup: 0.211023, loss_mps: 0.042686, loss_cps: 0.079835
[12:52:23.844] iteration 7714: total_loss: 0.208175, loss_sup: 0.046666, loss_mps: 0.054629, loss_cps: 0.106880
[12:52:23.990] iteration 7715: total_loss: 0.171595, loss_sup: 0.063104, loss_mps: 0.039701, loss_cps: 0.068790
[12:52:24.136] iteration 7716: total_loss: 0.340890, loss_sup: 0.233127, loss_mps: 0.039834, loss_cps: 0.067929
[12:52:24.282] iteration 7717: total_loss: 0.258131, loss_sup: 0.103451, loss_mps: 0.053276, loss_cps: 0.101405
[12:52:24.431] iteration 7718: total_loss: 0.091191, loss_sup: 0.014620, loss_mps: 0.028795, loss_cps: 0.047776
[12:52:24.577] iteration 7719: total_loss: 0.234135, loss_sup: 0.131993, loss_mps: 0.037613, loss_cps: 0.064529
[12:52:24.722] iteration 7720: total_loss: 0.225318, loss_sup: 0.090005, loss_mps: 0.048171, loss_cps: 0.087142
[12:52:24.868] iteration 7721: total_loss: 0.490801, loss_sup: 0.335853, loss_mps: 0.054468, loss_cps: 0.100480
[12:52:25.014] iteration 7722: total_loss: 0.463216, loss_sup: 0.295958, loss_mps: 0.056110, loss_cps: 0.111148
[12:52:25.159] iteration 7723: total_loss: 0.114304, loss_sup: 0.029429, loss_mps: 0.032651, loss_cps: 0.052224
[12:52:25.304] iteration 7724: total_loss: 0.288859, loss_sup: 0.149196, loss_mps: 0.048174, loss_cps: 0.091489
[12:52:25.450] iteration 7725: total_loss: 0.194343, loss_sup: 0.060205, loss_mps: 0.047504, loss_cps: 0.086634
[12:52:25.595] iteration 7726: total_loss: 0.256218, loss_sup: 0.148663, loss_mps: 0.039098, loss_cps: 0.068458
[12:52:25.740] iteration 7727: total_loss: 0.254011, loss_sup: 0.095115, loss_mps: 0.054614, loss_cps: 0.104281
[12:52:25.889] iteration 7728: total_loss: 0.275417, loss_sup: 0.167294, loss_mps: 0.041470, loss_cps: 0.066652
[12:52:26.035] iteration 7729: total_loss: 0.237866, loss_sup: 0.105693, loss_mps: 0.047154, loss_cps: 0.085019
[12:52:26.181] iteration 7730: total_loss: 0.197960, loss_sup: 0.074718, loss_mps: 0.044491, loss_cps: 0.078751
[12:52:26.326] iteration 7731: total_loss: 0.181336, loss_sup: 0.042893, loss_mps: 0.050249, loss_cps: 0.088195
[12:52:26.472] iteration 7732: total_loss: 0.217079, loss_sup: 0.129770, loss_mps: 0.031865, loss_cps: 0.055444
[12:52:26.617] iteration 7733: total_loss: 0.249145, loss_sup: 0.067454, loss_mps: 0.060453, loss_cps: 0.121238
[12:52:26.763] iteration 7734: total_loss: 0.475305, loss_sup: 0.227983, loss_mps: 0.083819, loss_cps: 0.163503
[12:52:26.909] iteration 7735: total_loss: 0.200198, loss_sup: 0.048198, loss_mps: 0.052129, loss_cps: 0.099871
[12:52:27.054] iteration 7736: total_loss: 0.201822, loss_sup: 0.118535, loss_mps: 0.032309, loss_cps: 0.050978
[12:52:27.200] iteration 7737: total_loss: 0.142020, loss_sup: 0.031429, loss_mps: 0.041172, loss_cps: 0.069420
[12:52:27.345] iteration 7738: total_loss: 0.158930, loss_sup: 0.034124, loss_mps: 0.042210, loss_cps: 0.082596
[12:52:27.491] iteration 7739: total_loss: 0.244099, loss_sup: 0.123647, loss_mps: 0.042733, loss_cps: 0.077719
[12:52:27.637] iteration 7740: total_loss: 0.170342, loss_sup: 0.039283, loss_mps: 0.044983, loss_cps: 0.086077
[12:52:27.783] iteration 7741: total_loss: 0.167116, loss_sup: 0.065238, loss_mps: 0.035147, loss_cps: 0.066731
[12:52:27.929] iteration 7742: total_loss: 0.139372, loss_sup: 0.018322, loss_mps: 0.044163, loss_cps: 0.076887
[12:52:28.074] iteration 7743: total_loss: 0.172601, loss_sup: 0.030941, loss_mps: 0.048267, loss_cps: 0.093393
[12:52:28.220] iteration 7744: total_loss: 0.292796, loss_sup: 0.134915, loss_mps: 0.052223, loss_cps: 0.105657
[12:52:28.365] iteration 7745: total_loss: 0.200426, loss_sup: 0.065819, loss_mps: 0.048189, loss_cps: 0.086418
[12:52:28.510] iteration 7746: total_loss: 0.159923, loss_sup: 0.066174, loss_mps: 0.033912, loss_cps: 0.059837
[12:52:28.655] iteration 7747: total_loss: 0.397416, loss_sup: 0.262837, loss_mps: 0.047011, loss_cps: 0.087568
[12:52:28.802] iteration 7748: total_loss: 0.209293, loss_sup: 0.079616, loss_mps: 0.044093, loss_cps: 0.085583
[12:52:28.948] iteration 7749: total_loss: 0.158690, loss_sup: 0.044442, loss_mps: 0.039619, loss_cps: 0.074629
[12:52:29.093] iteration 7750: total_loss: 0.263433, loss_sup: 0.159333, loss_mps: 0.037300, loss_cps: 0.066800
[12:52:29.237] iteration 7751: total_loss: 0.320699, loss_sup: 0.167854, loss_mps: 0.052712, loss_cps: 0.100133
[12:52:29.383] iteration 7752: total_loss: 0.137681, loss_sup: 0.016851, loss_mps: 0.043469, loss_cps: 0.077361
[12:52:29.529] iteration 7753: total_loss: 0.279600, loss_sup: 0.113996, loss_mps: 0.052871, loss_cps: 0.112733
[12:52:29.674] iteration 7754: total_loss: 0.204097, loss_sup: 0.114154, loss_mps: 0.033529, loss_cps: 0.056414
[12:52:29.819] iteration 7755: total_loss: 0.185909, loss_sup: 0.040598, loss_mps: 0.047182, loss_cps: 0.098128
[12:52:29.965] iteration 7756: total_loss: 0.321520, loss_sup: 0.107604, loss_mps: 0.067531, loss_cps: 0.146384
[12:52:30.110] iteration 7757: total_loss: 0.184578, loss_sup: 0.063174, loss_mps: 0.040941, loss_cps: 0.080463
[12:52:30.255] iteration 7758: total_loss: 0.216929, loss_sup: 0.091076, loss_mps: 0.041423, loss_cps: 0.084430
[12:52:30.402] iteration 7759: total_loss: 0.238239, loss_sup: 0.107854, loss_mps: 0.045284, loss_cps: 0.085101
[12:52:30.548] iteration 7760: total_loss: 0.211439, loss_sup: 0.087512, loss_mps: 0.043396, loss_cps: 0.080531
[12:52:30.695] iteration 7761: total_loss: 0.119169, loss_sup: 0.015024, loss_mps: 0.037320, loss_cps: 0.066825
[12:52:30.840] iteration 7762: total_loss: 0.223852, loss_sup: 0.066036, loss_mps: 0.052811, loss_cps: 0.105005
[12:52:30.986] iteration 7763: total_loss: 0.133811, loss_sup: 0.045150, loss_mps: 0.032213, loss_cps: 0.056447
[12:52:31.131] iteration 7764: total_loss: 0.165107, loss_sup: 0.040474, loss_mps: 0.041794, loss_cps: 0.082839
[12:52:31.277] iteration 7765: total_loss: 0.149223, loss_sup: 0.039860, loss_mps: 0.036899, loss_cps: 0.072464
[12:52:31.422] iteration 7766: total_loss: 0.199634, loss_sup: 0.108330, loss_mps: 0.032428, loss_cps: 0.058875
[12:52:31.568] iteration 7767: total_loss: 0.223681, loss_sup: 0.086561, loss_mps: 0.045170, loss_cps: 0.091950
[12:52:31.713] iteration 7768: total_loss: 0.333436, loss_sup: 0.180805, loss_mps: 0.049339, loss_cps: 0.103292
[12:52:31.859] iteration 7769: total_loss: 0.206867, loss_sup: 0.059968, loss_mps: 0.048758, loss_cps: 0.098140
[12:52:32.005] iteration 7770: total_loss: 0.191898, loss_sup: 0.061770, loss_mps: 0.045010, loss_cps: 0.085117
[12:52:32.151] iteration 7771: total_loss: 0.212676, loss_sup: 0.046163, loss_mps: 0.054287, loss_cps: 0.112227
[12:52:32.298] iteration 7772: total_loss: 0.108632, loss_sup: 0.038532, loss_mps: 0.027727, loss_cps: 0.042372
[12:52:32.444] iteration 7773: total_loss: 0.201031, loss_sup: 0.072847, loss_mps: 0.044496, loss_cps: 0.083689
[12:52:32.590] iteration 7774: total_loss: 0.119423, loss_sup: 0.030418, loss_mps: 0.032675, loss_cps: 0.056330
[12:52:32.736] iteration 7775: total_loss: 0.185823, loss_sup: 0.075568, loss_mps: 0.037101, loss_cps: 0.073154
[12:52:32.882] iteration 7776: total_loss: 0.266378, loss_sup: 0.133710, loss_mps: 0.046677, loss_cps: 0.085992
[12:52:33.030] iteration 7777: total_loss: 0.148552, loss_sup: 0.054289, loss_mps: 0.033711, loss_cps: 0.060552
[12:52:33.177] iteration 7778: total_loss: 0.176706, loss_sup: 0.062240, loss_mps: 0.039441, loss_cps: 0.075024
[12:52:33.323] iteration 7779: total_loss: 0.270157, loss_sup: 0.135580, loss_mps: 0.045555, loss_cps: 0.089022
[12:52:33.468] iteration 7780: total_loss: 0.224892, loss_sup: 0.114497, loss_mps: 0.037494, loss_cps: 0.072901
[12:52:33.614] iteration 7781: total_loss: 0.250386, loss_sup: 0.096544, loss_mps: 0.051211, loss_cps: 0.102631
[12:52:33.760] iteration 7782: total_loss: 0.231971, loss_sup: 0.093973, loss_mps: 0.045747, loss_cps: 0.092250
[12:52:33.906] iteration 7783: total_loss: 0.301223, loss_sup: 0.100082, loss_mps: 0.066869, loss_cps: 0.134272
[12:52:34.051] iteration 7784: total_loss: 0.242141, loss_sup: 0.083289, loss_mps: 0.053038, loss_cps: 0.105814
[12:52:34.196] iteration 7785: total_loss: 0.177816, loss_sup: 0.090096, loss_mps: 0.031849, loss_cps: 0.055872
[12:52:34.342] iteration 7786: total_loss: 0.119641, loss_sup: 0.035726, loss_mps: 0.031721, loss_cps: 0.052194
[12:52:34.488] iteration 7787: total_loss: 0.182927, loss_sup: 0.051094, loss_mps: 0.042561, loss_cps: 0.089271
[12:52:34.633] iteration 7788: total_loss: 0.286086, loss_sup: 0.105969, loss_mps: 0.060426, loss_cps: 0.119691
[12:52:34.780] iteration 7789: total_loss: 0.185942, loss_sup: 0.085937, loss_mps: 0.036195, loss_cps: 0.063810
[12:52:34.926] iteration 7790: total_loss: 0.192448, loss_sup: 0.090368, loss_mps: 0.035211, loss_cps: 0.066869
[12:52:35.072] iteration 7791: total_loss: 0.134742, loss_sup: 0.047614, loss_mps: 0.031913, loss_cps: 0.055215
[12:52:35.217] iteration 7792: total_loss: 0.236437, loss_sup: 0.107428, loss_mps: 0.044568, loss_cps: 0.084441
[12:52:35.363] iteration 7793: total_loss: 0.158090, loss_sup: 0.040674, loss_mps: 0.040694, loss_cps: 0.076721
[12:52:35.508] iteration 7794: total_loss: 0.182088, loss_sup: 0.038540, loss_mps: 0.047064, loss_cps: 0.096484
[12:52:35.654] iteration 7795: total_loss: 0.149264, loss_sup: 0.021123, loss_mps: 0.043383, loss_cps: 0.084758
[12:52:35.802] iteration 7796: total_loss: 0.166822, loss_sup: 0.036059, loss_mps: 0.042717, loss_cps: 0.088046
[12:52:35.948] iteration 7797: total_loss: 0.176251, loss_sup: 0.020117, loss_mps: 0.050401, loss_cps: 0.105733
[12:52:36.093] iteration 7798: total_loss: 0.264027, loss_sup: 0.142689, loss_mps: 0.041241, loss_cps: 0.080097
[12:52:36.239] iteration 7799: total_loss: 0.263754, loss_sup: 0.110760, loss_mps: 0.053313, loss_cps: 0.099681
[12:52:36.385] iteration 7800: total_loss: 0.295629, loss_sup: 0.167821, loss_mps: 0.044608, loss_cps: 0.083201
[12:52:36.385] Evaluation Started ==>
[12:52:47.837] ==> valid iteration 7800: unet metrics: {'dc': 0.6533992045710929, 'jc': 0.5249633947336018, 'pre': 0.7088471147548068, 'hd': 6.153681827457719}, ynet metrics: {'dc': 0.5955012410795769, 'jc': 0.47215498902205694, 'pre': 0.758399703612574, 'hd': 5.926929592951794}.
[12:52:47.897] ==> New best valid dice for unet: 0.653399, at iteration 7800
[12:52:48.059] ==> New best valid dice for ynet: 0.595501, at iteration 7800
[12:52:48.061] Evaluation Finished!⏹️
[12:52:48.214] iteration 7801: total_loss: 0.235006, loss_sup: 0.062515, loss_mps: 0.056810, loss_cps: 0.115681
[12:52:48.361] iteration 7802: total_loss: 0.218963, loss_sup: 0.068621, loss_mps: 0.049090, loss_cps: 0.101251
[12:52:48.507] iteration 7803: total_loss: 0.364265, loss_sup: 0.136088, loss_mps: 0.071679, loss_cps: 0.156498
[12:52:48.651] iteration 7804: total_loss: 0.160666, loss_sup: 0.086794, loss_mps: 0.026741, loss_cps: 0.047131
[12:52:48.797] iteration 7805: total_loss: 0.249940, loss_sup: 0.112222, loss_mps: 0.046040, loss_cps: 0.091677
[12:52:48.942] iteration 7806: total_loss: 0.265738, loss_sup: 0.138332, loss_mps: 0.041639, loss_cps: 0.085767
[12:52:49.087] iteration 7807: total_loss: 0.294712, loss_sup: 0.171788, loss_mps: 0.039867, loss_cps: 0.083057
[12:52:49.233] iteration 7808: total_loss: 0.389773, loss_sup: 0.232754, loss_mps: 0.049895, loss_cps: 0.107125
[12:52:49.378] iteration 7809: total_loss: 0.174567, loss_sup: 0.066047, loss_mps: 0.036447, loss_cps: 0.072073
[12:52:49.524] iteration 7810: total_loss: 0.165609, loss_sup: 0.031571, loss_mps: 0.045010, loss_cps: 0.089028
[12:52:49.669] iteration 7811: total_loss: 0.183407, loss_sup: 0.093094, loss_mps: 0.033514, loss_cps: 0.056799
[12:52:49.814] iteration 7812: total_loss: 0.087511, loss_sup: 0.016533, loss_mps: 0.027038, loss_cps: 0.043939
[12:52:49.959] iteration 7813: total_loss: 0.253808, loss_sup: 0.095054, loss_mps: 0.053238, loss_cps: 0.105516
[12:52:50.105] iteration 7814: total_loss: 0.169563, loss_sup: 0.067332, loss_mps: 0.035170, loss_cps: 0.067061
[12:52:50.250] iteration 7815: total_loss: 0.138110, loss_sup: 0.040179, loss_mps: 0.034831, loss_cps: 0.063099
[12:52:50.396] iteration 7816: total_loss: 0.254576, loss_sup: 0.086349, loss_mps: 0.056387, loss_cps: 0.111841
[12:52:50.541] iteration 7817: total_loss: 0.325901, loss_sup: 0.184883, loss_mps: 0.049377, loss_cps: 0.091641
[12:52:50.686] iteration 7818: total_loss: 0.240385, loss_sup: 0.077847, loss_mps: 0.053819, loss_cps: 0.108719
[12:52:50.831] iteration 7819: total_loss: 0.151470, loss_sup: 0.024560, loss_mps: 0.044009, loss_cps: 0.082901
[12:52:50.977] iteration 7820: total_loss: 0.433357, loss_sup: 0.255662, loss_mps: 0.058553, loss_cps: 0.119141
[12:52:51.123] iteration 7821: total_loss: 0.133856, loss_sup: 0.060464, loss_mps: 0.027847, loss_cps: 0.045545
[12:52:51.268] iteration 7822: total_loss: 0.365023, loss_sup: 0.171010, loss_mps: 0.063629, loss_cps: 0.130384
[12:52:51.413] iteration 7823: total_loss: 0.330166, loss_sup: 0.114734, loss_mps: 0.070073, loss_cps: 0.145359
[12:52:51.559] iteration 7824: total_loss: 0.140772, loss_sup: 0.023226, loss_mps: 0.043233, loss_cps: 0.074313
[12:52:51.704] iteration 7825: total_loss: 0.358922, loss_sup: 0.227541, loss_mps: 0.046087, loss_cps: 0.085294
[12:52:51.849] iteration 7826: total_loss: 0.173712, loss_sup: 0.064015, loss_mps: 0.039667, loss_cps: 0.070031
[12:52:51.994] iteration 7827: total_loss: 0.208041, loss_sup: 0.067102, loss_mps: 0.047286, loss_cps: 0.093652
[12:52:52.140] iteration 7828: total_loss: 0.145280, loss_sup: 0.029581, loss_mps: 0.041230, loss_cps: 0.074469
[12:52:52.286] iteration 7829: total_loss: 0.380612, loss_sup: 0.212422, loss_mps: 0.057057, loss_cps: 0.111133
[12:52:52.431] iteration 7830: total_loss: 0.159383, loss_sup: 0.019265, loss_mps: 0.047646, loss_cps: 0.092471
[12:52:52.576] iteration 7831: total_loss: 0.120274, loss_sup: 0.037373, loss_mps: 0.032409, loss_cps: 0.050492
[12:52:52.722] iteration 7832: total_loss: 0.128401, loss_sup: 0.034603, loss_mps: 0.034339, loss_cps: 0.059459
[12:52:52.867] iteration 7833: total_loss: 0.178934, loss_sup: 0.044405, loss_mps: 0.044926, loss_cps: 0.089603
[12:52:53.012] iteration 7834: total_loss: 0.161854, loss_sup: 0.047454, loss_mps: 0.041786, loss_cps: 0.072614
[12:52:53.158] iteration 7835: total_loss: 0.627828, loss_sup: 0.472156, loss_mps: 0.051214, loss_cps: 0.104458
[12:52:53.305] iteration 7836: total_loss: 0.273202, loss_sup: 0.117352, loss_mps: 0.052936, loss_cps: 0.102914
[12:52:53.451] iteration 7837: total_loss: 0.346299, loss_sup: 0.200263, loss_mps: 0.048903, loss_cps: 0.097133
[12:52:53.597] iteration 7838: total_loss: 0.531931, loss_sup: 0.330981, loss_mps: 0.065585, loss_cps: 0.135365
[12:52:53.743] iteration 7839: total_loss: 0.210869, loss_sup: 0.083220, loss_mps: 0.044941, loss_cps: 0.082708
[12:52:53.888] iteration 7840: total_loss: 0.201401, loss_sup: 0.047907, loss_mps: 0.052221, loss_cps: 0.101274
[12:52:54.033] iteration 7841: total_loss: 0.386481, loss_sup: 0.169352, loss_mps: 0.068913, loss_cps: 0.148216
[12:52:54.179] iteration 7842: total_loss: 0.199500, loss_sup: 0.083620, loss_mps: 0.042456, loss_cps: 0.073424
[12:52:54.325] iteration 7843: total_loss: 0.167127, loss_sup: 0.023453, loss_mps: 0.050002, loss_cps: 0.093673
[12:52:54.470] iteration 7844: total_loss: 0.185631, loss_sup: 0.055729, loss_mps: 0.047090, loss_cps: 0.082812
[12:52:54.615] iteration 7845: total_loss: 0.277160, loss_sup: 0.121125, loss_mps: 0.054593, loss_cps: 0.101441
[12:52:54.761] iteration 7846: total_loss: 0.248524, loss_sup: 0.092220, loss_mps: 0.053546, loss_cps: 0.102757
[12:52:54.907] iteration 7847: total_loss: 0.113763, loss_sup: 0.016898, loss_mps: 0.037512, loss_cps: 0.059354
[12:52:55.053] iteration 7848: total_loss: 0.191296, loss_sup: 0.071317, loss_mps: 0.042367, loss_cps: 0.077613
[12:52:55.198] iteration 7849: total_loss: 0.274823, loss_sup: 0.156888, loss_mps: 0.043738, loss_cps: 0.074198
[12:52:55.344] iteration 7850: total_loss: 0.284510, loss_sup: 0.145905, loss_mps: 0.048899, loss_cps: 0.089706
[12:52:55.491] iteration 7851: total_loss: 0.240822, loss_sup: 0.068044, loss_mps: 0.057745, loss_cps: 0.115034
[12:52:55.637] iteration 7852: total_loss: 0.205986, loss_sup: 0.093050, loss_mps: 0.039700, loss_cps: 0.073236
[12:52:55.782] iteration 7853: total_loss: 0.232314, loss_sup: 0.092278, loss_mps: 0.048193, loss_cps: 0.091843
[12:52:55.927] iteration 7854: total_loss: 0.313084, loss_sup: 0.130078, loss_mps: 0.058885, loss_cps: 0.124121
[12:52:56.073] iteration 7855: total_loss: 0.250423, loss_sup: 0.076389, loss_mps: 0.056693, loss_cps: 0.117341
[12:52:56.218] iteration 7856: total_loss: 0.376197, loss_sup: 0.226081, loss_mps: 0.052501, loss_cps: 0.097615
[12:52:56.363] iteration 7857: total_loss: 0.247727, loss_sup: 0.123722, loss_mps: 0.043558, loss_cps: 0.080448
[12:52:56.509] iteration 7858: total_loss: 0.142906, loss_sup: 0.028501, loss_mps: 0.039104, loss_cps: 0.075302
[12:52:56.655] iteration 7859: total_loss: 0.238961, loss_sup: 0.145034, loss_mps: 0.034207, loss_cps: 0.059721
[12:52:56.800] iteration 7860: total_loss: 0.178574, loss_sup: 0.080627, loss_mps: 0.035901, loss_cps: 0.062046
[12:52:56.946] iteration 7861: total_loss: 0.372854, loss_sup: 0.128814, loss_mps: 0.076797, loss_cps: 0.167243
[12:52:57.091] iteration 7862: total_loss: 0.132561, loss_sup: 0.046635, loss_mps: 0.033550, loss_cps: 0.052376
[12:52:57.236] iteration 7863: total_loss: 0.369391, loss_sup: 0.130120, loss_mps: 0.072636, loss_cps: 0.166634
[12:52:57.382] iteration 7864: total_loss: 0.222955, loss_sup: 0.112560, loss_mps: 0.039310, loss_cps: 0.071085
[12:52:57.527] iteration 7865: total_loss: 0.304975, loss_sup: 0.135267, loss_mps: 0.058788, loss_cps: 0.110920
[12:52:57.674] iteration 7866: total_loss: 0.195080, loss_sup: 0.065424, loss_mps: 0.046979, loss_cps: 0.082677
[12:52:57.819] iteration 7867: total_loss: 0.359999, loss_sup: 0.209152, loss_mps: 0.051100, loss_cps: 0.099747
[12:52:57.965] iteration 7868: total_loss: 0.139463, loss_sup: 0.030470, loss_mps: 0.039516, loss_cps: 0.069477
[12:52:58.110] iteration 7869: total_loss: 0.136090, loss_sup: 0.035172, loss_mps: 0.037285, loss_cps: 0.063633
[12:52:58.255] iteration 7870: total_loss: 0.137447, loss_sup: 0.025123, loss_mps: 0.040314, loss_cps: 0.072011
[12:52:58.401] iteration 7871: total_loss: 0.275705, loss_sup: 0.156528, loss_mps: 0.042639, loss_cps: 0.076538
[12:52:58.546] iteration 7872: total_loss: 0.338818, loss_sup: 0.148453, loss_mps: 0.061598, loss_cps: 0.128767
[12:52:58.692] iteration 7873: total_loss: 0.295940, loss_sup: 0.198350, loss_mps: 0.036349, loss_cps: 0.061241
[12:52:58.839] iteration 7874: total_loss: 0.126144, loss_sup: 0.047821, loss_mps: 0.029939, loss_cps: 0.048384
[12:52:58.985] iteration 7875: total_loss: 0.371051, loss_sup: 0.178552, loss_mps: 0.065342, loss_cps: 0.127157
[12:52:59.131] iteration 7876: total_loss: 0.318739, loss_sup: 0.202349, loss_mps: 0.040203, loss_cps: 0.076188
[12:52:59.276] iteration 7877: total_loss: 0.188504, loss_sup: 0.033592, loss_mps: 0.053841, loss_cps: 0.101071
[12:52:59.422] iteration 7878: total_loss: 0.377652, loss_sup: 0.235553, loss_mps: 0.052027, loss_cps: 0.090072
[12:52:59.567] iteration 7879: total_loss: 0.297710, loss_sup: 0.138398, loss_mps: 0.054822, loss_cps: 0.104490
[12:52:59.713] iteration 7880: total_loss: 0.280103, loss_sup: 0.153222, loss_mps: 0.046329, loss_cps: 0.080552
[12:52:59.859] iteration 7881: total_loss: 0.131343, loss_sup: 0.010787, loss_mps: 0.042995, loss_cps: 0.077562
[12:53:00.005] iteration 7882: total_loss: 0.170225, loss_sup: 0.040458, loss_mps: 0.046031, loss_cps: 0.083735
[12:53:00.151] iteration 7883: total_loss: 0.205087, loss_sup: 0.054281, loss_mps: 0.051372, loss_cps: 0.099434
[12:53:00.298] iteration 7884: total_loss: 0.414971, loss_sup: 0.116423, loss_mps: 0.090254, loss_cps: 0.208294
[12:53:00.443] iteration 7885: total_loss: 0.191357, loss_sup: 0.095079, loss_mps: 0.036040, loss_cps: 0.060239
[12:53:00.589] iteration 7886: total_loss: 0.133350, loss_sup: 0.025061, loss_mps: 0.037188, loss_cps: 0.071102
[12:53:00.735] iteration 7887: total_loss: 0.290273, loss_sup: 0.113923, loss_mps: 0.059035, loss_cps: 0.117315
[12:53:00.881] iteration 7888: total_loss: 0.197321, loss_sup: 0.012742, loss_mps: 0.060532, loss_cps: 0.124047
[12:53:01.027] iteration 7889: total_loss: 0.233364, loss_sup: 0.114152, loss_mps: 0.042827, loss_cps: 0.076384
[12:53:01.174] iteration 7890: total_loss: 0.281964, loss_sup: 0.053717, loss_mps: 0.071417, loss_cps: 0.156831
[12:53:01.324] iteration 7891: total_loss: 0.351513, loss_sup: 0.108532, loss_mps: 0.076811, loss_cps: 0.166171
[12:53:01.471] iteration 7892: total_loss: 0.319628, loss_sup: 0.107569, loss_mps: 0.069874, loss_cps: 0.142185
[12:53:01.617] iteration 7893: total_loss: 0.163249, loss_sup: 0.056281, loss_mps: 0.038561, loss_cps: 0.068407
[12:53:01.763] iteration 7894: total_loss: 0.149688, loss_sup: 0.059331, loss_mps: 0.034685, loss_cps: 0.055672
[12:53:01.909] iteration 7895: total_loss: 0.183909, loss_sup: 0.086203, loss_mps: 0.036426, loss_cps: 0.061279
[12:53:02.055] iteration 7896: total_loss: 0.341997, loss_sup: 0.172635, loss_mps: 0.055368, loss_cps: 0.113993
[12:53:02.201] iteration 7897: total_loss: 0.384269, loss_sup: 0.200982, loss_mps: 0.059870, loss_cps: 0.123417
[12:53:02.346] iteration 7898: total_loss: 0.496658, loss_sup: 0.405128, loss_mps: 0.034973, loss_cps: 0.056557
[12:53:02.493] iteration 7899: total_loss: 0.197862, loss_sup: 0.087479, loss_mps: 0.040087, loss_cps: 0.070296
[12:53:02.640] iteration 7900: total_loss: 0.105954, loss_sup: 0.028769, loss_mps: 0.029291, loss_cps: 0.047894
[12:53:02.640] Evaluation Started ==>
[12:53:14.029] ==> valid iteration 7900: unet metrics: {'dc': 0.6417876916151528, 'jc': 0.5136327727527529, 'pre': 0.700921895591832, 'hd': 6.085818377940984}, ynet metrics: {'dc': 0.6041304365539789, 'jc': 0.4857009722500868, 'pre': 0.7733920827933936, 'hd': 5.767121258911714}.
[12:53:14.189] ==> New best valid dice for ynet: 0.604130, at iteration 7900
[12:53:14.191] Evaluation Finished!⏹️
[12:53:14.342] iteration 7901: total_loss: 0.246647, loss_sup: 0.091643, loss_mps: 0.052493, loss_cps: 0.102510
[12:53:14.491] iteration 7902: total_loss: 0.196756, loss_sup: 0.088427, loss_mps: 0.037981, loss_cps: 0.070348
[12:53:14.638] iteration 7903: total_loss: 0.255930, loss_sup: 0.058370, loss_mps: 0.065196, loss_cps: 0.132365
[12:53:14.784] iteration 7904: total_loss: 0.271502, loss_sup: 0.108821, loss_mps: 0.055521, loss_cps: 0.107160
[12:53:14.930] iteration 7905: total_loss: 0.286968, loss_sup: 0.103661, loss_mps: 0.059862, loss_cps: 0.123445
[12:53:15.079] iteration 7906: total_loss: 0.141519, loss_sup: 0.024608, loss_mps: 0.040269, loss_cps: 0.076642
[12:53:15.229] iteration 7907: total_loss: 0.188671, loss_sup: 0.043424, loss_mps: 0.052537, loss_cps: 0.092710
[12:53:15.375] iteration 7908: total_loss: 0.155014, loss_sup: 0.017648, loss_mps: 0.047900, loss_cps: 0.089465
[12:53:15.521] iteration 7909: total_loss: 0.435105, loss_sup: 0.203221, loss_mps: 0.078587, loss_cps: 0.153297
[12:53:15.667] iteration 7910: total_loss: 0.154598, loss_sup: 0.081443, loss_mps: 0.028711, loss_cps: 0.044443
[12:53:15.815] iteration 7911: total_loss: 0.184009, loss_sup: 0.074685, loss_mps: 0.040627, loss_cps: 0.068698
[12:53:15.962] iteration 7912: total_loss: 0.300006, loss_sup: 0.148363, loss_mps: 0.054246, loss_cps: 0.097398
[12:53:16.108] iteration 7913: total_loss: 0.219660, loss_sup: 0.091673, loss_mps: 0.046093, loss_cps: 0.081894
[12:53:16.255] iteration 7914: total_loss: 0.483177, loss_sup: 0.253526, loss_mps: 0.075892, loss_cps: 0.153759
[12:53:16.402] iteration 7915: total_loss: 0.180319, loss_sup: 0.068717, loss_mps: 0.040930, loss_cps: 0.070672
[12:53:16.549] iteration 7916: total_loss: 0.153705, loss_sup: 0.051086, loss_mps: 0.037871, loss_cps: 0.064749
[12:53:16.696] iteration 7917: total_loss: 0.230541, loss_sup: 0.076642, loss_mps: 0.057038, loss_cps: 0.096861
[12:53:16.843] iteration 7918: total_loss: 0.138557, loss_sup: 0.027534, loss_mps: 0.041562, loss_cps: 0.069461
[12:53:16.991] iteration 7919: total_loss: 0.164459, loss_sup: 0.044367, loss_mps: 0.043785, loss_cps: 0.076307
[12:53:17.137] iteration 7920: total_loss: 0.326576, loss_sup: 0.255096, loss_mps: 0.026884, loss_cps: 0.044596
[12:53:17.284] iteration 7921: total_loss: 0.183677, loss_sup: 0.074501, loss_mps: 0.039636, loss_cps: 0.069540
[12:53:17.430] iteration 7922: total_loss: 0.261148, loss_sup: 0.118835, loss_mps: 0.049108, loss_cps: 0.093205
[12:53:17.576] iteration 7923: total_loss: 0.146624, loss_sup: 0.040898, loss_mps: 0.037681, loss_cps: 0.068045
[12:53:17.722] iteration 7924: total_loss: 0.147734, loss_sup: 0.030537, loss_mps: 0.041030, loss_cps: 0.076167
[12:53:17.868] iteration 7925: total_loss: 0.223039, loss_sup: 0.030583, loss_mps: 0.064830, loss_cps: 0.127625
[12:53:18.015] iteration 7926: total_loss: 0.315134, loss_sup: 0.150055, loss_mps: 0.056612, loss_cps: 0.108467
[12:53:18.161] iteration 7927: total_loss: 0.205228, loss_sup: 0.073325, loss_mps: 0.045438, loss_cps: 0.086465
[12:53:18.307] iteration 7928: total_loss: 0.141349, loss_sup: 0.032631, loss_mps: 0.039643, loss_cps: 0.069074
[12:53:18.454] iteration 7929: total_loss: 0.204417, loss_sup: 0.056327, loss_mps: 0.049955, loss_cps: 0.098134
[12:53:18.600] iteration 7930: total_loss: 0.280525, loss_sup: 0.138865, loss_mps: 0.047863, loss_cps: 0.093797
[12:53:18.746] iteration 7931: total_loss: 0.219437, loss_sup: 0.033388, loss_mps: 0.064697, loss_cps: 0.121352
[12:53:18.893] iteration 7932: total_loss: 0.236406, loss_sup: 0.094429, loss_mps: 0.047755, loss_cps: 0.094222
[12:53:19.039] iteration 7933: total_loss: 0.297838, loss_sup: 0.171757, loss_mps: 0.043902, loss_cps: 0.082179
[12:53:19.186] iteration 7934: total_loss: 0.237146, loss_sup: 0.054236, loss_mps: 0.058696, loss_cps: 0.124214
[12:53:19.332] iteration 7935: total_loss: 0.188298, loss_sup: 0.076979, loss_mps: 0.040177, loss_cps: 0.071142
[12:53:19.478] iteration 7936: total_loss: 0.290256, loss_sup: 0.127499, loss_mps: 0.056198, loss_cps: 0.106559
[12:53:19.625] iteration 7937: total_loss: 0.229554, loss_sup: 0.084213, loss_mps: 0.049264, loss_cps: 0.096077
[12:53:19.771] iteration 7938: total_loss: 0.104649, loss_sup: 0.030026, loss_mps: 0.029239, loss_cps: 0.045384
[12:53:19.917] iteration 7939: total_loss: 0.267466, loss_sup: 0.113070, loss_mps: 0.054038, loss_cps: 0.100359
[12:53:20.063] iteration 7940: total_loss: 0.199912, loss_sup: 0.030314, loss_mps: 0.057197, loss_cps: 0.112401
[12:53:20.209] iteration 7941: total_loss: 0.179828, loss_sup: 0.050552, loss_mps: 0.043425, loss_cps: 0.085850
[12:53:20.274] iteration 7942: total_loss: 0.463388, loss_sup: 0.277248, loss_mps: 0.062516, loss_cps: 0.123623
[12:53:21.488] iteration 7943: total_loss: 0.223073, loss_sup: 0.073729, loss_mps: 0.051913, loss_cps: 0.097431
[12:53:21.636] iteration 7944: total_loss: 0.187827, loss_sup: 0.076392, loss_mps: 0.038587, loss_cps: 0.072848
[12:53:21.783] iteration 7945: total_loss: 0.280830, loss_sup: 0.097415, loss_mps: 0.062518, loss_cps: 0.120896
[12:53:21.933] iteration 7946: total_loss: 0.139374, loss_sup: 0.019397, loss_mps: 0.040600, loss_cps: 0.079378
[12:53:22.081] iteration 7947: total_loss: 0.090463, loss_sup: 0.014622, loss_mps: 0.027764, loss_cps: 0.048077
[12:53:22.227] iteration 7948: total_loss: 0.362891, loss_sup: 0.150381, loss_mps: 0.068252, loss_cps: 0.144259
[12:53:22.373] iteration 7949: total_loss: 0.262173, loss_sup: 0.107772, loss_mps: 0.053314, loss_cps: 0.101087
[12:53:22.522] iteration 7950: total_loss: 0.228703, loss_sup: 0.096314, loss_mps: 0.044925, loss_cps: 0.087465
[12:53:22.668] iteration 7951: total_loss: 0.110115, loss_sup: 0.012810, loss_mps: 0.034597, loss_cps: 0.062707
[12:53:22.816] iteration 7952: total_loss: 0.133446, loss_sup: 0.020927, loss_mps: 0.040238, loss_cps: 0.072282
[12:53:22.962] iteration 7953: total_loss: 0.199591, loss_sup: 0.071167, loss_mps: 0.045599, loss_cps: 0.082825
[12:53:23.113] iteration 7954: total_loss: 0.296035, loss_sup: 0.146372, loss_mps: 0.052342, loss_cps: 0.097321
[12:53:23.260] iteration 7955: total_loss: 0.217156, loss_sup: 0.039536, loss_mps: 0.057294, loss_cps: 0.120325
[12:53:23.408] iteration 7956: total_loss: 0.210527, loss_sup: 0.017302, loss_mps: 0.063907, loss_cps: 0.129318
[12:53:23.557] iteration 7957: total_loss: 0.295739, loss_sup: 0.189944, loss_mps: 0.038922, loss_cps: 0.066872
[12:53:23.703] iteration 7958: total_loss: 0.162800, loss_sup: 0.054983, loss_mps: 0.037998, loss_cps: 0.069819
[12:53:23.850] iteration 7959: total_loss: 0.267562, loss_sup: 0.124115, loss_mps: 0.050899, loss_cps: 0.092548
[12:53:23.997] iteration 7960: total_loss: 0.217785, loss_sup: 0.092894, loss_mps: 0.043461, loss_cps: 0.081429
[12:53:24.143] iteration 7961: total_loss: 0.307312, loss_sup: 0.100869, loss_mps: 0.068832, loss_cps: 0.137612
[12:53:24.290] iteration 7962: total_loss: 0.124376, loss_sup: 0.038498, loss_mps: 0.031768, loss_cps: 0.054110
[12:53:24.438] iteration 7963: total_loss: 0.189507, loss_sup: 0.074660, loss_mps: 0.040228, loss_cps: 0.074619
[12:53:24.586] iteration 7964: total_loss: 0.184188, loss_sup: 0.040226, loss_mps: 0.049929, loss_cps: 0.094033
[12:53:24.733] iteration 7965: total_loss: 0.315997, loss_sup: 0.211084, loss_mps: 0.035013, loss_cps: 0.069899
[12:53:24.879] iteration 7966: total_loss: 0.155003, loss_sup: 0.023342, loss_mps: 0.044474, loss_cps: 0.087186
[12:53:25.027] iteration 7967: total_loss: 0.291178, loss_sup: 0.134628, loss_mps: 0.053358, loss_cps: 0.103192
[12:53:25.175] iteration 7968: total_loss: 0.138625, loss_sup: 0.030099, loss_mps: 0.040655, loss_cps: 0.067871
[12:53:25.321] iteration 7969: total_loss: 0.235791, loss_sup: 0.099680, loss_mps: 0.044412, loss_cps: 0.091699
[12:53:25.467] iteration 7970: total_loss: 0.239133, loss_sup: 0.049547, loss_mps: 0.060913, loss_cps: 0.128673
[12:53:25.613] iteration 7971: total_loss: 0.130282, loss_sup: 0.021821, loss_mps: 0.039176, loss_cps: 0.069285
[12:53:25.760] iteration 7972: total_loss: 0.259483, loss_sup: 0.116849, loss_mps: 0.049647, loss_cps: 0.092986
[12:53:25.907] iteration 7973: total_loss: 0.449453, loss_sup: 0.133201, loss_mps: 0.100442, loss_cps: 0.215810
[12:53:26.053] iteration 7974: total_loss: 0.260152, loss_sup: 0.140005, loss_mps: 0.039130, loss_cps: 0.081016
[12:53:26.199] iteration 7975: total_loss: 0.298452, loss_sup: 0.215543, loss_mps: 0.030531, loss_cps: 0.052377
[12:53:26.345] iteration 7976: total_loss: 0.270423, loss_sup: 0.098621, loss_mps: 0.057273, loss_cps: 0.114530
[12:53:26.492] iteration 7977: total_loss: 0.103246, loss_sup: 0.016507, loss_mps: 0.031775, loss_cps: 0.054963
[12:53:26.641] iteration 7978: total_loss: 0.209137, loss_sup: 0.099590, loss_mps: 0.038280, loss_cps: 0.071267
[12:53:26.787] iteration 7979: total_loss: 0.124770, loss_sup: 0.016353, loss_mps: 0.037834, loss_cps: 0.070583
[12:53:26.934] iteration 7980: total_loss: 0.177011, loss_sup: 0.046396, loss_mps: 0.046750, loss_cps: 0.083865
[12:53:27.080] iteration 7981: total_loss: 0.213055, loss_sup: 0.051606, loss_mps: 0.054260, loss_cps: 0.107189
[12:53:27.228] iteration 7982: total_loss: 0.224816, loss_sup: 0.089487, loss_mps: 0.046701, loss_cps: 0.088628
[12:53:27.374] iteration 7983: total_loss: 0.250257, loss_sup: 0.062763, loss_mps: 0.060662, loss_cps: 0.126832
[12:53:27.521] iteration 7984: total_loss: 0.273027, loss_sup: 0.091528, loss_mps: 0.058491, loss_cps: 0.123008
[12:53:27.667] iteration 7985: total_loss: 0.493661, loss_sup: 0.361671, loss_mps: 0.045288, loss_cps: 0.086702
[12:53:27.813] iteration 7986: total_loss: 0.204090, loss_sup: 0.066539, loss_mps: 0.045405, loss_cps: 0.092147
[12:53:27.959] iteration 7987: total_loss: 0.208190, loss_sup: 0.039284, loss_mps: 0.058995, loss_cps: 0.109910
[12:53:28.106] iteration 7988: total_loss: 0.313753, loss_sup: 0.148628, loss_mps: 0.056870, loss_cps: 0.108255
[12:53:28.252] iteration 7989: total_loss: 0.307410, loss_sup: 0.075459, loss_mps: 0.072999, loss_cps: 0.158953
[12:53:28.399] iteration 7990: total_loss: 0.271495, loss_sup: 0.091775, loss_mps: 0.058744, loss_cps: 0.120977
[12:53:28.547] iteration 7991: total_loss: 0.243649, loss_sup: 0.077585, loss_mps: 0.055531, loss_cps: 0.110533
[12:53:28.694] iteration 7992: total_loss: 0.316620, loss_sup: 0.179180, loss_mps: 0.048077, loss_cps: 0.089364
[12:53:28.842] iteration 7993: total_loss: 0.180834, loss_sup: 0.057006, loss_mps: 0.042823, loss_cps: 0.081005
[12:53:28.990] iteration 7994: total_loss: 0.148290, loss_sup: 0.061769, loss_mps: 0.032160, loss_cps: 0.054361
[12:53:29.136] iteration 7995: total_loss: 0.254960, loss_sup: 0.103325, loss_mps: 0.051226, loss_cps: 0.100409
[12:53:29.284] iteration 7996: total_loss: 0.144036, loss_sup: 0.043167, loss_mps: 0.035026, loss_cps: 0.065843
[12:53:29.433] iteration 7997: total_loss: 0.242209, loss_sup: 0.104866, loss_mps: 0.047051, loss_cps: 0.090292
[12:53:29.580] iteration 7998: total_loss: 0.266501, loss_sup: 0.050875, loss_mps: 0.070482, loss_cps: 0.145144
[12:53:29.727] iteration 7999: total_loss: 0.302500, loss_sup: 0.150523, loss_mps: 0.051637, loss_cps: 0.100340
[12:53:29.873] iteration 8000: total_loss: 0.260318, loss_sup: 0.107594, loss_mps: 0.050846, loss_cps: 0.101877
[12:53:29.874] Evaluation Started ==>
[12:53:41.263] ==> valid iteration 8000: unet metrics: {'dc': 0.6178096987851057, 'jc': 0.4920687653935009, 'pre': 0.6663114051121983, 'hd': 6.34524841475918}, ynet metrics: {'dc': 0.5592479270558091, 'jc': 0.4406041484991201, 'pre': 0.7582917608340692, 'hd': 5.9015842418834445}.
[12:53:41.265] Evaluation Finished!⏹️
[12:53:41.416] iteration 8001: total_loss: 0.095733, loss_sup: 0.017970, loss_mps: 0.029880, loss_cps: 0.047883
[12:53:41.563] iteration 8002: total_loss: 0.226734, loss_sup: 0.118278, loss_mps: 0.039418, loss_cps: 0.069039
[12:53:41.708] iteration 8003: total_loss: 0.176174, loss_sup: 0.065927, loss_mps: 0.039409, loss_cps: 0.070837
[12:53:41.855] iteration 8004: total_loss: 0.235840, loss_sup: 0.079762, loss_mps: 0.051941, loss_cps: 0.104137
[12:53:42.006] iteration 8005: total_loss: 0.246509, loss_sup: 0.132232, loss_mps: 0.039762, loss_cps: 0.074516
[12:53:42.153] iteration 8006: total_loss: 0.171210, loss_sup: 0.048551, loss_mps: 0.044108, loss_cps: 0.078552
[12:53:42.298] iteration 8007: total_loss: 0.347046, loss_sup: 0.187596, loss_mps: 0.055052, loss_cps: 0.104398
[12:53:42.444] iteration 8008: total_loss: 0.178291, loss_sup: 0.033255, loss_mps: 0.048886, loss_cps: 0.096150
[12:53:42.591] iteration 8009: total_loss: 0.220268, loss_sup: 0.104260, loss_mps: 0.041845, loss_cps: 0.074163
[12:53:42.739] iteration 8010: total_loss: 0.315364, loss_sup: 0.111175, loss_mps: 0.065671, loss_cps: 0.138518
[12:53:42.885] iteration 8011: total_loss: 0.128194, loss_sup: 0.025553, loss_mps: 0.036507, loss_cps: 0.066134
[12:53:43.031] iteration 8012: total_loss: 0.207790, loss_sup: 0.134557, loss_mps: 0.027847, loss_cps: 0.045385
[12:53:43.176] iteration 8013: total_loss: 0.221740, loss_sup: 0.090965, loss_mps: 0.046109, loss_cps: 0.084666
[12:53:43.322] iteration 8014: total_loss: 0.289672, loss_sup: 0.163048, loss_mps: 0.044015, loss_cps: 0.082609
[12:53:43.467] iteration 8015: total_loss: 0.250717, loss_sup: 0.054631, loss_mps: 0.062451, loss_cps: 0.133635
[12:53:43.612] iteration 8016: total_loss: 0.380022, loss_sup: 0.215115, loss_mps: 0.054035, loss_cps: 0.110873
[12:53:43.761] iteration 8017: total_loss: 0.246804, loss_sup: 0.062491, loss_mps: 0.057808, loss_cps: 0.126505
[12:53:43.906] iteration 8018: total_loss: 0.225882, loss_sup: 0.095854, loss_mps: 0.045515, loss_cps: 0.084513
[12:53:44.052] iteration 8019: total_loss: 0.285597, loss_sup: 0.130921, loss_mps: 0.053616, loss_cps: 0.101060
[12:53:44.197] iteration 8020: total_loss: 0.160254, loss_sup: 0.023905, loss_mps: 0.049116, loss_cps: 0.087233
[12:53:44.343] iteration 8021: total_loss: 0.150099, loss_sup: 0.048830, loss_mps: 0.036352, loss_cps: 0.064916
[12:53:44.489] iteration 8022: total_loss: 0.544879, loss_sup: 0.230207, loss_mps: 0.101712, loss_cps: 0.212960
[12:53:44.637] iteration 8023: total_loss: 0.168433, loss_sup: 0.066042, loss_mps: 0.039323, loss_cps: 0.063067
[12:53:44.783] iteration 8024: total_loss: 0.213727, loss_sup: 0.083156, loss_mps: 0.045562, loss_cps: 0.085010
[12:53:44.928] iteration 8025: total_loss: 0.376633, loss_sup: 0.101193, loss_mps: 0.084249, loss_cps: 0.191192
[12:53:45.074] iteration 8026: total_loss: 0.135447, loss_sup: 0.043204, loss_mps: 0.032656, loss_cps: 0.059586
[12:53:45.220] iteration 8027: total_loss: 0.222995, loss_sup: 0.050099, loss_mps: 0.056630, loss_cps: 0.116265
[12:53:45.365] iteration 8028: total_loss: 0.329243, loss_sup: 0.164207, loss_mps: 0.054895, loss_cps: 0.110141
[12:53:45.511] iteration 8029: total_loss: 0.220551, loss_sup: 0.086110, loss_mps: 0.048943, loss_cps: 0.085498
[12:53:45.657] iteration 8030: total_loss: 0.205174, loss_sup: 0.095688, loss_mps: 0.039903, loss_cps: 0.069584
[12:53:45.805] iteration 8031: total_loss: 0.141491, loss_sup: 0.050688, loss_mps: 0.033466, loss_cps: 0.057336
[12:53:45.952] iteration 8032: total_loss: 0.096446, loss_sup: 0.020464, loss_mps: 0.028948, loss_cps: 0.047034
[12:53:46.097] iteration 8033: total_loss: 0.202419, loss_sup: 0.090005, loss_mps: 0.039221, loss_cps: 0.073193
[12:53:46.243] iteration 8034: total_loss: 0.411269, loss_sup: 0.191397, loss_mps: 0.069076, loss_cps: 0.150796
[12:53:46.389] iteration 8035: total_loss: 0.256272, loss_sup: 0.131852, loss_mps: 0.044234, loss_cps: 0.080185
[12:53:46.535] iteration 8036: total_loss: 0.297505, loss_sup: 0.115264, loss_mps: 0.061231, loss_cps: 0.121010
[12:53:46.680] iteration 8037: total_loss: 0.152476, loss_sup: 0.054806, loss_mps: 0.038227, loss_cps: 0.059443
[12:53:46.827] iteration 8038: total_loss: 0.281454, loss_sup: 0.129158, loss_mps: 0.050295, loss_cps: 0.102001
[12:53:46.972] iteration 8039: total_loss: 0.176926, loss_sup: 0.042922, loss_mps: 0.046875, loss_cps: 0.087130
[12:53:47.118] iteration 8040: total_loss: 0.413222, loss_sup: 0.254510, loss_mps: 0.055039, loss_cps: 0.103673
[12:53:47.264] iteration 8041: total_loss: 0.224419, loss_sup: 0.123065, loss_mps: 0.037102, loss_cps: 0.064252
[12:53:47.412] iteration 8042: total_loss: 0.204794, loss_sup: 0.042549, loss_mps: 0.054551, loss_cps: 0.107694
[12:53:47.559] iteration 8043: total_loss: 0.200138, loss_sup: 0.073144, loss_mps: 0.046316, loss_cps: 0.080678
[12:53:47.704] iteration 8044: total_loss: 0.162335, loss_sup: 0.038505, loss_mps: 0.043190, loss_cps: 0.080640
[12:53:47.850] iteration 8045: total_loss: 0.231104, loss_sup: 0.125211, loss_mps: 0.039510, loss_cps: 0.066383
[12:53:47.996] iteration 8046: total_loss: 0.166556, loss_sup: 0.040382, loss_mps: 0.044410, loss_cps: 0.081764
[12:53:48.142] iteration 8047: total_loss: 0.190849, loss_sup: 0.083289, loss_mps: 0.039444, loss_cps: 0.068117
[12:53:48.291] iteration 8048: total_loss: 0.148845, loss_sup: 0.025110, loss_mps: 0.043311, loss_cps: 0.080424
[12:53:48.437] iteration 8049: total_loss: 0.172841, loss_sup: 0.035709, loss_mps: 0.048335, loss_cps: 0.088797
[12:53:48.582] iteration 8050: total_loss: 0.134082, loss_sup: 0.059820, loss_mps: 0.028604, loss_cps: 0.045658
[12:53:48.728] iteration 8051: total_loss: 0.207415, loss_sup: 0.068657, loss_mps: 0.046167, loss_cps: 0.092591
[12:53:48.873] iteration 8052: total_loss: 0.235641, loss_sup: 0.126708, loss_mps: 0.039583, loss_cps: 0.069349
[12:53:49.019] iteration 8053: total_loss: 0.166545, loss_sup: 0.027946, loss_mps: 0.047435, loss_cps: 0.091163
[12:53:49.167] iteration 8054: total_loss: 0.261567, loss_sup: 0.087657, loss_mps: 0.055190, loss_cps: 0.118720
[12:53:49.315] iteration 8055: total_loss: 0.373479, loss_sup: 0.129070, loss_mps: 0.076709, loss_cps: 0.167699
[12:53:49.461] iteration 8056: total_loss: 0.296909, loss_sup: 0.123198, loss_mps: 0.056247, loss_cps: 0.117464
[12:53:49.607] iteration 8057: total_loss: 0.123322, loss_sup: 0.015015, loss_mps: 0.039302, loss_cps: 0.069004
[12:53:49.755] iteration 8058: total_loss: 0.158350, loss_sup: 0.013429, loss_mps: 0.048222, loss_cps: 0.096700
[12:53:49.900] iteration 8059: total_loss: 0.146024, loss_sup: 0.032103, loss_mps: 0.040314, loss_cps: 0.073607
[12:53:50.046] iteration 8060: total_loss: 0.396707, loss_sup: 0.144846, loss_mps: 0.080165, loss_cps: 0.171697
[12:53:50.193] iteration 8061: total_loss: 0.190072, loss_sup: 0.057989, loss_mps: 0.045492, loss_cps: 0.086590
[12:53:50.339] iteration 8062: total_loss: 0.150999, loss_sup: 0.044482, loss_mps: 0.037025, loss_cps: 0.069492
[12:53:50.486] iteration 8063: total_loss: 0.317301, loss_sup: 0.145084, loss_mps: 0.055793, loss_cps: 0.116424
[12:53:50.632] iteration 8064: total_loss: 0.237592, loss_sup: 0.133550, loss_mps: 0.036534, loss_cps: 0.067507
[12:53:50.778] iteration 8065: total_loss: 0.164940, loss_sup: 0.029771, loss_mps: 0.045840, loss_cps: 0.089329
[12:53:50.924] iteration 8066: total_loss: 0.186847, loss_sup: 0.051902, loss_mps: 0.045224, loss_cps: 0.089721
[12:53:51.070] iteration 8067: total_loss: 0.389360, loss_sup: 0.249124, loss_mps: 0.047351, loss_cps: 0.092885
[12:53:51.216] iteration 8068: total_loss: 0.153360, loss_sup: 0.025797, loss_mps: 0.042846, loss_cps: 0.084717
[12:53:51.362] iteration 8069: total_loss: 0.239113, loss_sup: 0.050895, loss_mps: 0.061648, loss_cps: 0.126570
[12:53:51.509] iteration 8070: total_loss: 0.587055, loss_sup: 0.414995, loss_mps: 0.055825, loss_cps: 0.116234
[12:53:51.654] iteration 8071: total_loss: 0.174870, loss_sup: 0.024290, loss_mps: 0.048084, loss_cps: 0.102496
[12:53:51.801] iteration 8072: total_loss: 0.211554, loss_sup: 0.076836, loss_mps: 0.047461, loss_cps: 0.087258
[12:53:51.947] iteration 8073: total_loss: 0.232485, loss_sup: 0.062760, loss_mps: 0.057274, loss_cps: 0.112451
[12:53:52.093] iteration 8074: total_loss: 0.325309, loss_sup: 0.158918, loss_mps: 0.053877, loss_cps: 0.112514
[12:53:52.242] iteration 8075: total_loss: 0.182145, loss_sup: 0.048162, loss_mps: 0.046422, loss_cps: 0.087560
[12:53:52.388] iteration 8076: total_loss: 0.189937, loss_sup: 0.074227, loss_mps: 0.041862, loss_cps: 0.073848
[12:53:52.534] iteration 8077: total_loss: 0.203732, loss_sup: 0.046836, loss_mps: 0.054990, loss_cps: 0.101906
[12:53:52.681] iteration 8078: total_loss: 0.447804, loss_sup: 0.260995, loss_mps: 0.059714, loss_cps: 0.127094
[12:53:52.826] iteration 8079: total_loss: 0.201538, loss_sup: 0.036730, loss_mps: 0.055240, loss_cps: 0.109568
[12:53:52.972] iteration 8080: total_loss: 0.353701, loss_sup: 0.248092, loss_mps: 0.038604, loss_cps: 0.067004
[12:53:53.118] iteration 8081: total_loss: 0.254404, loss_sup: 0.108170, loss_mps: 0.050129, loss_cps: 0.096105
[12:53:53.265] iteration 8082: total_loss: 0.286946, loss_sup: 0.132087, loss_mps: 0.051860, loss_cps: 0.102999
[12:53:53.410] iteration 8083: total_loss: 0.216086, loss_sup: 0.112539, loss_mps: 0.037752, loss_cps: 0.065796
[12:53:53.558] iteration 8084: total_loss: 0.135692, loss_sup: 0.028153, loss_mps: 0.039483, loss_cps: 0.068056
[12:53:53.704] iteration 8085: total_loss: 0.288269, loss_sup: 0.059691, loss_mps: 0.072296, loss_cps: 0.156282
[12:53:53.850] iteration 8086: total_loss: 0.228055, loss_sup: 0.082650, loss_mps: 0.050026, loss_cps: 0.095379
[12:53:53.997] iteration 8087: total_loss: 0.329460, loss_sup: 0.200433, loss_mps: 0.046490, loss_cps: 0.082537
[12:53:54.143] iteration 8088: total_loss: 0.291462, loss_sup: 0.112563, loss_mps: 0.061920, loss_cps: 0.116978
[12:53:54.290] iteration 8089: total_loss: 0.247791, loss_sup: 0.088377, loss_mps: 0.056966, loss_cps: 0.102449
[12:53:54.437] iteration 8090: total_loss: 0.264467, loss_sup: 0.058316, loss_mps: 0.066122, loss_cps: 0.140030
[12:53:54.583] iteration 8091: total_loss: 0.247782, loss_sup: 0.115328, loss_mps: 0.047278, loss_cps: 0.085177
[12:53:54.729] iteration 8092: total_loss: 0.336506, loss_sup: 0.166679, loss_mps: 0.056047, loss_cps: 0.113781
[12:53:54.875] iteration 8093: total_loss: 0.164007, loss_sup: 0.065870, loss_mps: 0.035199, loss_cps: 0.062938
[12:53:55.023] iteration 8094: total_loss: 0.282074, loss_sup: 0.127852, loss_mps: 0.052391, loss_cps: 0.101831
[12:53:55.169] iteration 8095: total_loss: 0.191827, loss_sup: 0.057114, loss_mps: 0.046837, loss_cps: 0.087876
[12:53:55.318] iteration 8096: total_loss: 0.169222, loss_sup: 0.060985, loss_mps: 0.039752, loss_cps: 0.068485
[12:53:55.465] iteration 8097: total_loss: 0.228718, loss_sup: 0.092109, loss_mps: 0.046486, loss_cps: 0.090123
[12:53:55.612] iteration 8098: total_loss: 0.163112, loss_sup: 0.041115, loss_mps: 0.043114, loss_cps: 0.078883
[12:53:55.758] iteration 8099: total_loss: 0.316885, loss_sup: 0.232148, loss_mps: 0.031348, loss_cps: 0.053389
[12:53:55.905] iteration 8100: total_loss: 0.209379, loss_sup: 0.056626, loss_mps: 0.053288, loss_cps: 0.099466
[12:53:55.905] Evaluation Started ==>
[12:54:07.251] ==> valid iteration 8100: unet metrics: {'dc': 0.6342307715701836, 'jc': 0.5107662040543883, 'pre': 0.7191614989791285, 'hd': 6.009181716046039}, ynet metrics: {'dc': 0.5654046654081123, 'jc': 0.45087725748287283, 'pre': 0.7385551550956868, 'hd': 6.04375234738195}.
[12:54:07.253] Evaluation Finished!⏹️
[12:54:07.407] iteration 8101: total_loss: 0.182416, loss_sup: 0.037479, loss_mps: 0.051545, loss_cps: 0.093393
[12:54:07.554] iteration 8102: total_loss: 0.341345, loss_sup: 0.202480, loss_mps: 0.048897, loss_cps: 0.089968
[12:54:07.700] iteration 8103: total_loss: 0.361625, loss_sup: 0.212439, loss_mps: 0.054239, loss_cps: 0.094947
[12:54:07.845] iteration 8104: total_loss: 0.165976, loss_sup: 0.061087, loss_mps: 0.039132, loss_cps: 0.065756
[12:54:07.990] iteration 8105: total_loss: 0.159571, loss_sup: 0.009015, loss_mps: 0.051813, loss_cps: 0.098743
[12:54:08.136] iteration 8106: total_loss: 0.257128, loss_sup: 0.104653, loss_mps: 0.053537, loss_cps: 0.098937
[12:54:08.281] iteration 8107: total_loss: 0.456180, loss_sup: 0.268479, loss_mps: 0.064015, loss_cps: 0.123686
[12:54:08.426] iteration 8108: total_loss: 0.317924, loss_sup: 0.179952, loss_mps: 0.049032, loss_cps: 0.088940
[12:54:08.571] iteration 8109: total_loss: 0.183379, loss_sup: 0.052236, loss_mps: 0.045974, loss_cps: 0.085169
[12:54:08.717] iteration 8110: total_loss: 0.315986, loss_sup: 0.198939, loss_mps: 0.042481, loss_cps: 0.074566
[12:54:08.863] iteration 8111: total_loss: 0.172389, loss_sup: 0.063435, loss_mps: 0.040941, loss_cps: 0.068013
[12:54:09.009] iteration 8112: total_loss: 0.207636, loss_sup: 0.062455, loss_mps: 0.050987, loss_cps: 0.094193
[12:54:09.157] iteration 8113: total_loss: 0.292883, loss_sup: 0.152859, loss_mps: 0.049427, loss_cps: 0.090597
[12:54:09.303] iteration 8114: total_loss: 0.257300, loss_sup: 0.105231, loss_mps: 0.053517, loss_cps: 0.098551
[12:54:09.451] iteration 8115: total_loss: 0.171094, loss_sup: 0.040836, loss_mps: 0.046455, loss_cps: 0.083803
[12:54:09.597] iteration 8116: total_loss: 0.321961, loss_sup: 0.075398, loss_mps: 0.080892, loss_cps: 0.165670
[12:54:09.743] iteration 8117: total_loss: 0.376054, loss_sup: 0.161742, loss_mps: 0.071059, loss_cps: 0.143253
[12:54:09.890] iteration 8118: total_loss: 0.332496, loss_sup: 0.164715, loss_mps: 0.057521, loss_cps: 0.110259
[12:54:10.036] iteration 8119: total_loss: 0.250489, loss_sup: 0.127616, loss_mps: 0.045466, loss_cps: 0.077407
[12:54:10.182] iteration 8120: total_loss: 0.204475, loss_sup: 0.044371, loss_mps: 0.057795, loss_cps: 0.102309
[12:54:10.327] iteration 8121: total_loss: 0.207568, loss_sup: 0.048734, loss_mps: 0.055378, loss_cps: 0.103455
[12:54:10.473] iteration 8122: total_loss: 0.218169, loss_sup: 0.067181, loss_mps: 0.055366, loss_cps: 0.095622
[12:54:10.618] iteration 8123: total_loss: 0.307835, loss_sup: 0.139984, loss_mps: 0.056531, loss_cps: 0.111319
[12:54:10.763] iteration 8124: total_loss: 0.198116, loss_sup: 0.098171, loss_mps: 0.038134, loss_cps: 0.061811
[12:54:10.909] iteration 8125: total_loss: 0.170990, loss_sup: 0.030492, loss_mps: 0.049874, loss_cps: 0.090624
[12:54:11.055] iteration 8126: total_loss: 0.230501, loss_sup: 0.072402, loss_mps: 0.057576, loss_cps: 0.100524
[12:54:11.201] iteration 8127: total_loss: 0.276747, loss_sup: 0.135778, loss_mps: 0.049250, loss_cps: 0.091719
[12:54:11.346] iteration 8128: total_loss: 0.285136, loss_sup: 0.164828, loss_mps: 0.044967, loss_cps: 0.075341
[12:54:11.492] iteration 8129: total_loss: 0.235974, loss_sup: 0.136045, loss_mps: 0.037709, loss_cps: 0.062221
[12:54:11.638] iteration 8130: total_loss: 0.310881, loss_sup: 0.164733, loss_mps: 0.050068, loss_cps: 0.096080
[12:54:11.783] iteration 8131: total_loss: 0.162834, loss_sup: 0.052778, loss_mps: 0.041514, loss_cps: 0.068542
[12:54:11.928] iteration 8132: total_loss: 0.227312, loss_sup: 0.055230, loss_mps: 0.057510, loss_cps: 0.114572
[12:54:12.075] iteration 8133: total_loss: 0.341378, loss_sup: 0.176808, loss_mps: 0.056898, loss_cps: 0.107672
[12:54:12.220] iteration 8134: total_loss: 0.240202, loss_sup: 0.107747, loss_mps: 0.045953, loss_cps: 0.086503
[12:54:12.368] iteration 8135: total_loss: 0.131710, loss_sup: 0.043601, loss_mps: 0.033114, loss_cps: 0.054996
[12:54:12.513] iteration 8136: total_loss: 0.340493, loss_sup: 0.248364, loss_mps: 0.034554, loss_cps: 0.057575
[12:54:12.660] iteration 8137: total_loss: 0.166177, loss_sup: 0.055238, loss_mps: 0.039974, loss_cps: 0.070966
[12:54:12.806] iteration 8138: total_loss: 0.246996, loss_sup: 0.125529, loss_mps: 0.043679, loss_cps: 0.077788
[12:54:12.952] iteration 8139: total_loss: 0.141673, loss_sup: 0.038492, loss_mps: 0.038228, loss_cps: 0.064953
[12:54:13.098] iteration 8140: total_loss: 0.182893, loss_sup: 0.081575, loss_mps: 0.038424, loss_cps: 0.062895
[12:54:13.243] iteration 8141: total_loss: 0.206228, loss_sup: 0.084053, loss_mps: 0.044428, loss_cps: 0.077747
[12:54:13.388] iteration 8142: total_loss: 0.251524, loss_sup: 0.120802, loss_mps: 0.044851, loss_cps: 0.085871
[12:54:13.533] iteration 8143: total_loss: 0.243898, loss_sup: 0.125078, loss_mps: 0.041904, loss_cps: 0.076916
[12:54:13.678] iteration 8144: total_loss: 0.254639, loss_sup: 0.069628, loss_mps: 0.061839, loss_cps: 0.123172
[12:54:13.823] iteration 8145: total_loss: 0.358196, loss_sup: 0.135845, loss_mps: 0.072488, loss_cps: 0.149862
[12:54:13.969] iteration 8146: total_loss: 0.227298, loss_sup: 0.041866, loss_mps: 0.061319, loss_cps: 0.124114
[12:54:14.115] iteration 8147: total_loss: 0.192944, loss_sup: 0.076126, loss_mps: 0.042595, loss_cps: 0.074224
[12:54:14.260] iteration 8148: total_loss: 0.185175, loss_sup: 0.058792, loss_mps: 0.044343, loss_cps: 0.082040
[12:54:14.405] iteration 8149: total_loss: 0.244563, loss_sup: 0.090092, loss_mps: 0.055215, loss_cps: 0.099256
[12:54:14.551] iteration 8150: total_loss: 0.156096, loss_sup: 0.060657, loss_mps: 0.034020, loss_cps: 0.061419
[12:54:14.697] iteration 8151: total_loss: 0.130189, loss_sup: 0.040547, loss_mps: 0.033956, loss_cps: 0.055685
[12:54:14.842] iteration 8152: total_loss: 0.215359, loss_sup: 0.052331, loss_mps: 0.054967, loss_cps: 0.108061
[12:54:14.987] iteration 8153: total_loss: 0.261603, loss_sup: 0.126331, loss_mps: 0.047875, loss_cps: 0.087396
[12:54:15.134] iteration 8154: total_loss: 0.138904, loss_sup: 0.011800, loss_mps: 0.044796, loss_cps: 0.082309
[12:54:15.279] iteration 8155: total_loss: 0.155491, loss_sup: 0.030511, loss_mps: 0.043450, loss_cps: 0.081530
[12:54:15.424] iteration 8156: total_loss: 0.136250, loss_sup: 0.033706, loss_mps: 0.036622, loss_cps: 0.065922
[12:54:15.569] iteration 8157: total_loss: 0.274293, loss_sup: 0.110150, loss_mps: 0.053029, loss_cps: 0.111115
[12:54:15.715] iteration 8158: total_loss: 0.252806, loss_sup: 0.128299, loss_mps: 0.046309, loss_cps: 0.078198
[12:54:15.860] iteration 8159: total_loss: 0.303152, loss_sup: 0.161082, loss_mps: 0.048978, loss_cps: 0.093092
[12:54:16.005] iteration 8160: total_loss: 0.262414, loss_sup: 0.172289, loss_mps: 0.033350, loss_cps: 0.056774
[12:54:16.151] iteration 8161: total_loss: 0.305518, loss_sup: 0.095099, loss_mps: 0.067834, loss_cps: 0.142585
[12:54:16.297] iteration 8162: total_loss: 0.230825, loss_sup: 0.114254, loss_mps: 0.042237, loss_cps: 0.074333
[12:54:16.445] iteration 8163: total_loss: 0.262262, loss_sup: 0.147528, loss_mps: 0.040160, loss_cps: 0.074574
[12:54:16.591] iteration 8164: total_loss: 0.408826, loss_sup: 0.238549, loss_mps: 0.057829, loss_cps: 0.112448
[12:54:16.737] iteration 8165: total_loss: 0.140224, loss_sup: 0.030947, loss_mps: 0.038413, loss_cps: 0.070864
[12:54:16.882] iteration 8166: total_loss: 0.328927, loss_sup: 0.143402, loss_mps: 0.060490, loss_cps: 0.125035
[12:54:17.028] iteration 8167: total_loss: 0.246541, loss_sup: 0.095774, loss_mps: 0.050997, loss_cps: 0.099770
[12:54:17.174] iteration 8168: total_loss: 0.189581, loss_sup: 0.067637, loss_mps: 0.042054, loss_cps: 0.079890
[12:54:17.320] iteration 8169: total_loss: 0.278511, loss_sup: 0.109884, loss_mps: 0.058300, loss_cps: 0.110327
[12:54:17.466] iteration 8170: total_loss: 0.301223, loss_sup: 0.082622, loss_mps: 0.071742, loss_cps: 0.146859
[12:54:17.611] iteration 8171: total_loss: 0.300696, loss_sup: 0.114096, loss_mps: 0.059926, loss_cps: 0.126675
[12:54:17.757] iteration 8172: total_loss: 0.570980, loss_sup: 0.337011, loss_mps: 0.077813, loss_cps: 0.156156
[12:54:17.902] iteration 8173: total_loss: 0.186207, loss_sup: 0.058875, loss_mps: 0.045302, loss_cps: 0.082029
[12:54:18.049] iteration 8174: total_loss: 0.317819, loss_sup: 0.196227, loss_mps: 0.042205, loss_cps: 0.079387
[12:54:18.196] iteration 8175: total_loss: 0.182561, loss_sup: 0.038688, loss_mps: 0.049274, loss_cps: 0.094600
[12:54:18.342] iteration 8176: total_loss: 0.126016, loss_sup: 0.015737, loss_mps: 0.039037, loss_cps: 0.071241
[12:54:18.488] iteration 8177: total_loss: 0.197671, loss_sup: 0.111811, loss_mps: 0.032798, loss_cps: 0.053062
[12:54:18.633] iteration 8178: total_loss: 0.266267, loss_sup: 0.133769, loss_mps: 0.047142, loss_cps: 0.085356
[12:54:18.779] iteration 8179: total_loss: 0.212052, loss_sup: 0.073216, loss_mps: 0.047938, loss_cps: 0.090899
[12:54:18.925] iteration 8180: total_loss: 0.363381, loss_sup: 0.207673, loss_mps: 0.052111, loss_cps: 0.103598
[12:54:19.070] iteration 8181: total_loss: 0.296588, loss_sup: 0.112176, loss_mps: 0.061782, loss_cps: 0.122630
[12:54:19.217] iteration 8182: total_loss: 0.160625, loss_sup: 0.027139, loss_mps: 0.046795, loss_cps: 0.086691
[12:54:19.363] iteration 8183: total_loss: 0.323705, loss_sup: 0.183349, loss_mps: 0.048797, loss_cps: 0.091558
[12:54:19.509] iteration 8184: total_loss: 0.516369, loss_sup: 0.181672, loss_mps: 0.103470, loss_cps: 0.231227
[12:54:19.656] iteration 8185: total_loss: 0.223613, loss_sup: 0.064534, loss_mps: 0.055265, loss_cps: 0.103814
[12:54:19.801] iteration 8186: total_loss: 0.224214, loss_sup: 0.092490, loss_mps: 0.048179, loss_cps: 0.083545
[12:54:19.948] iteration 8187: total_loss: 0.159884, loss_sup: 0.059827, loss_mps: 0.036885, loss_cps: 0.063172
[12:54:20.094] iteration 8188: total_loss: 0.416330, loss_sup: 0.255969, loss_mps: 0.057006, loss_cps: 0.103355
[12:54:20.242] iteration 8189: total_loss: 0.235854, loss_sup: 0.112615, loss_mps: 0.046055, loss_cps: 0.077184
[12:54:20.389] iteration 8190: total_loss: 0.363747, loss_sup: 0.099969, loss_mps: 0.088851, loss_cps: 0.174927
[12:54:20.538] iteration 8191: total_loss: 0.184032, loss_sup: 0.048208, loss_mps: 0.051699, loss_cps: 0.084125
[12:54:20.684] iteration 8192: total_loss: 0.317365, loss_sup: 0.139423, loss_mps: 0.060198, loss_cps: 0.117744
[12:54:20.831] iteration 8193: total_loss: 0.309169, loss_sup: 0.162154, loss_mps: 0.052109, loss_cps: 0.094906
[12:54:20.977] iteration 8194: total_loss: 0.339754, loss_sup: 0.121118, loss_mps: 0.071339, loss_cps: 0.147296
[12:54:21.123] iteration 8195: total_loss: 0.151444, loss_sup: 0.028449, loss_mps: 0.045996, loss_cps: 0.076999
[12:54:21.270] iteration 8196: total_loss: 0.316969, loss_sup: 0.166269, loss_mps: 0.051558, loss_cps: 0.099142
[12:54:21.415] iteration 8197: total_loss: 0.204946, loss_sup: 0.089448, loss_mps: 0.042269, loss_cps: 0.073229
[12:54:21.561] iteration 8198: total_loss: 0.272073, loss_sup: 0.067390, loss_mps: 0.069425, loss_cps: 0.135258
[12:54:21.707] iteration 8199: total_loss: 0.164300, loss_sup: 0.051448, loss_mps: 0.040001, loss_cps: 0.072851
[12:54:21.853] iteration 8200: total_loss: 0.197468, loss_sup: 0.048250, loss_mps: 0.052670, loss_cps: 0.096549
[12:54:21.853] Evaluation Started ==>
[12:54:33.225] ==> valid iteration 8200: unet metrics: {'dc': 0.6096689592994375, 'jc': 0.4847185014756391, 'pre': 0.675084396125756, 'hd': 6.234808726482583}, ynet metrics: {'dc': 0.5778534579899135, 'jc': 0.4547173575209473, 'pre': 0.7071129999081551, 'hd': 6.202018074640467}.
[12:54:33.227] Evaluation Finished!⏹️
[12:54:33.376] iteration 8201: total_loss: 0.211709, loss_sup: 0.090517, loss_mps: 0.040362, loss_cps: 0.080830
[12:54:33.524] iteration 8202: total_loss: 0.552889, loss_sup: 0.329468, loss_mps: 0.070594, loss_cps: 0.152827
[12:54:33.670] iteration 8203: total_loss: 0.236226, loss_sup: 0.043181, loss_mps: 0.062847, loss_cps: 0.130197
[12:54:33.817] iteration 8204: total_loss: 0.159034, loss_sup: 0.041111, loss_mps: 0.042394, loss_cps: 0.075528
[12:54:33.964] iteration 8205: total_loss: 0.202604, loss_sup: 0.096736, loss_mps: 0.039405, loss_cps: 0.066462
[12:54:34.109] iteration 8206: total_loss: 0.176048, loss_sup: 0.049326, loss_mps: 0.045937, loss_cps: 0.080785
[12:54:34.254] iteration 8207: total_loss: 0.238320, loss_sup: 0.094213, loss_mps: 0.050512, loss_cps: 0.093595
[12:54:34.400] iteration 8208: total_loss: 0.181658, loss_sup: 0.079666, loss_mps: 0.038230, loss_cps: 0.063762
[12:54:34.545] iteration 8209: total_loss: 0.119955, loss_sup: 0.027756, loss_mps: 0.036113, loss_cps: 0.056086
[12:54:34.691] iteration 8210: total_loss: 0.276440, loss_sup: 0.133367, loss_mps: 0.049042, loss_cps: 0.094032
[12:54:34.839] iteration 8211: total_loss: 0.177289, loss_sup: 0.045353, loss_mps: 0.047328, loss_cps: 0.084609
[12:54:34.985] iteration 8212: total_loss: 0.281727, loss_sup: 0.116663, loss_mps: 0.054961, loss_cps: 0.110104
[12:54:35.132] iteration 8213: total_loss: 0.236385, loss_sup: 0.088325, loss_mps: 0.050974, loss_cps: 0.097086
[12:54:35.277] iteration 8214: total_loss: 0.241426, loss_sup: 0.075853, loss_mps: 0.055438, loss_cps: 0.110135
[12:54:35.423] iteration 8215: total_loss: 0.278056, loss_sup: 0.114365, loss_mps: 0.055123, loss_cps: 0.108568
[12:54:35.570] iteration 8216: total_loss: 0.120008, loss_sup: 0.018776, loss_mps: 0.036378, loss_cps: 0.064854
[12:54:35.715] iteration 8217: total_loss: 0.132230, loss_sup: 0.041555, loss_mps: 0.031907, loss_cps: 0.058768
[12:54:35.862] iteration 8218: total_loss: 0.215821, loss_sup: 0.061065, loss_mps: 0.053374, loss_cps: 0.101381
[12:54:36.013] iteration 8219: total_loss: 0.247419, loss_sup: 0.117364, loss_mps: 0.046499, loss_cps: 0.083556
[12:54:36.160] iteration 8220: total_loss: 0.360958, loss_sup: 0.120245, loss_mps: 0.076574, loss_cps: 0.164140
[12:54:36.306] iteration 8221: total_loss: 0.295451, loss_sup: 0.151475, loss_mps: 0.051586, loss_cps: 0.092390
[12:54:36.453] iteration 8222: total_loss: 0.145247, loss_sup: 0.022432, loss_mps: 0.043203, loss_cps: 0.079612
[12:54:36.598] iteration 8223: total_loss: 0.348779, loss_sup: 0.148577, loss_mps: 0.066870, loss_cps: 0.133332
[12:54:36.744] iteration 8224: total_loss: 0.377183, loss_sup: 0.177702, loss_mps: 0.066399, loss_cps: 0.133083
[12:54:36.890] iteration 8225: total_loss: 0.283828, loss_sup: 0.143243, loss_mps: 0.050126, loss_cps: 0.090459
[12:54:37.035] iteration 8226: total_loss: 0.127944, loss_sup: 0.028069, loss_mps: 0.035418, loss_cps: 0.064458
[12:54:37.181] iteration 8227: total_loss: 0.200607, loss_sup: 0.037878, loss_mps: 0.055326, loss_cps: 0.107403
[12:54:37.326] iteration 8228: total_loss: 0.128311, loss_sup: 0.008270, loss_mps: 0.043127, loss_cps: 0.076914
[12:54:37.472] iteration 8229: total_loss: 0.309526, loss_sup: 0.167112, loss_mps: 0.050351, loss_cps: 0.092062
[12:54:37.618] iteration 8230: total_loss: 0.290530, loss_sup: 0.072068, loss_mps: 0.070943, loss_cps: 0.147519
[12:54:37.764] iteration 8231: total_loss: 0.126713, loss_sup: 0.007720, loss_mps: 0.042673, loss_cps: 0.076319
[12:54:37.909] iteration 8232: total_loss: 0.250665, loss_sup: 0.080160, loss_mps: 0.056053, loss_cps: 0.114451
[12:54:38.055] iteration 8233: total_loss: 0.261865, loss_sup: 0.104829, loss_mps: 0.053994, loss_cps: 0.103042
[12:54:38.205] iteration 8234: total_loss: 0.290684, loss_sup: 0.136019, loss_mps: 0.053943, loss_cps: 0.100722
[12:54:38.351] iteration 8235: total_loss: 0.241318, loss_sup: 0.126018, loss_mps: 0.042920, loss_cps: 0.072380
[12:54:38.496] iteration 8236: total_loss: 0.187073, loss_sup: 0.066314, loss_mps: 0.044801, loss_cps: 0.075958
[12:54:38.643] iteration 8237: total_loss: 0.228440, loss_sup: 0.071471, loss_mps: 0.053047, loss_cps: 0.103922
[12:54:38.790] iteration 8238: total_loss: 0.166938, loss_sup: 0.049372, loss_mps: 0.045317, loss_cps: 0.072249
[12:54:38.936] iteration 8239: total_loss: 0.148134, loss_sup: 0.029274, loss_mps: 0.044800, loss_cps: 0.074060
[12:54:39.081] iteration 8240: total_loss: 0.273593, loss_sup: 0.129860, loss_mps: 0.049565, loss_cps: 0.094168
[12:54:39.227] iteration 8241: total_loss: 0.180629, loss_sup: 0.045432, loss_mps: 0.047146, loss_cps: 0.088051
[12:54:39.373] iteration 8242: total_loss: 0.170808, loss_sup: 0.041779, loss_mps: 0.047256, loss_cps: 0.081773
[12:54:39.518] iteration 8243: total_loss: 0.215028, loss_sup: 0.101783, loss_mps: 0.042137, loss_cps: 0.071109
[12:54:39.665] iteration 8244: total_loss: 0.414605, loss_sup: 0.232897, loss_mps: 0.060361, loss_cps: 0.121346
[12:54:39.810] iteration 8245: total_loss: 0.372073, loss_sup: 0.187604, loss_mps: 0.062317, loss_cps: 0.122152
[12:54:39.957] iteration 8246: total_loss: 0.205994, loss_sup: 0.039423, loss_mps: 0.058885, loss_cps: 0.107686
[12:54:40.104] iteration 8247: total_loss: 0.358275, loss_sup: 0.208780, loss_mps: 0.052163, loss_cps: 0.097332
[12:54:40.250] iteration 8248: total_loss: 0.238153, loss_sup: 0.103445, loss_mps: 0.046351, loss_cps: 0.088357
[12:54:40.395] iteration 8249: total_loss: 0.169676, loss_sup: 0.069139, loss_mps: 0.036941, loss_cps: 0.063597
[12:54:40.544] iteration 8250: total_loss: 0.393909, loss_sup: 0.213061, loss_mps: 0.061891, loss_cps: 0.118957
[12:54:40.690] iteration 8251: total_loss: 0.151510, loss_sup: 0.029759, loss_mps: 0.041923, loss_cps: 0.079828
[12:54:40.838] iteration 8252: total_loss: 0.228827, loss_sup: 0.084025, loss_mps: 0.052538, loss_cps: 0.092264
[12:54:40.985] iteration 8253: total_loss: 0.208570, loss_sup: 0.076632, loss_mps: 0.045719, loss_cps: 0.086219
[12:54:41.132] iteration 8254: total_loss: 0.097177, loss_sup: 0.012270, loss_mps: 0.031996, loss_cps: 0.052911
[12:54:41.277] iteration 8255: total_loss: 0.261907, loss_sup: 0.123941, loss_mps: 0.047493, loss_cps: 0.090473
[12:54:41.424] iteration 8256: total_loss: 0.268541, loss_sup: 0.044805, loss_mps: 0.072246, loss_cps: 0.151489
[12:54:41.569] iteration 8257: total_loss: 0.392053, loss_sup: 0.259804, loss_mps: 0.047044, loss_cps: 0.085205
[12:54:41.715] iteration 8258: total_loss: 0.242013, loss_sup: 0.140773, loss_mps: 0.036966, loss_cps: 0.064274
[12:54:41.864] iteration 8259: total_loss: 0.330448, loss_sup: 0.196130, loss_mps: 0.046329, loss_cps: 0.087989
[12:54:42.009] iteration 8260: total_loss: 0.258183, loss_sup: 0.089774, loss_mps: 0.055392, loss_cps: 0.113016
[12:54:42.155] iteration 8261: total_loss: 0.280082, loss_sup: 0.113069, loss_mps: 0.057122, loss_cps: 0.109891
[12:54:42.300] iteration 8262: total_loss: 0.237016, loss_sup: 0.108611, loss_mps: 0.046455, loss_cps: 0.081950
[12:54:42.449] iteration 8263: total_loss: 0.159351, loss_sup: 0.087059, loss_mps: 0.029201, loss_cps: 0.043092
[12:54:42.594] iteration 8264: total_loss: 0.140639, loss_sup: 0.054495, loss_mps: 0.032744, loss_cps: 0.053400
[12:54:42.740] iteration 8265: total_loss: 0.322461, loss_sup: 0.135254, loss_mps: 0.062427, loss_cps: 0.124781
[12:54:42.886] iteration 8266: total_loss: 0.235781, loss_sup: 0.124800, loss_mps: 0.040475, loss_cps: 0.070506
[12:54:43.032] iteration 8267: total_loss: 0.209328, loss_sup: 0.084110, loss_mps: 0.044372, loss_cps: 0.080846
[12:54:43.177] iteration 8268: total_loss: 0.216266, loss_sup: 0.070073, loss_mps: 0.049152, loss_cps: 0.097042
[12:54:43.323] iteration 8269: total_loss: 0.198131, loss_sup: 0.048037, loss_mps: 0.051119, loss_cps: 0.098975
[12:54:43.469] iteration 8270: total_loss: 0.158573, loss_sup: 0.032776, loss_mps: 0.043363, loss_cps: 0.082434
[12:54:43.614] iteration 8271: total_loss: 0.196230, loss_sup: 0.048097, loss_mps: 0.049942, loss_cps: 0.098191
[12:54:43.761] iteration 8272: total_loss: 0.267296, loss_sup: 0.089440, loss_mps: 0.061388, loss_cps: 0.116469
[12:54:43.907] iteration 8273: total_loss: 0.243836, loss_sup: 0.096801, loss_mps: 0.051612, loss_cps: 0.095423
[12:54:44.052] iteration 8274: total_loss: 0.165279, loss_sup: 0.022487, loss_mps: 0.051207, loss_cps: 0.091585
[12:54:44.199] iteration 8275: total_loss: 0.281519, loss_sup: 0.114747, loss_mps: 0.057413, loss_cps: 0.109358
[12:54:44.346] iteration 8276: total_loss: 0.499459, loss_sup: 0.309574, loss_mps: 0.063836, loss_cps: 0.126049
[12:54:44.492] iteration 8277: total_loss: 0.130426, loss_sup: 0.028475, loss_mps: 0.036650, loss_cps: 0.065301
[12:54:44.638] iteration 8278: total_loss: 0.172025, loss_sup: 0.053150, loss_mps: 0.042832, loss_cps: 0.076042
[12:54:44.785] iteration 8279: total_loss: 0.194282, loss_sup: 0.057084, loss_mps: 0.048288, loss_cps: 0.088910
[12:54:44.932] iteration 8280: total_loss: 0.359962, loss_sup: 0.194857, loss_mps: 0.056709, loss_cps: 0.108396
[12:54:45.082] iteration 8281: total_loss: 0.435281, loss_sup: 0.211775, loss_mps: 0.073506, loss_cps: 0.150000
[12:54:45.229] iteration 8282: total_loss: 0.159765, loss_sup: 0.028604, loss_mps: 0.045771, loss_cps: 0.085390
[12:54:45.377] iteration 8283: total_loss: 0.156816, loss_sup: 0.039763, loss_mps: 0.041485, loss_cps: 0.075568
[12:54:45.525] iteration 8284: total_loss: 0.383492, loss_sup: 0.287360, loss_mps: 0.035670, loss_cps: 0.060462
[12:54:45.671] iteration 8285: total_loss: 0.257784, loss_sup: 0.106728, loss_mps: 0.052665, loss_cps: 0.098391
[12:54:45.817] iteration 8286: total_loss: 0.206265, loss_sup: 0.049596, loss_mps: 0.053403, loss_cps: 0.103267
[12:54:45.965] iteration 8287: total_loss: 0.278348, loss_sup: 0.047998, loss_mps: 0.072728, loss_cps: 0.157622
[12:54:46.112] iteration 8288: total_loss: 0.102647, loss_sup: 0.005565, loss_mps: 0.037989, loss_cps: 0.059092
[12:54:46.259] iteration 8289: total_loss: 0.128155, loss_sup: 0.025173, loss_mps: 0.038880, loss_cps: 0.064102
[12:54:46.405] iteration 8290: total_loss: 0.332799, loss_sup: 0.236363, loss_mps: 0.035828, loss_cps: 0.060609
[12:54:46.551] iteration 8291: total_loss: 0.172633, loss_sup: 0.027982, loss_mps: 0.050398, loss_cps: 0.094253
[12:54:46.697] iteration 8292: total_loss: 0.163658, loss_sup: 0.045704, loss_mps: 0.043394, loss_cps: 0.074561
[12:54:46.843] iteration 8293: total_loss: 0.228965, loss_sup: 0.090589, loss_mps: 0.047436, loss_cps: 0.090940
[12:54:46.990] iteration 8294: total_loss: 0.451125, loss_sup: 0.256164, loss_mps: 0.065591, loss_cps: 0.129370
[12:54:47.137] iteration 8295: total_loss: 0.240877, loss_sup: 0.068166, loss_mps: 0.059006, loss_cps: 0.113705
[12:54:47.285] iteration 8296: total_loss: 0.242101, loss_sup: 0.124884, loss_mps: 0.042494, loss_cps: 0.074724
[12:54:47.432] iteration 8297: total_loss: 0.322771, loss_sup: 0.184407, loss_mps: 0.050508, loss_cps: 0.087856
[12:54:47.579] iteration 8298: total_loss: 0.296358, loss_sup: 0.104982, loss_mps: 0.064882, loss_cps: 0.126493
[12:54:47.725] iteration 8299: total_loss: 0.242952, loss_sup: 0.107805, loss_mps: 0.047335, loss_cps: 0.087812
[12:54:47.873] iteration 8300: total_loss: 0.156716, loss_sup: 0.041490, loss_mps: 0.043653, loss_cps: 0.071573
[12:54:47.873] Evaluation Started ==>
[12:54:59.247] ==> valid iteration 8300: unet metrics: {'dc': 0.6455518572950049, 'jc': 0.519332601659435, 'pre': 0.7237312837129719, 'hd': 6.00039982741171}, ynet metrics: {'dc': 0.5656015089335935, 'jc': 0.45060503288976816, 'pre': 0.7489064625460391, 'hd': 6.092847407037119}.
[12:54:59.250] Evaluation Finished!⏹️
[12:54:59.404] iteration 8301: total_loss: 0.222309, loss_sup: 0.103464, loss_mps: 0.044255, loss_cps: 0.074590
[12:54:59.552] iteration 8302: total_loss: 0.179006, loss_sup: 0.051075, loss_mps: 0.047316, loss_cps: 0.080615
[12:54:59.698] iteration 8303: total_loss: 0.133972, loss_sup: 0.020416, loss_mps: 0.039413, loss_cps: 0.074143
[12:54:59.843] iteration 8304: total_loss: 0.198907, loss_sup: 0.053604, loss_mps: 0.052441, loss_cps: 0.092861
[12:54:59.988] iteration 8305: total_loss: 0.200134, loss_sup: 0.065029, loss_mps: 0.048278, loss_cps: 0.086828
[12:55:00.134] iteration 8306: total_loss: 0.176327, loss_sup: 0.065302, loss_mps: 0.041435, loss_cps: 0.069590
[12:55:00.280] iteration 8307: total_loss: 0.118613, loss_sup: 0.031676, loss_mps: 0.033878, loss_cps: 0.053060
[12:55:00.426] iteration 8308: total_loss: 0.114807, loss_sup: 0.043737, loss_mps: 0.029685, loss_cps: 0.041384
[12:55:00.573] iteration 8309: total_loss: 0.167399, loss_sup: 0.044695, loss_mps: 0.040797, loss_cps: 0.081906
[12:55:00.720] iteration 8310: total_loss: 0.246842, loss_sup: 0.168941, loss_mps: 0.030169, loss_cps: 0.047732
[12:55:00.865] iteration 8311: total_loss: 0.264415, loss_sup: 0.163296, loss_mps: 0.038233, loss_cps: 0.062886
[12:55:01.011] iteration 8312: total_loss: 0.296742, loss_sup: 0.164726, loss_mps: 0.047877, loss_cps: 0.084139
[12:55:01.157] iteration 8313: total_loss: 0.183441, loss_sup: 0.089190, loss_mps: 0.035487, loss_cps: 0.058764
[12:55:01.303] iteration 8314: total_loss: 0.321381, loss_sup: 0.135078, loss_mps: 0.062216, loss_cps: 0.124087
[12:55:01.448] iteration 8315: total_loss: 0.389802, loss_sup: 0.172197, loss_mps: 0.069676, loss_cps: 0.147929
[12:55:01.594] iteration 8316: total_loss: 0.304228, loss_sup: 0.183814, loss_mps: 0.042759, loss_cps: 0.077655
[12:55:01.742] iteration 8317: total_loss: 0.130401, loss_sup: 0.011936, loss_mps: 0.041549, loss_cps: 0.076916
[12:55:01.889] iteration 8318: total_loss: 0.224608, loss_sup: 0.099235, loss_mps: 0.044351, loss_cps: 0.081022
[12:55:02.034] iteration 8319: total_loss: 0.185665, loss_sup: 0.053376, loss_mps: 0.045559, loss_cps: 0.086730
[12:55:02.180] iteration 8320: total_loss: 0.423956, loss_sup: 0.116865, loss_mps: 0.095370, loss_cps: 0.211721
[12:55:02.326] iteration 8321: total_loss: 0.370073, loss_sup: 0.122599, loss_mps: 0.080613, loss_cps: 0.166862
[12:55:02.477] iteration 8322: total_loss: 0.197785, loss_sup: 0.101108, loss_mps: 0.038134, loss_cps: 0.058542
[12:55:02.622] iteration 8323: total_loss: 0.221919, loss_sup: 0.098728, loss_mps: 0.044661, loss_cps: 0.078531
[12:55:02.767] iteration 8324: total_loss: 0.139476, loss_sup: 0.061817, loss_mps: 0.029344, loss_cps: 0.048315
[12:55:02.913] iteration 8325: total_loss: 0.246564, loss_sup: 0.109407, loss_mps: 0.049056, loss_cps: 0.088101
[12:55:03.063] iteration 8326: total_loss: 0.145089, loss_sup: 0.066221, loss_mps: 0.029944, loss_cps: 0.048924
[12:55:03.211] iteration 8327: total_loss: 0.318849, loss_sup: 0.196859, loss_mps: 0.042461, loss_cps: 0.079529
[12:55:03.358] iteration 8328: total_loss: 0.148725, loss_sup: 0.040600, loss_mps: 0.039664, loss_cps: 0.068461
[12:55:03.507] iteration 8329: total_loss: 0.272991, loss_sup: 0.120679, loss_mps: 0.050360, loss_cps: 0.101951
[12:55:03.652] iteration 8330: total_loss: 0.133651, loss_sup: 0.019623, loss_mps: 0.038979, loss_cps: 0.075049
[12:55:03.798] iteration 8331: total_loss: 0.184931, loss_sup: 0.067141, loss_mps: 0.041277, loss_cps: 0.076513
[12:55:03.943] iteration 8332: total_loss: 0.297998, loss_sup: 0.132002, loss_mps: 0.055517, loss_cps: 0.110480
[12:55:04.089] iteration 8333: total_loss: 0.403542, loss_sup: 0.140176, loss_mps: 0.083531, loss_cps: 0.179835
[12:55:04.234] iteration 8334: total_loss: 0.146163, loss_sup: 0.022109, loss_mps: 0.044709, loss_cps: 0.079345
[12:55:04.380] iteration 8335: total_loss: 0.193438, loss_sup: 0.075950, loss_mps: 0.041857, loss_cps: 0.075632
[12:55:04.529] iteration 8336: total_loss: 0.165374, loss_sup: 0.038181, loss_mps: 0.044381, loss_cps: 0.082811
[12:55:04.675] iteration 8337: total_loss: 0.163183, loss_sup: 0.058721, loss_mps: 0.037923, loss_cps: 0.066538
[12:55:04.820] iteration 8338: total_loss: 0.153052, loss_sup: 0.042128, loss_mps: 0.041142, loss_cps: 0.069782
[12:55:04.966] iteration 8339: total_loss: 0.396567, loss_sup: 0.279049, loss_mps: 0.042648, loss_cps: 0.074869
[12:55:05.111] iteration 8340: total_loss: 0.227016, loss_sup: 0.077594, loss_mps: 0.050586, loss_cps: 0.098836
[12:55:05.257] iteration 8341: total_loss: 0.309795, loss_sup: 0.166327, loss_mps: 0.051148, loss_cps: 0.092321
[12:55:05.403] iteration 8342: total_loss: 0.211662, loss_sup: 0.013630, loss_mps: 0.064711, loss_cps: 0.133321
[12:55:05.548] iteration 8343: total_loss: 0.150660, loss_sup: 0.025047, loss_mps: 0.043721, loss_cps: 0.081892
[12:55:05.696] iteration 8344: total_loss: 0.288943, loss_sup: 0.170574, loss_mps: 0.042580, loss_cps: 0.075788
[12:55:05.841] iteration 8345: total_loss: 0.164518, loss_sup: 0.025807, loss_mps: 0.050041, loss_cps: 0.088671
[12:55:05.986] iteration 8346: total_loss: 0.127323, loss_sup: 0.028243, loss_mps: 0.035578, loss_cps: 0.063502
[12:55:06.132] iteration 8347: total_loss: 0.147400, loss_sup: 0.032187, loss_mps: 0.040373, loss_cps: 0.074841
[12:55:06.277] iteration 8348: total_loss: 0.275905, loss_sup: 0.124151, loss_mps: 0.051570, loss_cps: 0.100184
[12:55:06.425] iteration 8349: total_loss: 0.228213, loss_sup: 0.071189, loss_mps: 0.053389, loss_cps: 0.103635
[12:55:06.570] iteration 8350: total_loss: 0.168525, loss_sup: 0.049874, loss_mps: 0.040815, loss_cps: 0.077836
[12:55:06.716] iteration 8351: total_loss: 0.375428, loss_sup: 0.180391, loss_mps: 0.065942, loss_cps: 0.129095
[12:55:06.861] iteration 8352: total_loss: 0.267441, loss_sup: 0.126686, loss_mps: 0.049130, loss_cps: 0.091625
[12:55:07.007] iteration 8353: total_loss: 0.227108, loss_sup: 0.075998, loss_mps: 0.051307, loss_cps: 0.099803
[12:55:07.153] iteration 8354: total_loss: 0.119930, loss_sup: 0.026426, loss_mps: 0.034600, loss_cps: 0.058904
[12:55:07.298] iteration 8355: total_loss: 0.193134, loss_sup: 0.044976, loss_mps: 0.051557, loss_cps: 0.096601
[12:55:07.445] iteration 8356: total_loss: 0.305831, loss_sup: 0.173498, loss_mps: 0.045136, loss_cps: 0.087197
[12:55:07.590] iteration 8357: total_loss: 0.315408, loss_sup: 0.082278, loss_mps: 0.073933, loss_cps: 0.159197
[12:55:07.736] iteration 8358: total_loss: 0.178809, loss_sup: 0.047337, loss_mps: 0.043865, loss_cps: 0.087607
[12:55:07.881] iteration 8359: total_loss: 0.394715, loss_sup: 0.195799, loss_mps: 0.064927, loss_cps: 0.133989
[12:55:07.942] iteration 8360: total_loss: 0.058986, loss_sup: 0.006535, loss_mps: 0.020731, loss_cps: 0.031720
[12:55:09.137] iteration 8361: total_loss: 0.347443, loss_sup: 0.186784, loss_mps: 0.055861, loss_cps: 0.104797
[12:55:09.286] iteration 8362: total_loss: 0.289349, loss_sup: 0.068995, loss_mps: 0.069994, loss_cps: 0.150360
[12:55:09.432] iteration 8363: total_loss: 0.180453, loss_sup: 0.049940, loss_mps: 0.043940, loss_cps: 0.086573
[12:55:09.578] iteration 8364: total_loss: 0.310636, loss_sup: 0.145282, loss_mps: 0.058933, loss_cps: 0.106421
[12:55:09.725] iteration 8365: total_loss: 0.188724, loss_sup: 0.039189, loss_mps: 0.053230, loss_cps: 0.096305
[12:55:09.873] iteration 8366: total_loss: 0.242923, loss_sup: 0.112577, loss_mps: 0.045564, loss_cps: 0.084782
[12:55:10.021] iteration 8367: total_loss: 0.284346, loss_sup: 0.124438, loss_mps: 0.053831, loss_cps: 0.106076
[12:55:10.171] iteration 8368: total_loss: 0.391533, loss_sup: 0.131925, loss_mps: 0.082827, loss_cps: 0.176781
[12:55:10.317] iteration 8369: total_loss: 0.184624, loss_sup: 0.059752, loss_mps: 0.045598, loss_cps: 0.079274
[12:55:10.463] iteration 8370: total_loss: 0.259001, loss_sup: 0.102920, loss_mps: 0.053153, loss_cps: 0.102928
[12:55:10.609] iteration 8371: total_loss: 0.214031, loss_sup: 0.100909, loss_mps: 0.039724, loss_cps: 0.073398
[12:55:10.754] iteration 8372: total_loss: 0.369386, loss_sup: 0.115766, loss_mps: 0.080951, loss_cps: 0.172669
[12:55:10.900] iteration 8373: total_loss: 0.218061, loss_sup: 0.053856, loss_mps: 0.055558, loss_cps: 0.108647
[12:55:11.046] iteration 8374: total_loss: 0.300936, loss_sup: 0.084471, loss_mps: 0.068471, loss_cps: 0.147994
[12:55:11.193] iteration 8375: total_loss: 0.188196, loss_sup: 0.051188, loss_mps: 0.049641, loss_cps: 0.087367
[12:55:11.341] iteration 8376: total_loss: 0.218301, loss_sup: 0.077787, loss_mps: 0.050315, loss_cps: 0.090199
[12:55:11.487] iteration 8377: total_loss: 0.306756, loss_sup: 0.065537, loss_mps: 0.078881, loss_cps: 0.162338
[12:55:11.633] iteration 8378: total_loss: 0.195641, loss_sup: 0.028348, loss_mps: 0.055631, loss_cps: 0.111662
[12:55:11.780] iteration 8379: total_loss: 0.256974, loss_sup: 0.099275, loss_mps: 0.054771, loss_cps: 0.102928
[12:55:11.926] iteration 8380: total_loss: 0.359460, loss_sup: 0.144425, loss_mps: 0.073055, loss_cps: 0.141980
[12:55:12.072] iteration 8381: total_loss: 0.259306, loss_sup: 0.068788, loss_mps: 0.063793, loss_cps: 0.126726
[12:55:12.217] iteration 8382: total_loss: 0.564890, loss_sup: 0.285203, loss_mps: 0.088590, loss_cps: 0.191097
[12:55:12.364] iteration 8383: total_loss: 0.227556, loss_sup: 0.053695, loss_mps: 0.058755, loss_cps: 0.115106
[12:55:12.510] iteration 8384: total_loss: 0.163793, loss_sup: 0.045523, loss_mps: 0.041666, loss_cps: 0.076604
[12:55:12.656] iteration 8385: total_loss: 0.166634, loss_sup: 0.051579, loss_mps: 0.042680, loss_cps: 0.072375
[12:55:12.801] iteration 8386: total_loss: 0.179895, loss_sup: 0.051227, loss_mps: 0.044926, loss_cps: 0.083742
[12:55:12.947] iteration 8387: total_loss: 0.261095, loss_sup: 0.082287, loss_mps: 0.060773, loss_cps: 0.118035
[12:55:13.093] iteration 8388: total_loss: 0.153267, loss_sup: 0.015765, loss_mps: 0.046106, loss_cps: 0.091396
[12:55:13.238] iteration 8389: total_loss: 0.187768, loss_sup: 0.094571, loss_mps: 0.035708, loss_cps: 0.057488
[12:55:13.384] iteration 8390: total_loss: 0.185676, loss_sup: 0.077414, loss_mps: 0.037607, loss_cps: 0.070655
[12:55:13.530] iteration 8391: total_loss: 0.185538, loss_sup: 0.034698, loss_mps: 0.051527, loss_cps: 0.099313
[12:55:13.676] iteration 8392: total_loss: 0.297326, loss_sup: 0.123138, loss_mps: 0.056369, loss_cps: 0.117818
[12:55:13.822] iteration 8393: total_loss: 0.139221, loss_sup: 0.032523, loss_mps: 0.040358, loss_cps: 0.066340
[12:55:13.967] iteration 8394: total_loss: 0.252812, loss_sup: 0.156784, loss_mps: 0.034963, loss_cps: 0.061066
[12:55:14.113] iteration 8395: total_loss: 0.502506, loss_sup: 0.290606, loss_mps: 0.070589, loss_cps: 0.141311
[12:55:14.259] iteration 8396: total_loss: 0.373490, loss_sup: 0.108070, loss_mps: 0.082435, loss_cps: 0.182985
[12:55:14.405] iteration 8397: total_loss: 0.321505, loss_sup: 0.080781, loss_mps: 0.080430, loss_cps: 0.160294
[12:55:14.550] iteration 8398: total_loss: 0.271404, loss_sup: 0.137669, loss_mps: 0.047226, loss_cps: 0.086509
[12:55:14.697] iteration 8399: total_loss: 0.288998, loss_sup: 0.104301, loss_mps: 0.059420, loss_cps: 0.125276
[12:55:14.844] iteration 8400: total_loss: 0.211677, loss_sup: 0.062142, loss_mps: 0.052222, loss_cps: 0.097313
[12:55:14.844] Evaluation Started ==>
[12:55:26.264] ==> valid iteration 8400: unet metrics: {'dc': 0.6495829827960952, 'jc': 0.5250881494740263, 'pre': 0.7339451999391228, 'hd': 5.901201800588904}, ynet metrics: {'dc': 0.549275433708735, 'jc': 0.4374075807720335, 'pre': 0.7262337358268772, 'hd': 6.1154287357209}.
[12:55:26.266] Evaluation Finished!⏹️
[12:55:26.419] iteration 8401: total_loss: 0.126792, loss_sup: 0.033259, loss_mps: 0.037299, loss_cps: 0.056234
[12:55:26.565] iteration 8402: total_loss: 0.194934, loss_sup: 0.081862, loss_mps: 0.042711, loss_cps: 0.070361
[12:55:26.710] iteration 8403: total_loss: 0.151234, loss_sup: 0.039213, loss_mps: 0.040545, loss_cps: 0.071476
[12:55:26.855] iteration 8404: total_loss: 0.247134, loss_sup: 0.093652, loss_mps: 0.055002, loss_cps: 0.098480
[12:55:27.000] iteration 8405: total_loss: 0.150865, loss_sup: 0.025866, loss_mps: 0.044663, loss_cps: 0.080337
[12:55:27.145] iteration 8406: total_loss: 0.152637, loss_sup: 0.028695, loss_mps: 0.044577, loss_cps: 0.079364
[12:55:27.291] iteration 8407: total_loss: 0.168222, loss_sup: 0.010874, loss_mps: 0.054028, loss_cps: 0.103320
[12:55:27.436] iteration 8408: total_loss: 0.302272, loss_sup: 0.141510, loss_mps: 0.055077, loss_cps: 0.105685
[12:55:27.584] iteration 8409: total_loss: 0.166729, loss_sup: 0.074065, loss_mps: 0.034065, loss_cps: 0.058598
[12:55:27.729] iteration 8410: total_loss: 0.327569, loss_sup: 0.085626, loss_mps: 0.076215, loss_cps: 0.165729
[12:55:27.874] iteration 8411: total_loss: 0.158227, loss_sup: 0.064718, loss_mps: 0.035429, loss_cps: 0.058079
[12:55:28.019] iteration 8412: total_loss: 0.245840, loss_sup: 0.118797, loss_mps: 0.044625, loss_cps: 0.082417
[12:55:28.164] iteration 8413: total_loss: 0.267915, loss_sup: 0.125052, loss_mps: 0.051283, loss_cps: 0.091579
[12:55:28.309] iteration 8414: total_loss: 0.420673, loss_sup: 0.195563, loss_mps: 0.072249, loss_cps: 0.152861
[12:55:28.455] iteration 8415: total_loss: 0.135458, loss_sup: 0.048497, loss_mps: 0.032414, loss_cps: 0.054547
[12:55:28.601] iteration 8416: total_loss: 0.216229, loss_sup: 0.035323, loss_mps: 0.061113, loss_cps: 0.119792
[12:55:28.746] iteration 8417: total_loss: 0.230036, loss_sup: 0.116149, loss_mps: 0.040930, loss_cps: 0.072957
[12:55:28.893] iteration 8418: total_loss: 0.265482, loss_sup: 0.086680, loss_mps: 0.060826, loss_cps: 0.117976
[12:55:29.040] iteration 8419: total_loss: 0.165240, loss_sup: 0.034809, loss_mps: 0.044330, loss_cps: 0.086100
[12:55:29.186] iteration 8420: total_loss: 0.272670, loss_sup: 0.098844, loss_mps: 0.057713, loss_cps: 0.116114
[12:55:29.335] iteration 8421: total_loss: 0.110457, loss_sup: 0.010841, loss_mps: 0.035784, loss_cps: 0.063832
[12:55:29.481] iteration 8422: total_loss: 0.258516, loss_sup: 0.050286, loss_mps: 0.066850, loss_cps: 0.141379
[12:55:29.626] iteration 8423: total_loss: 0.217354, loss_sup: 0.049954, loss_mps: 0.057800, loss_cps: 0.109599
[12:55:29.772] iteration 8424: total_loss: 0.247282, loss_sup: 0.068379, loss_mps: 0.061456, loss_cps: 0.117447
[12:55:29.919] iteration 8425: total_loss: 0.109677, loss_sup: 0.004607, loss_mps: 0.037945, loss_cps: 0.067126
[12:55:30.065] iteration 8426: total_loss: 0.296235, loss_sup: 0.175502, loss_mps: 0.042129, loss_cps: 0.078604
[12:55:30.211] iteration 8427: total_loss: 0.188350, loss_sup: 0.085419, loss_mps: 0.037513, loss_cps: 0.065418
[12:55:30.357] iteration 8428: total_loss: 0.133206, loss_sup: 0.026813, loss_mps: 0.038519, loss_cps: 0.067874
[12:55:30.503] iteration 8429: total_loss: 0.195418, loss_sup: 0.032364, loss_mps: 0.058440, loss_cps: 0.104613
[12:55:30.651] iteration 8430: total_loss: 0.164935, loss_sup: 0.042255, loss_mps: 0.043256, loss_cps: 0.079425
[12:55:30.798] iteration 8431: total_loss: 0.327108, loss_sup: 0.191130, loss_mps: 0.048358, loss_cps: 0.087620
[12:55:30.944] iteration 8432: total_loss: 0.198559, loss_sup: 0.018702, loss_mps: 0.060796, loss_cps: 0.119061
[12:55:31.090] iteration 8433: total_loss: 0.124586, loss_sup: 0.037288, loss_mps: 0.031653, loss_cps: 0.055645
[12:55:31.236] iteration 8434: total_loss: 0.327099, loss_sup: 0.157387, loss_mps: 0.057742, loss_cps: 0.111970
[12:55:31.385] iteration 8435: total_loss: 0.152071, loss_sup: 0.041112, loss_mps: 0.038333, loss_cps: 0.072625
[12:55:31.531] iteration 8436: total_loss: 0.183788, loss_sup: 0.076599, loss_mps: 0.038832, loss_cps: 0.068357
[12:55:31.679] iteration 8437: total_loss: 0.161855, loss_sup: 0.024918, loss_mps: 0.046240, loss_cps: 0.090696
[12:55:31.825] iteration 8438: total_loss: 0.189485, loss_sup: 0.066886, loss_mps: 0.043299, loss_cps: 0.079299
[12:55:31.971] iteration 8439: total_loss: 0.170246, loss_sup: 0.069770, loss_mps: 0.037391, loss_cps: 0.063085
[12:55:32.117] iteration 8440: total_loss: 0.261934, loss_sup: 0.110922, loss_mps: 0.050714, loss_cps: 0.100299
[12:55:32.263] iteration 8441: total_loss: 0.185806, loss_sup: 0.052980, loss_mps: 0.045977, loss_cps: 0.086849
[12:55:32.409] iteration 8442: total_loss: 0.306936, loss_sup: 0.167951, loss_mps: 0.047950, loss_cps: 0.091035
[12:55:32.555] iteration 8443: total_loss: 0.168391, loss_sup: 0.016309, loss_mps: 0.049885, loss_cps: 0.102197
[12:55:32.702] iteration 8444: total_loss: 0.303259, loss_sup: 0.070181, loss_mps: 0.075972, loss_cps: 0.157107
[12:55:32.848] iteration 8445: total_loss: 0.171880, loss_sup: 0.034289, loss_mps: 0.046074, loss_cps: 0.091517
[12:55:32.995] iteration 8446: total_loss: 0.141334, loss_sup: 0.056509, loss_mps: 0.030655, loss_cps: 0.054170
[12:55:33.142] iteration 8447: total_loss: 0.205737, loss_sup: 0.041474, loss_mps: 0.053731, loss_cps: 0.110532
[12:55:33.289] iteration 8448: total_loss: 0.247270, loss_sup: 0.097966, loss_mps: 0.048650, loss_cps: 0.100653
[12:55:33.437] iteration 8449: total_loss: 0.263142, loss_sup: 0.126193, loss_mps: 0.045990, loss_cps: 0.090959
[12:55:33.583] iteration 8450: total_loss: 0.186311, loss_sup: 0.061800, loss_mps: 0.041271, loss_cps: 0.083240
[12:55:33.729] iteration 8451: total_loss: 0.171249, loss_sup: 0.037819, loss_mps: 0.042835, loss_cps: 0.090595
[12:55:33.874] iteration 8452: total_loss: 0.191809, loss_sup: 0.036966, loss_mps: 0.049790, loss_cps: 0.105053
[12:55:34.020] iteration 8453: total_loss: 0.192919, loss_sup: 0.052631, loss_mps: 0.046873, loss_cps: 0.093416
[12:55:34.166] iteration 8454: total_loss: 0.217849, loss_sup: 0.089249, loss_mps: 0.044038, loss_cps: 0.084562
[12:55:34.311] iteration 8455: total_loss: 0.154432, loss_sup: 0.043131, loss_mps: 0.039193, loss_cps: 0.072109
[12:55:34.457] iteration 8456: total_loss: 0.123857, loss_sup: 0.048976, loss_mps: 0.028174, loss_cps: 0.046708
[12:55:34.604] iteration 8457: total_loss: 0.209621, loss_sup: 0.125416, loss_mps: 0.031878, loss_cps: 0.052327
[12:55:34.750] iteration 8458: total_loss: 0.220960, loss_sup: 0.073863, loss_mps: 0.049953, loss_cps: 0.097144
[12:55:34.895] iteration 8459: total_loss: 0.206806, loss_sup: 0.059177, loss_mps: 0.049149, loss_cps: 0.098480
[12:55:35.041] iteration 8460: total_loss: 0.276328, loss_sup: 0.114960, loss_mps: 0.055776, loss_cps: 0.105592
[12:55:35.186] iteration 8461: total_loss: 0.279602, loss_sup: 0.104961, loss_mps: 0.060072, loss_cps: 0.114569
[12:55:35.331] iteration 8462: total_loss: 0.117983, loss_sup: 0.056300, loss_mps: 0.023469, loss_cps: 0.038214
[12:55:35.477] iteration 8463: total_loss: 0.175976, loss_sup: 0.062209, loss_mps: 0.040732, loss_cps: 0.073035
[12:55:35.622] iteration 8464: total_loss: 0.221780, loss_sup: 0.085936, loss_mps: 0.046785, loss_cps: 0.089059
[12:55:35.768] iteration 8465: total_loss: 0.181533, loss_sup: 0.041271, loss_mps: 0.049081, loss_cps: 0.091182
[12:55:35.918] iteration 8466: total_loss: 0.197778, loss_sup: 0.073680, loss_mps: 0.042338, loss_cps: 0.081761
[12:55:36.064] iteration 8467: total_loss: 0.227383, loss_sup: 0.055463, loss_mps: 0.058578, loss_cps: 0.113342
[12:55:36.210] iteration 8468: total_loss: 0.187643, loss_sup: 0.045685, loss_mps: 0.047566, loss_cps: 0.094392
[12:55:36.356] iteration 8469: total_loss: 0.240852, loss_sup: 0.028703, loss_mps: 0.069986, loss_cps: 0.142163
[12:55:36.502] iteration 8470: total_loss: 0.168393, loss_sup: 0.066527, loss_mps: 0.036458, loss_cps: 0.065407
[12:55:36.648] iteration 8471: total_loss: 0.183469, loss_sup: 0.060159, loss_mps: 0.042946, loss_cps: 0.080363
[12:55:36.793] iteration 8472: total_loss: 0.455913, loss_sup: 0.331600, loss_mps: 0.043725, loss_cps: 0.080588
[12:55:36.939] iteration 8473: total_loss: 0.312544, loss_sup: 0.080895, loss_mps: 0.077886, loss_cps: 0.153764
[12:55:37.085] iteration 8474: total_loss: 0.245675, loss_sup: 0.109319, loss_mps: 0.047397, loss_cps: 0.088959
[12:55:37.234] iteration 8475: total_loss: 0.220108, loss_sup: 0.071090, loss_mps: 0.051383, loss_cps: 0.097635
[12:55:37.380] iteration 8476: total_loss: 0.198593, loss_sup: 0.078228, loss_mps: 0.043951, loss_cps: 0.076414
[12:55:37.526] iteration 8477: total_loss: 0.342219, loss_sup: 0.199314, loss_mps: 0.048606, loss_cps: 0.094299
[12:55:37.673] iteration 8478: total_loss: 0.241354, loss_sup: 0.081026, loss_mps: 0.052483, loss_cps: 0.107845
[12:55:37.820] iteration 8479: total_loss: 0.326804, loss_sup: 0.163180, loss_mps: 0.054666, loss_cps: 0.108957
[12:55:37.967] iteration 8480: total_loss: 0.199654, loss_sup: 0.049002, loss_mps: 0.051413, loss_cps: 0.099240
[12:55:38.114] iteration 8481: total_loss: 0.151634, loss_sup: 0.043908, loss_mps: 0.038823, loss_cps: 0.068903
[12:55:38.261] iteration 8482: total_loss: 0.157643, loss_sup: 0.027179, loss_mps: 0.046976, loss_cps: 0.083489
[12:55:38.407] iteration 8483: total_loss: 0.179628, loss_sup: 0.039497, loss_mps: 0.049634, loss_cps: 0.090497
[12:55:38.556] iteration 8484: total_loss: 0.152900, loss_sup: 0.061269, loss_mps: 0.034924, loss_cps: 0.056707
[12:55:38.702] iteration 8485: total_loss: 0.169657, loss_sup: 0.013638, loss_mps: 0.052659, loss_cps: 0.103360
[12:55:38.848] iteration 8486: total_loss: 0.155435, loss_sup: 0.010843, loss_mps: 0.049903, loss_cps: 0.094690
[12:55:38.994] iteration 8487: total_loss: 0.403797, loss_sup: 0.207915, loss_mps: 0.062775, loss_cps: 0.133106
[12:55:39.143] iteration 8488: total_loss: 0.305658, loss_sup: 0.133189, loss_mps: 0.058432, loss_cps: 0.114037
[12:55:39.289] iteration 8489: total_loss: 0.473260, loss_sup: 0.247228, loss_mps: 0.071312, loss_cps: 0.154720
[12:55:39.435] iteration 8490: total_loss: 0.254892, loss_sup: 0.080246, loss_mps: 0.058601, loss_cps: 0.116045
[12:55:39.582] iteration 8491: total_loss: 0.221241, loss_sup: 0.064600, loss_mps: 0.052262, loss_cps: 0.104379
[12:55:39.731] iteration 8492: total_loss: 0.327620, loss_sup: 0.199607, loss_mps: 0.043566, loss_cps: 0.084447
[12:55:39.877] iteration 8493: total_loss: 0.199548, loss_sup: 0.043741, loss_mps: 0.053231, loss_cps: 0.102576
[12:55:40.024] iteration 8494: total_loss: 0.238291, loss_sup: 0.068933, loss_mps: 0.058233, loss_cps: 0.111126
[12:55:40.169] iteration 8495: total_loss: 0.274751, loss_sup: 0.140094, loss_mps: 0.047242, loss_cps: 0.087414
[12:55:40.316] iteration 8496: total_loss: 0.242945, loss_sup: 0.058983, loss_mps: 0.059068, loss_cps: 0.124894
[12:55:40.462] iteration 8497: total_loss: 0.364393, loss_sup: 0.255382, loss_mps: 0.040196, loss_cps: 0.068815
[12:55:40.610] iteration 8498: total_loss: 0.239143, loss_sup: 0.116557, loss_mps: 0.044140, loss_cps: 0.078445
[12:55:40.756] iteration 8499: total_loss: 0.489960, loss_sup: 0.273231, loss_mps: 0.073048, loss_cps: 0.143681
[12:55:40.903] iteration 8500: total_loss: 0.118045, loss_sup: 0.008609, loss_mps: 0.042750, loss_cps: 0.066686
[12:55:40.903] Evaluation Started ==>
[12:55:52.282] ==> valid iteration 8500: unet metrics: {'dc': 0.614104092677075, 'jc': 0.48598286578417815, 'pre': 0.7093722859249868, 'hd': 6.216207998291359}, ynet metrics: {'dc': 0.607570943167208, 'jc': 0.4831464083720645, 'pre': 0.6885506022643231, 'hd': 6.243956978885226}.
[12:55:52.451] ==> New best valid dice for ynet: 0.607571, at iteration 8500
[12:55:52.453] Evaluation Finished!⏹️
[12:55:52.606] iteration 8501: total_loss: 0.222393, loss_sup: 0.042781, loss_mps: 0.059198, loss_cps: 0.120415
[12:55:52.753] iteration 8502: total_loss: 0.163093, loss_sup: 0.025202, loss_mps: 0.049344, loss_cps: 0.088546
[12:55:52.899] iteration 8503: total_loss: 0.263803, loss_sup: 0.127129, loss_mps: 0.048528, loss_cps: 0.088147
[12:55:53.044] iteration 8504: total_loss: 0.279285, loss_sup: 0.097815, loss_mps: 0.060490, loss_cps: 0.120980
[12:55:53.189] iteration 8505: total_loss: 0.128865, loss_sup: 0.012743, loss_mps: 0.042217, loss_cps: 0.073905
[12:55:53.335] iteration 8506: total_loss: 0.268373, loss_sup: 0.094652, loss_mps: 0.056072, loss_cps: 0.117649
[12:55:53.482] iteration 8507: total_loss: 0.373443, loss_sup: 0.204619, loss_mps: 0.056327, loss_cps: 0.112498
[12:55:53.629] iteration 8508: total_loss: 0.314902, loss_sup: 0.085130, loss_mps: 0.069927, loss_cps: 0.159846
[12:55:53.775] iteration 8509: total_loss: 0.215113, loss_sup: 0.059714, loss_mps: 0.053659, loss_cps: 0.101741
[12:55:53.921] iteration 8510: total_loss: 0.161094, loss_sup: 0.017893, loss_mps: 0.050856, loss_cps: 0.092344
[12:55:54.067] iteration 8511: total_loss: 0.186793, loss_sup: 0.074875, loss_mps: 0.042677, loss_cps: 0.069241
[12:55:54.212] iteration 8512: total_loss: 0.201139, loss_sup: 0.093036, loss_mps: 0.041134, loss_cps: 0.066969
[12:55:54.357] iteration 8513: total_loss: 0.251149, loss_sup: 0.093124, loss_mps: 0.053784, loss_cps: 0.104241
[12:55:54.503] iteration 8514: total_loss: 0.244759, loss_sup: 0.094685, loss_mps: 0.052471, loss_cps: 0.097604
[12:55:54.649] iteration 8515: total_loss: 0.124609, loss_sup: 0.047510, loss_mps: 0.029549, loss_cps: 0.047551
[12:55:54.794] iteration 8516: total_loss: 0.215964, loss_sup: 0.103160, loss_mps: 0.040550, loss_cps: 0.072253
[12:55:54.939] iteration 8517: total_loss: 0.281892, loss_sup: 0.085420, loss_mps: 0.068846, loss_cps: 0.127626
[12:55:55.085] iteration 8518: total_loss: 0.432233, loss_sup: 0.123127, loss_mps: 0.096122, loss_cps: 0.212984
[12:55:55.230] iteration 8519: total_loss: 0.198103, loss_sup: 0.088469, loss_mps: 0.041836, loss_cps: 0.067798
[12:55:55.378] iteration 8520: total_loss: 0.161847, loss_sup: 0.029443, loss_mps: 0.045307, loss_cps: 0.087097
[12:55:55.524] iteration 8521: total_loss: 0.168833, loss_sup: 0.030419, loss_mps: 0.048247, loss_cps: 0.090166
[12:55:55.669] iteration 8522: total_loss: 0.165874, loss_sup: 0.024170, loss_mps: 0.051158, loss_cps: 0.090546
[12:55:55.815] iteration 8523: total_loss: 0.212430, loss_sup: 0.109387, loss_mps: 0.037618, loss_cps: 0.065425
[12:55:55.961] iteration 8524: total_loss: 0.361125, loss_sup: 0.218581, loss_mps: 0.048659, loss_cps: 0.093884
[12:55:56.107] iteration 8525: total_loss: 0.263950, loss_sup: 0.075931, loss_mps: 0.061165, loss_cps: 0.126854
[12:55:56.253] iteration 8526: total_loss: 0.209130, loss_sup: 0.095856, loss_mps: 0.041665, loss_cps: 0.071609
[12:55:56.399] iteration 8527: total_loss: 0.172253, loss_sup: 0.016285, loss_mps: 0.055230, loss_cps: 0.100737
[12:55:56.544] iteration 8528: total_loss: 0.161149, loss_sup: 0.023349, loss_mps: 0.046192, loss_cps: 0.091609
[12:55:56.690] iteration 8529: total_loss: 0.501607, loss_sup: 0.292079, loss_mps: 0.068473, loss_cps: 0.141054
[12:55:56.837] iteration 8530: total_loss: 0.251506, loss_sup: 0.113418, loss_mps: 0.046717, loss_cps: 0.091371
[12:55:56.984] iteration 8531: total_loss: 0.171297, loss_sup: 0.041083, loss_mps: 0.046104, loss_cps: 0.084111
[12:55:57.130] iteration 8532: total_loss: 0.307367, loss_sup: 0.156772, loss_mps: 0.052352, loss_cps: 0.098242
[12:55:57.276] iteration 8533: total_loss: 0.149259, loss_sup: 0.024137, loss_mps: 0.045395, loss_cps: 0.079727
[12:55:57.422] iteration 8534: total_loss: 0.187829, loss_sup: 0.072501, loss_mps: 0.040043, loss_cps: 0.075284
[12:55:57.568] iteration 8535: total_loss: 0.203087, loss_sup: 0.110703, loss_mps: 0.032323, loss_cps: 0.060061
[12:55:57.714] iteration 8536: total_loss: 0.212071, loss_sup: 0.080073, loss_mps: 0.044076, loss_cps: 0.087923
[12:55:57.861] iteration 8537: total_loss: 0.175732, loss_sup: 0.065008, loss_mps: 0.040135, loss_cps: 0.070590
[12:55:58.007] iteration 8538: total_loss: 0.141199, loss_sup: 0.016391, loss_mps: 0.044266, loss_cps: 0.080543
[12:55:58.152] iteration 8539: total_loss: 0.217817, loss_sup: 0.068960, loss_mps: 0.049030, loss_cps: 0.099828
[12:55:58.298] iteration 8540: total_loss: 0.333087, loss_sup: 0.196412, loss_mps: 0.046760, loss_cps: 0.089914
[12:55:58.443] iteration 8541: total_loss: 0.181569, loss_sup: 0.071548, loss_mps: 0.039992, loss_cps: 0.070029
[12:55:58.589] iteration 8542: total_loss: 0.461018, loss_sup: 0.238709, loss_mps: 0.072876, loss_cps: 0.149433
[12:55:58.735] iteration 8543: total_loss: 0.243934, loss_sup: 0.078167, loss_mps: 0.055463, loss_cps: 0.110304
[12:55:58.881] iteration 8544: total_loss: 0.151935, loss_sup: 0.021230, loss_mps: 0.045794, loss_cps: 0.084911
[12:55:59.026] iteration 8545: total_loss: 0.101205, loss_sup: 0.018243, loss_mps: 0.031660, loss_cps: 0.051303
[12:55:59.171] iteration 8546: total_loss: 0.146153, loss_sup: 0.025824, loss_mps: 0.042417, loss_cps: 0.077913
[12:55:59.317] iteration 8547: total_loss: 0.325523, loss_sup: 0.178194, loss_mps: 0.050764, loss_cps: 0.096565
[12:55:59.463] iteration 8548: total_loss: 0.169623, loss_sup: 0.021188, loss_mps: 0.048853, loss_cps: 0.099581
[12:55:59.608] iteration 8549: total_loss: 0.328797, loss_sup: 0.053530, loss_mps: 0.089281, loss_cps: 0.185986
[12:55:59.755] iteration 8550: total_loss: 0.378428, loss_sup: 0.157549, loss_mps: 0.069635, loss_cps: 0.151244
[12:55:59.901] iteration 8551: total_loss: 0.107780, loss_sup: 0.041316, loss_mps: 0.023885, loss_cps: 0.042579
[12:56:00.046] iteration 8552: total_loss: 0.321855, loss_sup: 0.086943, loss_mps: 0.075971, loss_cps: 0.158941
[12:56:00.192] iteration 8553: total_loss: 0.206576, loss_sup: 0.066240, loss_mps: 0.051402, loss_cps: 0.088934
[12:56:00.337] iteration 8554: total_loss: 0.279412, loss_sup: 0.108899, loss_mps: 0.056843, loss_cps: 0.113671
[12:56:00.485] iteration 8555: total_loss: 0.234005, loss_sup: 0.097696, loss_mps: 0.046633, loss_cps: 0.089676
[12:56:00.630] iteration 8556: total_loss: 0.265469, loss_sup: 0.079710, loss_mps: 0.061228, loss_cps: 0.124531
[12:56:00.776] iteration 8557: total_loss: 0.125964, loss_sup: 0.036571, loss_mps: 0.033038, loss_cps: 0.056355
[12:56:00.922] iteration 8558: total_loss: 0.149438, loss_sup: 0.018155, loss_mps: 0.046020, loss_cps: 0.085263
[12:56:01.067] iteration 8559: total_loss: 0.325809, loss_sup: 0.186347, loss_mps: 0.049205, loss_cps: 0.090257
[12:56:01.213] iteration 8560: total_loss: 0.126418, loss_sup: 0.016964, loss_mps: 0.038209, loss_cps: 0.071245
[12:56:01.359] iteration 8561: total_loss: 0.240807, loss_sup: 0.054121, loss_mps: 0.061627, loss_cps: 0.125059
[12:56:01.508] iteration 8562: total_loss: 0.231856, loss_sup: 0.113710, loss_mps: 0.041144, loss_cps: 0.077002
[12:56:01.656] iteration 8563: total_loss: 0.175231, loss_sup: 0.057466, loss_mps: 0.040690, loss_cps: 0.077076
[12:56:01.802] iteration 8564: total_loss: 0.182745, loss_sup: 0.037224, loss_mps: 0.048268, loss_cps: 0.097253
[12:56:01.947] iteration 8565: total_loss: 0.126331, loss_sup: 0.036967, loss_mps: 0.033838, loss_cps: 0.055525
[12:56:02.093] iteration 8566: total_loss: 0.299626, loss_sup: 0.102725, loss_mps: 0.066950, loss_cps: 0.129952
[12:56:02.241] iteration 8567: total_loss: 0.183887, loss_sup: 0.049649, loss_mps: 0.046442, loss_cps: 0.087796
[12:56:02.387] iteration 8568: total_loss: 0.462739, loss_sup: 0.212220, loss_mps: 0.078787, loss_cps: 0.171732
[12:56:02.534] iteration 8569: total_loss: 0.185379, loss_sup: 0.060150, loss_mps: 0.045313, loss_cps: 0.079917
[12:56:02.680] iteration 8570: total_loss: 0.176993, loss_sup: 0.027580, loss_mps: 0.049353, loss_cps: 0.100060
[12:56:02.827] iteration 8571: total_loss: 0.335693, loss_sup: 0.107832, loss_mps: 0.074267, loss_cps: 0.153593
[12:56:02.974] iteration 8572: total_loss: 0.200320, loss_sup: 0.052409, loss_mps: 0.049656, loss_cps: 0.098255
[12:56:03.120] iteration 8573: total_loss: 0.237511, loss_sup: 0.020801, loss_mps: 0.069650, loss_cps: 0.147059
[12:56:03.268] iteration 8574: total_loss: 0.252877, loss_sup: 0.087043, loss_mps: 0.055609, loss_cps: 0.110225
[12:56:03.414] iteration 8575: total_loss: 0.404040, loss_sup: 0.203799, loss_mps: 0.061694, loss_cps: 0.138548
[12:56:03.561] iteration 8576: total_loss: 0.216516, loss_sup: 0.085944, loss_mps: 0.045942, loss_cps: 0.084629
[12:56:03.707] iteration 8577: total_loss: 0.347488, loss_sup: 0.153224, loss_mps: 0.064717, loss_cps: 0.129546
[12:56:03.854] iteration 8578: total_loss: 0.258802, loss_sup: 0.115514, loss_mps: 0.050844, loss_cps: 0.092444
[12:56:04.000] iteration 8579: total_loss: 0.159347, loss_sup: 0.014198, loss_mps: 0.047209, loss_cps: 0.097941
[12:56:04.147] iteration 8580: total_loss: 0.394218, loss_sup: 0.149145, loss_mps: 0.076329, loss_cps: 0.168743
[12:56:04.294] iteration 8581: total_loss: 0.243647, loss_sup: 0.043585, loss_mps: 0.065466, loss_cps: 0.134597
[12:56:04.441] iteration 8582: total_loss: 0.251789, loss_sup: 0.037707, loss_mps: 0.072827, loss_cps: 0.141255
[12:56:04.587] iteration 8583: total_loss: 0.404082, loss_sup: 0.243298, loss_mps: 0.055094, loss_cps: 0.105690
[12:56:04.733] iteration 8584: total_loss: 0.130777, loss_sup: 0.022318, loss_mps: 0.039500, loss_cps: 0.068959
[12:56:04.879] iteration 8585: total_loss: 0.253304, loss_sup: 0.114840, loss_mps: 0.050762, loss_cps: 0.087701
[12:56:05.025] iteration 8586: total_loss: 0.276584, loss_sup: 0.136510, loss_mps: 0.049097, loss_cps: 0.090977
[12:56:05.171] iteration 8587: total_loss: 0.252192, loss_sup: 0.097040, loss_mps: 0.050593, loss_cps: 0.104560
[12:56:05.319] iteration 8588: total_loss: 0.310769, loss_sup: 0.068854, loss_mps: 0.079148, loss_cps: 0.162767
[12:56:05.464] iteration 8589: total_loss: 0.183661, loss_sup: 0.044890, loss_mps: 0.049275, loss_cps: 0.089496
[12:56:05.610] iteration 8590: total_loss: 0.349158, loss_sup: 0.205062, loss_mps: 0.051120, loss_cps: 0.092976
[12:56:05.756] iteration 8591: total_loss: 0.364791, loss_sup: 0.149596, loss_mps: 0.070337, loss_cps: 0.144859
[12:56:05.902] iteration 8592: total_loss: 0.288264, loss_sup: 0.144586, loss_mps: 0.047648, loss_cps: 0.096029
[12:56:06.047] iteration 8593: total_loss: 0.156611, loss_sup: 0.033927, loss_mps: 0.045690, loss_cps: 0.076994
[12:56:06.193] iteration 8594: total_loss: 0.375315, loss_sup: 0.171626, loss_mps: 0.066688, loss_cps: 0.137001
[12:56:06.341] iteration 8595: total_loss: 0.227751, loss_sup: 0.066756, loss_mps: 0.053662, loss_cps: 0.107333
[12:56:06.487] iteration 8596: total_loss: 0.249347, loss_sup: 0.108628, loss_mps: 0.049561, loss_cps: 0.091159
[12:56:06.632] iteration 8597: total_loss: 0.328088, loss_sup: 0.175233, loss_mps: 0.053977, loss_cps: 0.098878
[12:56:06.779] iteration 8598: total_loss: 0.150914, loss_sup: 0.041203, loss_mps: 0.039856, loss_cps: 0.069855
[12:56:06.925] iteration 8599: total_loss: 0.234394, loss_sup: 0.115391, loss_mps: 0.042407, loss_cps: 0.076596
[12:56:07.071] iteration 8600: total_loss: 0.186067, loss_sup: 0.061327, loss_mps: 0.044509, loss_cps: 0.080231
[12:56:07.071] Evaluation Started ==>
[12:56:18.464] ==> valid iteration 8600: unet metrics: {'dc': 0.601511765391017, 'jc': 0.47848904138555964, 'pre': 0.7374467869769011, 'hd': 5.956726302512456}, ynet metrics: {'dc': 0.578091970996582, 'jc': 0.4580790384048197, 'pre': 0.7397893989799017, 'hd': 5.866982838832749}.
[12:56:18.466] Evaluation Finished!⏹️
[12:56:18.620] iteration 8601: total_loss: 0.310619, loss_sup: 0.107178, loss_mps: 0.066768, loss_cps: 0.136672
[12:56:18.768] iteration 8602: total_loss: 0.322191, loss_sup: 0.179560, loss_mps: 0.050647, loss_cps: 0.091984
[12:56:18.914] iteration 8603: total_loss: 0.369706, loss_sup: 0.239888, loss_mps: 0.046811, loss_cps: 0.083007
[12:56:19.059] iteration 8604: total_loss: 0.377421, loss_sup: 0.224540, loss_mps: 0.053649, loss_cps: 0.099233
[12:56:19.204] iteration 8605: total_loss: 0.255189, loss_sup: 0.099143, loss_mps: 0.052101, loss_cps: 0.103945
[12:56:19.350] iteration 8606: total_loss: 0.790109, loss_sup: 0.470090, loss_mps: 0.098630, loss_cps: 0.221388
[12:56:19.495] iteration 8607: total_loss: 0.315864, loss_sup: 0.171297, loss_mps: 0.050892, loss_cps: 0.093675
[12:56:19.642] iteration 8608: total_loss: 0.315789, loss_sup: 0.159449, loss_mps: 0.053375, loss_cps: 0.102965
[12:56:19.787] iteration 8609: total_loss: 0.227872, loss_sup: 0.088214, loss_mps: 0.050016, loss_cps: 0.089643
[12:56:19.933] iteration 8610: total_loss: 0.221095, loss_sup: 0.069856, loss_mps: 0.057207, loss_cps: 0.094032
[12:56:20.079] iteration 8611: total_loss: 0.195611, loss_sup: 0.035662, loss_mps: 0.055828, loss_cps: 0.104121
[12:56:20.225] iteration 8612: total_loss: 0.210853, loss_sup: 0.040938, loss_mps: 0.057946, loss_cps: 0.111968
[12:56:20.371] iteration 8613: total_loss: 0.247135, loss_sup: 0.112615, loss_mps: 0.049206, loss_cps: 0.085313
[12:56:20.516] iteration 8614: total_loss: 0.353490, loss_sup: 0.159664, loss_mps: 0.067133, loss_cps: 0.126694
[12:56:20.662] iteration 8615: total_loss: 0.383809, loss_sup: 0.190102, loss_mps: 0.065767, loss_cps: 0.127941
[12:56:20.810] iteration 8616: total_loss: 0.177211, loss_sup: 0.034398, loss_mps: 0.050648, loss_cps: 0.092165
[12:56:20.956] iteration 8617: total_loss: 0.354736, loss_sup: 0.146275, loss_mps: 0.071502, loss_cps: 0.136959
[12:56:21.101] iteration 8618: total_loss: 0.271578, loss_sup: 0.072474, loss_mps: 0.069303, loss_cps: 0.129801
[12:56:21.248] iteration 8619: total_loss: 0.309119, loss_sup: 0.121011, loss_mps: 0.067430, loss_cps: 0.120678
[12:56:21.396] iteration 8620: total_loss: 0.265839, loss_sup: 0.100588, loss_mps: 0.059775, loss_cps: 0.105476
[12:56:21.541] iteration 8621: total_loss: 0.169791, loss_sup: 0.044357, loss_mps: 0.047662, loss_cps: 0.077772
[12:56:21.686] iteration 8622: total_loss: 0.418122, loss_sup: 0.163414, loss_mps: 0.087946, loss_cps: 0.166762
[12:56:21.831] iteration 8623: total_loss: 0.232205, loss_sup: 0.085221, loss_mps: 0.052153, loss_cps: 0.094831
[12:56:21.977] iteration 8624: total_loss: 0.441412, loss_sup: 0.215875, loss_mps: 0.074614, loss_cps: 0.150923
[12:56:22.122] iteration 8625: total_loss: 0.158292, loss_sup: 0.041218, loss_mps: 0.043420, loss_cps: 0.073654
[12:56:22.267] iteration 8626: total_loss: 0.099772, loss_sup: 0.012156, loss_mps: 0.034965, loss_cps: 0.052652
[12:56:22.416] iteration 8627: total_loss: 0.196070, loss_sup: 0.090151, loss_mps: 0.040185, loss_cps: 0.065735
[12:56:22.562] iteration 8628: total_loss: 0.313974, loss_sup: 0.129099, loss_mps: 0.064120, loss_cps: 0.120755
[12:56:22.707] iteration 8629: total_loss: 0.164991, loss_sup: 0.065641, loss_mps: 0.036736, loss_cps: 0.062614
[12:56:22.852] iteration 8630: total_loss: 0.152336, loss_sup: 0.057473, loss_mps: 0.036083, loss_cps: 0.058779
[12:56:22.999] iteration 8631: total_loss: 0.375760, loss_sup: 0.173111, loss_mps: 0.069046, loss_cps: 0.133603
[12:56:23.146] iteration 8632: total_loss: 0.236692, loss_sup: 0.061473, loss_mps: 0.061003, loss_cps: 0.114216
[12:56:23.291] iteration 8633: total_loss: 0.191675, loss_sup: 0.055209, loss_mps: 0.053189, loss_cps: 0.083277
[12:56:23.437] iteration 8634: total_loss: 0.251698, loss_sup: 0.104680, loss_mps: 0.053452, loss_cps: 0.093566
[12:56:23.582] iteration 8635: total_loss: 0.282948, loss_sup: 0.122135, loss_mps: 0.056669, loss_cps: 0.104144
[12:56:23.728] iteration 8636: total_loss: 0.129889, loss_sup: 0.013814, loss_mps: 0.043369, loss_cps: 0.072707
[12:56:23.876] iteration 8637: total_loss: 0.158763, loss_sup: 0.027147, loss_mps: 0.047526, loss_cps: 0.084090
[12:56:24.022] iteration 8638: total_loss: 0.133247, loss_sup: 0.057967, loss_mps: 0.028192, loss_cps: 0.047088
[12:56:24.168] iteration 8639: total_loss: 0.317482, loss_sup: 0.096843, loss_mps: 0.074036, loss_cps: 0.146603
[12:56:24.314] iteration 8640: total_loss: 0.397047, loss_sup: 0.246922, loss_mps: 0.051119, loss_cps: 0.099006
[12:56:24.460] iteration 8641: total_loss: 0.233613, loss_sup: 0.071428, loss_mps: 0.056648, loss_cps: 0.105537
[12:56:24.605] iteration 8642: total_loss: 0.214176, loss_sup: 0.074398, loss_mps: 0.047925, loss_cps: 0.091853
[12:56:24.751] iteration 8643: total_loss: 0.233245, loss_sup: 0.073694, loss_mps: 0.054509, loss_cps: 0.105042
[12:56:24.896] iteration 8644: total_loss: 0.218344, loss_sup: 0.088877, loss_mps: 0.046275, loss_cps: 0.083191
[12:56:25.041] iteration 8645: total_loss: 0.127260, loss_sup: 0.031597, loss_mps: 0.035860, loss_cps: 0.059803
[12:56:25.187] iteration 8646: total_loss: 0.161362, loss_sup: 0.039294, loss_mps: 0.045501, loss_cps: 0.076567
[12:56:25.335] iteration 8647: total_loss: 0.228337, loss_sup: 0.023203, loss_mps: 0.067405, loss_cps: 0.137728
[12:56:25.481] iteration 8648: total_loss: 0.353504, loss_sup: 0.179687, loss_mps: 0.058745, loss_cps: 0.115073
[12:56:25.627] iteration 8649: total_loss: 0.080666, loss_sup: 0.014776, loss_mps: 0.026533, loss_cps: 0.039357
[12:56:25.773] iteration 8650: total_loss: 0.097368, loss_sup: 0.024314, loss_mps: 0.028547, loss_cps: 0.044507
[12:56:25.919] iteration 8651: total_loss: 0.444172, loss_sup: 0.296651, loss_mps: 0.051086, loss_cps: 0.096435
[12:56:26.065] iteration 8652: total_loss: 0.165833, loss_sup: 0.024777, loss_mps: 0.047471, loss_cps: 0.093586
[12:56:26.211] iteration 8653: total_loss: 0.271319, loss_sup: 0.131222, loss_mps: 0.047180, loss_cps: 0.092916
[12:56:26.358] iteration 8654: total_loss: 0.207720, loss_sup: 0.074392, loss_mps: 0.045138, loss_cps: 0.088190
[12:56:26.504] iteration 8655: total_loss: 0.335780, loss_sup: 0.142159, loss_mps: 0.062059, loss_cps: 0.131562
[12:56:26.651] iteration 8656: total_loss: 0.366870, loss_sup: 0.203390, loss_mps: 0.057462, loss_cps: 0.106019
[12:56:26.797] iteration 8657: total_loss: 0.375024, loss_sup: 0.222907, loss_mps: 0.052539, loss_cps: 0.099578
[12:56:26.943] iteration 8658: total_loss: 0.205133, loss_sup: 0.057153, loss_mps: 0.049839, loss_cps: 0.098141
[12:56:27.089] iteration 8659: total_loss: 0.248785, loss_sup: 0.099679, loss_mps: 0.050923, loss_cps: 0.098183
[12:56:27.235] iteration 8660: total_loss: 0.171709, loss_sup: 0.071602, loss_mps: 0.037710, loss_cps: 0.062397
[12:56:27.381] iteration 8661: total_loss: 0.142686, loss_sup: 0.036679, loss_mps: 0.040536, loss_cps: 0.065470
[12:56:27.527] iteration 8662: total_loss: 0.272241, loss_sup: 0.060551, loss_mps: 0.067962, loss_cps: 0.143727
[12:56:27.672] iteration 8663: total_loss: 0.455443, loss_sup: 0.277664, loss_mps: 0.060451, loss_cps: 0.117328
[12:56:27.820] iteration 8664: total_loss: 0.128124, loss_sup: 0.031680, loss_mps: 0.034962, loss_cps: 0.061481
[12:56:27.965] iteration 8665: total_loss: 0.124812, loss_sup: 0.048440, loss_mps: 0.029774, loss_cps: 0.046597
[12:56:28.111] iteration 8666: total_loss: 0.146847, loss_sup: 0.065773, loss_mps: 0.031783, loss_cps: 0.049291
[12:56:28.257] iteration 8667: total_loss: 0.333107, loss_sup: 0.134213, loss_mps: 0.067375, loss_cps: 0.131520
[12:56:28.403] iteration 8668: total_loss: 0.166290, loss_sup: 0.043278, loss_mps: 0.042298, loss_cps: 0.080714
[12:56:28.550] iteration 8669: total_loss: 0.278746, loss_sup: 0.108017, loss_mps: 0.057736, loss_cps: 0.112993
[12:56:28.698] iteration 8670: total_loss: 0.310374, loss_sup: 0.125972, loss_mps: 0.063579, loss_cps: 0.120823
[12:56:28.847] iteration 8671: total_loss: 0.152526, loss_sup: 0.039615, loss_mps: 0.039697, loss_cps: 0.073214
[12:56:28.993] iteration 8672: total_loss: 0.473446, loss_sup: 0.334328, loss_mps: 0.050423, loss_cps: 0.088695
[12:56:29.140] iteration 8673: total_loss: 0.198635, loss_sup: 0.033212, loss_mps: 0.055944, loss_cps: 0.109480
[12:56:29.287] iteration 8674: total_loss: 0.223459, loss_sup: 0.077022, loss_mps: 0.051765, loss_cps: 0.094672
[12:56:29.436] iteration 8675: total_loss: 0.290241, loss_sup: 0.117965, loss_mps: 0.056966, loss_cps: 0.115310
[12:56:29.583] iteration 8676: total_loss: 0.212495, loss_sup: 0.069167, loss_mps: 0.049642, loss_cps: 0.093685
[12:56:29.729] iteration 8677: total_loss: 0.198876, loss_sup: 0.036832, loss_mps: 0.053614, loss_cps: 0.108430
[12:56:29.875] iteration 8678: total_loss: 0.206418, loss_sup: 0.037735, loss_mps: 0.057921, loss_cps: 0.110761
[12:56:30.020] iteration 8679: total_loss: 0.164892, loss_sup: 0.053735, loss_mps: 0.039566, loss_cps: 0.071591
[12:56:30.166] iteration 8680: total_loss: 0.160141, loss_sup: 0.039525, loss_mps: 0.043431, loss_cps: 0.077185
[12:56:30.313] iteration 8681: total_loss: 0.265678, loss_sup: 0.126477, loss_mps: 0.047134, loss_cps: 0.092067
[12:56:30.460] iteration 8682: total_loss: 0.494478, loss_sup: 0.264335, loss_mps: 0.074595, loss_cps: 0.155549
[12:56:30.610] iteration 8683: total_loss: 0.295440, loss_sup: 0.081551, loss_mps: 0.070258, loss_cps: 0.143631
[12:56:30.758] iteration 8684: total_loss: 0.257949, loss_sup: 0.101463, loss_mps: 0.055195, loss_cps: 0.101291
[12:56:30.905] iteration 8685: total_loss: 0.163843, loss_sup: 0.037827, loss_mps: 0.045749, loss_cps: 0.080267
[12:56:31.052] iteration 8686: total_loss: 0.285363, loss_sup: 0.088989, loss_mps: 0.067418, loss_cps: 0.128955
[12:56:31.197] iteration 8687: total_loss: 0.241908, loss_sup: 0.050497, loss_mps: 0.064133, loss_cps: 0.127278
[12:56:31.349] iteration 8688: total_loss: 0.340653, loss_sup: 0.128964, loss_mps: 0.071455, loss_cps: 0.140234
[12:56:31.496] iteration 8689: total_loss: 0.126779, loss_sup: 0.022761, loss_mps: 0.038400, loss_cps: 0.065617
[12:56:31.643] iteration 8690: total_loss: 0.207830, loss_sup: 0.083848, loss_mps: 0.042981, loss_cps: 0.081001
[12:56:31.793] iteration 8691: total_loss: 0.299106, loss_sup: 0.180092, loss_mps: 0.043474, loss_cps: 0.075540
[12:56:31.938] iteration 8692: total_loss: 0.120504, loss_sup: 0.014570, loss_mps: 0.040793, loss_cps: 0.065141
[12:56:32.087] iteration 8693: total_loss: 0.239611, loss_sup: 0.109325, loss_mps: 0.046180, loss_cps: 0.084106
[12:56:32.233] iteration 8694: total_loss: 0.207549, loss_sup: 0.073217, loss_mps: 0.046609, loss_cps: 0.087724
[12:56:32.379] iteration 8695: total_loss: 0.571449, loss_sup: 0.335742, loss_mps: 0.076959, loss_cps: 0.158747
[12:56:32.525] iteration 8696: total_loss: 0.185012, loss_sup: 0.058869, loss_mps: 0.045986, loss_cps: 0.080157
[12:56:32.673] iteration 8697: total_loss: 0.255021, loss_sup: 0.112897, loss_mps: 0.050810, loss_cps: 0.091314
[12:56:32.818] iteration 8698: total_loss: 0.424029, loss_sup: 0.243218, loss_mps: 0.063608, loss_cps: 0.117204
[12:56:32.965] iteration 8699: total_loss: 0.351982, loss_sup: 0.117121, loss_mps: 0.074374, loss_cps: 0.160487
[12:56:33.111] iteration 8700: total_loss: 0.190544, loss_sup: 0.052566, loss_mps: 0.047735, loss_cps: 0.090243
[12:56:33.111] Evaluation Started ==>
[12:56:44.574] ==> valid iteration 8700: unet metrics: {'dc': 0.6325136886574775, 'jc': 0.5098742033521779, 'pre': 0.7394031735732087, 'hd': 5.754297694892935}, ynet metrics: {'dc': 0.5698456246927177, 'jc': 0.4515479006885491, 'pre': 0.7686926638912143, 'hd': 5.8624089589187856}.
[12:56:44.576] Evaluation Finished!⏹️
[12:56:44.726] iteration 8701: total_loss: 0.294724, loss_sup: 0.182913, loss_mps: 0.039783, loss_cps: 0.072028
[12:56:44.874] iteration 8702: total_loss: 0.199444, loss_sup: 0.037575, loss_mps: 0.054370, loss_cps: 0.107498
[12:56:45.020] iteration 8703: total_loss: 0.245581, loss_sup: 0.080426, loss_mps: 0.056088, loss_cps: 0.109067
[12:56:45.166] iteration 8704: total_loss: 0.328708, loss_sup: 0.182174, loss_mps: 0.051774, loss_cps: 0.094760
[12:56:45.312] iteration 8705: total_loss: 0.243107, loss_sup: 0.107658, loss_mps: 0.049444, loss_cps: 0.086006
[12:56:45.459] iteration 8706: total_loss: 0.239621, loss_sup: 0.041814, loss_mps: 0.068122, loss_cps: 0.129684
[12:56:45.605] iteration 8707: total_loss: 0.306709, loss_sup: 0.065136, loss_mps: 0.076543, loss_cps: 0.165031
[12:56:45.750] iteration 8708: total_loss: 0.181487, loss_sup: 0.071984, loss_mps: 0.039450, loss_cps: 0.070052
[12:56:45.895] iteration 8709: total_loss: 0.108706, loss_sup: 0.038402, loss_mps: 0.028001, loss_cps: 0.042303
[12:56:46.041] iteration 8710: total_loss: 0.269307, loss_sup: 0.099622, loss_mps: 0.059077, loss_cps: 0.110608
[12:56:46.189] iteration 8711: total_loss: 0.267738, loss_sup: 0.108653, loss_mps: 0.055292, loss_cps: 0.103793
[12:56:46.334] iteration 8712: total_loss: 0.199915, loss_sup: 0.068181, loss_mps: 0.048650, loss_cps: 0.083085
[12:56:46.480] iteration 8713: total_loss: 0.243518, loss_sup: 0.087928, loss_mps: 0.054924, loss_cps: 0.100666
[12:56:46.625] iteration 8714: total_loss: 0.219908, loss_sup: 0.068802, loss_mps: 0.052308, loss_cps: 0.098798
[12:56:46.771] iteration 8715: total_loss: 0.312531, loss_sup: 0.075136, loss_mps: 0.078357, loss_cps: 0.159038
[12:56:46.917] iteration 8716: total_loss: 0.234816, loss_sup: 0.091125, loss_mps: 0.049388, loss_cps: 0.094303
[12:56:47.062] iteration 8717: total_loss: 0.233873, loss_sup: 0.087910, loss_mps: 0.051363, loss_cps: 0.094600
[12:56:47.208] iteration 8718: total_loss: 0.162767, loss_sup: 0.036353, loss_mps: 0.044640, loss_cps: 0.081773
[12:56:47.353] iteration 8719: total_loss: 0.252381, loss_sup: 0.155256, loss_mps: 0.035243, loss_cps: 0.061882
[12:56:47.499] iteration 8720: total_loss: 0.169195, loss_sup: 0.050803, loss_mps: 0.043670, loss_cps: 0.074723
[12:56:47.644] iteration 8721: total_loss: 0.196477, loss_sup: 0.041726, loss_mps: 0.055405, loss_cps: 0.099346
[12:56:47.790] iteration 8722: total_loss: 0.255701, loss_sup: 0.082616, loss_mps: 0.061335, loss_cps: 0.111749
[12:56:47.936] iteration 8723: total_loss: 0.192557, loss_sup: 0.055336, loss_mps: 0.048789, loss_cps: 0.088432
[12:56:48.082] iteration 8724: total_loss: 0.157578, loss_sup: 0.042246, loss_mps: 0.041730, loss_cps: 0.073602
[12:56:48.227] iteration 8725: total_loss: 0.359501, loss_sup: 0.127565, loss_mps: 0.074437, loss_cps: 0.157500
[12:56:48.373] iteration 8726: total_loss: 0.155263, loss_sup: 0.014010, loss_mps: 0.048581, loss_cps: 0.092672
[12:56:48.520] iteration 8727: total_loss: 0.217382, loss_sup: 0.088475, loss_mps: 0.043333, loss_cps: 0.085574
[12:56:48.668] iteration 8728: total_loss: 0.163094, loss_sup: 0.044411, loss_mps: 0.042509, loss_cps: 0.076175
[12:56:48.813] iteration 8729: total_loss: 0.207761, loss_sup: 0.063552, loss_mps: 0.050495, loss_cps: 0.093714
[12:56:48.959] iteration 8730: total_loss: 0.240368, loss_sup: 0.094678, loss_mps: 0.048963, loss_cps: 0.096728
[12:56:49.108] iteration 8731: total_loss: 0.139581, loss_sup: 0.015681, loss_mps: 0.043465, loss_cps: 0.080435
[12:56:49.253] iteration 8732: total_loss: 0.267016, loss_sup: 0.083535, loss_mps: 0.059638, loss_cps: 0.123843
[12:56:49.399] iteration 8733: total_loss: 0.354784, loss_sup: 0.134501, loss_mps: 0.070762, loss_cps: 0.149521
[12:56:49.545] iteration 8734: total_loss: 0.420125, loss_sup: 0.280825, loss_mps: 0.048469, loss_cps: 0.090831
[12:56:49.691] iteration 8735: total_loss: 0.311683, loss_sup: 0.182740, loss_mps: 0.044116, loss_cps: 0.084827
[12:56:49.836] iteration 8736: total_loss: 0.274097, loss_sup: 0.146925, loss_mps: 0.043613, loss_cps: 0.083559
[12:56:49.982] iteration 8737: total_loss: 0.270970, loss_sup: 0.107190, loss_mps: 0.054471, loss_cps: 0.109309
[12:56:50.128] iteration 8738: total_loss: 0.169019, loss_sup: 0.072911, loss_mps: 0.035273, loss_cps: 0.060835
[12:56:50.273] iteration 8739: total_loss: 0.231758, loss_sup: 0.063808, loss_mps: 0.056239, loss_cps: 0.111712
[12:56:50.419] iteration 8740: total_loss: 0.261714, loss_sup: 0.109537, loss_mps: 0.050754, loss_cps: 0.101423
[12:56:50.565] iteration 8741: total_loss: 0.189229, loss_sup: 0.046631, loss_mps: 0.052341, loss_cps: 0.090257
[12:56:50.711] iteration 8742: total_loss: 0.234046, loss_sup: 0.075198, loss_mps: 0.052632, loss_cps: 0.106216
[12:56:50.856] iteration 8743: total_loss: 0.283480, loss_sup: 0.120017, loss_mps: 0.056102, loss_cps: 0.107360
[12:56:51.001] iteration 8744: total_loss: 0.399999, loss_sup: 0.251762, loss_mps: 0.051389, loss_cps: 0.096848
[12:56:51.147] iteration 8745: total_loss: 0.145770, loss_sup: 0.015224, loss_mps: 0.045503, loss_cps: 0.085042
[12:56:51.292] iteration 8746: total_loss: 0.481207, loss_sup: 0.340421, loss_mps: 0.049469, loss_cps: 0.091317
[12:56:51.438] iteration 8747: total_loss: 0.278215, loss_sup: 0.138982, loss_mps: 0.049615, loss_cps: 0.089618
[12:56:51.584] iteration 8748: total_loss: 0.207495, loss_sup: 0.049988, loss_mps: 0.054793, loss_cps: 0.102714
[12:56:51.729] iteration 8749: total_loss: 0.191016, loss_sup: 0.046164, loss_mps: 0.050991, loss_cps: 0.093861
[12:56:51.875] iteration 8750: total_loss: 0.169215, loss_sup: 0.031874, loss_mps: 0.048039, loss_cps: 0.089302
[12:56:52.020] iteration 8751: total_loss: 0.205915, loss_sup: 0.082663, loss_mps: 0.046232, loss_cps: 0.077020
[12:56:52.166] iteration 8752: total_loss: 0.264876, loss_sup: 0.114320, loss_mps: 0.052060, loss_cps: 0.098496
[12:56:52.311] iteration 8753: total_loss: 0.260188, loss_sup: 0.077520, loss_mps: 0.060269, loss_cps: 0.122399
[12:56:52.457] iteration 8754: total_loss: 0.300559, loss_sup: 0.089584, loss_mps: 0.070614, loss_cps: 0.140361
[12:56:52.603] iteration 8755: total_loss: 0.253131, loss_sup: 0.108259, loss_mps: 0.050003, loss_cps: 0.094869
[12:56:52.748] iteration 8756: total_loss: 0.271311, loss_sup: 0.143346, loss_mps: 0.044189, loss_cps: 0.083775
[12:56:52.893] iteration 8757: total_loss: 0.137713, loss_sup: 0.030893, loss_mps: 0.039882, loss_cps: 0.066938
[12:56:53.039] iteration 8758: total_loss: 0.179184, loss_sup: 0.058286, loss_mps: 0.044647, loss_cps: 0.076252
[12:56:53.184] iteration 8759: total_loss: 0.147986, loss_sup: 0.049454, loss_mps: 0.037535, loss_cps: 0.060996
[12:56:53.330] iteration 8760: total_loss: 0.333726, loss_sup: 0.120054, loss_mps: 0.074032, loss_cps: 0.139640
[12:56:53.479] iteration 8761: total_loss: 0.112517, loss_sup: 0.007343, loss_mps: 0.038645, loss_cps: 0.066529
[12:56:53.625] iteration 8762: total_loss: 0.268053, loss_sup: 0.069837, loss_mps: 0.066834, loss_cps: 0.131382
[12:56:53.770] iteration 8763: total_loss: 0.296539, loss_sup: 0.145232, loss_mps: 0.052879, loss_cps: 0.098427
[12:56:53.916] iteration 8764: total_loss: 0.190385, loss_sup: 0.053394, loss_mps: 0.049693, loss_cps: 0.087297
[12:56:54.061] iteration 8765: total_loss: 0.417104, loss_sup: 0.253687, loss_mps: 0.058570, loss_cps: 0.104848
[12:56:54.206] iteration 8766: total_loss: 0.263043, loss_sup: 0.132597, loss_mps: 0.046669, loss_cps: 0.083778
[12:56:54.352] iteration 8767: total_loss: 0.201642, loss_sup: 0.050232, loss_mps: 0.054331, loss_cps: 0.097080
[12:56:54.498] iteration 8768: total_loss: 0.226298, loss_sup: 0.090937, loss_mps: 0.047815, loss_cps: 0.087546
[12:56:54.643] iteration 8769: total_loss: 0.145239, loss_sup: 0.012663, loss_mps: 0.047956, loss_cps: 0.084621
[12:56:54.789] iteration 8770: total_loss: 0.166856, loss_sup: 0.032338, loss_mps: 0.048268, loss_cps: 0.086250
[12:56:54.934] iteration 8771: total_loss: 0.139393, loss_sup: 0.012287, loss_mps: 0.045325, loss_cps: 0.081781
[12:56:55.080] iteration 8772: total_loss: 0.238715, loss_sup: 0.054602, loss_mps: 0.063523, loss_cps: 0.120589
[12:56:55.227] iteration 8773: total_loss: 0.187006, loss_sup: 0.027671, loss_mps: 0.056389, loss_cps: 0.102946
[12:56:55.373] iteration 8774: total_loss: 0.453052, loss_sup: 0.121521, loss_mps: 0.103959, loss_cps: 0.227572
[12:56:55.519] iteration 8775: total_loss: 0.174832, loss_sup: 0.024013, loss_mps: 0.052869, loss_cps: 0.097950
[12:56:55.664] iteration 8776: total_loss: 0.337905, loss_sup: 0.126665, loss_mps: 0.068518, loss_cps: 0.142722
[12:56:55.809] iteration 8777: total_loss: 0.291099, loss_sup: 0.156285, loss_mps: 0.047344, loss_cps: 0.087470
[12:56:55.874] iteration 8778: total_loss: 0.266497, loss_sup: 0.087909, loss_mps: 0.060464, loss_cps: 0.118124
[12:56:57.096] iteration 8779: total_loss: 0.399232, loss_sup: 0.223512, loss_mps: 0.060873, loss_cps: 0.114847
[12:56:57.244] iteration 8780: total_loss: 0.331149, loss_sup: 0.132067, loss_mps: 0.068084, loss_cps: 0.130998
[12:56:57.390] iteration 8781: total_loss: 0.207002, loss_sup: 0.021407, loss_mps: 0.060814, loss_cps: 0.124782
[12:56:57.539] iteration 8782: total_loss: 0.308348, loss_sup: 0.197280, loss_mps: 0.040916, loss_cps: 0.070152
[12:56:57.684] iteration 8783: total_loss: 0.298496, loss_sup: 0.123513, loss_mps: 0.059820, loss_cps: 0.115163
[12:56:57.830] iteration 8784: total_loss: 0.155252, loss_sup: 0.051470, loss_mps: 0.038783, loss_cps: 0.065000
[12:56:57.977] iteration 8785: total_loss: 0.219523, loss_sup: 0.049213, loss_mps: 0.056931, loss_cps: 0.113379
[12:56:58.127] iteration 8786: total_loss: 0.248488, loss_sup: 0.069053, loss_mps: 0.060991, loss_cps: 0.118444
[12:56:58.273] iteration 8787: total_loss: 0.317140, loss_sup: 0.147608, loss_mps: 0.058802, loss_cps: 0.110729
[12:56:58.419] iteration 8788: total_loss: 0.254143, loss_sup: 0.088556, loss_mps: 0.056128, loss_cps: 0.109459
[12:56:58.569] iteration 8789: total_loss: 0.134634, loss_sup: 0.027142, loss_mps: 0.037823, loss_cps: 0.069669
[12:56:58.718] iteration 8790: total_loss: 0.268340, loss_sup: 0.080549, loss_mps: 0.063263, loss_cps: 0.124528
[12:56:58.864] iteration 8791: total_loss: 0.266092, loss_sup: 0.076723, loss_mps: 0.063983, loss_cps: 0.125385
[12:56:59.010] iteration 8792: total_loss: 0.268860, loss_sup: 0.056086, loss_mps: 0.071096, loss_cps: 0.141678
[12:56:59.156] iteration 8793: total_loss: 0.169371, loss_sup: 0.043249, loss_mps: 0.046687, loss_cps: 0.079435
[12:56:59.305] iteration 8794: total_loss: 0.261553, loss_sup: 0.050719, loss_mps: 0.067207, loss_cps: 0.143627
[12:56:59.451] iteration 8795: total_loss: 0.149131, loss_sup: 0.041727, loss_mps: 0.039817, loss_cps: 0.067587
[12:56:59.602] iteration 8796: total_loss: 0.143226, loss_sup: 0.009269, loss_mps: 0.048761, loss_cps: 0.085196
[12:56:59.749] iteration 8797: total_loss: 0.349565, loss_sup: 0.138453, loss_mps: 0.069494, loss_cps: 0.141618
[12:56:59.896] iteration 8798: total_loss: 0.305718, loss_sup: 0.099751, loss_mps: 0.069229, loss_cps: 0.136738
[12:57:00.042] iteration 8799: total_loss: 0.265563, loss_sup: 0.125495, loss_mps: 0.049014, loss_cps: 0.091054
[12:57:00.188] iteration 8800: total_loss: 0.322622, loss_sup: 0.113542, loss_mps: 0.067924, loss_cps: 0.141156
[12:57:00.188] Evaluation Started ==>
[12:57:11.566] ==> valid iteration 8800: unet metrics: {'dc': 0.630152357396841, 'jc': 0.5062141373720448, 'pre': 0.6990437995317083, 'hd': 6.18064321842414}, ynet metrics: {'dc': 0.574803793355254, 'jc': 0.4589815692058974, 'pre': 0.7466353955121626, 'hd': 6.045324375049654}.
[12:57:11.568] Evaluation Finished!⏹️
[12:57:11.721] iteration 8801: total_loss: 0.178778, loss_sup: 0.043165, loss_mps: 0.047938, loss_cps: 0.087676
[12:57:11.868] iteration 8802: total_loss: 0.242221, loss_sup: 0.102459, loss_mps: 0.048279, loss_cps: 0.091483
[12:57:12.014] iteration 8803: total_loss: 0.189403, loss_sup: 0.037342, loss_mps: 0.051000, loss_cps: 0.101062
[12:57:12.159] iteration 8804: total_loss: 0.281994, loss_sup: 0.095298, loss_mps: 0.062414, loss_cps: 0.124282
[12:57:12.306] iteration 8805: total_loss: 0.215175, loss_sup: 0.108297, loss_mps: 0.039385, loss_cps: 0.067493
[12:57:12.452] iteration 8806: total_loss: 0.328404, loss_sup: 0.178232, loss_mps: 0.050357, loss_cps: 0.099815
[12:57:12.598] iteration 8807: total_loss: 0.397561, loss_sup: 0.242115, loss_mps: 0.057408, loss_cps: 0.098037
[12:57:12.744] iteration 8808: total_loss: 0.226624, loss_sup: 0.095548, loss_mps: 0.049052, loss_cps: 0.082024
[12:57:12.889] iteration 8809: total_loss: 0.215154, loss_sup: 0.123718, loss_mps: 0.035896, loss_cps: 0.055540
[12:57:13.034] iteration 8810: total_loss: 0.379762, loss_sup: 0.235072, loss_mps: 0.049696, loss_cps: 0.094994
[12:57:13.180] iteration 8811: total_loss: 0.250086, loss_sup: 0.133413, loss_mps: 0.041216, loss_cps: 0.075457
[12:57:13.325] iteration 8812: total_loss: 0.197510, loss_sup: 0.096962, loss_mps: 0.039053, loss_cps: 0.061495
[12:57:13.472] iteration 8813: total_loss: 0.257205, loss_sup: 0.058192, loss_mps: 0.068124, loss_cps: 0.130889
[12:57:13.619] iteration 8814: total_loss: 0.105589, loss_sup: 0.011820, loss_mps: 0.035073, loss_cps: 0.058696
[12:57:13.765] iteration 8815: total_loss: 0.394008, loss_sup: 0.104363, loss_mps: 0.091835, loss_cps: 0.197810
[12:57:13.911] iteration 8816: total_loss: 0.344756, loss_sup: 0.154312, loss_mps: 0.065870, loss_cps: 0.124574
[12:57:14.056] iteration 8817: total_loss: 0.195021, loss_sup: 0.061187, loss_mps: 0.048084, loss_cps: 0.085749
[12:57:14.203] iteration 8818: total_loss: 0.179997, loss_sup: 0.038780, loss_mps: 0.049333, loss_cps: 0.091884
[12:57:14.349] iteration 8819: total_loss: 0.182108, loss_sup: 0.051489, loss_mps: 0.048244, loss_cps: 0.082375
[12:57:14.494] iteration 8820: total_loss: 0.260641, loss_sup: 0.102311, loss_mps: 0.054816, loss_cps: 0.103514
[12:57:14.640] iteration 8821: total_loss: 0.213907, loss_sup: 0.034446, loss_mps: 0.060913, loss_cps: 0.118548
[12:57:14.785] iteration 8822: total_loss: 0.457927, loss_sup: 0.139188, loss_mps: 0.103646, loss_cps: 0.215093
[12:57:14.931] iteration 8823: total_loss: 0.443248, loss_sup: 0.148160, loss_mps: 0.092330, loss_cps: 0.202758
[12:57:15.080] iteration 8824: total_loss: 0.247629, loss_sup: 0.078991, loss_mps: 0.057424, loss_cps: 0.111214
[12:57:15.225] iteration 8825: total_loss: 0.157000, loss_sup: 0.045820, loss_mps: 0.041083, loss_cps: 0.070097
[12:57:15.372] iteration 8826: total_loss: 0.254554, loss_sup: 0.129060, loss_mps: 0.045951, loss_cps: 0.079543
[12:57:15.518] iteration 8827: total_loss: 0.184726, loss_sup: 0.057250, loss_mps: 0.046557, loss_cps: 0.080919
[12:57:15.664] iteration 8828: total_loss: 0.276549, loss_sup: 0.088206, loss_mps: 0.062096, loss_cps: 0.126248
[12:57:15.809] iteration 8829: total_loss: 0.239483, loss_sup: 0.105064, loss_mps: 0.048536, loss_cps: 0.085883
[12:57:15.955] iteration 8830: total_loss: 0.149310, loss_sup: 0.029636, loss_mps: 0.043028, loss_cps: 0.076646
[12:57:16.100] iteration 8831: total_loss: 0.213708, loss_sup: 0.021856, loss_mps: 0.065893, loss_cps: 0.125958
[12:57:16.246] iteration 8832: total_loss: 0.206864, loss_sup: 0.058361, loss_mps: 0.053713, loss_cps: 0.094790
[12:57:16.396] iteration 8833: total_loss: 0.257440, loss_sup: 0.127030, loss_mps: 0.048205, loss_cps: 0.082205
[12:57:16.543] iteration 8834: total_loss: 0.154215, loss_sup: 0.049870, loss_mps: 0.039599, loss_cps: 0.064746
[12:57:16.689] iteration 8835: total_loss: 0.166697, loss_sup: 0.059441, loss_mps: 0.038297, loss_cps: 0.068959
[12:57:16.835] iteration 8836: total_loss: 0.194556, loss_sup: 0.029147, loss_mps: 0.058827, loss_cps: 0.106583
[12:57:16.981] iteration 8837: total_loss: 0.159323, loss_sup: 0.031819, loss_mps: 0.045301, loss_cps: 0.082204
[12:57:17.128] iteration 8838: total_loss: 0.255785, loss_sup: 0.076254, loss_mps: 0.062770, loss_cps: 0.116761
[12:57:17.274] iteration 8839: total_loss: 0.220243, loss_sup: 0.040780, loss_mps: 0.059365, loss_cps: 0.120099
[12:57:17.420] iteration 8840: total_loss: 0.321623, loss_sup: 0.106448, loss_mps: 0.072079, loss_cps: 0.143095
[12:57:17.566] iteration 8841: total_loss: 0.168753, loss_sup: 0.015239, loss_mps: 0.051883, loss_cps: 0.101631
[12:57:17.713] iteration 8842: total_loss: 0.182969, loss_sup: 0.057973, loss_mps: 0.045369, loss_cps: 0.079627
[12:57:17.860] iteration 8843: total_loss: 0.206912, loss_sup: 0.056331, loss_mps: 0.053153, loss_cps: 0.097429
[12:57:18.011] iteration 8844: total_loss: 0.114762, loss_sup: 0.023172, loss_mps: 0.033853, loss_cps: 0.057737
[12:57:18.157] iteration 8845: total_loss: 0.243173, loss_sup: 0.091894, loss_mps: 0.051966, loss_cps: 0.099313
[12:57:18.303] iteration 8846: total_loss: 0.232480, loss_sup: 0.098949, loss_mps: 0.045614, loss_cps: 0.087917
[12:57:18.450] iteration 8847: total_loss: 0.229866, loss_sup: 0.082150, loss_mps: 0.049167, loss_cps: 0.098549
[12:57:18.596] iteration 8848: total_loss: 0.245845, loss_sup: 0.096387, loss_mps: 0.048608, loss_cps: 0.100850
[12:57:18.741] iteration 8849: total_loss: 0.284024, loss_sup: 0.179051, loss_mps: 0.038009, loss_cps: 0.066964
[12:57:18.889] iteration 8850: total_loss: 0.179654, loss_sup: 0.036840, loss_mps: 0.049398, loss_cps: 0.093416
[12:57:19.036] iteration 8851: total_loss: 0.404014, loss_sup: 0.197375, loss_mps: 0.067929, loss_cps: 0.138709
[12:57:19.182] iteration 8852: total_loss: 0.137445, loss_sup: 0.020374, loss_mps: 0.040652, loss_cps: 0.076419
[12:57:19.328] iteration 8853: total_loss: 0.362724, loss_sup: 0.185184, loss_mps: 0.057990, loss_cps: 0.119550
[12:57:19.475] iteration 8854: total_loss: 0.144772, loss_sup: 0.033512, loss_mps: 0.038858, loss_cps: 0.072403
[12:57:19.620] iteration 8855: total_loss: 0.205527, loss_sup: 0.051648, loss_mps: 0.051443, loss_cps: 0.102436
[12:57:19.767] iteration 8856: total_loss: 0.151734, loss_sup: 0.074888, loss_mps: 0.029965, loss_cps: 0.046882
[12:57:19.913] iteration 8857: total_loss: 0.119316, loss_sup: 0.029791, loss_mps: 0.034257, loss_cps: 0.055267
[12:57:20.059] iteration 8858: total_loss: 0.269061, loss_sup: 0.111687, loss_mps: 0.053955, loss_cps: 0.103418
[12:57:20.204] iteration 8859: total_loss: 0.157194, loss_sup: 0.061627, loss_mps: 0.034616, loss_cps: 0.060951
[12:57:20.351] iteration 8860: total_loss: 0.126729, loss_sup: 0.006146, loss_mps: 0.043442, loss_cps: 0.077141
[12:57:20.496] iteration 8861: total_loss: 0.165904, loss_sup: 0.041415, loss_mps: 0.042355, loss_cps: 0.082134
[12:57:20.642] iteration 8862: total_loss: 0.212486, loss_sup: 0.099510, loss_mps: 0.039891, loss_cps: 0.073085
[12:57:20.788] iteration 8863: total_loss: 0.518433, loss_sup: 0.170457, loss_mps: 0.107336, loss_cps: 0.240641
[12:57:20.934] iteration 8864: total_loss: 0.227504, loss_sup: 0.111206, loss_mps: 0.041579, loss_cps: 0.074719
[12:57:21.080] iteration 8865: total_loss: 0.245425, loss_sup: 0.135737, loss_mps: 0.039871, loss_cps: 0.069817
[12:57:21.230] iteration 8866: total_loss: 0.167597, loss_sup: 0.029840, loss_mps: 0.046121, loss_cps: 0.091636
[12:57:21.376] iteration 8867: total_loss: 0.229245, loss_sup: 0.098626, loss_mps: 0.046341, loss_cps: 0.084278
[12:57:21.521] iteration 8868: total_loss: 0.185047, loss_sup: 0.094853, loss_mps: 0.031710, loss_cps: 0.058484
[12:57:21.667] iteration 8869: total_loss: 0.297244, loss_sup: 0.130080, loss_mps: 0.056854, loss_cps: 0.110310
[12:57:21.813] iteration 8870: total_loss: 0.108870, loss_sup: 0.013536, loss_mps: 0.036089, loss_cps: 0.059245
[12:57:21.959] iteration 8871: total_loss: 0.146108, loss_sup: 0.036921, loss_mps: 0.041331, loss_cps: 0.067855
[12:57:22.106] iteration 8872: total_loss: 0.201774, loss_sup: 0.033636, loss_mps: 0.057980, loss_cps: 0.110157
[12:57:22.252] iteration 8873: total_loss: 0.252370, loss_sup: 0.090796, loss_mps: 0.054818, loss_cps: 0.106756
[12:57:22.398] iteration 8874: total_loss: 0.277392, loss_sup: 0.103043, loss_mps: 0.058083, loss_cps: 0.116266
[12:57:22.544] iteration 8875: total_loss: 0.175088, loss_sup: 0.044620, loss_mps: 0.044896, loss_cps: 0.085571
[12:57:22.692] iteration 8876: total_loss: 0.170588, loss_sup: 0.034236, loss_mps: 0.048279, loss_cps: 0.088074
[12:57:22.838] iteration 8877: total_loss: 0.342236, loss_sup: 0.089600, loss_mps: 0.079592, loss_cps: 0.173044
[12:57:22.988] iteration 8878: total_loss: 0.233499, loss_sup: 0.061181, loss_mps: 0.059118, loss_cps: 0.113200
[12:57:23.134] iteration 8879: total_loss: 0.338615, loss_sup: 0.168682, loss_mps: 0.057225, loss_cps: 0.112708
[12:57:23.280] iteration 8880: total_loss: 0.578241, loss_sup: 0.266779, loss_mps: 0.100224, loss_cps: 0.211238
[12:57:23.425] iteration 8881: total_loss: 0.273328, loss_sup: 0.129475, loss_mps: 0.049713, loss_cps: 0.094140
[12:57:23.571] iteration 8882: total_loss: 0.469517, loss_sup: 0.214323, loss_mps: 0.083559, loss_cps: 0.171635
[12:57:23.717] iteration 8883: total_loss: 0.149690, loss_sup: 0.020491, loss_mps: 0.045138, loss_cps: 0.084061
[12:57:23.862] iteration 8884: total_loss: 0.134398, loss_sup: 0.019134, loss_mps: 0.041938, loss_cps: 0.073326
[12:57:24.008] iteration 8885: total_loss: 0.375053, loss_sup: 0.192946, loss_mps: 0.059909, loss_cps: 0.122197
[12:57:24.155] iteration 8886: total_loss: 0.382925, loss_sup: 0.180177, loss_mps: 0.070203, loss_cps: 0.132544
[12:57:24.302] iteration 8887: total_loss: 0.171396, loss_sup: 0.020571, loss_mps: 0.053346, loss_cps: 0.097478
[12:57:24.448] iteration 8888: total_loss: 0.137077, loss_sup: 0.037735, loss_mps: 0.038411, loss_cps: 0.060930
[12:57:24.594] iteration 8889: total_loss: 0.315220, loss_sup: 0.087260, loss_mps: 0.075496, loss_cps: 0.152464
[12:57:24.742] iteration 8890: total_loss: 0.564646, loss_sup: 0.315199, loss_mps: 0.082456, loss_cps: 0.166991
[12:57:24.888] iteration 8891: total_loss: 0.378959, loss_sup: 0.175592, loss_mps: 0.064712, loss_cps: 0.138654
[12:57:25.034] iteration 8892: total_loss: 0.417336, loss_sup: 0.173188, loss_mps: 0.077239, loss_cps: 0.166909
[12:57:25.180] iteration 8893: total_loss: 0.229214, loss_sup: 0.086946, loss_mps: 0.050062, loss_cps: 0.092206
[12:57:25.326] iteration 8894: total_loss: 0.377009, loss_sup: 0.202547, loss_mps: 0.059465, loss_cps: 0.114997
[12:57:25.472] iteration 8895: total_loss: 0.304301, loss_sup: 0.047499, loss_mps: 0.082242, loss_cps: 0.174559
[12:57:25.618] iteration 8896: total_loss: 0.273453, loss_sup: 0.101618, loss_mps: 0.061593, loss_cps: 0.110242
[12:57:25.764] iteration 8897: total_loss: 0.197455, loss_sup: 0.033411, loss_mps: 0.054691, loss_cps: 0.109353
[12:57:25.910] iteration 8898: total_loss: 0.260926, loss_sup: 0.102202, loss_mps: 0.057294, loss_cps: 0.101430
[12:57:26.056] iteration 8899: total_loss: 0.331018, loss_sup: 0.105014, loss_mps: 0.075393, loss_cps: 0.150611
[12:57:26.202] iteration 8900: total_loss: 0.422047, loss_sup: 0.113465, loss_mps: 0.097135, loss_cps: 0.211448
[12:57:26.202] Evaluation Started ==>
[12:57:37.628] ==> valid iteration 8900: unet metrics: {'dc': 0.618846052753341, 'jc': 0.493973600502882, 'pre': 0.6908559937628928, 'hd': 6.3059910486949535}, ynet metrics: {'dc': 0.5461262757673803, 'jc': 0.4347871110530166, 'pre': 0.7373998654094933, 'hd': 6.080971460338043}.
[12:57:37.631] Evaluation Finished!⏹️
[12:57:37.783] iteration 8901: total_loss: 0.163911, loss_sup: 0.060857, loss_mps: 0.039241, loss_cps: 0.063813
[12:57:37.931] iteration 8902: total_loss: 0.341343, loss_sup: 0.208626, loss_mps: 0.048271, loss_cps: 0.084445
[12:57:38.081] iteration 8903: total_loss: 0.227193, loss_sup: 0.027115, loss_mps: 0.067046, loss_cps: 0.133031
[12:57:38.227] iteration 8904: total_loss: 0.236827, loss_sup: 0.081654, loss_mps: 0.054338, loss_cps: 0.100835
[12:57:38.373] iteration 8905: total_loss: 0.192198, loss_sup: 0.048988, loss_mps: 0.051615, loss_cps: 0.091595
[12:57:38.517] iteration 8906: total_loss: 0.287693, loss_sup: 0.063086, loss_mps: 0.073371, loss_cps: 0.151237
[12:57:38.663] iteration 8907: total_loss: 0.144199, loss_sup: 0.020424, loss_mps: 0.046033, loss_cps: 0.077743
[12:57:38.809] iteration 8908: total_loss: 0.211959, loss_sup: 0.079282, loss_mps: 0.046810, loss_cps: 0.085868
[12:57:38.956] iteration 8909: total_loss: 0.316772, loss_sup: 0.166112, loss_mps: 0.053997, loss_cps: 0.096663
[12:57:39.107] iteration 8910: total_loss: 0.311912, loss_sup: 0.191830, loss_mps: 0.045081, loss_cps: 0.075001
[12:57:39.253] iteration 8911: total_loss: 0.469271, loss_sup: 0.224667, loss_mps: 0.079479, loss_cps: 0.165125
[12:57:39.399] iteration 8912: total_loss: 0.431125, loss_sup: 0.146864, loss_mps: 0.095319, loss_cps: 0.188942
[12:57:39.546] iteration 8913: total_loss: 0.231253, loss_sup: 0.137647, loss_mps: 0.037013, loss_cps: 0.056592
[12:57:39.694] iteration 8914: total_loss: 0.212234, loss_sup: 0.060461, loss_mps: 0.053065, loss_cps: 0.098709
[12:57:39.839] iteration 8915: total_loss: 0.214339, loss_sup: 0.059372, loss_mps: 0.055739, loss_cps: 0.099228
[12:57:39.985] iteration 8916: total_loss: 0.148096, loss_sup: 0.013752, loss_mps: 0.046819, loss_cps: 0.087524
[12:57:40.135] iteration 8917: total_loss: 0.283504, loss_sup: 0.116599, loss_mps: 0.058334, loss_cps: 0.108572
[12:57:40.280] iteration 8918: total_loss: 0.224940, loss_sup: 0.043991, loss_mps: 0.063911, loss_cps: 0.117038
[12:57:40.426] iteration 8919: total_loss: 0.230450, loss_sup: 0.046123, loss_mps: 0.062775, loss_cps: 0.121552
[12:57:40.571] iteration 8920: total_loss: 0.215600, loss_sup: 0.057061, loss_mps: 0.054149, loss_cps: 0.104390
[12:57:40.719] iteration 8921: total_loss: 0.262734, loss_sup: 0.147489, loss_mps: 0.041609, loss_cps: 0.073636
[12:57:40.865] iteration 8922: total_loss: 0.180776, loss_sup: 0.030898, loss_mps: 0.051691, loss_cps: 0.098187
[12:57:41.011] iteration 8923: total_loss: 0.362268, loss_sup: 0.191149, loss_mps: 0.061621, loss_cps: 0.109498
[12:57:41.157] iteration 8924: total_loss: 0.272133, loss_sup: 0.091769, loss_mps: 0.061385, loss_cps: 0.118979
[12:57:41.304] iteration 8925: total_loss: 0.233231, loss_sup: 0.071818, loss_mps: 0.056799, loss_cps: 0.104615
[12:57:41.450] iteration 8926: total_loss: 0.248129, loss_sup: 0.118918, loss_mps: 0.046868, loss_cps: 0.082343
[12:57:41.597] iteration 8927: total_loss: 0.380689, loss_sup: 0.143175, loss_mps: 0.076660, loss_cps: 0.160853
[12:57:41.747] iteration 8928: total_loss: 0.363015, loss_sup: 0.203420, loss_mps: 0.056666, loss_cps: 0.102929
[12:57:41.894] iteration 8929: total_loss: 0.208057, loss_sup: 0.062558, loss_mps: 0.051446, loss_cps: 0.094053
[12:57:42.040] iteration 8930: total_loss: 0.175103, loss_sup: 0.042615, loss_mps: 0.048269, loss_cps: 0.084218
[12:57:42.186] iteration 8931: total_loss: 0.148858, loss_sup: 0.047212, loss_mps: 0.038861, loss_cps: 0.062785
[12:57:42.332] iteration 8932: total_loss: 0.307606, loss_sup: 0.039446, loss_mps: 0.089148, loss_cps: 0.179012
[12:57:42.479] iteration 8933: total_loss: 0.186967, loss_sup: 0.049836, loss_mps: 0.049087, loss_cps: 0.088044
[12:57:42.626] iteration 8934: total_loss: 0.263040, loss_sup: 0.113600, loss_mps: 0.054410, loss_cps: 0.095030
[12:57:42.772] iteration 8935: total_loss: 0.180868, loss_sup: 0.080627, loss_mps: 0.038349, loss_cps: 0.061891
[12:57:42.917] iteration 8936: total_loss: 0.144154, loss_sup: 0.036369, loss_mps: 0.040864, loss_cps: 0.066920
[12:57:43.063] iteration 8937: total_loss: 0.159596, loss_sup: 0.034432, loss_mps: 0.042716, loss_cps: 0.082448
[12:57:43.209] iteration 8938: total_loss: 0.195902, loss_sup: 0.033321, loss_mps: 0.056409, loss_cps: 0.106171
[12:57:43.355] iteration 8939: total_loss: 0.278238, loss_sup: 0.060690, loss_mps: 0.071982, loss_cps: 0.145566
[12:57:43.503] iteration 8940: total_loss: 0.254333, loss_sup: 0.090268, loss_mps: 0.057499, loss_cps: 0.106566
[12:57:43.649] iteration 8941: total_loss: 0.194692, loss_sup: 0.041231, loss_mps: 0.055698, loss_cps: 0.097763
[12:57:43.795] iteration 8942: total_loss: 0.160604, loss_sup: 0.050056, loss_mps: 0.041305, loss_cps: 0.069243
[12:57:43.942] iteration 8943: total_loss: 0.501566, loss_sup: 0.311933, loss_mps: 0.067383, loss_cps: 0.122250
[12:57:44.087] iteration 8944: total_loss: 0.130498, loss_sup: 0.037587, loss_mps: 0.035498, loss_cps: 0.057413
[12:57:44.234] iteration 8945: total_loss: 0.294172, loss_sup: 0.023087, loss_mps: 0.085289, loss_cps: 0.185797
[12:57:44.379] iteration 8946: total_loss: 0.305913, loss_sup: 0.101893, loss_mps: 0.065803, loss_cps: 0.138217
[12:57:44.526] iteration 8947: total_loss: 0.502821, loss_sup: 0.186229, loss_mps: 0.097624, loss_cps: 0.218968
[12:57:44.672] iteration 8948: total_loss: 0.227960, loss_sup: 0.058050, loss_mps: 0.056992, loss_cps: 0.112918
[12:57:44.817] iteration 8949: total_loss: 0.176808, loss_sup: 0.035248, loss_mps: 0.050558, loss_cps: 0.091003
[12:57:44.962] iteration 8950: total_loss: 0.453659, loss_sup: 0.239571, loss_mps: 0.073102, loss_cps: 0.140985
[12:57:45.108] iteration 8951: total_loss: 0.187467, loss_sup: 0.066024, loss_mps: 0.042658, loss_cps: 0.078785
[12:57:45.253] iteration 8952: total_loss: 0.606792, loss_sup: 0.300842, loss_mps: 0.098901, loss_cps: 0.207050
[12:57:45.399] iteration 8953: total_loss: 0.157110, loss_sup: 0.034011, loss_mps: 0.045476, loss_cps: 0.077624
[12:57:45.544] iteration 8954: total_loss: 0.241102, loss_sup: 0.090780, loss_mps: 0.050106, loss_cps: 0.100217
[12:57:45.690] iteration 8955: total_loss: 0.260766, loss_sup: 0.052426, loss_mps: 0.069282, loss_cps: 0.139058
[12:57:45.836] iteration 8956: total_loss: 0.362265, loss_sup: 0.165131, loss_mps: 0.066093, loss_cps: 0.131041
[12:57:45.982] iteration 8957: total_loss: 0.180379, loss_sup: 0.042280, loss_mps: 0.048806, loss_cps: 0.089293
[12:57:46.128] iteration 8958: total_loss: 0.294482, loss_sup: 0.087454, loss_mps: 0.067445, loss_cps: 0.139583
[12:57:46.273] iteration 8959: total_loss: 0.278527, loss_sup: 0.125075, loss_mps: 0.052448, loss_cps: 0.101004
[12:57:46.419] iteration 8960: total_loss: 0.151467, loss_sup: 0.035616, loss_mps: 0.042913, loss_cps: 0.072938
[12:57:46.565] iteration 8961: total_loss: 0.146302, loss_sup: 0.021151, loss_mps: 0.043393, loss_cps: 0.081759
[12:57:46.711] iteration 8962: total_loss: 0.171165, loss_sup: 0.053418, loss_mps: 0.043673, loss_cps: 0.074074
[12:57:46.857] iteration 8963: total_loss: 0.388606, loss_sup: 0.177368, loss_mps: 0.070047, loss_cps: 0.141191
[12:57:47.003] iteration 8964: total_loss: 0.263572, loss_sup: 0.111691, loss_mps: 0.057083, loss_cps: 0.094798
[12:57:47.148] iteration 8965: total_loss: 0.274891, loss_sup: 0.083636, loss_mps: 0.067817, loss_cps: 0.123438
[12:57:47.294] iteration 8966: total_loss: 0.318033, loss_sup: 0.044236, loss_mps: 0.090721, loss_cps: 0.183075
[12:57:47.440] iteration 8967: total_loss: 0.179627, loss_sup: 0.075511, loss_mps: 0.038602, loss_cps: 0.065514
[12:57:47.586] iteration 8968: total_loss: 0.332613, loss_sup: 0.170334, loss_mps: 0.057723, loss_cps: 0.104556
[12:57:47.731] iteration 8969: total_loss: 0.231377, loss_sup: 0.086669, loss_mps: 0.051345, loss_cps: 0.093363
[12:57:47.876] iteration 8970: total_loss: 0.228138, loss_sup: 0.095357, loss_mps: 0.046232, loss_cps: 0.086550
[12:57:48.022] iteration 8971: total_loss: 0.235050, loss_sup: 0.077565, loss_mps: 0.052523, loss_cps: 0.104962
[12:57:48.168] iteration 8972: total_loss: 0.226060, loss_sup: 0.074359, loss_mps: 0.053412, loss_cps: 0.098289
[12:57:48.313] iteration 8973: total_loss: 0.317389, loss_sup: 0.068052, loss_mps: 0.079489, loss_cps: 0.169848
[12:57:48.459] iteration 8974: total_loss: 0.255169, loss_sup: 0.042529, loss_mps: 0.073002, loss_cps: 0.139638
[12:57:48.604] iteration 8975: total_loss: 0.262353, loss_sup: 0.040324, loss_mps: 0.074106, loss_cps: 0.147924
[12:57:48.750] iteration 8976: total_loss: 0.231401, loss_sup: 0.061908, loss_mps: 0.057904, loss_cps: 0.111589
[12:57:48.895] iteration 8977: total_loss: 0.183426, loss_sup: 0.049658, loss_mps: 0.049273, loss_cps: 0.084496
[12:57:49.040] iteration 8978: total_loss: 0.245759, loss_sup: 0.072920, loss_mps: 0.062395, loss_cps: 0.110444
[12:57:49.186] iteration 8979: total_loss: 0.212087, loss_sup: 0.075247, loss_mps: 0.050739, loss_cps: 0.086101
[12:57:49.333] iteration 8980: total_loss: 0.218216, loss_sup: 0.040206, loss_mps: 0.061038, loss_cps: 0.116972
[12:57:49.478] iteration 8981: total_loss: 0.164184, loss_sup: 0.009826, loss_mps: 0.054092, loss_cps: 0.100266
[12:57:49.624] iteration 8982: total_loss: 0.178509, loss_sup: 0.031094, loss_mps: 0.051095, loss_cps: 0.096320
[12:57:49.770] iteration 8983: total_loss: 0.153310, loss_sup: 0.039225, loss_mps: 0.040811, loss_cps: 0.073274
[12:57:49.916] iteration 8984: total_loss: 0.191511, loss_sup: 0.040148, loss_mps: 0.051865, loss_cps: 0.099498
[12:57:50.061] iteration 8985: total_loss: 0.167716, loss_sup: 0.021814, loss_mps: 0.049112, loss_cps: 0.096789
[12:57:50.207] iteration 8986: total_loss: 0.160046, loss_sup: 0.019512, loss_mps: 0.049570, loss_cps: 0.090964
[12:57:50.354] iteration 8987: total_loss: 0.380428, loss_sup: 0.190556, loss_mps: 0.063353, loss_cps: 0.126519
[12:57:50.502] iteration 8988: total_loss: 0.215332, loss_sup: 0.090902, loss_mps: 0.042533, loss_cps: 0.081898
[12:57:50.648] iteration 8989: total_loss: 0.202457, loss_sup: 0.109921, loss_mps: 0.034635, loss_cps: 0.057901
[12:57:50.793] iteration 8990: total_loss: 0.329594, loss_sup: 0.163915, loss_mps: 0.055892, loss_cps: 0.109787
[12:57:50.939] iteration 8991: total_loss: 0.312932, loss_sup: 0.137591, loss_mps: 0.058455, loss_cps: 0.116886
[12:57:51.086] iteration 8992: total_loss: 0.199994, loss_sup: 0.042993, loss_mps: 0.053214, loss_cps: 0.103787
[12:57:51.231] iteration 8993: total_loss: 0.227786, loss_sup: 0.128906, loss_mps: 0.037616, loss_cps: 0.061265
[12:57:51.378] iteration 8994: total_loss: 0.298550, loss_sup: 0.132916, loss_mps: 0.055205, loss_cps: 0.110428
[12:57:51.524] iteration 8995: total_loss: 0.341747, loss_sup: 0.158718, loss_mps: 0.063539, loss_cps: 0.119489
[12:57:51.670] iteration 8996: total_loss: 0.389386, loss_sup: 0.253342, loss_mps: 0.048363, loss_cps: 0.087681
[12:57:51.816] iteration 8997: total_loss: 0.269673, loss_sup: 0.095627, loss_mps: 0.058067, loss_cps: 0.115978
[12:57:51.962] iteration 8998: total_loss: 0.325747, loss_sup: 0.139994, loss_mps: 0.061956, loss_cps: 0.123797
[12:57:52.110] iteration 8999: total_loss: 0.141103, loss_sup: 0.026333, loss_mps: 0.043446, loss_cps: 0.071324
[12:57:52.255] iteration 9000: total_loss: 0.159742, loss_sup: 0.017510, loss_mps: 0.049546, loss_cps: 0.092686
[12:57:52.256] Evaluation Started ==>
[12:58:03.714] ==> valid iteration 9000: unet metrics: {'dc': 0.6186594244223196, 'jc': 0.4970268775562674, 'pre': 0.7168386155711156, 'hd': 6.022659692025106}, ynet metrics: {'dc': 0.5842888734077858, 'jc': 0.46274070923068733, 'pre': 0.722233597233556, 'hd': 6.110037965852498}.
[12:58:03.716] Evaluation Finished!⏹️
[12:58:03.866] iteration 9001: total_loss: 0.279646, loss_sup: 0.133824, loss_mps: 0.051711, loss_cps: 0.094111
[12:58:04.014] iteration 9002: total_loss: 0.339981, loss_sup: 0.114427, loss_mps: 0.074932, loss_cps: 0.150622
[12:58:04.160] iteration 9003: total_loss: 0.368962, loss_sup: 0.111128, loss_mps: 0.085391, loss_cps: 0.172443
[12:58:04.306] iteration 9004: total_loss: 0.229288, loss_sup: 0.105387, loss_mps: 0.046343, loss_cps: 0.077558
[12:58:04.452] iteration 9005: total_loss: 0.240672, loss_sup: 0.080875, loss_mps: 0.057146, loss_cps: 0.102651
[12:58:04.601] iteration 9006: total_loss: 0.238630, loss_sup: 0.065582, loss_mps: 0.060096, loss_cps: 0.112952
[12:58:04.747] iteration 9007: total_loss: 0.349737, loss_sup: 0.156282, loss_mps: 0.065832, loss_cps: 0.127624
[12:58:04.894] iteration 9008: total_loss: 0.418066, loss_sup: 0.242773, loss_mps: 0.061810, loss_cps: 0.113483
[12:58:05.040] iteration 9009: total_loss: 0.164577, loss_sup: 0.007744, loss_mps: 0.054822, loss_cps: 0.102011
[12:58:05.186] iteration 9010: total_loss: 0.199651, loss_sup: 0.065376, loss_mps: 0.048889, loss_cps: 0.085386
[12:58:05.338] iteration 9011: total_loss: 0.326016, loss_sup: 0.142532, loss_mps: 0.063176, loss_cps: 0.120308
[12:58:05.486] iteration 9012: total_loss: 0.274679, loss_sup: 0.100921, loss_mps: 0.061588, loss_cps: 0.112170
[12:58:05.632] iteration 9013: total_loss: 0.222416, loss_sup: 0.065789, loss_mps: 0.054420, loss_cps: 0.102207
[12:58:05.777] iteration 9014: total_loss: 0.360453, loss_sup: 0.139564, loss_mps: 0.073865, loss_cps: 0.147024
[12:58:05.923] iteration 9015: total_loss: 0.225869, loss_sup: 0.062040, loss_mps: 0.056713, loss_cps: 0.107117
[12:58:06.069] iteration 9016: total_loss: 0.283067, loss_sup: 0.090365, loss_mps: 0.064735, loss_cps: 0.127967
[12:58:06.215] iteration 9017: total_loss: 0.163126, loss_sup: 0.035088, loss_mps: 0.046465, loss_cps: 0.081573
[12:58:06.362] iteration 9018: total_loss: 0.457245, loss_sup: 0.262558, loss_mps: 0.067185, loss_cps: 0.127501
[12:58:06.507] iteration 9019: total_loss: 0.298516, loss_sup: 0.143922, loss_mps: 0.053584, loss_cps: 0.101010
[12:58:06.654] iteration 9020: total_loss: 0.219932, loss_sup: 0.095005, loss_mps: 0.045330, loss_cps: 0.079598
[12:58:06.800] iteration 9021: total_loss: 0.254471, loss_sup: 0.084439, loss_mps: 0.056655, loss_cps: 0.113378
[12:58:06.945] iteration 9022: total_loss: 0.203345, loss_sup: 0.010454, loss_mps: 0.062096, loss_cps: 0.130794
[12:58:07.091] iteration 9023: total_loss: 0.143327, loss_sup: 0.019861, loss_mps: 0.045093, loss_cps: 0.078372
[12:58:07.237] iteration 9024: total_loss: 0.262384, loss_sup: 0.141112, loss_mps: 0.043943, loss_cps: 0.077329
[12:58:07.383] iteration 9025: total_loss: 0.206258, loss_sup: 0.064221, loss_mps: 0.052027, loss_cps: 0.090010
[12:58:07.529] iteration 9026: total_loss: 0.202768, loss_sup: 0.077675, loss_mps: 0.044846, loss_cps: 0.080248
[12:58:07.674] iteration 9027: total_loss: 0.185402, loss_sup: 0.087599, loss_mps: 0.034555, loss_cps: 0.063247
[12:58:07.820] iteration 9028: total_loss: 0.137400, loss_sup: 0.041570, loss_mps: 0.034494, loss_cps: 0.061336
[12:58:07.966] iteration 9029: total_loss: 0.185232, loss_sup: 0.061316, loss_mps: 0.043591, loss_cps: 0.080324
[12:58:08.112] iteration 9030: total_loss: 0.119376, loss_sup: 0.027611, loss_mps: 0.035565, loss_cps: 0.056201
[12:58:08.259] iteration 9031: total_loss: 0.182600, loss_sup: 0.041926, loss_mps: 0.048597, loss_cps: 0.092077
[12:58:08.405] iteration 9032: total_loss: 0.194258, loss_sup: 0.095738, loss_mps: 0.036619, loss_cps: 0.061901
[12:58:08.552] iteration 9033: total_loss: 0.308119, loss_sup: 0.105159, loss_mps: 0.066149, loss_cps: 0.136811
[12:58:08.699] iteration 9034: total_loss: 0.314866, loss_sup: 0.077518, loss_mps: 0.075109, loss_cps: 0.162239
[12:58:08.845] iteration 9035: total_loss: 0.470739, loss_sup: 0.300192, loss_mps: 0.060600, loss_cps: 0.109947
[12:58:08.991] iteration 9036: total_loss: 0.275537, loss_sup: 0.128887, loss_mps: 0.051018, loss_cps: 0.095632
[12:58:09.140] iteration 9037: total_loss: 0.284620, loss_sup: 0.080101, loss_mps: 0.068578, loss_cps: 0.135940
[12:58:09.288] iteration 9038: total_loss: 0.210921, loss_sup: 0.063633, loss_mps: 0.048492, loss_cps: 0.098796
[12:58:09.435] iteration 9039: total_loss: 0.249679, loss_sup: 0.054532, loss_mps: 0.067884, loss_cps: 0.127263
[12:58:09.582] iteration 9040: total_loss: 0.388230, loss_sup: 0.191736, loss_mps: 0.066386, loss_cps: 0.130108
[12:58:09.728] iteration 9041: total_loss: 0.264550, loss_sup: 0.045873, loss_mps: 0.072285, loss_cps: 0.146392
[12:58:09.875] iteration 9042: total_loss: 0.150805, loss_sup: 0.015599, loss_mps: 0.049816, loss_cps: 0.085389
[12:58:10.022] iteration 9043: total_loss: 0.146373, loss_sup: 0.043246, loss_mps: 0.038086, loss_cps: 0.065041
[12:58:10.169] iteration 9044: total_loss: 0.127943, loss_sup: 0.041235, loss_mps: 0.034279, loss_cps: 0.052429
[12:58:10.315] iteration 9045: total_loss: 0.195218, loss_sup: 0.031480, loss_mps: 0.057535, loss_cps: 0.106203
[12:58:10.461] iteration 9046: total_loss: 0.297892, loss_sup: 0.128771, loss_mps: 0.057938, loss_cps: 0.111182
[12:58:10.606] iteration 9047: total_loss: 0.165463, loss_sup: 0.040217, loss_mps: 0.044367, loss_cps: 0.080879
[12:58:10.753] iteration 9048: total_loss: 0.347040, loss_sup: 0.102022, loss_mps: 0.078447, loss_cps: 0.166571
[12:58:10.903] iteration 9049: total_loss: 0.367431, loss_sup: 0.119664, loss_mps: 0.079544, loss_cps: 0.168223
[12:58:11.049] iteration 9050: total_loss: 0.469386, loss_sup: 0.236029, loss_mps: 0.075087, loss_cps: 0.158270
[12:58:11.196] iteration 9051: total_loss: 0.183894, loss_sup: 0.046838, loss_mps: 0.049507, loss_cps: 0.087550
[12:58:11.342] iteration 9052: total_loss: 0.155912, loss_sup: 0.029571, loss_mps: 0.044718, loss_cps: 0.081623
[12:58:11.490] iteration 9053: total_loss: 0.294510, loss_sup: 0.121070, loss_mps: 0.058416, loss_cps: 0.115024
[12:58:11.636] iteration 9054: total_loss: 0.392894, loss_sup: 0.111700, loss_mps: 0.088676, loss_cps: 0.192518
[12:58:11.782] iteration 9055: total_loss: 0.192310, loss_sup: 0.052565, loss_mps: 0.049409, loss_cps: 0.090336
[12:58:11.931] iteration 9056: total_loss: 0.209367, loss_sup: 0.042827, loss_mps: 0.056339, loss_cps: 0.110202
[12:58:12.078] iteration 9057: total_loss: 0.178473, loss_sup: 0.048997, loss_mps: 0.045811, loss_cps: 0.083665
[12:58:12.224] iteration 9058: total_loss: 0.179132, loss_sup: 0.071002, loss_mps: 0.043029, loss_cps: 0.065101
[12:58:12.371] iteration 9059: total_loss: 0.271857, loss_sup: 0.145817, loss_mps: 0.044277, loss_cps: 0.081762
[12:58:12.517] iteration 9060: total_loss: 0.308341, loss_sup: 0.078802, loss_mps: 0.075887, loss_cps: 0.153652
[12:58:12.665] iteration 9061: total_loss: 0.180034, loss_sup: 0.038989, loss_mps: 0.047789, loss_cps: 0.093256
[12:58:12.812] iteration 9062: total_loss: 0.168812, loss_sup: 0.042421, loss_mps: 0.047058, loss_cps: 0.079332
[12:58:12.958] iteration 9063: total_loss: 0.249348, loss_sup: 0.050552, loss_mps: 0.067413, loss_cps: 0.131383
[12:58:13.104] iteration 9064: total_loss: 0.238102, loss_sup: 0.114822, loss_mps: 0.043088, loss_cps: 0.080192
[12:58:13.251] iteration 9065: total_loss: 0.166473, loss_sup: 0.023606, loss_mps: 0.049656, loss_cps: 0.093211
[12:58:13.397] iteration 9066: total_loss: 0.312188, loss_sup: 0.139687, loss_mps: 0.058002, loss_cps: 0.114499
[12:58:13.543] iteration 9067: total_loss: 0.239607, loss_sup: 0.042716, loss_mps: 0.066707, loss_cps: 0.130184
[12:58:13.689] iteration 9068: total_loss: 0.179659, loss_sup: 0.019814, loss_mps: 0.054660, loss_cps: 0.105185
[12:58:13.836] iteration 9069: total_loss: 0.284084, loss_sup: 0.078269, loss_mps: 0.065584, loss_cps: 0.140231
[12:58:13.983] iteration 9070: total_loss: 0.293619, loss_sup: 0.088583, loss_mps: 0.071020, loss_cps: 0.134017
[12:58:14.129] iteration 9071: total_loss: 0.269883, loss_sup: 0.114777, loss_mps: 0.051076, loss_cps: 0.104031
[12:58:14.275] iteration 9072: total_loss: 0.462731, loss_sup: 0.276465, loss_mps: 0.060117, loss_cps: 0.126149
[12:58:14.421] iteration 9073: total_loss: 0.410967, loss_sup: 0.087243, loss_mps: 0.101655, loss_cps: 0.222068
[12:58:14.566] iteration 9074: total_loss: 0.245828, loss_sup: 0.137954, loss_mps: 0.037728, loss_cps: 0.070146
[12:58:14.714] iteration 9075: total_loss: 0.257493, loss_sup: 0.148736, loss_mps: 0.040160, loss_cps: 0.068597
[12:58:14.860] iteration 9076: total_loss: 0.164564, loss_sup: 0.012735, loss_mps: 0.051842, loss_cps: 0.099987
[12:58:15.007] iteration 9077: total_loss: 0.222914, loss_sup: 0.042933, loss_mps: 0.060269, loss_cps: 0.119712
[12:58:15.153] iteration 9078: total_loss: 0.193675, loss_sup: 0.038864, loss_mps: 0.051996, loss_cps: 0.102815
[12:58:15.300] iteration 9079: total_loss: 0.161200, loss_sup: 0.032335, loss_mps: 0.046025, loss_cps: 0.082840
[12:58:15.446] iteration 9080: total_loss: 0.217471, loss_sup: 0.059060, loss_mps: 0.054817, loss_cps: 0.103595
[12:58:15.592] iteration 9081: total_loss: 0.176991, loss_sup: 0.045219, loss_mps: 0.045565, loss_cps: 0.086207
[12:58:15.739] iteration 9082: total_loss: 0.356126, loss_sup: 0.173565, loss_mps: 0.063269, loss_cps: 0.119292
[12:58:15.885] iteration 9083: total_loss: 0.214139, loss_sup: 0.054284, loss_mps: 0.057651, loss_cps: 0.102204
[12:58:16.031] iteration 9084: total_loss: 0.107230, loss_sup: 0.012182, loss_mps: 0.036382, loss_cps: 0.058667
[12:58:16.178] iteration 9085: total_loss: 0.254270, loss_sup: 0.123603, loss_mps: 0.045989, loss_cps: 0.084678
[12:58:16.325] iteration 9086: total_loss: 0.126897, loss_sup: 0.027701, loss_mps: 0.036399, loss_cps: 0.062797
[12:58:16.471] iteration 9087: total_loss: 0.159167, loss_sup: 0.063775, loss_mps: 0.035304, loss_cps: 0.060088
[12:58:16.618] iteration 9088: total_loss: 0.216645, loss_sup: 0.059820, loss_mps: 0.054448, loss_cps: 0.102378
[12:58:16.764] iteration 9089: total_loss: 0.489950, loss_sup: 0.213607, loss_mps: 0.093174, loss_cps: 0.183169
[12:58:16.910] iteration 9090: total_loss: 0.172911, loss_sup: 0.072875, loss_mps: 0.036634, loss_cps: 0.063402
[12:58:17.057] iteration 9091: total_loss: 0.179934, loss_sup: 0.013269, loss_mps: 0.055914, loss_cps: 0.110751
[12:58:17.203] iteration 9092: total_loss: 0.203257, loss_sup: 0.041128, loss_mps: 0.053387, loss_cps: 0.108742
[12:58:17.349] iteration 9093: total_loss: 0.158899, loss_sup: 0.007589, loss_mps: 0.050833, loss_cps: 0.100477
[12:58:17.500] iteration 9094: total_loss: 0.198237, loss_sup: 0.051091, loss_mps: 0.048094, loss_cps: 0.099053
[12:58:17.645] iteration 9095: total_loss: 0.301367, loss_sup: 0.083915, loss_mps: 0.070747, loss_cps: 0.146705
[12:58:17.792] iteration 9096: total_loss: 0.308626, loss_sup: 0.099265, loss_mps: 0.067541, loss_cps: 0.141820
[12:58:17.938] iteration 9097: total_loss: 0.249328, loss_sup: 0.099218, loss_mps: 0.053251, loss_cps: 0.096859
[12:58:18.084] iteration 9098: total_loss: 0.264720, loss_sup: 0.086085, loss_mps: 0.060145, loss_cps: 0.118489
[12:58:18.230] iteration 9099: total_loss: 0.342762, loss_sup: 0.202485, loss_mps: 0.049012, loss_cps: 0.091265
[12:58:18.377] iteration 9100: total_loss: 0.290957, loss_sup: 0.124977, loss_mps: 0.056328, loss_cps: 0.109652
[12:58:18.377] Evaluation Started ==>
[12:58:29.769] ==> valid iteration 9100: unet metrics: {'dc': 0.6371094849266167, 'jc': 0.5117561469444153, 'pre': 0.7347774057796316, 'hd': 5.89330908007699}, ynet metrics: {'dc': 0.5830468387518729, 'jc': 0.46024334831029673, 'pre': 0.7433752480041621, 'hd': 5.962853375935491}.
[12:58:29.771] Evaluation Finished!⏹️
[12:58:29.926] iteration 9101: total_loss: 0.167229, loss_sup: 0.035440, loss_mps: 0.045641, loss_cps: 0.086148
[12:58:30.075] iteration 9102: total_loss: 0.232844, loss_sup: 0.064630, loss_mps: 0.055732, loss_cps: 0.112482
[12:58:30.221] iteration 9103: total_loss: 0.228015, loss_sup: 0.063140, loss_mps: 0.055869, loss_cps: 0.109006
[12:58:30.367] iteration 9104: total_loss: 0.232872, loss_sup: 0.089531, loss_mps: 0.049407, loss_cps: 0.093934
[12:58:30.513] iteration 9105: total_loss: 0.149146, loss_sup: 0.058801, loss_mps: 0.033713, loss_cps: 0.056632
[12:58:30.658] iteration 9106: total_loss: 0.429048, loss_sup: 0.268659, loss_mps: 0.055592, loss_cps: 0.104798
[12:58:30.804] iteration 9107: total_loss: 0.221616, loss_sup: 0.053491, loss_mps: 0.056841, loss_cps: 0.111284
[12:58:30.950] iteration 9108: total_loss: 0.279553, loss_sup: 0.105810, loss_mps: 0.061010, loss_cps: 0.112733
[12:58:31.095] iteration 9109: total_loss: 0.431954, loss_sup: 0.217778, loss_mps: 0.071637, loss_cps: 0.142539
[12:58:31.240] iteration 9110: total_loss: 0.339703, loss_sup: 0.081706, loss_mps: 0.084443, loss_cps: 0.173553
[12:58:31.386] iteration 9111: total_loss: 0.302430, loss_sup: 0.133427, loss_mps: 0.057660, loss_cps: 0.111343
[12:58:31.532] iteration 9112: total_loss: 0.250867, loss_sup: 0.135319, loss_mps: 0.040823, loss_cps: 0.074725
[12:58:31.679] iteration 9113: total_loss: 0.352817, loss_sup: 0.172402, loss_mps: 0.062985, loss_cps: 0.117430
[12:58:31.824] iteration 9114: total_loss: 0.345764, loss_sup: 0.152937, loss_mps: 0.062825, loss_cps: 0.130002
[12:58:31.969] iteration 9115: total_loss: 0.244516, loss_sup: 0.027719, loss_mps: 0.071654, loss_cps: 0.145143
[12:58:32.117] iteration 9116: total_loss: 0.133424, loss_sup: 0.026011, loss_mps: 0.040612, loss_cps: 0.066801
[12:58:32.262] iteration 9117: total_loss: 0.254108, loss_sup: 0.126407, loss_mps: 0.047114, loss_cps: 0.080587
[12:58:32.408] iteration 9118: total_loss: 0.185554, loss_sup: 0.059872, loss_mps: 0.048560, loss_cps: 0.077122
[12:58:32.554] iteration 9119: total_loss: 0.179465, loss_sup: 0.027183, loss_mps: 0.056487, loss_cps: 0.095795
[12:58:32.701] iteration 9120: total_loss: 0.348859, loss_sup: 0.127623, loss_mps: 0.075375, loss_cps: 0.145861
[12:58:32.847] iteration 9121: total_loss: 0.227689, loss_sup: 0.075390, loss_mps: 0.055519, loss_cps: 0.096779
[12:58:32.992] iteration 9122: total_loss: 0.197951, loss_sup: 0.063394, loss_mps: 0.048599, loss_cps: 0.085957
[12:58:33.138] iteration 9123: total_loss: 0.237917, loss_sup: 0.059643, loss_mps: 0.059578, loss_cps: 0.118696
[12:58:33.286] iteration 9124: total_loss: 0.155500, loss_sup: 0.043355, loss_mps: 0.042458, loss_cps: 0.069688
[12:58:33.435] iteration 9125: total_loss: 0.440301, loss_sup: 0.128440, loss_mps: 0.094734, loss_cps: 0.217127
[12:58:33.583] iteration 9126: total_loss: 0.340177, loss_sup: 0.137929, loss_mps: 0.067394, loss_cps: 0.134855
[12:58:33.729] iteration 9127: total_loss: 0.204733, loss_sup: 0.059918, loss_mps: 0.051596, loss_cps: 0.093219
[12:58:33.876] iteration 9128: total_loss: 0.429759, loss_sup: 0.208528, loss_mps: 0.073683, loss_cps: 0.147548
[12:58:34.022] iteration 9129: total_loss: 0.231356, loss_sup: 0.071141, loss_mps: 0.056123, loss_cps: 0.104093
[12:58:34.168] iteration 9130: total_loss: 0.233097, loss_sup: 0.111946, loss_mps: 0.043582, loss_cps: 0.077569
[12:58:34.314] iteration 9131: total_loss: 0.308591, loss_sup: 0.155530, loss_mps: 0.055089, loss_cps: 0.097971
[12:58:34.461] iteration 9132: total_loss: 0.232669, loss_sup: 0.041616, loss_mps: 0.064353, loss_cps: 0.126700
[12:58:34.607] iteration 9133: total_loss: 0.395490, loss_sup: 0.242358, loss_mps: 0.052854, loss_cps: 0.100279
[12:58:34.755] iteration 9134: total_loss: 0.118429, loss_sup: 0.018476, loss_mps: 0.037556, loss_cps: 0.062397
[12:58:34.900] iteration 9135: total_loss: 0.219088, loss_sup: 0.041314, loss_mps: 0.061086, loss_cps: 0.116688
[12:58:35.046] iteration 9136: total_loss: 0.121271, loss_sup: 0.032083, loss_mps: 0.034114, loss_cps: 0.055075
[12:58:35.193] iteration 9137: total_loss: 0.233733, loss_sup: 0.082973, loss_mps: 0.053485, loss_cps: 0.097275
[12:58:35.339] iteration 9138: total_loss: 0.170940, loss_sup: 0.050074, loss_mps: 0.042320, loss_cps: 0.078545
[12:58:35.485] iteration 9139: total_loss: 0.179391, loss_sup: 0.068170, loss_mps: 0.040784, loss_cps: 0.070437
[12:58:35.630] iteration 9140: total_loss: 0.189027, loss_sup: 0.067800, loss_mps: 0.042873, loss_cps: 0.078353
[12:58:35.776] iteration 9141: total_loss: 0.164793, loss_sup: 0.044413, loss_mps: 0.044859, loss_cps: 0.075521
[12:58:35.922] iteration 9142: total_loss: 0.228966, loss_sup: 0.051323, loss_mps: 0.062469, loss_cps: 0.115174
[12:58:36.068] iteration 9143: total_loss: 0.301457, loss_sup: 0.156661, loss_mps: 0.051082, loss_cps: 0.093714
[12:58:36.214] iteration 9144: total_loss: 0.717413, loss_sup: 0.411581, loss_mps: 0.096660, loss_cps: 0.209172
[12:58:36.360] iteration 9145: total_loss: 0.248172, loss_sup: 0.021568, loss_mps: 0.073860, loss_cps: 0.152745
[12:58:36.507] iteration 9146: total_loss: 0.273212, loss_sup: 0.063878, loss_mps: 0.072246, loss_cps: 0.137087
[12:58:36.654] iteration 9147: total_loss: 0.157237, loss_sup: 0.057916, loss_mps: 0.039645, loss_cps: 0.059675
[12:58:36.799] iteration 9148: total_loss: 0.217193, loss_sup: 0.103443, loss_mps: 0.042468, loss_cps: 0.071282
[12:58:36.946] iteration 9149: total_loss: 0.201913, loss_sup: 0.050825, loss_mps: 0.052831, loss_cps: 0.098258
[12:58:37.092] iteration 9150: total_loss: 0.240291, loss_sup: 0.057365, loss_mps: 0.065998, loss_cps: 0.116928
[12:58:37.239] iteration 9151: total_loss: 0.293869, loss_sup: 0.099246, loss_mps: 0.066345, loss_cps: 0.128279
[12:58:37.385] iteration 9152: total_loss: 0.204626, loss_sup: 0.083562, loss_mps: 0.043969, loss_cps: 0.077096
[12:58:37.532] iteration 9153: total_loss: 0.149636, loss_sup: 0.048832, loss_mps: 0.038395, loss_cps: 0.062409
[12:58:37.678] iteration 9154: total_loss: 0.135054, loss_sup: 0.041904, loss_mps: 0.035505, loss_cps: 0.057644
[12:58:37.824] iteration 9155: total_loss: 0.261838, loss_sup: 0.027023, loss_mps: 0.080223, loss_cps: 0.154592
[12:58:37.970] iteration 9156: total_loss: 0.352091, loss_sup: 0.131626, loss_mps: 0.072775, loss_cps: 0.147690
[12:58:38.115] iteration 9157: total_loss: 0.157538, loss_sup: 0.027775, loss_mps: 0.047220, loss_cps: 0.082542
[12:58:38.264] iteration 9158: total_loss: 0.302925, loss_sup: 0.070187, loss_mps: 0.076432, loss_cps: 0.156306
[12:58:38.412] iteration 9159: total_loss: 0.243400, loss_sup: 0.080894, loss_mps: 0.056257, loss_cps: 0.106248
[12:58:38.558] iteration 9160: total_loss: 0.149109, loss_sup: 0.017353, loss_mps: 0.047720, loss_cps: 0.084036
[12:58:38.703] iteration 9161: total_loss: 0.213833, loss_sup: 0.035503, loss_mps: 0.059537, loss_cps: 0.118792
[12:58:38.849] iteration 9162: total_loss: 0.318597, loss_sup: 0.113457, loss_mps: 0.068207, loss_cps: 0.136934
[12:58:38.996] iteration 9163: total_loss: 0.216461, loss_sup: 0.083587, loss_mps: 0.046995, loss_cps: 0.085879
[12:58:39.142] iteration 9164: total_loss: 0.331338, loss_sup: 0.108164, loss_mps: 0.074114, loss_cps: 0.149059
[12:58:39.289] iteration 9165: total_loss: 0.237214, loss_sup: 0.083552, loss_mps: 0.051757, loss_cps: 0.101904
[12:58:39.435] iteration 9166: total_loss: 0.215459, loss_sup: 0.093052, loss_mps: 0.044607, loss_cps: 0.077799
[12:58:39.585] iteration 9167: total_loss: 0.184431, loss_sup: 0.038727, loss_mps: 0.050241, loss_cps: 0.095463
[12:58:39.730] iteration 9168: total_loss: 0.231811, loss_sup: 0.058669, loss_mps: 0.056803, loss_cps: 0.116339
[12:58:39.876] iteration 9169: total_loss: 0.171946, loss_sup: 0.032495, loss_mps: 0.048282, loss_cps: 0.091169
[12:58:40.023] iteration 9170: total_loss: 0.520763, loss_sup: 0.294067, loss_mps: 0.072890, loss_cps: 0.153806
[12:58:40.171] iteration 9171: total_loss: 0.268768, loss_sup: 0.116862, loss_mps: 0.052557, loss_cps: 0.099349
[12:58:40.318] iteration 9172: total_loss: 0.370231, loss_sup: 0.178148, loss_mps: 0.062323, loss_cps: 0.129760
[12:58:40.464] iteration 9173: total_loss: 0.185079, loss_sup: 0.076564, loss_mps: 0.040116, loss_cps: 0.068399
[12:58:40.610] iteration 9174: total_loss: 0.327741, loss_sup: 0.113296, loss_mps: 0.069371, loss_cps: 0.145073
[12:58:40.757] iteration 9175: total_loss: 0.292603, loss_sup: 0.130977, loss_mps: 0.056328, loss_cps: 0.105299
[12:58:40.903] iteration 9176: total_loss: 0.282640, loss_sup: 0.070789, loss_mps: 0.070981, loss_cps: 0.140870
[12:58:41.050] iteration 9177: total_loss: 0.161461, loss_sup: 0.010206, loss_mps: 0.053621, loss_cps: 0.097634
[12:58:41.195] iteration 9178: total_loss: 0.268204, loss_sup: 0.028419, loss_mps: 0.077486, loss_cps: 0.162299
[12:58:41.341] iteration 9179: total_loss: 0.327396, loss_sup: 0.069557, loss_mps: 0.080216, loss_cps: 0.177623
[12:58:41.487] iteration 9180: total_loss: 0.134928, loss_sup: 0.017592, loss_mps: 0.042617, loss_cps: 0.074719
[12:58:41.633] iteration 9181: total_loss: 0.213085, loss_sup: 0.099310, loss_mps: 0.041488, loss_cps: 0.072286
[12:58:41.780] iteration 9182: total_loss: 0.307374, loss_sup: 0.095289, loss_mps: 0.072046, loss_cps: 0.140039
[12:58:41.926] iteration 9183: total_loss: 0.395134, loss_sup: 0.183642, loss_mps: 0.070026, loss_cps: 0.141467
[12:58:42.071] iteration 9184: total_loss: 0.154973, loss_sup: 0.034410, loss_mps: 0.044505, loss_cps: 0.076058
[12:58:42.217] iteration 9185: total_loss: 0.263848, loss_sup: 0.059934, loss_mps: 0.067738, loss_cps: 0.136177
[12:58:42.363] iteration 9186: total_loss: 0.087320, loss_sup: 0.009806, loss_mps: 0.030841, loss_cps: 0.046673
[12:58:42.509] iteration 9187: total_loss: 0.433414, loss_sup: 0.120245, loss_mps: 0.098552, loss_cps: 0.214616
[12:58:42.655] iteration 9188: total_loss: 0.333572, loss_sup: 0.089933, loss_mps: 0.082417, loss_cps: 0.161222
[12:58:42.800] iteration 9189: total_loss: 0.171857, loss_sup: 0.033715, loss_mps: 0.049491, loss_cps: 0.088651
[12:58:42.946] iteration 9190: total_loss: 0.289896, loss_sup: 0.134375, loss_mps: 0.056757, loss_cps: 0.098764
[12:58:43.092] iteration 9191: total_loss: 0.311813, loss_sup: 0.115791, loss_mps: 0.064219, loss_cps: 0.131803
[12:58:43.237] iteration 9192: total_loss: 0.340157, loss_sup: 0.122272, loss_mps: 0.073909, loss_cps: 0.143975
[12:58:43.383] iteration 9193: total_loss: 0.165374, loss_sup: 0.028931, loss_mps: 0.049147, loss_cps: 0.087295
[12:58:43.529] iteration 9194: total_loss: 0.158426, loss_sup: 0.037455, loss_mps: 0.044853, loss_cps: 0.076118
[12:58:43.677] iteration 9195: total_loss: 0.248977, loss_sup: 0.051688, loss_mps: 0.066915, loss_cps: 0.130374
[12:58:43.742] iteration 9196: total_loss: 0.432858, loss_sup: 0.166979, loss_mps: 0.083071, loss_cps: 0.182809
[12:58:44.951] iteration 9197: total_loss: 0.336739, loss_sup: 0.173002, loss_mps: 0.054692, loss_cps: 0.109045
[12:58:45.100] iteration 9198: total_loss: 0.194527, loss_sup: 0.049404, loss_mps: 0.052681, loss_cps: 0.092442
[12:58:45.247] iteration 9199: total_loss: 0.227068, loss_sup: 0.108885, loss_mps: 0.042695, loss_cps: 0.075487
[12:58:45.393] iteration 9200: total_loss: 0.355174, loss_sup: 0.203279, loss_mps: 0.051292, loss_cps: 0.100602
[12:58:45.393] Evaluation Started ==>
[12:58:56.762] ==> valid iteration 9200: unet metrics: {'dc': 0.5918893550235433, 'jc': 0.4714290265282504, 'pre': 0.680835149428584, 'hd': 6.183909378954873}, ynet metrics: {'dc': 0.5844056096437781, 'jc': 0.4654740337416902, 'pre': 0.7537615339886059, 'hd': 5.920865547483442}.
[12:58:56.764] Evaluation Finished!⏹️
[12:58:56.915] iteration 9201: total_loss: 0.421670, loss_sup: 0.188893, loss_mps: 0.071061, loss_cps: 0.161716
[12:58:57.062] iteration 9202: total_loss: 0.254052, loss_sup: 0.026010, loss_mps: 0.071748, loss_cps: 0.156293
[12:58:57.208] iteration 9203: total_loss: 0.422328, loss_sup: 0.124532, loss_mps: 0.092541, loss_cps: 0.205255
[12:58:57.353] iteration 9204: total_loss: 0.349776, loss_sup: 0.112065, loss_mps: 0.074968, loss_cps: 0.162743
[12:58:57.499] iteration 9205: total_loss: 0.165850, loss_sup: 0.012052, loss_mps: 0.051391, loss_cps: 0.102407
[12:58:57.645] iteration 9206: total_loss: 0.358469, loss_sup: 0.021906, loss_mps: 0.105497, loss_cps: 0.231066
[12:58:57.794] iteration 9207: total_loss: 0.174028, loss_sup: 0.034196, loss_mps: 0.049614, loss_cps: 0.090219
[12:58:57.940] iteration 9208: total_loss: 0.195795, loss_sup: 0.060991, loss_mps: 0.049082, loss_cps: 0.085722
[12:58:58.087] iteration 9209: total_loss: 0.358623, loss_sup: 0.123133, loss_mps: 0.082972, loss_cps: 0.152519
[12:58:58.234] iteration 9210: total_loss: 0.122725, loss_sup: 0.039193, loss_mps: 0.032021, loss_cps: 0.051511
[12:58:58.380] iteration 9211: total_loss: 0.241939, loss_sup: 0.047281, loss_mps: 0.064911, loss_cps: 0.129747
[12:58:58.526] iteration 9212: total_loss: 0.181400, loss_sup: 0.044827, loss_mps: 0.048327, loss_cps: 0.088246
[12:58:58.672] iteration 9213: total_loss: 0.428516, loss_sup: 0.180959, loss_mps: 0.082418, loss_cps: 0.165138
[12:58:58.818] iteration 9214: total_loss: 0.429720, loss_sup: 0.184532, loss_mps: 0.080574, loss_cps: 0.164614
[12:58:58.964] iteration 9215: total_loss: 0.189509, loss_sup: 0.014646, loss_mps: 0.061840, loss_cps: 0.113022
[12:58:59.112] iteration 9216: total_loss: 0.244335, loss_sup: 0.076479, loss_mps: 0.061644, loss_cps: 0.106212
[12:58:59.261] iteration 9217: total_loss: 0.288207, loss_sup: 0.062383, loss_mps: 0.076144, loss_cps: 0.149680
[12:58:59.410] iteration 9218: total_loss: 0.243084, loss_sup: 0.082386, loss_mps: 0.059754, loss_cps: 0.100944
[12:58:59.556] iteration 9219: total_loss: 0.296822, loss_sup: 0.090658, loss_mps: 0.069097, loss_cps: 0.137066
[12:58:59.701] iteration 9220: total_loss: 0.234452, loss_sup: 0.086316, loss_mps: 0.051755, loss_cps: 0.096381
[12:58:59.849] iteration 9221: total_loss: 0.298686, loss_sup: 0.077294, loss_mps: 0.073224, loss_cps: 0.148168
[12:58:59.995] iteration 9222: total_loss: 0.251072, loss_sup: 0.073578, loss_mps: 0.061514, loss_cps: 0.115981
[12:59:00.141] iteration 9223: total_loss: 0.198698, loss_sup: 0.060716, loss_mps: 0.051419, loss_cps: 0.086563
[12:59:00.286] iteration 9224: total_loss: 0.233129, loss_sup: 0.082533, loss_mps: 0.051945, loss_cps: 0.098651
[12:59:00.431] iteration 9225: total_loss: 0.375550, loss_sup: 0.117545, loss_mps: 0.083812, loss_cps: 0.174194
[12:59:00.578] iteration 9226: total_loss: 0.241501, loss_sup: 0.029834, loss_mps: 0.070410, loss_cps: 0.141256
[12:59:00.723] iteration 9227: total_loss: 0.494325, loss_sup: 0.254863, loss_mps: 0.079710, loss_cps: 0.159752
[12:59:00.869] iteration 9228: total_loss: 0.241542, loss_sup: 0.016181, loss_mps: 0.074237, loss_cps: 0.151124
[12:59:01.015] iteration 9229: total_loss: 0.163366, loss_sup: 0.048556, loss_mps: 0.043712, loss_cps: 0.071099
[12:59:01.161] iteration 9230: total_loss: 0.156212, loss_sup: 0.034439, loss_mps: 0.043986, loss_cps: 0.077787
[12:59:01.306] iteration 9231: total_loss: 0.264610, loss_sup: 0.084386, loss_mps: 0.059445, loss_cps: 0.120779
[12:59:01.454] iteration 9232: total_loss: 0.356475, loss_sup: 0.079296, loss_mps: 0.088993, loss_cps: 0.188186
[12:59:01.603] iteration 9233: total_loss: 0.190567, loss_sup: 0.060191, loss_mps: 0.045441, loss_cps: 0.084935
[12:59:01.749] iteration 9234: total_loss: 0.136125, loss_sup: 0.038686, loss_mps: 0.036170, loss_cps: 0.061269
[12:59:01.896] iteration 9235: total_loss: 0.380154, loss_sup: 0.162723, loss_mps: 0.073740, loss_cps: 0.143691
[12:59:02.042] iteration 9236: total_loss: 0.257460, loss_sup: 0.079265, loss_mps: 0.059932, loss_cps: 0.118263
[12:59:02.188] iteration 9237: total_loss: 0.166252, loss_sup: 0.042019, loss_mps: 0.044820, loss_cps: 0.079414
[12:59:02.335] iteration 9238: total_loss: 0.241398, loss_sup: 0.089036, loss_mps: 0.052448, loss_cps: 0.099914
[12:59:02.481] iteration 9239: total_loss: 0.286011, loss_sup: 0.092170, loss_mps: 0.063905, loss_cps: 0.129936
[12:59:02.628] iteration 9240: total_loss: 0.161626, loss_sup: 0.036569, loss_mps: 0.047401, loss_cps: 0.077656
[12:59:02.775] iteration 9241: total_loss: 0.193772, loss_sup: 0.045727, loss_mps: 0.051216, loss_cps: 0.096829
[12:59:02.923] iteration 9242: total_loss: 0.179787, loss_sup: 0.031186, loss_mps: 0.051144, loss_cps: 0.097458
[12:59:03.069] iteration 9243: total_loss: 0.462432, loss_sup: 0.286108, loss_mps: 0.060715, loss_cps: 0.115609
[12:59:03.215] iteration 9244: total_loss: 0.296663, loss_sup: 0.104038, loss_mps: 0.064998, loss_cps: 0.127627
[12:59:03.360] iteration 9245: total_loss: 0.507468, loss_sup: 0.171700, loss_mps: 0.101180, loss_cps: 0.234588
[12:59:03.506] iteration 9246: total_loss: 0.214071, loss_sup: 0.040903, loss_mps: 0.060200, loss_cps: 0.112968
[12:59:03.652] iteration 9247: total_loss: 0.186151, loss_sup: 0.042515, loss_mps: 0.051346, loss_cps: 0.092290
[12:59:03.798] iteration 9248: total_loss: 0.183091, loss_sup: 0.038810, loss_mps: 0.049611, loss_cps: 0.094670
[12:59:03.944] iteration 9249: total_loss: 0.205073, loss_sup: 0.038144, loss_mps: 0.056898, loss_cps: 0.110032
[12:59:04.090] iteration 9250: total_loss: 0.198174, loss_sup: 0.077821, loss_mps: 0.041793, loss_cps: 0.078560
[12:59:04.236] iteration 9251: total_loss: 0.495097, loss_sup: 0.219824, loss_mps: 0.088011, loss_cps: 0.187262
[12:59:04.383] iteration 9252: total_loss: 0.235584, loss_sup: 0.077740, loss_mps: 0.053298, loss_cps: 0.104546
[12:59:04.529] iteration 9253: total_loss: 0.241733, loss_sup: 0.047872, loss_mps: 0.063564, loss_cps: 0.130297
[12:59:04.677] iteration 9254: total_loss: 0.351614, loss_sup: 0.134299, loss_mps: 0.072194, loss_cps: 0.145120
[12:59:04.824] iteration 9255: total_loss: 0.266846, loss_sup: 0.102111, loss_mps: 0.058719, loss_cps: 0.106015
[12:59:04.970] iteration 9256: total_loss: 0.254972, loss_sup: 0.073551, loss_mps: 0.060076, loss_cps: 0.121345
[12:59:05.115] iteration 9257: total_loss: 0.452950, loss_sup: 0.188070, loss_mps: 0.086344, loss_cps: 0.178537
[12:59:05.262] iteration 9258: total_loss: 0.363648, loss_sup: 0.045833, loss_mps: 0.097975, loss_cps: 0.219840
[12:59:05.408] iteration 9259: total_loss: 0.136674, loss_sup: 0.037800, loss_mps: 0.037320, loss_cps: 0.061553
[12:59:05.554] iteration 9260: total_loss: 0.401283, loss_sup: 0.219072, loss_mps: 0.062230, loss_cps: 0.119981
[12:59:05.701] iteration 9261: total_loss: 0.339065, loss_sup: 0.082017, loss_mps: 0.084017, loss_cps: 0.173031
[12:59:05.846] iteration 9262: total_loss: 0.404110, loss_sup: 0.119362, loss_mps: 0.094533, loss_cps: 0.190215
[12:59:05.992] iteration 9263: total_loss: 0.321121, loss_sup: 0.080767, loss_mps: 0.080612, loss_cps: 0.159742
[12:59:06.139] iteration 9264: total_loss: 0.233405, loss_sup: 0.037474, loss_mps: 0.065873, loss_cps: 0.130058
[12:59:06.286] iteration 9265: total_loss: 0.166164, loss_sup: 0.026931, loss_mps: 0.051919, loss_cps: 0.087314
[12:59:06.432] iteration 9266: total_loss: 0.188634, loss_sup: 0.044137, loss_mps: 0.051097, loss_cps: 0.093400
[12:59:06.578] iteration 9267: total_loss: 0.186167, loss_sup: 0.032048, loss_mps: 0.053727, loss_cps: 0.100392
[12:59:06.724] iteration 9268: total_loss: 0.156815, loss_sup: 0.042792, loss_mps: 0.044157, loss_cps: 0.069866
[12:59:06.872] iteration 9269: total_loss: 0.212821, loss_sup: 0.061422, loss_mps: 0.053434, loss_cps: 0.097964
[12:59:07.022] iteration 9270: total_loss: 0.412721, loss_sup: 0.124872, loss_mps: 0.092128, loss_cps: 0.195721
[12:59:07.170] iteration 9271: total_loss: 0.303952, loss_sup: 0.063867, loss_mps: 0.079203, loss_cps: 0.160883
[12:59:07.316] iteration 9272: total_loss: 0.248325, loss_sup: 0.116378, loss_mps: 0.048579, loss_cps: 0.083368
[12:59:07.462] iteration 9273: total_loss: 0.338568, loss_sup: 0.147830, loss_mps: 0.065084, loss_cps: 0.125655
[12:59:07.607] iteration 9274: total_loss: 0.183352, loss_sup: 0.046850, loss_mps: 0.050204, loss_cps: 0.086298
[12:59:07.756] iteration 9275: total_loss: 0.219893, loss_sup: 0.060580, loss_mps: 0.054837, loss_cps: 0.104476
[12:59:07.903] iteration 9276: total_loss: 0.165216, loss_sup: 0.035887, loss_mps: 0.048141, loss_cps: 0.081188
[12:59:08.048] iteration 9277: total_loss: 0.219614, loss_sup: 0.062578, loss_mps: 0.055130, loss_cps: 0.101906
[12:59:08.195] iteration 9278: total_loss: 0.201440, loss_sup: 0.063896, loss_mps: 0.048354, loss_cps: 0.089191
[12:59:08.341] iteration 9279: total_loss: 0.265641, loss_sup: 0.129745, loss_mps: 0.048643, loss_cps: 0.087253
[12:59:08.487] iteration 9280: total_loss: 0.292851, loss_sup: 0.118151, loss_mps: 0.059440, loss_cps: 0.115260
[12:59:08.632] iteration 9281: total_loss: 0.183067, loss_sup: 0.067790, loss_mps: 0.043705, loss_cps: 0.071572
[12:59:08.778] iteration 9282: total_loss: 0.288893, loss_sup: 0.106015, loss_mps: 0.065099, loss_cps: 0.117779
[12:59:08.924] iteration 9283: total_loss: 0.265784, loss_sup: 0.044856, loss_mps: 0.074621, loss_cps: 0.146307
[12:59:09.070] iteration 9284: total_loss: 0.492827, loss_sup: 0.284053, loss_mps: 0.069956, loss_cps: 0.138818
[12:59:09.217] iteration 9285: total_loss: 0.297954, loss_sup: 0.101062, loss_mps: 0.065489, loss_cps: 0.131403
[12:59:09.364] iteration 9286: total_loss: 0.165524, loss_sup: 0.037989, loss_mps: 0.046661, loss_cps: 0.080875
[12:59:09.510] iteration 9287: total_loss: 0.195601, loss_sup: 0.049806, loss_mps: 0.052020, loss_cps: 0.093776
[12:59:09.656] iteration 9288: total_loss: 0.156025, loss_sup: 0.051854, loss_mps: 0.038851, loss_cps: 0.065320
[12:59:09.802] iteration 9289: total_loss: 0.223751, loss_sup: 0.107969, loss_mps: 0.042209, loss_cps: 0.073572
[12:59:09.949] iteration 9290: total_loss: 0.178998, loss_sup: 0.047686, loss_mps: 0.047042, loss_cps: 0.084269
[12:59:10.096] iteration 9291: total_loss: 0.165075, loss_sup: 0.034420, loss_mps: 0.046994, loss_cps: 0.083661
[12:59:10.245] iteration 9292: total_loss: 0.161699, loss_sup: 0.057505, loss_mps: 0.038509, loss_cps: 0.065685
[12:59:10.392] iteration 9293: total_loss: 0.346223, loss_sup: 0.178583, loss_mps: 0.058479, loss_cps: 0.109160
[12:59:10.539] iteration 9294: total_loss: 0.290030, loss_sup: 0.075632, loss_mps: 0.073149, loss_cps: 0.141249
[12:59:10.685] iteration 9295: total_loss: 0.228272, loss_sup: 0.050851, loss_mps: 0.060729, loss_cps: 0.116692
[12:59:10.831] iteration 9296: total_loss: 0.189505, loss_sup: 0.049129, loss_mps: 0.051572, loss_cps: 0.088804
[12:59:10.977] iteration 9297: total_loss: 0.133552, loss_sup: 0.017632, loss_mps: 0.042798, loss_cps: 0.073121
[12:59:11.124] iteration 9298: total_loss: 0.272536, loss_sup: 0.169000, loss_mps: 0.038018, loss_cps: 0.065519
[12:59:11.271] iteration 9299: total_loss: 0.210105, loss_sup: 0.031862, loss_mps: 0.061788, loss_cps: 0.116454
[12:59:11.417] iteration 9300: total_loss: 0.134809, loss_sup: 0.017212, loss_mps: 0.044641, loss_cps: 0.072957
[12:59:11.417] Evaluation Started ==>
[12:59:22.778] ==> valid iteration 9300: unet metrics: {'dc': 0.6615368174458814, 'jc': 0.5376609269133188, 'pre': 0.755687063022832, 'hd': 5.779310456976551}, ynet metrics: {'dc': 0.5867955914296294, 'jc': 0.46928477222625187, 'pre': 0.7554430702017965, 'hd': 5.829321215112237}.
[12:59:22.836] ==> New best valid dice for unet: 0.661537, at iteration 9300
[12:59:22.837] Evaluation Finished!⏹️
[12:59:22.990] iteration 9301: total_loss: 0.208710, loss_sup: 0.045938, loss_mps: 0.054361, loss_cps: 0.108411
[12:59:23.140] iteration 9302: total_loss: 0.124173, loss_sup: 0.002366, loss_mps: 0.041818, loss_cps: 0.079989
[12:59:23.286] iteration 9303: total_loss: 0.308668, loss_sup: 0.150844, loss_mps: 0.052963, loss_cps: 0.104860
[12:59:23.434] iteration 9304: total_loss: 0.167493, loss_sup: 0.040705, loss_mps: 0.043892, loss_cps: 0.082896
[12:59:23.582] iteration 9305: total_loss: 0.270645, loss_sup: 0.046540, loss_mps: 0.073769, loss_cps: 0.150337
[12:59:23.731] iteration 9306: total_loss: 0.349345, loss_sup: 0.068468, loss_mps: 0.087939, loss_cps: 0.192938
[12:59:23.877] iteration 9307: total_loss: 0.191033, loss_sup: 0.082413, loss_mps: 0.038150, loss_cps: 0.070471
[12:59:24.022] iteration 9308: total_loss: 0.171511, loss_sup: 0.025572, loss_mps: 0.050128, loss_cps: 0.095812
[12:59:24.167] iteration 9309: total_loss: 0.257380, loss_sup: 0.102522, loss_mps: 0.050943, loss_cps: 0.103915
[12:59:24.312] iteration 9310: total_loss: 0.218960, loss_sup: 0.041973, loss_mps: 0.058702, loss_cps: 0.118286
[12:59:24.458] iteration 9311: total_loss: 0.138063, loss_sup: 0.006527, loss_mps: 0.044730, loss_cps: 0.086805
[12:59:24.604] iteration 9312: total_loss: 0.293239, loss_sup: 0.135448, loss_mps: 0.053475, loss_cps: 0.104316
[12:59:24.750] iteration 9313: total_loss: 0.194990, loss_sup: 0.050102, loss_mps: 0.049447, loss_cps: 0.095441
[12:59:24.896] iteration 9314: total_loss: 0.189704, loss_sup: 0.084322, loss_mps: 0.038102, loss_cps: 0.067280
[12:59:25.041] iteration 9315: total_loss: 0.197907, loss_sup: 0.089157, loss_mps: 0.038443, loss_cps: 0.070307
[12:59:25.189] iteration 9316: total_loss: 0.194508, loss_sup: 0.032202, loss_mps: 0.054649, loss_cps: 0.107657
[12:59:25.337] iteration 9317: total_loss: 0.223932, loss_sup: 0.074950, loss_mps: 0.050689, loss_cps: 0.098293
[12:59:25.483] iteration 9318: total_loss: 0.164689, loss_sup: 0.040347, loss_mps: 0.045342, loss_cps: 0.079000
[12:59:25.629] iteration 9319: total_loss: 0.378808, loss_sup: 0.223675, loss_mps: 0.051702, loss_cps: 0.103432
[12:59:25.776] iteration 9320: total_loss: 0.217666, loss_sup: 0.026556, loss_mps: 0.062389, loss_cps: 0.128722
[12:59:25.922] iteration 9321: total_loss: 0.191925, loss_sup: 0.043526, loss_mps: 0.050633, loss_cps: 0.097766
[12:59:26.068] iteration 9322: total_loss: 0.395933, loss_sup: 0.217066, loss_mps: 0.061181, loss_cps: 0.117686
[12:59:26.214] iteration 9323: total_loss: 0.187370, loss_sup: 0.050216, loss_mps: 0.046842, loss_cps: 0.090312
[12:59:26.361] iteration 9324: total_loss: 0.122476, loss_sup: 0.028456, loss_mps: 0.034126, loss_cps: 0.059894
[12:59:26.507] iteration 9325: total_loss: 0.360664, loss_sup: 0.119848, loss_mps: 0.077375, loss_cps: 0.163441
[12:59:26.655] iteration 9326: total_loss: 0.212474, loss_sup: 0.072646, loss_mps: 0.047529, loss_cps: 0.092299
[12:59:26.801] iteration 9327: total_loss: 0.323488, loss_sup: 0.119067, loss_mps: 0.065465, loss_cps: 0.138956
[12:59:26.947] iteration 9328: total_loss: 0.152673, loss_sup: 0.052012, loss_mps: 0.035529, loss_cps: 0.065131
[12:59:27.094] iteration 9329: total_loss: 0.356723, loss_sup: 0.150509, loss_mps: 0.066867, loss_cps: 0.139347
[12:59:27.239] iteration 9330: total_loss: 0.229359, loss_sup: 0.027064, loss_mps: 0.068644, loss_cps: 0.133651
[12:59:27.386] iteration 9331: total_loss: 0.113613, loss_sup: 0.016194, loss_mps: 0.036774, loss_cps: 0.060645
[12:59:27.532] iteration 9332: total_loss: 0.230263, loss_sup: 0.086638, loss_mps: 0.050017, loss_cps: 0.093607
[12:59:27.678] iteration 9333: total_loss: 0.286004, loss_sup: 0.102859, loss_mps: 0.062590, loss_cps: 0.120554
[12:59:27.823] iteration 9334: total_loss: 0.180340, loss_sup: 0.024625, loss_mps: 0.052280, loss_cps: 0.103435
[12:59:27.972] iteration 9335: total_loss: 0.272142, loss_sup: 0.067578, loss_mps: 0.066193, loss_cps: 0.138372
[12:59:28.120] iteration 9336: total_loss: 0.213402, loss_sup: 0.119423, loss_mps: 0.034761, loss_cps: 0.059218
[12:59:28.267] iteration 9337: total_loss: 0.141243, loss_sup: 0.036403, loss_mps: 0.038582, loss_cps: 0.066259
[12:59:28.413] iteration 9338: total_loss: 0.182374, loss_sup: 0.015886, loss_mps: 0.059141, loss_cps: 0.107347
[12:59:28.562] iteration 9339: total_loss: 0.251280, loss_sup: 0.046415, loss_mps: 0.068159, loss_cps: 0.136706
[12:59:28.708] iteration 9340: total_loss: 0.238455, loss_sup: 0.060482, loss_mps: 0.060214, loss_cps: 0.117759
[12:59:28.854] iteration 9341: total_loss: 0.188532, loss_sup: 0.053708, loss_mps: 0.049912, loss_cps: 0.084911
[12:59:29.001] iteration 9342: total_loss: 0.317622, loss_sup: 0.128972, loss_mps: 0.064846, loss_cps: 0.123804
[12:59:29.147] iteration 9343: total_loss: 0.344701, loss_sup: 0.141970, loss_mps: 0.071292, loss_cps: 0.131439
[12:59:29.297] iteration 9344: total_loss: 0.232463, loss_sup: 0.050110, loss_mps: 0.064587, loss_cps: 0.117767
[12:59:29.442] iteration 9345: total_loss: 0.198062, loss_sup: 0.026244, loss_mps: 0.058558, loss_cps: 0.113260
[12:59:29.588] iteration 9346: total_loss: 0.176006, loss_sup: 0.068743, loss_mps: 0.040614, loss_cps: 0.066649
[12:59:29.734] iteration 9347: total_loss: 0.218107, loss_sup: 0.093694, loss_mps: 0.045949, loss_cps: 0.078465
[12:59:29.881] iteration 9348: total_loss: 0.252230, loss_sup: 0.066378, loss_mps: 0.061576, loss_cps: 0.124276
[12:59:30.028] iteration 9349: total_loss: 0.152016, loss_sup: 0.006581, loss_mps: 0.050711, loss_cps: 0.094724
[12:59:30.174] iteration 9350: total_loss: 0.132458, loss_sup: 0.017959, loss_mps: 0.042246, loss_cps: 0.072253
[12:59:30.320] iteration 9351: total_loss: 0.190706, loss_sup: 0.093578, loss_mps: 0.035048, loss_cps: 0.062081
[12:59:30.466] iteration 9352: total_loss: 0.686419, loss_sup: 0.352206, loss_mps: 0.102375, loss_cps: 0.231838
[12:59:30.615] iteration 9353: total_loss: 0.250885, loss_sup: 0.055786, loss_mps: 0.065570, loss_cps: 0.129529
[12:59:30.761] iteration 9354: total_loss: 0.333384, loss_sup: 0.153625, loss_mps: 0.060594, loss_cps: 0.119164
[12:59:30.908] iteration 9355: total_loss: 0.274574, loss_sup: 0.067741, loss_mps: 0.066520, loss_cps: 0.140313
[12:59:31.054] iteration 9356: total_loss: 0.221326, loss_sup: 0.055133, loss_mps: 0.057620, loss_cps: 0.108572
[12:59:31.200] iteration 9357: total_loss: 0.392817, loss_sup: 0.171722, loss_mps: 0.072802, loss_cps: 0.148293
[12:59:31.346] iteration 9358: total_loss: 0.264646, loss_sup: 0.099173, loss_mps: 0.057514, loss_cps: 0.107959
[12:59:31.492] iteration 9359: total_loss: 0.149402, loss_sup: 0.019347, loss_mps: 0.047208, loss_cps: 0.082847
[12:59:31.640] iteration 9360: total_loss: 0.208394, loss_sup: 0.073716, loss_mps: 0.048067, loss_cps: 0.086611
[12:59:31.786] iteration 9361: total_loss: 0.139334, loss_sup: 0.024941, loss_mps: 0.043476, loss_cps: 0.070917
[12:59:31.932] iteration 9362: total_loss: 0.124361, loss_sup: 0.018946, loss_mps: 0.039771, loss_cps: 0.065645
[12:59:32.078] iteration 9363: total_loss: 0.327608, loss_sup: 0.202475, loss_mps: 0.045131, loss_cps: 0.080002
[12:59:32.225] iteration 9364: total_loss: 0.149088, loss_sup: 0.018428, loss_mps: 0.045269, loss_cps: 0.085391
[12:59:32.373] iteration 9365: total_loss: 0.192414, loss_sup: 0.049478, loss_mps: 0.051457, loss_cps: 0.091479
[12:59:32.519] iteration 9366: total_loss: 0.250891, loss_sup: 0.106535, loss_mps: 0.051812, loss_cps: 0.092544
[12:59:32.665] iteration 9367: total_loss: 0.271405, loss_sup: 0.128585, loss_mps: 0.051171, loss_cps: 0.091648
[12:59:32.813] iteration 9368: total_loss: 0.322655, loss_sup: 0.153699, loss_mps: 0.056467, loss_cps: 0.112489
[12:59:32.959] iteration 9369: total_loss: 0.191684, loss_sup: 0.017827, loss_mps: 0.057638, loss_cps: 0.116219
[12:59:33.105] iteration 9370: total_loss: 0.106536, loss_sup: 0.006871, loss_mps: 0.038836, loss_cps: 0.060830
[12:59:33.251] iteration 9371: total_loss: 0.202358, loss_sup: 0.041047, loss_mps: 0.055960, loss_cps: 0.105351
[12:59:33.397] iteration 9372: total_loss: 0.191258, loss_sup: 0.035974, loss_mps: 0.052707, loss_cps: 0.102576
[12:59:33.542] iteration 9373: total_loss: 0.148640, loss_sup: 0.029513, loss_mps: 0.045070, loss_cps: 0.074057
[12:59:33.688] iteration 9374: total_loss: 0.199718, loss_sup: 0.056105, loss_mps: 0.050530, loss_cps: 0.093083
[12:59:33.834] iteration 9375: total_loss: 0.086979, loss_sup: 0.007464, loss_mps: 0.031593, loss_cps: 0.047922
[12:59:33.981] iteration 9376: total_loss: 0.295551, loss_sup: 0.074996, loss_mps: 0.073075, loss_cps: 0.147481
[12:59:34.127] iteration 9377: total_loss: 0.354591, loss_sup: 0.197056, loss_mps: 0.055543, loss_cps: 0.101993
[12:59:34.273] iteration 9378: total_loss: 0.198418, loss_sup: 0.052360, loss_mps: 0.048686, loss_cps: 0.097372
[12:59:34.419] iteration 9379: total_loss: 0.146062, loss_sup: 0.008575, loss_mps: 0.046408, loss_cps: 0.091079
[12:59:34.565] iteration 9380: total_loss: 0.545699, loss_sup: 0.220159, loss_mps: 0.099176, loss_cps: 0.226363
[12:59:34.711] iteration 9381: total_loss: 0.374049, loss_sup: 0.184759, loss_mps: 0.065385, loss_cps: 0.123905
[12:59:34.858] iteration 9382: total_loss: 0.370269, loss_sup: 0.138539, loss_mps: 0.074226, loss_cps: 0.157504
[12:59:35.004] iteration 9383: total_loss: 0.229142, loss_sup: 0.072821, loss_mps: 0.053797, loss_cps: 0.102524
[12:59:35.150] iteration 9384: total_loss: 0.300264, loss_sup: 0.077343, loss_mps: 0.072243, loss_cps: 0.150679
[12:59:35.297] iteration 9385: total_loss: 0.184902, loss_sup: 0.022692, loss_mps: 0.053035, loss_cps: 0.109175
[12:59:35.445] iteration 9386: total_loss: 0.144497, loss_sup: 0.032184, loss_mps: 0.039869, loss_cps: 0.072445
[12:59:35.592] iteration 9387: total_loss: 0.269231, loss_sup: 0.058887, loss_mps: 0.068820, loss_cps: 0.141524
[12:59:35.739] iteration 9388: total_loss: 0.223207, loss_sup: 0.106302, loss_mps: 0.039940, loss_cps: 0.076964
[12:59:35.890] iteration 9389: total_loss: 0.386002, loss_sup: 0.090235, loss_mps: 0.090550, loss_cps: 0.205216
[12:59:36.035] iteration 9390: total_loss: 0.254421, loss_sup: 0.114622, loss_mps: 0.047084, loss_cps: 0.092716
[12:59:36.181] iteration 9391: total_loss: 0.219677, loss_sup: 0.083014, loss_mps: 0.047308, loss_cps: 0.089356
[12:59:36.327] iteration 9392: total_loss: 0.465447, loss_sup: 0.232523, loss_mps: 0.077039, loss_cps: 0.155884
[12:59:36.475] iteration 9393: total_loss: 0.387769, loss_sup: 0.218345, loss_mps: 0.057194, loss_cps: 0.112230
[12:59:36.621] iteration 9394: total_loss: 0.320018, loss_sup: 0.143996, loss_mps: 0.061210, loss_cps: 0.114812
[12:59:36.768] iteration 9395: total_loss: 0.160189, loss_sup: 0.033238, loss_mps: 0.047433, loss_cps: 0.079518
[12:59:36.914] iteration 9396: total_loss: 0.255522, loss_sup: 0.082100, loss_mps: 0.058615, loss_cps: 0.114807
[12:59:37.060] iteration 9397: total_loss: 0.304789, loss_sup: 0.139634, loss_mps: 0.058958, loss_cps: 0.106197
[12:59:37.207] iteration 9398: total_loss: 0.271534, loss_sup: 0.101483, loss_mps: 0.058324, loss_cps: 0.111727
[12:59:37.352] iteration 9399: total_loss: 0.467898, loss_sup: 0.181757, loss_mps: 0.092332, loss_cps: 0.193809
[12:59:37.498] iteration 9400: total_loss: 0.341895, loss_sup: 0.117616, loss_mps: 0.075955, loss_cps: 0.148325
[12:59:37.498] Evaluation Started ==>
[12:59:48.904] ==> valid iteration 9400: unet metrics: {'dc': 0.6300146411922811, 'jc': 0.5076878361196067, 'pre': 0.7326402326704454, 'hd': 5.847682187438037}, ynet metrics: {'dc': 0.5494150768125069, 'jc': 0.43941914050857445, 'pre': 0.7036418165755924, 'hd': 5.933593418600872}.
[12:59:48.906] Evaluation Finished!⏹️
[12:59:49.058] iteration 9401: total_loss: 0.289607, loss_sup: 0.065229, loss_mps: 0.075827, loss_cps: 0.148552
[12:59:49.205] iteration 9402: total_loss: 0.415215, loss_sup: 0.186135, loss_mps: 0.076713, loss_cps: 0.152367
[12:59:49.351] iteration 9403: total_loss: 0.176056, loss_sup: 0.042349, loss_mps: 0.050413, loss_cps: 0.083293
[12:59:49.496] iteration 9404: total_loss: 0.325992, loss_sup: 0.123786, loss_mps: 0.067194, loss_cps: 0.135011
[12:59:49.647] iteration 9405: total_loss: 0.358590, loss_sup: 0.132161, loss_mps: 0.075503, loss_cps: 0.150926
[12:59:49.794] iteration 9406: total_loss: 0.186868, loss_sup: 0.080550, loss_mps: 0.041390, loss_cps: 0.064929
[12:59:49.940] iteration 9407: total_loss: 0.408824, loss_sup: 0.260126, loss_mps: 0.055541, loss_cps: 0.093156
[12:59:50.087] iteration 9408: total_loss: 0.383577, loss_sup: 0.154088, loss_mps: 0.077846, loss_cps: 0.151643
[12:59:50.234] iteration 9409: total_loss: 0.415372, loss_sup: 0.150438, loss_mps: 0.086861, loss_cps: 0.178073
[12:59:50.381] iteration 9410: total_loss: 0.287098, loss_sup: 0.131001, loss_mps: 0.056698, loss_cps: 0.099399
[12:59:50.529] iteration 9411: total_loss: 0.257079, loss_sup: 0.056530, loss_mps: 0.066375, loss_cps: 0.134175
[12:59:50.676] iteration 9412: total_loss: 0.392264, loss_sup: 0.110553, loss_mps: 0.091658, loss_cps: 0.190053
[12:59:50.824] iteration 9413: total_loss: 0.187565, loss_sup: 0.051834, loss_mps: 0.048554, loss_cps: 0.087177
[12:59:50.971] iteration 9414: total_loss: 0.191473, loss_sup: 0.049103, loss_mps: 0.051778, loss_cps: 0.090591
[12:59:51.117] iteration 9415: total_loss: 0.272611, loss_sup: 0.074767, loss_mps: 0.067936, loss_cps: 0.129908
[12:59:51.268] iteration 9416: total_loss: 0.230549, loss_sup: 0.040562, loss_mps: 0.064327, loss_cps: 0.125660
[12:59:51.416] iteration 9417: total_loss: 0.358683, loss_sup: 0.186669, loss_mps: 0.061312, loss_cps: 0.110703
[12:59:51.563] iteration 9418: total_loss: 0.341810, loss_sup: 0.079534, loss_mps: 0.085597, loss_cps: 0.176679
[12:59:51.709] iteration 9419: total_loss: 0.160690, loss_sup: 0.053155, loss_mps: 0.041430, loss_cps: 0.066106
[12:59:51.855] iteration 9420: total_loss: 0.278814, loss_sup: 0.156584, loss_mps: 0.046213, loss_cps: 0.076017
[12:59:52.001] iteration 9421: total_loss: 0.490936, loss_sup: 0.185503, loss_mps: 0.094210, loss_cps: 0.211224
[12:59:52.147] iteration 9422: total_loss: 0.203756, loss_sup: 0.091298, loss_mps: 0.044700, loss_cps: 0.067758
[12:59:52.293] iteration 9423: total_loss: 0.416905, loss_sup: 0.245182, loss_mps: 0.059985, loss_cps: 0.111739
[12:59:52.439] iteration 9424: total_loss: 0.280012, loss_sup: 0.157409, loss_mps: 0.047372, loss_cps: 0.075231
[12:59:52.586] iteration 9425: total_loss: 0.169066, loss_sup: 0.023505, loss_mps: 0.051935, loss_cps: 0.093625
[12:59:52.732] iteration 9426: total_loss: 0.238294, loss_sup: 0.067028, loss_mps: 0.060706, loss_cps: 0.110560
[12:59:52.878] iteration 9427: total_loss: 0.163837, loss_sup: 0.062505, loss_mps: 0.040357, loss_cps: 0.060975
[12:59:53.024] iteration 9428: total_loss: 0.351723, loss_sup: 0.082194, loss_mps: 0.088252, loss_cps: 0.181278
[12:59:53.172] iteration 9429: total_loss: 0.361460, loss_sup: 0.148865, loss_mps: 0.071816, loss_cps: 0.140779
[12:59:53.318] iteration 9430: total_loss: 0.231317, loss_sup: 0.045061, loss_mps: 0.065872, loss_cps: 0.120384
[12:59:53.464] iteration 9431: total_loss: 0.205201, loss_sup: 0.036621, loss_mps: 0.057475, loss_cps: 0.111105
[12:59:53.612] iteration 9432: total_loss: 0.210243, loss_sup: 0.038309, loss_mps: 0.057930, loss_cps: 0.114005
[12:59:53.758] iteration 9433: total_loss: 0.303833, loss_sup: 0.067083, loss_mps: 0.080308, loss_cps: 0.156442
[12:59:53.904] iteration 9434: total_loss: 0.319469, loss_sup: 0.120058, loss_mps: 0.066521, loss_cps: 0.132890
[12:59:54.050] iteration 9435: total_loss: 0.207048, loss_sup: 0.067186, loss_mps: 0.049940, loss_cps: 0.089921
[12:59:54.196] iteration 9436: total_loss: 0.466183, loss_sup: 0.131410, loss_mps: 0.107320, loss_cps: 0.227453
[12:59:54.342] iteration 9437: total_loss: 0.273839, loss_sup: 0.073097, loss_mps: 0.067128, loss_cps: 0.133614
[12:59:54.489] iteration 9438: total_loss: 0.228130, loss_sup: 0.098525, loss_mps: 0.047003, loss_cps: 0.082602
[12:59:54.635] iteration 9439: total_loss: 0.174114, loss_sup: 0.058279, loss_mps: 0.041431, loss_cps: 0.074405
[12:59:54.781] iteration 9440: total_loss: 0.148959, loss_sup: 0.016200, loss_mps: 0.049219, loss_cps: 0.083540
[12:59:54.927] iteration 9441: total_loss: 0.249002, loss_sup: 0.110708, loss_mps: 0.047790, loss_cps: 0.090504
[12:59:55.073] iteration 9442: total_loss: 0.281260, loss_sup: 0.137330, loss_mps: 0.050563, loss_cps: 0.093366
[12:59:55.220] iteration 9443: total_loss: 0.393322, loss_sup: 0.183649, loss_mps: 0.069883, loss_cps: 0.139791
[12:59:55.366] iteration 9444: total_loss: 0.683589, loss_sup: 0.462359, loss_mps: 0.073034, loss_cps: 0.148196
[12:59:55.514] iteration 9445: total_loss: 0.246928, loss_sup: 0.098548, loss_mps: 0.053315, loss_cps: 0.095065
[12:59:55.661] iteration 9446: total_loss: 0.170343, loss_sup: 0.039695, loss_mps: 0.045010, loss_cps: 0.085639
[12:59:55.807] iteration 9447: total_loss: 0.136993, loss_sup: 0.028208, loss_mps: 0.038803, loss_cps: 0.069982
[12:59:55.953] iteration 9448: total_loss: 0.419306, loss_sup: 0.214311, loss_mps: 0.067004, loss_cps: 0.137991
[12:59:56.099] iteration 9449: total_loss: 0.272982, loss_sup: 0.070986, loss_mps: 0.068293, loss_cps: 0.133703
[12:59:56.247] iteration 9450: total_loss: 0.215288, loss_sup: 0.088481, loss_mps: 0.044912, loss_cps: 0.081896
[12:59:56.396] iteration 9451: total_loss: 0.232234, loss_sup: 0.025223, loss_mps: 0.067557, loss_cps: 0.139454
[12:59:56.544] iteration 9452: total_loss: 0.146396, loss_sup: 0.017761, loss_mps: 0.044847, loss_cps: 0.083787
[12:59:56.691] iteration 9453: total_loss: 0.336969, loss_sup: 0.103989, loss_mps: 0.074966, loss_cps: 0.158013
[12:59:56.837] iteration 9454: total_loss: 0.208522, loss_sup: 0.033889, loss_mps: 0.060325, loss_cps: 0.114308
[12:59:56.983] iteration 9455: total_loss: 0.279369, loss_sup: 0.056236, loss_mps: 0.077135, loss_cps: 0.145998
[12:59:57.129] iteration 9456: total_loss: 0.255394, loss_sup: 0.077121, loss_mps: 0.064584, loss_cps: 0.113688
[12:59:57.275] iteration 9457: total_loss: 0.422365, loss_sup: 0.159005, loss_mps: 0.088060, loss_cps: 0.175300
[12:59:57.425] iteration 9458: total_loss: 0.132514, loss_sup: 0.013918, loss_mps: 0.044455, loss_cps: 0.074141
[12:59:57.573] iteration 9459: total_loss: 0.210907, loss_sup: 0.046947, loss_mps: 0.059224, loss_cps: 0.104737
[12:59:57.720] iteration 9460: total_loss: 0.367754, loss_sup: 0.103677, loss_mps: 0.088010, loss_cps: 0.176067
[12:59:57.866] iteration 9461: total_loss: 0.312034, loss_sup: 0.137827, loss_mps: 0.060137, loss_cps: 0.114070
[12:59:58.014] iteration 9462: total_loss: 0.249390, loss_sup: 0.113034, loss_mps: 0.048001, loss_cps: 0.088354
[12:59:58.161] iteration 9463: total_loss: 0.229954, loss_sup: 0.087496, loss_mps: 0.051714, loss_cps: 0.090743
[12:59:58.307] iteration 9464: total_loss: 0.271658, loss_sup: 0.089044, loss_mps: 0.063799, loss_cps: 0.118815
[12:59:58.457] iteration 9465: total_loss: 0.201341, loss_sup: 0.048642, loss_mps: 0.054512, loss_cps: 0.098186
[12:59:58.605] iteration 9466: total_loss: 0.451570, loss_sup: 0.125102, loss_mps: 0.103094, loss_cps: 0.223373
[12:59:58.752] iteration 9467: total_loss: 0.458145, loss_sup: 0.215254, loss_mps: 0.084350, loss_cps: 0.158541
[12:59:58.898] iteration 9468: total_loss: 0.133788, loss_sup: 0.032918, loss_mps: 0.039936, loss_cps: 0.060933
[12:59:59.044] iteration 9469: total_loss: 0.237431, loss_sup: 0.069803, loss_mps: 0.060161, loss_cps: 0.107468
[12:59:59.190] iteration 9470: total_loss: 0.185441, loss_sup: 0.050630, loss_mps: 0.048128, loss_cps: 0.086683
[12:59:59.335] iteration 9471: total_loss: 0.193825, loss_sup: 0.045547, loss_mps: 0.051343, loss_cps: 0.096935
[12:59:59.482] iteration 9472: total_loss: 0.220040, loss_sup: 0.053939, loss_mps: 0.057785, loss_cps: 0.108317
[12:59:59.630] iteration 9473: total_loss: 0.286891, loss_sup: 0.031450, loss_mps: 0.084072, loss_cps: 0.171369
[12:59:59.775] iteration 9474: total_loss: 0.286744, loss_sup: 0.084614, loss_mps: 0.069389, loss_cps: 0.132740
[12:59:59.923] iteration 9475: total_loss: 0.125597, loss_sup: 0.010471, loss_mps: 0.043382, loss_cps: 0.071744
[13:00:00.069] iteration 9476: total_loss: 0.171625, loss_sup: 0.036767, loss_mps: 0.047037, loss_cps: 0.087821
[13:00:00.217] iteration 9477: total_loss: 0.223852, loss_sup: 0.068123, loss_mps: 0.056026, loss_cps: 0.099702
[13:00:00.364] iteration 9478: total_loss: 0.391954, loss_sup: 0.165925, loss_mps: 0.076692, loss_cps: 0.149337
[13:00:00.510] iteration 9479: total_loss: 0.343100, loss_sup: 0.141147, loss_mps: 0.065002, loss_cps: 0.136952
[13:00:00.656] iteration 9480: total_loss: 0.213053, loss_sup: 0.033666, loss_mps: 0.061702, loss_cps: 0.117685
[13:00:00.802] iteration 9481: total_loss: 0.308508, loss_sup: 0.101824, loss_mps: 0.073220, loss_cps: 0.133463
[13:00:00.950] iteration 9482: total_loss: 0.324653, loss_sup: 0.143539, loss_mps: 0.060857, loss_cps: 0.120257
[13:00:01.098] iteration 9483: total_loss: 0.305598, loss_sup: 0.132547, loss_mps: 0.056488, loss_cps: 0.116563
[13:00:01.244] iteration 9484: total_loss: 0.115688, loss_sup: 0.017691, loss_mps: 0.036504, loss_cps: 0.061493
[13:00:01.391] iteration 9485: total_loss: 0.246859, loss_sup: 0.061915, loss_mps: 0.061061, loss_cps: 0.123882
[13:00:01.537] iteration 9486: total_loss: 0.147367, loss_sup: 0.010164, loss_mps: 0.047644, loss_cps: 0.089560
[13:00:01.685] iteration 9487: total_loss: 0.322714, loss_sup: 0.077398, loss_mps: 0.076214, loss_cps: 0.169102
[13:00:01.832] iteration 9488: total_loss: 0.597354, loss_sup: 0.301150, loss_mps: 0.093079, loss_cps: 0.203125
[13:00:01.981] iteration 9489: total_loss: 0.219321, loss_sup: 0.051782, loss_mps: 0.056803, loss_cps: 0.110736
[13:00:02.128] iteration 9490: total_loss: 0.236155, loss_sup: 0.082857, loss_mps: 0.051257, loss_cps: 0.102041
[13:00:02.274] iteration 9491: total_loss: 0.329871, loss_sup: 0.190790, loss_mps: 0.048131, loss_cps: 0.090950
[13:00:02.421] iteration 9492: total_loss: 0.234624, loss_sup: 0.091117, loss_mps: 0.048462, loss_cps: 0.095045
[13:00:02.568] iteration 9493: total_loss: 0.523206, loss_sup: 0.206231, loss_mps: 0.101481, loss_cps: 0.215494
[13:00:02.717] iteration 9494: total_loss: 0.375948, loss_sup: 0.209167, loss_mps: 0.058236, loss_cps: 0.108545
[13:00:02.865] iteration 9495: total_loss: 0.416784, loss_sup: 0.248451, loss_mps: 0.056246, loss_cps: 0.112087
[13:00:03.012] iteration 9496: total_loss: 0.155379, loss_sup: 0.023262, loss_mps: 0.047023, loss_cps: 0.085094
[13:00:03.159] iteration 9497: total_loss: 0.251327, loss_sup: 0.075564, loss_mps: 0.060012, loss_cps: 0.115752
[13:00:03.305] iteration 9498: total_loss: 0.220399, loss_sup: 0.068659, loss_mps: 0.054332, loss_cps: 0.097409
[13:00:03.452] iteration 9499: total_loss: 0.150227, loss_sup: 0.034648, loss_mps: 0.044044, loss_cps: 0.071535
[13:00:03.599] iteration 9500: total_loss: 0.228341, loss_sup: 0.042651, loss_mps: 0.064349, loss_cps: 0.121342
[13:00:03.599] Evaluation Started ==>
[13:00:14.979] ==> valid iteration 9500: unet metrics: {'dc': 0.6333788801064612, 'jc': 0.5076537878633497, 'pre': 0.7616159336465583, 'hd': 5.816466496791507}, ynet metrics: {'dc': 0.5708514765269009, 'jc': 0.4516224365402064, 'pre': 0.7513655091118523, 'hd': 5.9584473154040944}.
[13:00:14.981] Evaluation Finished!⏹️
[13:00:15.134] iteration 9501: total_loss: 0.230614, loss_sup: 0.085472, loss_mps: 0.050382, loss_cps: 0.094761
[13:00:15.281] iteration 9502: total_loss: 0.355823, loss_sup: 0.125863, loss_mps: 0.077948, loss_cps: 0.152013
[13:00:15.428] iteration 9503: total_loss: 0.217797, loss_sup: 0.050860, loss_mps: 0.058601, loss_cps: 0.108336
[13:00:15.573] iteration 9504: total_loss: 0.394853, loss_sup: 0.157722, loss_mps: 0.080263, loss_cps: 0.156868
[13:00:15.719] iteration 9505: total_loss: 0.194641, loss_sup: 0.024190, loss_mps: 0.060346, loss_cps: 0.110104
[13:00:15.864] iteration 9506: total_loss: 0.227325, loss_sup: 0.073463, loss_mps: 0.054050, loss_cps: 0.099811
[13:00:16.010] iteration 9507: total_loss: 0.228735, loss_sup: 0.042290, loss_mps: 0.064514, loss_cps: 0.121930
[13:00:16.156] iteration 9508: total_loss: 0.242283, loss_sup: 0.023761, loss_mps: 0.074581, loss_cps: 0.143941
[13:00:16.303] iteration 9509: total_loss: 0.155506, loss_sup: 0.038082, loss_mps: 0.043415, loss_cps: 0.074008
[13:00:16.448] iteration 9510: total_loss: 0.196335, loss_sup: 0.053916, loss_mps: 0.051404, loss_cps: 0.091014
[13:00:16.594] iteration 9511: total_loss: 0.464020, loss_sup: 0.189737, loss_mps: 0.087157, loss_cps: 0.187127
[13:00:16.740] iteration 9512: total_loss: 0.190773, loss_sup: 0.043280, loss_mps: 0.053128, loss_cps: 0.094364
[13:00:16.886] iteration 9513: total_loss: 0.217926, loss_sup: 0.056679, loss_mps: 0.056091, loss_cps: 0.105156
[13:00:17.031] iteration 9514: total_loss: 0.212049, loss_sup: 0.043035, loss_mps: 0.060662, loss_cps: 0.108352
[13:00:17.177] iteration 9515: total_loss: 0.373375, loss_sup: 0.223323, loss_mps: 0.055302, loss_cps: 0.094751
[13:00:17.323] iteration 9516: total_loss: 0.218396, loss_sup: 0.035893, loss_mps: 0.065336, loss_cps: 0.117167
[13:00:17.468] iteration 9517: total_loss: 0.163790, loss_sup: 0.029310, loss_mps: 0.050383, loss_cps: 0.084098
[13:00:17.614] iteration 9518: total_loss: 0.195691, loss_sup: 0.052382, loss_mps: 0.050109, loss_cps: 0.093200
[13:00:17.763] iteration 9519: total_loss: 0.209682, loss_sup: 0.082465, loss_mps: 0.048324, loss_cps: 0.078893
[13:00:17.909] iteration 9520: total_loss: 0.224339, loss_sup: 0.048175, loss_mps: 0.058666, loss_cps: 0.117498
[13:00:18.054] iteration 9521: total_loss: 0.233069, loss_sup: 0.053922, loss_mps: 0.059337, loss_cps: 0.119811
[13:00:18.202] iteration 9522: total_loss: 0.443293, loss_sup: 0.215055, loss_mps: 0.075954, loss_cps: 0.152285
[13:00:18.349] iteration 9523: total_loss: 0.225350, loss_sup: 0.050618, loss_mps: 0.058024, loss_cps: 0.116707
[13:00:18.495] iteration 9524: total_loss: 0.269140, loss_sup: 0.082350, loss_mps: 0.061963, loss_cps: 0.124827
[13:00:18.640] iteration 9525: total_loss: 0.275275, loss_sup: 0.124302, loss_mps: 0.049537, loss_cps: 0.101436
[13:00:18.787] iteration 9526: total_loss: 0.206241, loss_sup: 0.028743, loss_mps: 0.061548, loss_cps: 0.115950
[13:00:18.935] iteration 9527: total_loss: 0.145666, loss_sup: 0.009770, loss_mps: 0.047917, loss_cps: 0.087978
[13:00:19.080] iteration 9528: total_loss: 0.111817, loss_sup: 0.019449, loss_mps: 0.035170, loss_cps: 0.057198
[13:00:19.226] iteration 9529: total_loss: 0.212159, loss_sup: 0.044242, loss_mps: 0.057211, loss_cps: 0.110706
[13:00:19.371] iteration 9530: total_loss: 0.272145, loss_sup: 0.102483, loss_mps: 0.057266, loss_cps: 0.112396
[13:00:19.517] iteration 9531: total_loss: 0.156398, loss_sup: 0.014738, loss_mps: 0.049876, loss_cps: 0.091784
[13:00:19.663] iteration 9532: total_loss: 0.289058, loss_sup: 0.129392, loss_mps: 0.057181, loss_cps: 0.102485
[13:00:19.808] iteration 9533: total_loss: 0.219595, loss_sup: 0.097686, loss_mps: 0.042595, loss_cps: 0.079315
[13:00:19.958] iteration 9534: total_loss: 0.311879, loss_sup: 0.184835, loss_mps: 0.045553, loss_cps: 0.081490
[13:00:20.105] iteration 9535: total_loss: 0.219474, loss_sup: 0.103261, loss_mps: 0.039883, loss_cps: 0.076331
[13:00:20.251] iteration 9536: total_loss: 0.286712, loss_sup: 0.087836, loss_mps: 0.065972, loss_cps: 0.132903
[13:00:20.398] iteration 9537: total_loss: 0.257379, loss_sup: 0.103471, loss_mps: 0.055367, loss_cps: 0.098541
[13:00:20.543] iteration 9538: total_loss: 0.184535, loss_sup: 0.020975, loss_mps: 0.053434, loss_cps: 0.110126
[13:00:20.689] iteration 9539: total_loss: 0.178019, loss_sup: 0.039992, loss_mps: 0.047971, loss_cps: 0.090056
[13:00:20.834] iteration 9540: total_loss: 0.116607, loss_sup: 0.008525, loss_mps: 0.039117, loss_cps: 0.068965
[13:00:20.982] iteration 9541: total_loss: 0.598756, loss_sup: 0.272582, loss_mps: 0.101543, loss_cps: 0.224631
[13:00:21.129] iteration 9542: total_loss: 0.156035, loss_sup: 0.023446, loss_mps: 0.047812, loss_cps: 0.084778
[13:00:21.275] iteration 9543: total_loss: 0.231403, loss_sup: 0.093825, loss_mps: 0.048402, loss_cps: 0.089177
[13:00:21.424] iteration 9544: total_loss: 0.504683, loss_sup: 0.241171, loss_mps: 0.085488, loss_cps: 0.178024
[13:00:21.572] iteration 9545: total_loss: 0.218290, loss_sup: 0.032970, loss_mps: 0.062556, loss_cps: 0.122763
[13:00:21.719] iteration 9546: total_loss: 0.304414, loss_sup: 0.041169, loss_mps: 0.080570, loss_cps: 0.182675
[13:00:21.865] iteration 9547: total_loss: 0.196455, loss_sup: 0.060624, loss_mps: 0.046445, loss_cps: 0.089386
[13:00:22.012] iteration 9548: total_loss: 0.469441, loss_sup: 0.140220, loss_mps: 0.102093, loss_cps: 0.227128
[13:00:22.158] iteration 9549: total_loss: 0.227715, loss_sup: 0.080043, loss_mps: 0.052029, loss_cps: 0.095643
[13:00:22.305] iteration 9550: total_loss: 0.268926, loss_sup: 0.020069, loss_mps: 0.080996, loss_cps: 0.167861
[13:00:22.452] iteration 9551: total_loss: 0.278118, loss_sup: 0.060892, loss_mps: 0.074451, loss_cps: 0.142775
[13:00:22.598] iteration 9552: total_loss: 0.266368, loss_sup: 0.066139, loss_mps: 0.065575, loss_cps: 0.134653
[13:00:22.745] iteration 9553: total_loss: 0.138020, loss_sup: 0.035096, loss_mps: 0.038423, loss_cps: 0.064502
[13:00:22.891] iteration 9554: total_loss: 0.134198, loss_sup: 0.029034, loss_mps: 0.038900, loss_cps: 0.066264
[13:00:23.037] iteration 9555: total_loss: 0.232559, loss_sup: 0.129747, loss_mps: 0.039594, loss_cps: 0.063218
[13:00:23.183] iteration 9556: total_loss: 0.336067, loss_sup: 0.182687, loss_mps: 0.055964, loss_cps: 0.097415
[13:00:23.328] iteration 9557: total_loss: 0.285003, loss_sup: 0.156234, loss_mps: 0.047590, loss_cps: 0.081180
[13:00:23.475] iteration 9558: total_loss: 0.136703, loss_sup: 0.034794, loss_mps: 0.038576, loss_cps: 0.063333
[13:00:23.620] iteration 9559: total_loss: 0.285887, loss_sup: 0.015406, loss_mps: 0.087586, loss_cps: 0.182895
[13:00:23.766] iteration 9560: total_loss: 0.280265, loss_sup: 0.083790, loss_mps: 0.066076, loss_cps: 0.130398
[13:00:23.912] iteration 9561: total_loss: 0.308618, loss_sup: 0.108884, loss_mps: 0.065909, loss_cps: 0.133825
[13:00:24.060] iteration 9562: total_loss: 0.369913, loss_sup: 0.191156, loss_mps: 0.057751, loss_cps: 0.121007
[13:00:24.206] iteration 9563: total_loss: 0.218819, loss_sup: 0.132730, loss_mps: 0.033872, loss_cps: 0.052217
[13:00:24.352] iteration 9564: total_loss: 0.205825, loss_sup: 0.073822, loss_mps: 0.047989, loss_cps: 0.084013
[13:00:24.500] iteration 9565: total_loss: 0.318479, loss_sup: 0.085145, loss_mps: 0.073874, loss_cps: 0.159460
[13:00:24.647] iteration 9566: total_loss: 0.257873, loss_sup: 0.078602, loss_mps: 0.058678, loss_cps: 0.120593
[13:00:24.794] iteration 9567: total_loss: 0.242235, loss_sup: 0.083128, loss_mps: 0.054084, loss_cps: 0.105022
[13:00:24.943] iteration 9568: total_loss: 0.255897, loss_sup: 0.071759, loss_mps: 0.063347, loss_cps: 0.120791
[13:00:25.090] iteration 9569: total_loss: 0.239834, loss_sup: 0.059562, loss_mps: 0.060976, loss_cps: 0.119296
[13:00:25.235] iteration 9570: total_loss: 0.255851, loss_sup: 0.073051, loss_mps: 0.064991, loss_cps: 0.117809
[13:00:25.381] iteration 9571: total_loss: 0.258201, loss_sup: 0.087989, loss_mps: 0.059621, loss_cps: 0.110591
[13:00:25.529] iteration 9572: total_loss: 0.260868, loss_sup: 0.142728, loss_mps: 0.044378, loss_cps: 0.073762
[13:00:25.675] iteration 9573: total_loss: 0.432338, loss_sup: 0.289293, loss_mps: 0.051335, loss_cps: 0.091710
[13:00:25.821] iteration 9574: total_loss: 0.292807, loss_sup: 0.093524, loss_mps: 0.065679, loss_cps: 0.133605
[13:00:25.967] iteration 9575: total_loss: 0.329229, loss_sup: 0.127802, loss_mps: 0.066476, loss_cps: 0.134950
[13:00:26.113] iteration 9576: total_loss: 0.174177, loss_sup: 0.026700, loss_mps: 0.050229, loss_cps: 0.097248
[13:00:26.259] iteration 9577: total_loss: 0.285343, loss_sup: 0.095995, loss_mps: 0.064991, loss_cps: 0.124356
[13:00:26.404] iteration 9578: total_loss: 0.254418, loss_sup: 0.103310, loss_mps: 0.055295, loss_cps: 0.095813
[13:00:26.555] iteration 9579: total_loss: 0.148867, loss_sup: 0.035244, loss_mps: 0.043563, loss_cps: 0.070060
[13:00:26.701] iteration 9580: total_loss: 0.224236, loss_sup: 0.072628, loss_mps: 0.052851, loss_cps: 0.098758
[13:00:26.848] iteration 9581: total_loss: 0.395282, loss_sup: 0.195128, loss_mps: 0.071052, loss_cps: 0.129101
[13:00:26.995] iteration 9582: total_loss: 0.289664, loss_sup: 0.038413, loss_mps: 0.083505, loss_cps: 0.167746
[13:00:27.140] iteration 9583: total_loss: 0.255538, loss_sup: 0.056390, loss_mps: 0.070085, loss_cps: 0.129064
[13:00:27.286] iteration 9584: total_loss: 0.321249, loss_sup: 0.092067, loss_mps: 0.077647, loss_cps: 0.151536
[13:00:27.432] iteration 9585: total_loss: 0.200102, loss_sup: 0.057436, loss_mps: 0.052395, loss_cps: 0.090272
[13:00:27.577] iteration 9586: total_loss: 0.380218, loss_sup: 0.131932, loss_mps: 0.083041, loss_cps: 0.165245
[13:00:27.727] iteration 9587: total_loss: 0.383513, loss_sup: 0.104066, loss_mps: 0.089653, loss_cps: 0.189794
[13:00:27.875] iteration 9588: total_loss: 0.369173, loss_sup: 0.089965, loss_mps: 0.087565, loss_cps: 0.191643
[13:00:28.021] iteration 9589: total_loss: 0.587256, loss_sup: 0.291860, loss_mps: 0.095586, loss_cps: 0.199810
[13:00:28.167] iteration 9590: total_loss: 0.216110, loss_sup: 0.071082, loss_mps: 0.050784, loss_cps: 0.094243
[13:00:28.313] iteration 9591: total_loss: 0.287051, loss_sup: 0.116442, loss_mps: 0.058904, loss_cps: 0.111706
[13:00:28.460] iteration 9592: total_loss: 0.252197, loss_sup: 0.075222, loss_mps: 0.063620, loss_cps: 0.113354
[13:00:28.607] iteration 9593: total_loss: 0.153693, loss_sup: 0.014969, loss_mps: 0.050494, loss_cps: 0.088229
[13:00:28.752] iteration 9594: total_loss: 0.150693, loss_sup: 0.054067, loss_mps: 0.036703, loss_cps: 0.059923
[13:00:28.898] iteration 9595: total_loss: 0.362395, loss_sup: 0.065951, loss_mps: 0.095376, loss_cps: 0.201068
[13:00:29.046] iteration 9596: total_loss: 0.324284, loss_sup: 0.107711, loss_mps: 0.073858, loss_cps: 0.142715
[13:00:29.193] iteration 9597: total_loss: 0.268243, loss_sup: 0.072176, loss_mps: 0.067123, loss_cps: 0.128943
[13:00:29.339] iteration 9598: total_loss: 0.215098, loss_sup: 0.041920, loss_mps: 0.062574, loss_cps: 0.110604
[13:00:29.486] iteration 9599: total_loss: 0.223778, loss_sup: 0.103830, loss_mps: 0.043170, loss_cps: 0.076777
[13:00:29.632] iteration 9600: total_loss: 0.326956, loss_sup: 0.134745, loss_mps: 0.064460, loss_cps: 0.127751
[13:00:29.632] Evaluation Started ==>
[13:00:40.999] ==> valid iteration 9600: unet metrics: {'dc': 0.639301980405341, 'jc': 0.5151257335803562, 'pre': 0.7291629268665343, 'hd': 5.88394000701765}, ynet metrics: {'dc': 0.5964905869503507, 'jc': 0.4743183036411053, 'pre': 0.7373928236510422, 'hd': 6.035803696333424}.
[13:00:41.001] Evaluation Finished!⏹️
[13:00:41.154] iteration 9601: total_loss: 0.175459, loss_sup: 0.018258, loss_mps: 0.053558, loss_cps: 0.103642
[13:00:41.304] iteration 9602: total_loss: 0.205273, loss_sup: 0.033416, loss_mps: 0.060336, loss_cps: 0.111521
[13:00:41.452] iteration 9603: total_loss: 0.196735, loss_sup: 0.050989, loss_mps: 0.053435, loss_cps: 0.092311
[13:00:41.597] iteration 9604: total_loss: 0.266688, loss_sup: 0.041408, loss_mps: 0.073545, loss_cps: 0.151736
[13:00:41.742] iteration 9605: total_loss: 0.388985, loss_sup: 0.178328, loss_mps: 0.070802, loss_cps: 0.139855
[13:00:41.888] iteration 9606: total_loss: 0.216720, loss_sup: 0.049114, loss_mps: 0.060100, loss_cps: 0.107505
[13:00:42.034] iteration 9607: total_loss: 0.228405, loss_sup: 0.045158, loss_mps: 0.061598, loss_cps: 0.121648
[13:00:42.179] iteration 9608: total_loss: 0.220794, loss_sup: 0.037807, loss_mps: 0.066537, loss_cps: 0.116451
[13:00:42.325] iteration 9609: total_loss: 0.160590, loss_sup: 0.030226, loss_mps: 0.048611, loss_cps: 0.081752
[13:00:42.473] iteration 9610: total_loss: 0.172316, loss_sup: 0.057602, loss_mps: 0.042911, loss_cps: 0.071803
[13:00:42.619] iteration 9611: total_loss: 0.339376, loss_sup: 0.191471, loss_mps: 0.054159, loss_cps: 0.093747
[13:00:42.764] iteration 9612: total_loss: 0.098347, loss_sup: 0.006128, loss_mps: 0.035127, loss_cps: 0.057091
[13:00:42.910] iteration 9613: total_loss: 0.368763, loss_sup: 0.179959, loss_mps: 0.066884, loss_cps: 0.121920
[13:00:42.971] iteration 9614: total_loss: 0.379548, loss_sup: 0.077729, loss_mps: 0.098587, loss_cps: 0.203231
[13:00:44.208] iteration 9615: total_loss: 0.227530, loss_sup: 0.072446, loss_mps: 0.052543, loss_cps: 0.102541
[13:00:44.356] iteration 9616: total_loss: 0.227028, loss_sup: 0.050715, loss_mps: 0.061765, loss_cps: 0.114548
[13:00:44.502] iteration 9617: total_loss: 0.230258, loss_sup: 0.073117, loss_mps: 0.055905, loss_cps: 0.101236
[13:00:44.648] iteration 9618: total_loss: 0.389223, loss_sup: 0.146181, loss_mps: 0.081770, loss_cps: 0.161271
[13:00:44.794] iteration 9619: total_loss: 0.287075, loss_sup: 0.094001, loss_mps: 0.067490, loss_cps: 0.125583
[13:00:44.942] iteration 9620: total_loss: 0.206169, loss_sup: 0.086235, loss_mps: 0.044509, loss_cps: 0.075425
[13:00:45.088] iteration 9621: total_loss: 0.266318, loss_sup: 0.109263, loss_mps: 0.054654, loss_cps: 0.102400
[13:00:45.236] iteration 9622: total_loss: 0.209242, loss_sup: 0.031123, loss_mps: 0.060866, loss_cps: 0.117254
[13:00:45.382] iteration 9623: total_loss: 0.435373, loss_sup: 0.183603, loss_mps: 0.079676, loss_cps: 0.172094
[13:00:45.528] iteration 9624: total_loss: 0.351261, loss_sup: 0.066895, loss_mps: 0.094201, loss_cps: 0.190165
[13:00:45.673] iteration 9625: total_loss: 0.144492, loss_sup: 0.009430, loss_mps: 0.047968, loss_cps: 0.087093
[13:00:45.819] iteration 9626: total_loss: 0.350668, loss_sup: 0.131247, loss_mps: 0.074594, loss_cps: 0.144826
[13:00:45.966] iteration 9627: total_loss: 0.300450, loss_sup: 0.158890, loss_mps: 0.050808, loss_cps: 0.090753
[13:00:46.113] iteration 9628: total_loss: 0.245834, loss_sup: 0.094465, loss_mps: 0.055046, loss_cps: 0.096323
[13:00:46.259] iteration 9629: total_loss: 0.320657, loss_sup: 0.071637, loss_mps: 0.085170, loss_cps: 0.163851
[13:00:46.406] iteration 9630: total_loss: 0.235985, loss_sup: 0.078322, loss_mps: 0.053507, loss_cps: 0.104155
[13:00:46.553] iteration 9631: total_loss: 0.165285, loss_sup: 0.032424, loss_mps: 0.048697, loss_cps: 0.084164
[13:00:46.702] iteration 9632: total_loss: 0.452386, loss_sup: 0.292839, loss_mps: 0.054698, loss_cps: 0.104849
[13:00:46.848] iteration 9633: total_loss: 0.356270, loss_sup: 0.085016, loss_mps: 0.087004, loss_cps: 0.184250
[13:00:46.998] iteration 9634: total_loss: 0.144185, loss_sup: 0.025657, loss_mps: 0.044334, loss_cps: 0.074194
[13:00:47.144] iteration 9635: total_loss: 0.146853, loss_sup: 0.042300, loss_mps: 0.039159, loss_cps: 0.065394
[13:00:47.290] iteration 9636: total_loss: 0.332185, loss_sup: 0.148316, loss_mps: 0.063742, loss_cps: 0.120127
[13:00:47.436] iteration 9637: total_loss: 0.279509, loss_sup: 0.096127, loss_mps: 0.060537, loss_cps: 0.122845
[13:00:47.584] iteration 9638: total_loss: 0.359056, loss_sup: 0.149307, loss_mps: 0.070416, loss_cps: 0.139333
[13:00:47.731] iteration 9639: total_loss: 0.511571, loss_sup: 0.108945, loss_mps: 0.123338, loss_cps: 0.279288
[13:00:47.879] iteration 9640: total_loss: 0.136201, loss_sup: 0.016328, loss_mps: 0.044889, loss_cps: 0.074984
[13:00:48.025] iteration 9641: total_loss: 0.247361, loss_sup: 0.091999, loss_mps: 0.054677, loss_cps: 0.100685
[13:00:48.174] iteration 9642: total_loss: 0.266280, loss_sup: 0.151923, loss_mps: 0.042686, loss_cps: 0.071670
[13:00:48.320] iteration 9643: total_loss: 0.253698, loss_sup: 0.036093, loss_mps: 0.073893, loss_cps: 0.143712
[13:00:48.468] iteration 9644: total_loss: 0.297398, loss_sup: 0.045228, loss_mps: 0.084012, loss_cps: 0.168157
[13:00:48.616] iteration 9645: total_loss: 0.198267, loss_sup: 0.015782, loss_mps: 0.062399, loss_cps: 0.120087
[13:00:48.763] iteration 9646: total_loss: 0.464900, loss_sup: 0.235866, loss_mps: 0.072292, loss_cps: 0.156742
[13:00:48.909] iteration 9647: total_loss: 0.163731, loss_sup: 0.063274, loss_mps: 0.037449, loss_cps: 0.063007
[13:00:49.056] iteration 9648: total_loss: 0.281011, loss_sup: 0.128864, loss_mps: 0.054709, loss_cps: 0.097437
[13:00:49.202] iteration 9649: total_loss: 0.292895, loss_sup: 0.133422, loss_mps: 0.055863, loss_cps: 0.103610
[13:00:49.348] iteration 9650: total_loss: 0.129776, loss_sup: 0.012342, loss_mps: 0.042354, loss_cps: 0.075081
[13:00:49.494] iteration 9651: total_loss: 0.517465, loss_sup: 0.330718, loss_mps: 0.061955, loss_cps: 0.124792
[13:00:49.641] iteration 9652: total_loss: 0.337918, loss_sup: 0.122553, loss_mps: 0.070890, loss_cps: 0.144475
[13:00:49.787] iteration 9653: total_loss: 0.405693, loss_sup: 0.061914, loss_mps: 0.110999, loss_cps: 0.232780
[13:00:49.933] iteration 9654: total_loss: 0.210643, loss_sup: 0.010993, loss_mps: 0.065625, loss_cps: 0.134026
[13:00:50.079] iteration 9655: total_loss: 0.460752, loss_sup: 0.210224, loss_mps: 0.082371, loss_cps: 0.168158
[13:00:50.226] iteration 9656: total_loss: 0.270048, loss_sup: 0.032361, loss_mps: 0.078306, loss_cps: 0.159381
[13:00:50.377] iteration 9657: total_loss: 0.326220, loss_sup: 0.109364, loss_mps: 0.078446, loss_cps: 0.138410
[13:00:50.526] iteration 9658: total_loss: 0.262872, loss_sup: 0.089677, loss_mps: 0.061954, loss_cps: 0.111241
[13:00:50.675] iteration 9659: total_loss: 0.155148, loss_sup: 0.040826, loss_mps: 0.042802, loss_cps: 0.071520
[13:00:50.821] iteration 9660: total_loss: 0.320576, loss_sup: 0.052288, loss_mps: 0.090601, loss_cps: 0.177687
[13:00:50.967] iteration 9661: total_loss: 0.185923, loss_sup: 0.039401, loss_mps: 0.054231, loss_cps: 0.092291
[13:00:51.115] iteration 9662: total_loss: 0.293591, loss_sup: 0.068495, loss_mps: 0.074482, loss_cps: 0.150614
[13:00:51.262] iteration 9663: total_loss: 0.393301, loss_sup: 0.115117, loss_mps: 0.093142, loss_cps: 0.185042
[13:00:51.409] iteration 9664: total_loss: 0.172624, loss_sup: 0.036126, loss_mps: 0.048411, loss_cps: 0.088088
[13:00:51.556] iteration 9665: total_loss: 0.262928, loss_sup: 0.091928, loss_mps: 0.059595, loss_cps: 0.111404
[13:00:51.704] iteration 9666: total_loss: 0.317974, loss_sup: 0.165609, loss_mps: 0.055025, loss_cps: 0.097341
[13:00:51.850] iteration 9667: total_loss: 0.297073, loss_sup: 0.091293, loss_mps: 0.071348, loss_cps: 0.134432
[13:00:51.996] iteration 9668: total_loss: 0.447699, loss_sup: 0.210394, loss_mps: 0.080823, loss_cps: 0.156482
[13:00:52.142] iteration 9669: total_loss: 0.360521, loss_sup: 0.138345, loss_mps: 0.074922, loss_cps: 0.147253
[13:00:52.288] iteration 9670: total_loss: 0.249926, loss_sup: 0.083430, loss_mps: 0.057827, loss_cps: 0.108668
[13:00:52.434] iteration 9671: total_loss: 0.268652, loss_sup: 0.050224, loss_mps: 0.072554, loss_cps: 0.145874
[13:00:52.581] iteration 9672: total_loss: 0.475004, loss_sup: 0.111713, loss_mps: 0.113940, loss_cps: 0.249350
[13:00:52.729] iteration 9673: total_loss: 0.162311, loss_sup: 0.022245, loss_mps: 0.052490, loss_cps: 0.087576
[13:00:52.878] iteration 9674: total_loss: 0.271604, loss_sup: 0.064115, loss_mps: 0.071053, loss_cps: 0.136436
[13:00:53.027] iteration 9675: total_loss: 0.271996, loss_sup: 0.030362, loss_mps: 0.081569, loss_cps: 0.160065
[13:00:53.178] iteration 9676: total_loss: 0.308880, loss_sup: 0.060796, loss_mps: 0.080870, loss_cps: 0.167214
[13:00:53.324] iteration 9677: total_loss: 0.286951, loss_sup: 0.070451, loss_mps: 0.074010, loss_cps: 0.142489
[13:00:53.471] iteration 9678: total_loss: 0.177539, loss_sup: 0.024586, loss_mps: 0.054351, loss_cps: 0.098602
[13:00:53.617] iteration 9679: total_loss: 0.403644, loss_sup: 0.084754, loss_mps: 0.103426, loss_cps: 0.215464
[13:00:53.765] iteration 9680: total_loss: 0.294926, loss_sup: 0.115312, loss_mps: 0.061365, loss_cps: 0.118249
[13:00:53.913] iteration 9681: total_loss: 0.282055, loss_sup: 0.142533, loss_mps: 0.051244, loss_cps: 0.088278
[13:00:54.060] iteration 9682: total_loss: 0.144204, loss_sup: 0.019954, loss_mps: 0.044201, loss_cps: 0.080049
[13:00:54.206] iteration 9683: total_loss: 0.230565, loss_sup: 0.070936, loss_mps: 0.058556, loss_cps: 0.101073
[13:00:54.352] iteration 9684: total_loss: 0.358066, loss_sup: 0.150054, loss_mps: 0.069016, loss_cps: 0.138996
[13:00:54.499] iteration 9685: total_loss: 0.232427, loss_sup: 0.100137, loss_mps: 0.047362, loss_cps: 0.084928
[13:00:54.645] iteration 9686: total_loss: 0.279153, loss_sup: 0.130404, loss_mps: 0.054505, loss_cps: 0.094244
[13:00:54.792] iteration 9687: total_loss: 0.231297, loss_sup: 0.015126, loss_mps: 0.071489, loss_cps: 0.144683
[13:00:54.938] iteration 9688: total_loss: 0.118831, loss_sup: 0.010302, loss_mps: 0.039785, loss_cps: 0.068743
[13:00:55.087] iteration 9689: total_loss: 0.321694, loss_sup: 0.104896, loss_mps: 0.076112, loss_cps: 0.140686
[13:00:55.235] iteration 9690: total_loss: 0.323849, loss_sup: 0.198594, loss_mps: 0.043523, loss_cps: 0.081732
[13:00:55.382] iteration 9691: total_loss: 0.217518, loss_sup: 0.049892, loss_mps: 0.058587, loss_cps: 0.109039
[13:00:55.531] iteration 9692: total_loss: 0.201054, loss_sup: 0.034908, loss_mps: 0.056484, loss_cps: 0.109662
[13:00:55.678] iteration 9693: total_loss: 0.156532, loss_sup: 0.033660, loss_mps: 0.045865, loss_cps: 0.077007
[13:00:55.825] iteration 9694: total_loss: 0.208787, loss_sup: 0.065289, loss_mps: 0.051308, loss_cps: 0.092190
[13:00:55.971] iteration 9695: total_loss: 0.331039, loss_sup: 0.174176, loss_mps: 0.054857, loss_cps: 0.102007
[13:00:56.117] iteration 9696: total_loss: 0.163655, loss_sup: 0.041537, loss_mps: 0.044719, loss_cps: 0.077399
[13:00:56.263] iteration 9697: total_loss: 0.176960, loss_sup: 0.015466, loss_mps: 0.055678, loss_cps: 0.105815
[13:00:56.411] iteration 9698: total_loss: 0.290644, loss_sup: 0.123792, loss_mps: 0.057949, loss_cps: 0.108902
[13:00:56.557] iteration 9699: total_loss: 0.213344, loss_sup: 0.026847, loss_mps: 0.064991, loss_cps: 0.121505
[13:00:56.704] iteration 9700: total_loss: 0.417871, loss_sup: 0.065164, loss_mps: 0.111024, loss_cps: 0.241684
[13:00:56.704] Evaluation Started ==>
[13:01:08.109] ==> valid iteration 9700: unet metrics: {'dc': 0.6264222476481287, 'jc': 0.508394047576971, 'pre': 0.7061186854076835, 'hd': 6.01901371534168}, ynet metrics: {'dc': 0.5972562419979506, 'jc': 0.47868242490874524, 'pre': 0.7498976496641345, 'hd': 5.79665366206559}.
[13:01:08.110] Evaluation Finished!⏹️
[13:01:08.261] iteration 9701: total_loss: 0.507521, loss_sup: 0.152744, loss_mps: 0.111476, loss_cps: 0.243301
[13:01:08.408] iteration 9702: total_loss: 0.157111, loss_sup: 0.020266, loss_mps: 0.049313, loss_cps: 0.087531
[13:01:08.555] iteration 9703: total_loss: 0.168185, loss_sup: 0.045985, loss_mps: 0.044274, loss_cps: 0.077927
[13:01:08.701] iteration 9704: total_loss: 0.345923, loss_sup: 0.211865, loss_mps: 0.049387, loss_cps: 0.084671
[13:01:08.846] iteration 9705: total_loss: 0.201604, loss_sup: 0.026558, loss_mps: 0.055789, loss_cps: 0.119257
[13:01:08.992] iteration 9706: total_loss: 0.131818, loss_sup: 0.036968, loss_mps: 0.036628, loss_cps: 0.058222
[13:01:09.138] iteration 9707: total_loss: 0.247731, loss_sup: 0.080185, loss_mps: 0.058615, loss_cps: 0.108930
[13:01:09.284] iteration 9708: total_loss: 0.252138, loss_sup: 0.026876, loss_mps: 0.073067, loss_cps: 0.152195
[13:01:09.430] iteration 9709: total_loss: 0.227905, loss_sup: 0.114010, loss_mps: 0.041121, loss_cps: 0.072774
[13:01:09.576] iteration 9710: total_loss: 0.221218, loss_sup: 0.095734, loss_mps: 0.042512, loss_cps: 0.082972
[13:01:09.722] iteration 9711: total_loss: 0.376395, loss_sup: 0.140081, loss_mps: 0.076751, loss_cps: 0.159563
[13:01:09.869] iteration 9712: total_loss: 0.298068, loss_sup: 0.037818, loss_mps: 0.084300, loss_cps: 0.175950
[13:01:10.019] iteration 9713: total_loss: 0.316436, loss_sup: 0.187883, loss_mps: 0.046214, loss_cps: 0.082339
[13:01:10.166] iteration 9714: total_loss: 0.206681, loss_sup: 0.019614, loss_mps: 0.063596, loss_cps: 0.123471
[13:01:10.311] iteration 9715: total_loss: 0.201520, loss_sup: 0.021838, loss_mps: 0.059729, loss_cps: 0.119953
[13:01:10.458] iteration 9716: total_loss: 0.276843, loss_sup: 0.131671, loss_mps: 0.052144, loss_cps: 0.093029
[13:01:10.604] iteration 9717: total_loss: 0.242478, loss_sup: 0.097680, loss_mps: 0.050040, loss_cps: 0.094758
[13:01:10.750] iteration 9718: total_loss: 0.248971, loss_sup: 0.094286, loss_mps: 0.053808, loss_cps: 0.100877
[13:01:10.897] iteration 9719: total_loss: 0.159554, loss_sup: 0.003372, loss_mps: 0.053834, loss_cps: 0.102348
[13:01:11.044] iteration 9720: total_loss: 0.385298, loss_sup: 0.174317, loss_mps: 0.069379, loss_cps: 0.141602
[13:01:11.190] iteration 9721: total_loss: 0.435231, loss_sup: 0.203711, loss_mps: 0.077211, loss_cps: 0.154309
[13:01:11.337] iteration 9722: total_loss: 0.370274, loss_sup: 0.142931, loss_mps: 0.075540, loss_cps: 0.151803
[13:01:11.483] iteration 9723: total_loss: 0.235539, loss_sup: 0.095078, loss_mps: 0.048848, loss_cps: 0.091613
[13:01:11.630] iteration 9724: total_loss: 0.168042, loss_sup: 0.039681, loss_mps: 0.047479, loss_cps: 0.080882
[13:01:11.777] iteration 9725: total_loss: 0.272474, loss_sup: 0.056604, loss_mps: 0.073623, loss_cps: 0.142246
[13:01:11.927] iteration 9726: total_loss: 0.239851, loss_sup: 0.058931, loss_mps: 0.065431, loss_cps: 0.115489
[13:01:12.074] iteration 9727: total_loss: 0.339505, loss_sup: 0.191371, loss_mps: 0.052833, loss_cps: 0.095301
[13:01:12.222] iteration 9728: total_loss: 0.143632, loss_sup: 0.021942, loss_mps: 0.046574, loss_cps: 0.075115
[13:01:12.369] iteration 9729: total_loss: 0.250322, loss_sup: 0.051383, loss_mps: 0.070420, loss_cps: 0.128519
[13:01:12.516] iteration 9730: total_loss: 0.166720, loss_sup: 0.036625, loss_mps: 0.047599, loss_cps: 0.082496
[13:01:12.664] iteration 9731: total_loss: 0.224467, loss_sup: 0.047323, loss_mps: 0.064773, loss_cps: 0.112371
[13:01:12.811] iteration 9732: total_loss: 0.176486, loss_sup: 0.049124, loss_mps: 0.048642, loss_cps: 0.078720
[13:01:12.958] iteration 9733: total_loss: 0.196769, loss_sup: 0.007572, loss_mps: 0.065897, loss_cps: 0.123299
[13:01:13.104] iteration 9734: total_loss: 0.238095, loss_sup: 0.043834, loss_mps: 0.068143, loss_cps: 0.126118
[13:01:13.251] iteration 9735: total_loss: 0.283700, loss_sup: 0.141175, loss_mps: 0.049701, loss_cps: 0.092824
[13:01:13.398] iteration 9736: total_loss: 0.179976, loss_sup: 0.053327, loss_mps: 0.047540, loss_cps: 0.079110
[13:01:13.544] iteration 9737: total_loss: 0.180079, loss_sup: 0.029685, loss_mps: 0.053511, loss_cps: 0.096883
[13:01:13.691] iteration 9738: total_loss: 0.431177, loss_sup: 0.274697, loss_mps: 0.056309, loss_cps: 0.100171
[13:01:13.837] iteration 9739: total_loss: 0.192561, loss_sup: 0.047322, loss_mps: 0.052274, loss_cps: 0.092964
[13:01:13.986] iteration 9740: total_loss: 0.218117, loss_sup: 0.044350, loss_mps: 0.058746, loss_cps: 0.115021
[13:01:14.132] iteration 9741: total_loss: 0.301555, loss_sup: 0.043482, loss_mps: 0.085865, loss_cps: 0.172208
[13:01:14.278] iteration 9742: total_loss: 0.238281, loss_sup: 0.072334, loss_mps: 0.060168, loss_cps: 0.105778
[13:01:14.423] iteration 9743: total_loss: 0.173643, loss_sup: 0.037117, loss_mps: 0.049544, loss_cps: 0.086982
[13:01:14.570] iteration 9744: total_loss: 0.145771, loss_sup: 0.043446, loss_mps: 0.038770, loss_cps: 0.063555
[13:01:14.717] iteration 9745: total_loss: 0.229392, loss_sup: 0.081900, loss_mps: 0.051622, loss_cps: 0.095870
[13:01:14.863] iteration 9746: total_loss: 0.109711, loss_sup: 0.012986, loss_mps: 0.035855, loss_cps: 0.060869
[13:01:15.009] iteration 9747: total_loss: 0.422687, loss_sup: 0.133267, loss_mps: 0.093886, loss_cps: 0.195535
[13:01:15.155] iteration 9748: total_loss: 0.389758, loss_sup: 0.241869, loss_mps: 0.051677, loss_cps: 0.096211
[13:01:15.306] iteration 9749: total_loss: 0.365928, loss_sup: 0.111135, loss_mps: 0.083863, loss_cps: 0.170930
[13:01:15.453] iteration 9750: total_loss: 0.187383, loss_sup: 0.096079, loss_mps: 0.034432, loss_cps: 0.056871
[13:01:15.599] iteration 9751: total_loss: 0.369032, loss_sup: 0.173303, loss_mps: 0.066051, loss_cps: 0.129678
[13:01:15.746] iteration 9752: total_loss: 0.280478, loss_sup: 0.144887, loss_mps: 0.047563, loss_cps: 0.088029
[13:01:15.893] iteration 9753: total_loss: 0.273295, loss_sup: 0.093295, loss_mps: 0.060868, loss_cps: 0.119132
[13:01:16.039] iteration 9754: total_loss: 0.140944, loss_sup: 0.030367, loss_mps: 0.042036, loss_cps: 0.068540
[13:01:16.186] iteration 9755: total_loss: 0.257700, loss_sup: 0.109069, loss_mps: 0.053138, loss_cps: 0.095493
[13:01:16.336] iteration 9756: total_loss: 0.303426, loss_sup: 0.106643, loss_mps: 0.066243, loss_cps: 0.130540
[13:01:16.484] iteration 9757: total_loss: 0.246490, loss_sup: 0.082783, loss_mps: 0.054723, loss_cps: 0.108985
[13:01:16.630] iteration 9758: total_loss: 0.183185, loss_sup: 0.047219, loss_mps: 0.046263, loss_cps: 0.089703
[13:01:16.776] iteration 9759: total_loss: 0.217341, loss_sup: 0.079158, loss_mps: 0.048402, loss_cps: 0.089781
[13:01:16.922] iteration 9760: total_loss: 0.274364, loss_sup: 0.078593, loss_mps: 0.066289, loss_cps: 0.129482
[13:01:17.068] iteration 9761: total_loss: 0.246264, loss_sup: 0.040012, loss_mps: 0.070289, loss_cps: 0.135963
[13:01:17.214] iteration 9762: total_loss: 0.165443, loss_sup: 0.014803, loss_mps: 0.051232, loss_cps: 0.099407
[13:01:17.360] iteration 9763: total_loss: 0.216665, loss_sup: 0.059476, loss_mps: 0.054193, loss_cps: 0.102996
[13:01:17.506] iteration 9764: total_loss: 0.226346, loss_sup: 0.115545, loss_mps: 0.040505, loss_cps: 0.070295
[13:01:17.653] iteration 9765: total_loss: 0.176929, loss_sup: 0.063431, loss_mps: 0.042338, loss_cps: 0.071161
[13:01:17.799] iteration 9766: total_loss: 0.214466, loss_sup: 0.071638, loss_mps: 0.048297, loss_cps: 0.094530
[13:01:17.945] iteration 9767: total_loss: 0.181808, loss_sup: 0.042099, loss_mps: 0.049777, loss_cps: 0.089931
[13:01:18.091] iteration 9768: total_loss: 0.363895, loss_sup: 0.089849, loss_mps: 0.087355, loss_cps: 0.186692
[13:01:18.237] iteration 9769: total_loss: 0.276586, loss_sup: 0.039072, loss_mps: 0.076729, loss_cps: 0.160786
[13:01:18.384] iteration 9770: total_loss: 0.174241, loss_sup: 0.060370, loss_mps: 0.042711, loss_cps: 0.071160
[13:01:18.530] iteration 9771: total_loss: 0.214514, loss_sup: 0.016636, loss_mps: 0.065615, loss_cps: 0.132263
[13:01:18.678] iteration 9772: total_loss: 0.260915, loss_sup: 0.053284, loss_mps: 0.070496, loss_cps: 0.137135
[13:01:18.824] iteration 9773: total_loss: 0.195500, loss_sup: 0.032745, loss_mps: 0.054334, loss_cps: 0.108421
[13:01:18.970] iteration 9774: total_loss: 0.156221, loss_sup: 0.042845, loss_mps: 0.041962, loss_cps: 0.071414
[13:01:19.116] iteration 9775: total_loss: 0.155240, loss_sup: 0.025094, loss_mps: 0.046533, loss_cps: 0.083613
[13:01:19.263] iteration 9776: total_loss: 0.194015, loss_sup: 0.042278, loss_mps: 0.052827, loss_cps: 0.098911
[13:01:19.415] iteration 9777: total_loss: 0.266811, loss_sup: 0.051822, loss_mps: 0.070560, loss_cps: 0.144428
[13:01:19.561] iteration 9778: total_loss: 0.238968, loss_sup: 0.042911, loss_mps: 0.065490, loss_cps: 0.130567
[13:01:19.707] iteration 9779: total_loss: 0.306588, loss_sup: 0.112376, loss_mps: 0.067114, loss_cps: 0.127098
[13:01:19.856] iteration 9780: total_loss: 0.254400, loss_sup: 0.117775, loss_mps: 0.050972, loss_cps: 0.085654
[13:01:20.002] iteration 9781: total_loss: 0.273935, loss_sup: 0.132679, loss_mps: 0.048719, loss_cps: 0.092538
[13:01:20.148] iteration 9782: total_loss: 0.233297, loss_sup: 0.084887, loss_mps: 0.051355, loss_cps: 0.097056
[13:01:20.294] iteration 9783: total_loss: 0.276582, loss_sup: 0.104236, loss_mps: 0.060895, loss_cps: 0.111451
[13:01:20.446] iteration 9784: total_loss: 0.283721, loss_sup: 0.112464, loss_mps: 0.059912, loss_cps: 0.111345
[13:01:20.592] iteration 9785: total_loss: 0.133859, loss_sup: 0.020361, loss_mps: 0.041837, loss_cps: 0.071662
[13:01:20.739] iteration 9786: total_loss: 0.177548, loss_sup: 0.062892, loss_mps: 0.041117, loss_cps: 0.073538
[13:01:20.885] iteration 9787: total_loss: 0.455188, loss_sup: 0.203519, loss_mps: 0.083338, loss_cps: 0.168331
[13:01:21.031] iteration 9788: total_loss: 0.118507, loss_sup: 0.045076, loss_mps: 0.028736, loss_cps: 0.044695
[13:01:21.177] iteration 9789: total_loss: 0.216782, loss_sup: 0.044031, loss_mps: 0.057142, loss_cps: 0.115610
[13:01:21.324] iteration 9790: total_loss: 0.326877, loss_sup: 0.083077, loss_mps: 0.076779, loss_cps: 0.167021
[13:01:21.470] iteration 9791: total_loss: 0.254628, loss_sup: 0.123381, loss_mps: 0.045451, loss_cps: 0.085797
[13:01:21.618] iteration 9792: total_loss: 0.147838, loss_sup: 0.016470, loss_mps: 0.043924, loss_cps: 0.087445
[13:01:21.764] iteration 9793: total_loss: 0.237098, loss_sup: 0.084808, loss_mps: 0.054032, loss_cps: 0.098258
[13:01:21.911] iteration 9794: total_loss: 0.264256, loss_sup: 0.131601, loss_mps: 0.045745, loss_cps: 0.086911
[13:01:22.057] iteration 9795: total_loss: 0.158687, loss_sup: 0.028476, loss_mps: 0.046216, loss_cps: 0.083995
[13:01:22.207] iteration 9796: total_loss: 0.231550, loss_sup: 0.035485, loss_mps: 0.065556, loss_cps: 0.130508
[13:01:22.353] iteration 9797: total_loss: 0.188910, loss_sup: 0.042310, loss_mps: 0.049411, loss_cps: 0.097188
[13:01:22.503] iteration 9798: total_loss: 0.244773, loss_sup: 0.098229, loss_mps: 0.051834, loss_cps: 0.094710
[13:01:22.650] iteration 9799: total_loss: 0.192006, loss_sup: 0.026672, loss_mps: 0.056528, loss_cps: 0.108806
[13:01:22.796] iteration 9800: total_loss: 0.233609, loss_sup: 0.075452, loss_mps: 0.054740, loss_cps: 0.103418
[13:01:22.797] Evaluation Started ==>
[13:01:34.151] ==> valid iteration 9800: unet metrics: {'dc': 0.6284447148545959, 'jc': 0.512962471532193, 'pre': 0.7245450866571671, 'hd': 5.934235379081121}, ynet metrics: {'dc': 0.5725418088519272, 'jc': 0.4579638885334187, 'pre': 0.739358620052494, 'hd': 5.876266132206583}.
[13:01:34.153] Evaluation Finished!⏹️
[13:01:34.306] iteration 9801: total_loss: 0.272956, loss_sup: 0.033207, loss_mps: 0.075712, loss_cps: 0.164037
[13:01:34.453] iteration 9802: total_loss: 0.209022, loss_sup: 0.108500, loss_mps: 0.036699, loss_cps: 0.063822
[13:01:34.598] iteration 9803: total_loss: 0.094734, loss_sup: 0.018137, loss_mps: 0.030145, loss_cps: 0.046452
[13:01:34.746] iteration 9804: total_loss: 0.406469, loss_sup: 0.245860, loss_mps: 0.054131, loss_cps: 0.106478
[13:01:34.891] iteration 9805: total_loss: 0.272707, loss_sup: 0.050881, loss_mps: 0.073181, loss_cps: 0.148644
[13:01:35.036] iteration 9806: total_loss: 0.224701, loss_sup: 0.086374, loss_mps: 0.048393, loss_cps: 0.089934
[13:01:35.181] iteration 9807: total_loss: 0.190893, loss_sup: 0.023819, loss_mps: 0.056565, loss_cps: 0.110508
[13:01:35.326] iteration 9808: total_loss: 0.163499, loss_sup: 0.010857, loss_mps: 0.051440, loss_cps: 0.101202
[13:01:35.471] iteration 9809: total_loss: 0.330033, loss_sup: 0.132462, loss_mps: 0.068992, loss_cps: 0.128578
[13:01:35.616] iteration 9810: total_loss: 0.355274, loss_sup: 0.134237, loss_mps: 0.070839, loss_cps: 0.150198
[13:01:35.762] iteration 9811: total_loss: 0.402033, loss_sup: 0.217736, loss_mps: 0.061870, loss_cps: 0.122428
[13:01:35.908] iteration 9812: total_loss: 0.353882, loss_sup: 0.105038, loss_mps: 0.082942, loss_cps: 0.165902
[13:01:36.053] iteration 9813: total_loss: 0.295881, loss_sup: 0.128172, loss_mps: 0.057893, loss_cps: 0.109816
[13:01:36.198] iteration 9814: total_loss: 0.212297, loss_sup: 0.044600, loss_mps: 0.056780, loss_cps: 0.110918
[13:01:36.343] iteration 9815: total_loss: 0.326803, loss_sup: 0.173020, loss_mps: 0.054588, loss_cps: 0.099195
[13:01:36.489] iteration 9816: total_loss: 0.459164, loss_sup: 0.256034, loss_mps: 0.066765, loss_cps: 0.136365
[13:01:36.635] iteration 9817: total_loss: 0.215015, loss_sup: 0.025740, loss_mps: 0.065696, loss_cps: 0.123579
[13:01:36.783] iteration 9818: total_loss: 0.229169, loss_sup: 0.085129, loss_mps: 0.051859, loss_cps: 0.092181
[13:01:36.930] iteration 9819: total_loss: 0.500098, loss_sup: 0.204090, loss_mps: 0.093474, loss_cps: 0.202534
[13:01:37.076] iteration 9820: total_loss: 0.148771, loss_sup: 0.038143, loss_mps: 0.040666, loss_cps: 0.069962
[13:01:37.222] iteration 9821: total_loss: 0.246232, loss_sup: 0.020088, loss_mps: 0.074029, loss_cps: 0.152115
[13:01:37.368] iteration 9822: total_loss: 0.508927, loss_sup: 0.236349, loss_mps: 0.089574, loss_cps: 0.183005
[13:01:37.515] iteration 9823: total_loss: 0.262719, loss_sup: 0.125708, loss_mps: 0.051488, loss_cps: 0.085523
[13:01:37.660] iteration 9824: total_loss: 0.239117, loss_sup: 0.034204, loss_mps: 0.072856, loss_cps: 0.132057
[13:01:37.806] iteration 9825: total_loss: 0.162380, loss_sup: 0.032480, loss_mps: 0.047122, loss_cps: 0.082778
[13:01:37.958] iteration 9826: total_loss: 0.363220, loss_sup: 0.140342, loss_mps: 0.074097, loss_cps: 0.148781
[13:01:38.104] iteration 9827: total_loss: 0.187923, loss_sup: 0.052981, loss_mps: 0.049371, loss_cps: 0.085570
[13:01:38.253] iteration 9828: total_loss: 0.531814, loss_sup: 0.375179, loss_mps: 0.057063, loss_cps: 0.099573
[13:01:38.399] iteration 9829: total_loss: 0.293366, loss_sup: 0.030662, loss_mps: 0.085388, loss_cps: 0.177316
[13:01:38.545] iteration 9830: total_loss: 0.368135, loss_sup: 0.130550, loss_mps: 0.080922, loss_cps: 0.156663
[13:01:38.692] iteration 9831: total_loss: 0.176470, loss_sup: 0.023680, loss_mps: 0.055952, loss_cps: 0.096838
[13:01:38.839] iteration 9832: total_loss: 0.269922, loss_sup: 0.112293, loss_mps: 0.058644, loss_cps: 0.098985
[13:01:38.985] iteration 9833: total_loss: 0.215769, loss_sup: 0.100924, loss_mps: 0.042226, loss_cps: 0.072620
[13:01:39.131] iteration 9834: total_loss: 0.188465, loss_sup: 0.039816, loss_mps: 0.051013, loss_cps: 0.097636
[13:01:39.276] iteration 9835: total_loss: 0.190630, loss_sup: 0.055569, loss_mps: 0.047450, loss_cps: 0.087611
[13:01:39.422] iteration 9836: total_loss: 0.280377, loss_sup: 0.067316, loss_mps: 0.070296, loss_cps: 0.142765
[13:01:39.568] iteration 9837: total_loss: 0.377797, loss_sup: 0.145745, loss_mps: 0.078240, loss_cps: 0.153812
[13:01:39.714] iteration 9838: total_loss: 0.267370, loss_sup: 0.079168, loss_mps: 0.064842, loss_cps: 0.123360
[13:01:39.859] iteration 9839: total_loss: 0.377334, loss_sup: 0.158862, loss_mps: 0.073942, loss_cps: 0.144529
[13:01:40.005] iteration 9840: total_loss: 0.252904, loss_sup: 0.039848, loss_mps: 0.073322, loss_cps: 0.139734
[13:01:40.151] iteration 9841: total_loss: 0.216887, loss_sup: 0.065249, loss_mps: 0.056271, loss_cps: 0.095366
[13:01:40.297] iteration 9842: total_loss: 0.100768, loss_sup: 0.017016, loss_mps: 0.031533, loss_cps: 0.052219
[13:01:40.443] iteration 9843: total_loss: 0.328145, loss_sup: 0.125099, loss_mps: 0.071343, loss_cps: 0.131703
[13:01:40.589] iteration 9844: total_loss: 0.315774, loss_sup: 0.097299, loss_mps: 0.072864, loss_cps: 0.145611
[13:01:40.735] iteration 9845: total_loss: 0.210903, loss_sup: 0.067787, loss_mps: 0.051230, loss_cps: 0.091887
[13:01:40.881] iteration 9846: total_loss: 0.261213, loss_sup: 0.092898, loss_mps: 0.059591, loss_cps: 0.108723
[13:01:41.027] iteration 9847: total_loss: 0.270664, loss_sup: 0.070783, loss_mps: 0.067294, loss_cps: 0.132586
[13:01:41.175] iteration 9848: total_loss: 0.142602, loss_sup: 0.017673, loss_mps: 0.044630, loss_cps: 0.080299
[13:01:41.320] iteration 9849: total_loss: 0.295792, loss_sup: 0.127759, loss_mps: 0.056092, loss_cps: 0.111941
[13:01:41.467] iteration 9850: total_loss: 0.349296, loss_sup: 0.156435, loss_mps: 0.066141, loss_cps: 0.126719
[13:01:41.613] iteration 9851: total_loss: 0.190677, loss_sup: 0.065063, loss_mps: 0.047127, loss_cps: 0.078486
[13:01:41.760] iteration 9852: total_loss: 0.475694, loss_sup: 0.186389, loss_mps: 0.096391, loss_cps: 0.192914
[13:01:41.906] iteration 9853: total_loss: 0.229862, loss_sup: 0.030274, loss_mps: 0.068699, loss_cps: 0.130889
[13:01:42.052] iteration 9854: total_loss: 0.335834, loss_sup: 0.100811, loss_mps: 0.073220, loss_cps: 0.161803
[13:01:42.199] iteration 9855: total_loss: 0.109614, loss_sup: 0.014575, loss_mps: 0.035888, loss_cps: 0.059152
[13:01:42.345] iteration 9856: total_loss: 0.171203, loss_sup: 0.052006, loss_mps: 0.043373, loss_cps: 0.075824
[13:01:42.500] iteration 9857: total_loss: 0.282703, loss_sup: 0.050699, loss_mps: 0.077974, loss_cps: 0.154030
[13:01:42.646] iteration 9858: total_loss: 0.237815, loss_sup: 0.080692, loss_mps: 0.056156, loss_cps: 0.100966
[13:01:42.793] iteration 9859: total_loss: 0.315931, loss_sup: 0.165855, loss_mps: 0.050260, loss_cps: 0.099817
[13:01:42.940] iteration 9860: total_loss: 0.269001, loss_sup: 0.066865, loss_mps: 0.068511, loss_cps: 0.133625
[13:01:43.086] iteration 9861: total_loss: 0.222584, loss_sup: 0.022869, loss_mps: 0.068961, loss_cps: 0.130755
[13:01:43.232] iteration 9862: total_loss: 0.375192, loss_sup: 0.165952, loss_mps: 0.069294, loss_cps: 0.139945
[13:01:43.378] iteration 9863: total_loss: 0.195255, loss_sup: 0.050002, loss_mps: 0.051749, loss_cps: 0.093503
[13:01:43.530] iteration 9864: total_loss: 0.189194, loss_sup: 0.038524, loss_mps: 0.052611, loss_cps: 0.098058
[13:01:43.675] iteration 9865: total_loss: 0.116085, loss_sup: 0.012081, loss_mps: 0.038767, loss_cps: 0.065237
[13:01:43.821] iteration 9866: total_loss: 0.193661, loss_sup: 0.033423, loss_mps: 0.058752, loss_cps: 0.101486
[13:01:43.967] iteration 9867: total_loss: 0.242023, loss_sup: 0.069398, loss_mps: 0.059895, loss_cps: 0.112730
[13:01:44.113] iteration 9868: total_loss: 0.351982, loss_sup: 0.109906, loss_mps: 0.079140, loss_cps: 0.162936
[13:01:44.259] iteration 9869: total_loss: 0.247826, loss_sup: 0.102950, loss_mps: 0.050424, loss_cps: 0.094452
[13:01:44.409] iteration 9870: total_loss: 0.166289, loss_sup: 0.044893, loss_mps: 0.041592, loss_cps: 0.079804
[13:01:44.554] iteration 9871: total_loss: 0.159556, loss_sup: 0.038793, loss_mps: 0.045979, loss_cps: 0.074784
[13:01:44.700] iteration 9872: total_loss: 0.228200, loss_sup: 0.071205, loss_mps: 0.055266, loss_cps: 0.101729
[13:01:44.847] iteration 9873: total_loss: 0.212820, loss_sup: 0.045541, loss_mps: 0.058904, loss_cps: 0.108375
[13:01:44.995] iteration 9874: total_loss: 0.182043, loss_sup: 0.074125, loss_mps: 0.038794, loss_cps: 0.069123
[13:01:45.140] iteration 9875: total_loss: 0.205842, loss_sup: 0.079249, loss_mps: 0.046384, loss_cps: 0.080208
[13:01:45.290] iteration 9876: total_loss: 0.158618, loss_sup: 0.059087, loss_mps: 0.037167, loss_cps: 0.062364
[13:01:45.436] iteration 9877: total_loss: 0.383851, loss_sup: 0.043479, loss_mps: 0.106719, loss_cps: 0.233653
[13:01:45.581] iteration 9878: total_loss: 0.135247, loss_sup: 0.012668, loss_mps: 0.045160, loss_cps: 0.077419
[13:01:45.727] iteration 9879: total_loss: 0.305193, loss_sup: 0.138494, loss_mps: 0.059330, loss_cps: 0.107369
[13:01:45.874] iteration 9880: total_loss: 0.202584, loss_sup: 0.035025, loss_mps: 0.058029, loss_cps: 0.109530
[13:01:46.020] iteration 9881: total_loss: 0.469279, loss_sup: 0.224400, loss_mps: 0.080888, loss_cps: 0.163991
[13:01:46.167] iteration 9882: total_loss: 0.113987, loss_sup: 0.003769, loss_mps: 0.038651, loss_cps: 0.071567
[13:01:46.313] iteration 9883: total_loss: 0.213535, loss_sup: 0.098097, loss_mps: 0.043166, loss_cps: 0.072272
[13:01:46.459] iteration 9884: total_loss: 0.311548, loss_sup: 0.121097, loss_mps: 0.062743, loss_cps: 0.127708
[13:01:46.608] iteration 9885: total_loss: 0.214086, loss_sup: 0.065066, loss_mps: 0.051306, loss_cps: 0.097714
[13:01:46.754] iteration 9886: total_loss: 0.559199, loss_sup: 0.285737, loss_mps: 0.088209, loss_cps: 0.185253
[13:01:46.900] iteration 9887: total_loss: 0.241471, loss_sup: 0.121928, loss_mps: 0.043336, loss_cps: 0.076206
[13:01:47.049] iteration 9888: total_loss: 0.116838, loss_sup: 0.012377, loss_mps: 0.039148, loss_cps: 0.065312
[13:01:47.194] iteration 9889: total_loss: 0.172210, loss_sup: 0.042718, loss_mps: 0.048783, loss_cps: 0.080708
[13:01:47.340] iteration 9890: total_loss: 0.175033, loss_sup: 0.064799, loss_mps: 0.039358, loss_cps: 0.070876
[13:01:47.486] iteration 9891: total_loss: 0.471878, loss_sup: 0.124401, loss_mps: 0.111460, loss_cps: 0.236017
[13:01:47.632] iteration 9892: total_loss: 0.276316, loss_sup: 0.120646, loss_mps: 0.056709, loss_cps: 0.098961
[13:01:47.779] iteration 9893: total_loss: 0.380280, loss_sup: 0.233933, loss_mps: 0.050920, loss_cps: 0.095427
[13:01:47.925] iteration 9894: total_loss: 0.214136, loss_sup: 0.045034, loss_mps: 0.058961, loss_cps: 0.110141
[13:01:48.071] iteration 9895: total_loss: 0.180280, loss_sup: 0.062168, loss_mps: 0.044943, loss_cps: 0.073169
[13:01:48.217] iteration 9896: total_loss: 0.197938, loss_sup: 0.075266, loss_mps: 0.045950, loss_cps: 0.076722
[13:01:48.365] iteration 9897: total_loss: 0.385819, loss_sup: 0.192003, loss_mps: 0.066695, loss_cps: 0.127121
[13:01:48.511] iteration 9898: total_loss: 0.403374, loss_sup: 0.208666, loss_mps: 0.067080, loss_cps: 0.127628
[13:01:48.656] iteration 9899: total_loss: 0.227229, loss_sup: 0.056338, loss_mps: 0.061346, loss_cps: 0.109545
[13:01:48.802] iteration 9900: total_loss: 0.196889, loss_sup: 0.042149, loss_mps: 0.055551, loss_cps: 0.099189
[13:01:48.802] Evaluation Started ==>
[13:02:00.165] ==> valid iteration 9900: unet metrics: {'dc': 0.6389285525210946, 'jc': 0.5157305826479449, 'pre': 0.7379634536767351, 'hd': 5.863294871193328}, ynet metrics: {'dc': 0.5925646294557352, 'jc': 0.4723453188792793, 'pre': 0.7697909774034326, 'hd': 5.736895298680276}.
[13:02:00.167] Evaluation Finished!⏹️
[13:02:00.321] iteration 9901: total_loss: 0.218920, loss_sup: 0.091768, loss_mps: 0.049344, loss_cps: 0.077808
[13:02:00.471] iteration 9902: total_loss: 0.221128, loss_sup: 0.031721, loss_mps: 0.066705, loss_cps: 0.122702
[13:02:00.617] iteration 9903: total_loss: 0.205347, loss_sup: 0.019708, loss_mps: 0.067906, loss_cps: 0.117733
[13:02:00.762] iteration 9904: total_loss: 0.168511, loss_sup: 0.043127, loss_mps: 0.046346, loss_cps: 0.079038
[13:02:00.907] iteration 9905: total_loss: 0.213756, loss_sup: 0.021694, loss_mps: 0.066726, loss_cps: 0.125336
[13:02:01.054] iteration 9906: total_loss: 0.261567, loss_sup: 0.027231, loss_mps: 0.078854, loss_cps: 0.155482
[13:02:01.204] iteration 9907: total_loss: 0.178672, loss_sup: 0.028434, loss_mps: 0.053884, loss_cps: 0.096355
[13:02:01.350] iteration 9908: total_loss: 0.375940, loss_sup: 0.145093, loss_mps: 0.075009, loss_cps: 0.155838
[13:02:01.498] iteration 9909: total_loss: 0.375800, loss_sup: 0.199551, loss_mps: 0.062235, loss_cps: 0.114014
[13:02:01.643] iteration 9910: total_loss: 0.135745, loss_sup: 0.012229, loss_mps: 0.044110, loss_cps: 0.079406
[13:02:01.789] iteration 9911: total_loss: 0.349956, loss_sup: 0.166439, loss_mps: 0.062415, loss_cps: 0.121102
[13:02:01.935] iteration 9912: total_loss: 0.163721, loss_sup: 0.026775, loss_mps: 0.050707, loss_cps: 0.086240
[13:02:02.082] iteration 9913: total_loss: 0.099628, loss_sup: 0.017476, loss_mps: 0.031394, loss_cps: 0.050758
[13:02:02.227] iteration 9914: total_loss: 0.242291, loss_sup: 0.068822, loss_mps: 0.059743, loss_cps: 0.113726
[13:02:02.372] iteration 9915: total_loss: 0.253303, loss_sup: 0.042008, loss_mps: 0.069667, loss_cps: 0.141628
[13:02:02.520] iteration 9916: total_loss: 0.188895, loss_sup: 0.039543, loss_mps: 0.053218, loss_cps: 0.096134
[13:02:02.665] iteration 9917: total_loss: 0.192335, loss_sup: 0.096184, loss_mps: 0.036176, loss_cps: 0.059975
[13:02:02.811] iteration 9918: total_loss: 0.164340, loss_sup: 0.048643, loss_mps: 0.041627, loss_cps: 0.074069
[13:02:02.956] iteration 9919: total_loss: 0.121790, loss_sup: 0.013416, loss_mps: 0.038919, loss_cps: 0.069456
[13:02:03.103] iteration 9920: total_loss: 0.336623, loss_sup: 0.125843, loss_mps: 0.070922, loss_cps: 0.139858
[13:02:03.250] iteration 9921: total_loss: 0.188373, loss_sup: 0.064532, loss_mps: 0.044053, loss_cps: 0.079788
[13:02:03.396] iteration 9922: total_loss: 0.349348, loss_sup: 0.192861, loss_mps: 0.054986, loss_cps: 0.101502
[13:02:03.541] iteration 9923: total_loss: 0.317174, loss_sup: 0.136065, loss_mps: 0.059883, loss_cps: 0.121225
[13:02:03.686] iteration 9924: total_loss: 0.265939, loss_sup: 0.069659, loss_mps: 0.064931, loss_cps: 0.131348
[13:02:03.832] iteration 9925: total_loss: 0.198432, loss_sup: 0.037171, loss_mps: 0.052831, loss_cps: 0.108430
[13:02:03.979] iteration 9926: total_loss: 0.295439, loss_sup: 0.171342, loss_mps: 0.045094, loss_cps: 0.079003
[13:02:04.126] iteration 9927: total_loss: 0.227429, loss_sup: 0.078849, loss_mps: 0.048625, loss_cps: 0.099956
[13:02:04.272] iteration 9928: total_loss: 0.146879, loss_sup: 0.030636, loss_mps: 0.041053, loss_cps: 0.075191
[13:02:04.419] iteration 9929: total_loss: 0.545782, loss_sup: 0.217829, loss_mps: 0.102843, loss_cps: 0.225110
[13:02:04.565] iteration 9930: total_loss: 0.119550, loss_sup: 0.006281, loss_mps: 0.039464, loss_cps: 0.073806
[13:02:04.712] iteration 9931: total_loss: 0.235901, loss_sup: 0.079531, loss_mps: 0.053595, loss_cps: 0.102775
[13:02:04.861] iteration 9932: total_loss: 0.271072, loss_sup: 0.145751, loss_mps: 0.045476, loss_cps: 0.079845
[13:02:05.007] iteration 9933: total_loss: 0.173373, loss_sup: 0.040325, loss_mps: 0.046972, loss_cps: 0.086077
[13:02:05.152] iteration 9934: total_loss: 0.181942, loss_sup: 0.016133, loss_mps: 0.054191, loss_cps: 0.111618
[13:02:05.298] iteration 9935: total_loss: 0.440645, loss_sup: 0.168917, loss_mps: 0.084523, loss_cps: 0.187205
[13:02:05.444] iteration 9936: total_loss: 0.182430, loss_sup: 0.038601, loss_mps: 0.050566, loss_cps: 0.093263
[13:02:05.590] iteration 9937: total_loss: 0.274008, loss_sup: 0.071431, loss_mps: 0.066653, loss_cps: 0.135924
[13:02:05.736] iteration 9938: total_loss: 0.215055, loss_sup: 0.041923, loss_mps: 0.058113, loss_cps: 0.115019
[13:02:05.882] iteration 9939: total_loss: 0.354448, loss_sup: 0.049664, loss_mps: 0.096618, loss_cps: 0.208166
[13:02:06.029] iteration 9940: total_loss: 0.293653, loss_sup: 0.067258, loss_mps: 0.073370, loss_cps: 0.153025
[13:02:06.176] iteration 9941: total_loss: 0.177601, loss_sup: 0.021809, loss_mps: 0.051353, loss_cps: 0.104439
[13:02:06.322] iteration 9942: total_loss: 0.309033, loss_sup: 0.145069, loss_mps: 0.054992, loss_cps: 0.108972
[13:02:06.468] iteration 9943: total_loss: 0.466622, loss_sup: 0.076325, loss_mps: 0.120981, loss_cps: 0.269316
[13:02:06.615] iteration 9944: total_loss: 0.282717, loss_sup: 0.085897, loss_mps: 0.067369, loss_cps: 0.129452
[13:02:06.761] iteration 9945: total_loss: 0.164153, loss_sup: 0.026645, loss_mps: 0.047663, loss_cps: 0.089845
[13:02:06.907] iteration 9946: total_loss: 0.256027, loss_sup: 0.078311, loss_mps: 0.058362, loss_cps: 0.119354
[13:02:07.054] iteration 9947: total_loss: 0.380605, loss_sup: 0.094720, loss_mps: 0.090201, loss_cps: 0.195684
[13:02:07.200] iteration 9948: total_loss: 0.408489, loss_sup: 0.101868, loss_mps: 0.099055, loss_cps: 0.207566
[13:02:07.345] iteration 9949: total_loss: 0.226142, loss_sup: 0.041233, loss_mps: 0.060563, loss_cps: 0.124346
[13:02:07.491] iteration 9950: total_loss: 0.324695, loss_sup: 0.152782, loss_mps: 0.058258, loss_cps: 0.113655
[13:02:07.637] iteration 9951: total_loss: 0.218261, loss_sup: 0.056816, loss_mps: 0.055458, loss_cps: 0.105987
[13:02:07.787] iteration 9952: total_loss: 0.210694, loss_sup: 0.066705, loss_mps: 0.050150, loss_cps: 0.093839
[13:02:07.934] iteration 9953: total_loss: 0.518087, loss_sup: 0.271139, loss_mps: 0.078940, loss_cps: 0.168008
[13:02:08.080] iteration 9954: total_loss: 0.285272, loss_sup: 0.121785, loss_mps: 0.055744, loss_cps: 0.107743
[13:02:08.226] iteration 9955: total_loss: 0.338697, loss_sup: 0.142911, loss_mps: 0.065597, loss_cps: 0.130188
[13:02:08.372] iteration 9956: total_loss: 0.320907, loss_sup: 0.057655, loss_mps: 0.085931, loss_cps: 0.177321
[13:02:08.518] iteration 9957: total_loss: 0.235976, loss_sup: 0.046275, loss_mps: 0.064214, loss_cps: 0.125487
[13:02:08.665] iteration 9958: total_loss: 0.229139, loss_sup: 0.065639, loss_mps: 0.057236, loss_cps: 0.106265
[13:02:08.811] iteration 9959: total_loss: 0.303475, loss_sup: 0.039194, loss_mps: 0.081267, loss_cps: 0.183014
[13:02:08.957] iteration 9960: total_loss: 0.189190, loss_sup: 0.067907, loss_mps: 0.045876, loss_cps: 0.075408
[13:02:09.103] iteration 9961: total_loss: 0.403424, loss_sup: 0.095622, loss_mps: 0.099090, loss_cps: 0.208712
[13:02:09.250] iteration 9962: total_loss: 0.312959, loss_sup: 0.044070, loss_mps: 0.087270, loss_cps: 0.181619
[13:02:09.396] iteration 9963: total_loss: 0.380499, loss_sup: 0.046494, loss_mps: 0.104956, loss_cps: 0.229049
[13:02:09.542] iteration 9964: total_loss: 0.284483, loss_sup: 0.059974, loss_mps: 0.077157, loss_cps: 0.147352
[13:02:09.688] iteration 9965: total_loss: 0.363684, loss_sup: 0.099850, loss_mps: 0.086320, loss_cps: 0.177515
[13:02:09.835] iteration 9966: total_loss: 0.243126, loss_sup: 0.033618, loss_mps: 0.070939, loss_cps: 0.138570
[13:02:09.981] iteration 9967: total_loss: 0.286533, loss_sup: 0.131797, loss_mps: 0.056189, loss_cps: 0.098546
[13:02:10.126] iteration 9968: total_loss: 0.242804, loss_sup: 0.062091, loss_mps: 0.065027, loss_cps: 0.115685
[13:02:10.272] iteration 9969: total_loss: 0.310110, loss_sup: 0.040935, loss_mps: 0.087189, loss_cps: 0.181986
[13:02:10.419] iteration 9970: total_loss: 0.294938, loss_sup: 0.038253, loss_mps: 0.087427, loss_cps: 0.169258
[13:02:10.565] iteration 9971: total_loss: 0.245644, loss_sup: 0.039646, loss_mps: 0.070828, loss_cps: 0.135171
[13:02:10.712] iteration 9972: total_loss: 0.193491, loss_sup: 0.042979, loss_mps: 0.054308, loss_cps: 0.096204
[13:02:10.859] iteration 9973: total_loss: 0.202582, loss_sup: 0.056857, loss_mps: 0.055528, loss_cps: 0.090197
[13:02:11.005] iteration 9974: total_loss: 0.208289, loss_sup: 0.071458, loss_mps: 0.051267, loss_cps: 0.085563
[13:02:11.151] iteration 9975: total_loss: 0.213813, loss_sup: 0.045813, loss_mps: 0.059565, loss_cps: 0.108436
[13:02:11.297] iteration 9976: total_loss: 0.288907, loss_sup: 0.135004, loss_mps: 0.056329, loss_cps: 0.097573
[13:02:11.443] iteration 9977: total_loss: 0.336052, loss_sup: 0.054678, loss_mps: 0.091185, loss_cps: 0.190189
[13:02:11.591] iteration 9978: total_loss: 0.198692, loss_sup: 0.008967, loss_mps: 0.063731, loss_cps: 0.125994
[13:02:11.737] iteration 9979: total_loss: 0.193586, loss_sup: 0.030992, loss_mps: 0.054115, loss_cps: 0.108479
[13:02:11.883] iteration 9980: total_loss: 0.417401, loss_sup: 0.186336, loss_mps: 0.075437, loss_cps: 0.155629
[13:02:12.028] iteration 9981: total_loss: 0.273114, loss_sup: 0.079681, loss_mps: 0.065601, loss_cps: 0.127833
[13:02:12.176] iteration 9982: total_loss: 0.171030, loss_sup: 0.049104, loss_mps: 0.045772, loss_cps: 0.076154
[13:02:12.322] iteration 9983: total_loss: 0.337312, loss_sup: 0.133509, loss_mps: 0.066288, loss_cps: 0.137515
[13:02:12.468] iteration 9984: total_loss: 0.304962, loss_sup: 0.049663, loss_mps: 0.083864, loss_cps: 0.171435
[13:02:12.613] iteration 9985: total_loss: 0.347090, loss_sup: 0.079688, loss_mps: 0.084541, loss_cps: 0.182861
[13:02:12.759] iteration 9986: total_loss: 0.540891, loss_sup: 0.138583, loss_mps: 0.122592, loss_cps: 0.279715
[13:02:12.905] iteration 9987: total_loss: 0.370187, loss_sup: 0.137416, loss_mps: 0.078309, loss_cps: 0.154462
[13:02:13.050] iteration 9988: total_loss: 0.392412, loss_sup: 0.137382, loss_mps: 0.083748, loss_cps: 0.171282
[13:02:13.197] iteration 9989: total_loss: 0.178671, loss_sup: 0.053252, loss_mps: 0.045049, loss_cps: 0.080371
[13:02:13.343] iteration 9990: total_loss: 0.197779, loss_sup: 0.052368, loss_mps: 0.052696, loss_cps: 0.092714
[13:02:13.489] iteration 9991: total_loss: 0.221283, loss_sup: 0.037432, loss_mps: 0.064005, loss_cps: 0.119847
[13:02:13.635] iteration 9992: total_loss: 0.385504, loss_sup: 0.122422, loss_mps: 0.087923, loss_cps: 0.175159
[13:02:13.781] iteration 9993: total_loss: 0.284710, loss_sup: 0.073397, loss_mps: 0.070990, loss_cps: 0.140323
[13:02:13.927] iteration 9994: total_loss: 0.224268, loss_sup: 0.046953, loss_mps: 0.058693, loss_cps: 0.118622
[13:02:14.073] iteration 9995: total_loss: 0.211081, loss_sup: 0.068791, loss_mps: 0.050458, loss_cps: 0.091832
[13:02:14.219] iteration 9996: total_loss: 0.226265, loss_sup: 0.031209, loss_mps: 0.065802, loss_cps: 0.129254
[13:02:14.365] iteration 9997: total_loss: 0.262079, loss_sup: 0.014966, loss_mps: 0.080655, loss_cps: 0.166458
[13:02:14.511] iteration 9998: total_loss: 0.295959, loss_sup: 0.119388, loss_mps: 0.060752, loss_cps: 0.115819
[13:02:14.657] iteration 9999: total_loss: 0.183326, loss_sup: 0.040848, loss_mps: 0.048566, loss_cps: 0.093912
[13:02:14.804] iteration 10000: total_loss: 0.270917, loss_sup: 0.154742, loss_mps: 0.045558, loss_cps: 0.070617
[13:02:14.804] Evaluation Started ==>
[13:02:26.154] ==> valid iteration 10000: unet metrics: {'dc': 0.6422576262537074, 'jc': 0.5147635255053478, 'pre': 0.7253813011238558, 'hd': 5.929885965511944}, ynet metrics: {'dc': 0.6055017561534376, 'jc': 0.48352923640826656, 'pre': 0.7639641475685216, 'hd': 5.8467248087181005}.
[13:02:26.156] Evaluation Finished!⏹️
[13:02:26.310] iteration 10001: total_loss: 0.275996, loss_sup: 0.063844, loss_mps: 0.072680, loss_cps: 0.139472
[13:02:26.462] iteration 10002: total_loss: 0.233509, loss_sup: 0.040652, loss_mps: 0.063750, loss_cps: 0.129107
[13:02:26.607] iteration 10003: total_loss: 0.275716, loss_sup: 0.054683, loss_mps: 0.070080, loss_cps: 0.150953
[13:02:26.755] iteration 10004: total_loss: 0.229072, loss_sup: 0.061520, loss_mps: 0.059198, loss_cps: 0.108354
[13:02:26.901] iteration 10005: total_loss: 0.346884, loss_sup: 0.177477, loss_mps: 0.056988, loss_cps: 0.112419
[13:02:27.046] iteration 10006: total_loss: 0.369113, loss_sup: 0.237571, loss_mps: 0.048361, loss_cps: 0.083181
[13:02:27.191] iteration 10007: total_loss: 0.324384, loss_sup: 0.105066, loss_mps: 0.075645, loss_cps: 0.143673
[13:02:27.336] iteration 10008: total_loss: 0.252865, loss_sup: 0.142432, loss_mps: 0.040367, loss_cps: 0.070065
[13:02:27.482] iteration 10009: total_loss: 0.248157, loss_sup: 0.081729, loss_mps: 0.058750, loss_cps: 0.107678
[13:02:27.628] iteration 10010: total_loss: 0.334441, loss_sup: 0.109185, loss_mps: 0.075702, loss_cps: 0.149553
[13:02:27.774] iteration 10011: total_loss: 0.158784, loss_sup: 0.019625, loss_mps: 0.050142, loss_cps: 0.089017
[13:02:27.919] iteration 10012: total_loss: 0.278029, loss_sup: 0.046285, loss_mps: 0.077133, loss_cps: 0.154611
[13:02:28.065] iteration 10013: total_loss: 0.200066, loss_sup: 0.073047, loss_mps: 0.048139, loss_cps: 0.078880
[13:02:28.210] iteration 10014: total_loss: 0.549243, loss_sup: 0.328595, loss_mps: 0.073590, loss_cps: 0.147058
[13:02:28.355] iteration 10015: total_loss: 0.329543, loss_sup: 0.065975, loss_mps: 0.088279, loss_cps: 0.175290
[13:02:28.503] iteration 10016: total_loss: 0.278029, loss_sup: 0.056836, loss_mps: 0.078885, loss_cps: 0.142309
[13:02:28.649] iteration 10017: total_loss: 0.436454, loss_sup: 0.123811, loss_mps: 0.098386, loss_cps: 0.214258
[13:02:28.794] iteration 10018: total_loss: 0.327827, loss_sup: 0.090510, loss_mps: 0.078659, loss_cps: 0.158658
[13:02:28.941] iteration 10019: total_loss: 0.140575, loss_sup: 0.026592, loss_mps: 0.040732, loss_cps: 0.073251
[13:02:29.087] iteration 10020: total_loss: 0.376051, loss_sup: 0.095032, loss_mps: 0.089171, loss_cps: 0.191849
[13:02:29.232] iteration 10021: total_loss: 0.508703, loss_sup: 0.051741, loss_mps: 0.146080, loss_cps: 0.310882
[13:02:29.377] iteration 10022: total_loss: 0.346566, loss_sup: 0.127498, loss_mps: 0.072306, loss_cps: 0.146762
[13:02:29.523] iteration 10023: total_loss: 0.459582, loss_sup: 0.193854, loss_mps: 0.087514, loss_cps: 0.178214
[13:02:29.668] iteration 10024: total_loss: 0.236927, loss_sup: 0.059934, loss_mps: 0.060334, loss_cps: 0.116660
[13:02:29.814] iteration 10025: total_loss: 0.319400, loss_sup: 0.079521, loss_mps: 0.080994, loss_cps: 0.158885
[13:02:29.959] iteration 10026: total_loss: 0.224721, loss_sup: 0.016876, loss_mps: 0.069747, loss_cps: 0.138097
[13:02:30.104] iteration 10027: total_loss: 0.292303, loss_sup: 0.054927, loss_mps: 0.081244, loss_cps: 0.156132
[13:02:30.249] iteration 10028: total_loss: 0.376544, loss_sup: 0.162387, loss_mps: 0.072939, loss_cps: 0.141218
[13:02:30.394] iteration 10029: total_loss: 0.296179, loss_sup: 0.120076, loss_mps: 0.060715, loss_cps: 0.115388
[13:02:30.540] iteration 10030: total_loss: 0.442628, loss_sup: 0.250396, loss_mps: 0.065606, loss_cps: 0.126625
[13:02:30.686] iteration 10031: total_loss: 0.213955, loss_sup: 0.030309, loss_mps: 0.061001, loss_cps: 0.122645
[13:02:30.751] iteration 10032: total_loss: 0.411823, loss_sup: 0.172038, loss_mps: 0.083156, loss_cps: 0.156628
[13:02:31.922] iteration 10033: total_loss: 0.149404, loss_sup: 0.028845, loss_mps: 0.044540, loss_cps: 0.076019
[13:02:32.072] iteration 10034: total_loss: 0.381060, loss_sup: 0.134048, loss_mps: 0.084957, loss_cps: 0.162056
[13:02:32.219] iteration 10035: total_loss: 0.203281, loss_sup: 0.017292, loss_mps: 0.064757, loss_cps: 0.121232
[13:02:32.366] iteration 10036: total_loss: 0.244097, loss_sup: 0.075681, loss_mps: 0.059994, loss_cps: 0.108422
[13:02:32.515] iteration 10037: total_loss: 0.158389, loss_sup: 0.031535, loss_mps: 0.046177, loss_cps: 0.080677
[13:02:32.661] iteration 10038: total_loss: 0.332252, loss_sup: 0.112756, loss_mps: 0.073311, loss_cps: 0.146186
[13:02:32.809] iteration 10039: total_loss: 0.275863, loss_sup: 0.060598, loss_mps: 0.074166, loss_cps: 0.141099
[13:02:32.955] iteration 10040: total_loss: 0.194774, loss_sup: 0.037399, loss_mps: 0.054491, loss_cps: 0.102884
[13:02:33.103] iteration 10041: total_loss: 0.373386, loss_sup: 0.096799, loss_mps: 0.090626, loss_cps: 0.185962
[13:02:33.250] iteration 10042: total_loss: 0.175209, loss_sup: 0.037680, loss_mps: 0.049810, loss_cps: 0.087720
[13:02:33.396] iteration 10043: total_loss: 0.196573, loss_sup: 0.037665, loss_mps: 0.055582, loss_cps: 0.103326
[13:02:33.542] iteration 10044: total_loss: 0.361470, loss_sup: 0.036018, loss_mps: 0.103334, loss_cps: 0.222118
[13:02:33.688] iteration 10045: total_loss: 0.240281, loss_sup: 0.102096, loss_mps: 0.048410, loss_cps: 0.089775
[13:02:33.836] iteration 10046: total_loss: 0.311189, loss_sup: 0.053545, loss_mps: 0.087261, loss_cps: 0.170383
[13:02:33.982] iteration 10047: total_loss: 0.323422, loss_sup: 0.067680, loss_mps: 0.085466, loss_cps: 0.170277
[13:02:34.128] iteration 10048: total_loss: 0.496711, loss_sup: 0.296812, loss_mps: 0.065448, loss_cps: 0.134451
[13:02:34.274] iteration 10049: total_loss: 0.226409, loss_sup: 0.061908, loss_mps: 0.059446, loss_cps: 0.105055
[13:02:34.420] iteration 10050: total_loss: 0.257774, loss_sup: 0.088451, loss_mps: 0.060793, loss_cps: 0.108531
[13:02:34.566] iteration 10051: total_loss: 0.186211, loss_sup: 0.010171, loss_mps: 0.062969, loss_cps: 0.113071
[13:02:34.713] iteration 10052: total_loss: 0.159966, loss_sup: 0.028749, loss_mps: 0.045610, loss_cps: 0.085607
[13:02:34.860] iteration 10053: total_loss: 0.271081, loss_sup: 0.115674, loss_mps: 0.058506, loss_cps: 0.096901
[13:02:35.006] iteration 10054: total_loss: 0.425994, loss_sup: 0.183929, loss_mps: 0.081208, loss_cps: 0.160856
[13:02:35.154] iteration 10055: total_loss: 0.338907, loss_sup: 0.116916, loss_mps: 0.076233, loss_cps: 0.145758
[13:02:35.301] iteration 10056: total_loss: 0.306293, loss_sup: 0.106570, loss_mps: 0.068097, loss_cps: 0.131626
[13:02:35.447] iteration 10057: total_loss: 0.218585, loss_sup: 0.046864, loss_mps: 0.058415, loss_cps: 0.113306
[13:02:35.594] iteration 10058: total_loss: 0.180435, loss_sup: 0.070014, loss_mps: 0.042469, loss_cps: 0.067951
[13:02:35.742] iteration 10059: total_loss: 0.268927, loss_sup: 0.104910, loss_mps: 0.060695, loss_cps: 0.103322
[13:02:35.889] iteration 10060: total_loss: 0.156898, loss_sup: 0.012283, loss_mps: 0.052837, loss_cps: 0.091778
[13:02:36.039] iteration 10061: total_loss: 0.211794, loss_sup: 0.026580, loss_mps: 0.063485, loss_cps: 0.121729
[13:02:36.189] iteration 10062: total_loss: 0.394965, loss_sup: 0.081068, loss_mps: 0.101815, loss_cps: 0.212083
[13:02:36.337] iteration 10063: total_loss: 0.191164, loss_sup: 0.055994, loss_mps: 0.048863, loss_cps: 0.086308
[13:02:36.484] iteration 10064: total_loss: 0.237667, loss_sup: 0.026726, loss_mps: 0.072644, loss_cps: 0.138297
[13:02:36.631] iteration 10065: total_loss: 0.228889, loss_sup: 0.070779, loss_mps: 0.054264, loss_cps: 0.103847
[13:02:36.777] iteration 10066: total_loss: 0.290881, loss_sup: 0.089529, loss_mps: 0.066312, loss_cps: 0.135040
[13:02:36.923] iteration 10067: total_loss: 0.221898, loss_sup: 0.072079, loss_mps: 0.053948, loss_cps: 0.095871
[13:02:37.069] iteration 10068: total_loss: 0.390788, loss_sup: 0.158263, loss_mps: 0.076669, loss_cps: 0.155856
[13:02:37.218] iteration 10069: total_loss: 0.180896, loss_sup: 0.012566, loss_mps: 0.059070, loss_cps: 0.109261
[13:02:37.364] iteration 10070: total_loss: 0.171346, loss_sup: 0.047278, loss_mps: 0.045557, loss_cps: 0.078511
[13:02:37.510] iteration 10071: total_loss: 0.273094, loss_sup: 0.081538, loss_mps: 0.064634, loss_cps: 0.126922
[13:02:37.660] iteration 10072: total_loss: 0.358747, loss_sup: 0.209918, loss_mps: 0.051220, loss_cps: 0.097609
[13:02:37.806] iteration 10073: total_loss: 0.415654, loss_sup: 0.095527, loss_mps: 0.101188, loss_cps: 0.218939
[13:02:37.957] iteration 10074: total_loss: 0.208004, loss_sup: 0.032712, loss_mps: 0.059766, loss_cps: 0.115525
[13:02:38.108] iteration 10075: total_loss: 0.348312, loss_sup: 0.098335, loss_mps: 0.085070, loss_cps: 0.164907
[13:02:38.254] iteration 10076: total_loss: 0.318876, loss_sup: 0.065241, loss_mps: 0.081628, loss_cps: 0.172007
[13:02:38.400] iteration 10077: total_loss: 0.609976, loss_sup: 0.438299, loss_mps: 0.061739, loss_cps: 0.109938
[13:02:38.547] iteration 10078: total_loss: 0.378850, loss_sup: 0.148831, loss_mps: 0.076827, loss_cps: 0.153192
[13:02:38.694] iteration 10079: total_loss: 0.289765, loss_sup: 0.018973, loss_mps: 0.092035, loss_cps: 0.178758
[13:02:38.840] iteration 10080: total_loss: 0.315560, loss_sup: 0.036691, loss_mps: 0.090648, loss_cps: 0.188222
[13:02:38.987] iteration 10081: total_loss: 0.364280, loss_sup: 0.180551, loss_mps: 0.066975, loss_cps: 0.116754
[13:02:39.134] iteration 10082: total_loss: 0.252534, loss_sup: 0.021544, loss_mps: 0.078241, loss_cps: 0.152749
[13:02:39.280] iteration 10083: total_loss: 0.238366, loss_sup: 0.065052, loss_mps: 0.061169, loss_cps: 0.112145
[13:02:39.426] iteration 10084: total_loss: 0.200942, loss_sup: 0.039730, loss_mps: 0.059946, loss_cps: 0.101266
[13:02:39.573] iteration 10085: total_loss: 0.361874, loss_sup: 0.091530, loss_mps: 0.088289, loss_cps: 0.182054
[13:02:39.719] iteration 10086: total_loss: 0.416058, loss_sup: 0.169127, loss_mps: 0.080419, loss_cps: 0.166512
[13:02:39.865] iteration 10087: total_loss: 0.569773, loss_sup: 0.220219, loss_mps: 0.114952, loss_cps: 0.234603
[13:02:40.012] iteration 10088: total_loss: 0.311411, loss_sup: 0.105308, loss_mps: 0.069981, loss_cps: 0.136122
[13:02:40.158] iteration 10089: total_loss: 0.386989, loss_sup: 0.083348, loss_mps: 0.099361, loss_cps: 0.204280
[13:02:40.305] iteration 10090: total_loss: 0.203930, loss_sup: 0.052159, loss_mps: 0.053966, loss_cps: 0.097805
[13:02:40.452] iteration 10091: total_loss: 0.184342, loss_sup: 0.047672, loss_mps: 0.050255, loss_cps: 0.086415
[13:02:40.599] iteration 10092: total_loss: 0.356107, loss_sup: 0.092883, loss_mps: 0.088933, loss_cps: 0.174291
[13:02:40.746] iteration 10093: total_loss: 0.325723, loss_sup: 0.059395, loss_mps: 0.086392, loss_cps: 0.179936
[13:02:40.893] iteration 10094: total_loss: 0.153459, loss_sup: 0.012511, loss_mps: 0.053214, loss_cps: 0.087735
[13:02:41.039] iteration 10095: total_loss: 0.175499, loss_sup: 0.016153, loss_mps: 0.056197, loss_cps: 0.103148
[13:02:41.188] iteration 10096: total_loss: 0.282588, loss_sup: 0.054084, loss_mps: 0.079944, loss_cps: 0.148560
[13:02:41.334] iteration 10097: total_loss: 0.259042, loss_sup: 0.059770, loss_mps: 0.070187, loss_cps: 0.129085
[13:02:41.481] iteration 10098: total_loss: 0.311315, loss_sup: 0.115730, loss_mps: 0.068543, loss_cps: 0.127042
[13:02:41.627] iteration 10099: total_loss: 0.149158, loss_sup: 0.022970, loss_mps: 0.045896, loss_cps: 0.080292
[13:02:41.773] iteration 10100: total_loss: 0.201189, loss_sup: 0.045052, loss_mps: 0.053958, loss_cps: 0.102180
[13:02:41.773] Evaluation Started ==>
[13:02:53.125] ==> valid iteration 10100: unet metrics: {'dc': 0.6362102337253455, 'jc': 0.509210138428238, 'pre': 0.754264965163824, 'hd': 5.892537793317711}, ynet metrics: {'dc': 0.5863865705783378, 'jc': 0.46493920551799345, 'pre': 0.7494643846221988, 'hd': 5.900278850979524}.
[13:02:53.126] Evaluation Finished!⏹️
[13:02:53.279] iteration 10101: total_loss: 0.205391, loss_sup: 0.030562, loss_mps: 0.060480, loss_cps: 0.114349
[13:02:53.426] iteration 10102: total_loss: 0.131497, loss_sup: 0.006507, loss_mps: 0.045694, loss_cps: 0.079297
[13:02:53.571] iteration 10103: total_loss: 0.324092, loss_sup: 0.108696, loss_mps: 0.074613, loss_cps: 0.140783
[13:02:53.716] iteration 10104: total_loss: 0.165559, loss_sup: 0.057367, loss_mps: 0.039069, loss_cps: 0.069123
[13:02:53.862] iteration 10105: total_loss: 0.225476, loss_sup: 0.101817, loss_mps: 0.045292, loss_cps: 0.078367
[13:02:54.007] iteration 10106: total_loss: 0.358446, loss_sup: 0.205457, loss_mps: 0.055207, loss_cps: 0.097782
[13:02:54.152] iteration 10107: total_loss: 0.270717, loss_sup: 0.139158, loss_mps: 0.048556, loss_cps: 0.083003
[13:02:54.298] iteration 10108: total_loss: 0.244481, loss_sup: 0.069336, loss_mps: 0.061431, loss_cps: 0.113714
[13:02:54.443] iteration 10109: total_loss: 0.318081, loss_sup: 0.062459, loss_mps: 0.084311, loss_cps: 0.171310
[13:02:54.590] iteration 10110: total_loss: 0.351332, loss_sup: 0.093112, loss_mps: 0.083226, loss_cps: 0.174994
[13:02:54.736] iteration 10111: total_loss: 0.126906, loss_sup: 0.013752, loss_mps: 0.041696, loss_cps: 0.071458
[13:02:54.882] iteration 10112: total_loss: 0.159779, loss_sup: 0.067562, loss_mps: 0.035283, loss_cps: 0.056933
[13:02:55.027] iteration 10113: total_loss: 0.282926, loss_sup: 0.108813, loss_mps: 0.058564, loss_cps: 0.115549
[13:02:55.174] iteration 10114: total_loss: 0.237216, loss_sup: 0.038684, loss_mps: 0.067476, loss_cps: 0.131056
[13:02:55.320] iteration 10115: total_loss: 0.232134, loss_sup: 0.011916, loss_mps: 0.071807, loss_cps: 0.148411
[13:02:55.465] iteration 10116: total_loss: 0.254410, loss_sup: 0.041656, loss_mps: 0.070064, loss_cps: 0.142689
[13:02:55.611] iteration 10117: total_loss: 0.171951, loss_sup: 0.046760, loss_mps: 0.044734, loss_cps: 0.080457
[13:02:55.756] iteration 10118: total_loss: 0.339775, loss_sup: 0.133046, loss_mps: 0.068523, loss_cps: 0.138207
[13:02:55.903] iteration 10119: total_loss: 0.208432, loss_sup: 0.031172, loss_mps: 0.062738, loss_cps: 0.114523
[13:02:56.049] iteration 10120: total_loss: 0.218892, loss_sup: 0.023200, loss_mps: 0.066964, loss_cps: 0.128727
[13:02:56.194] iteration 10121: total_loss: 0.233383, loss_sup: 0.073470, loss_mps: 0.053096, loss_cps: 0.106816
[13:02:56.340] iteration 10122: total_loss: 0.180413, loss_sup: 0.059892, loss_mps: 0.041596, loss_cps: 0.078925
[13:02:56.486] iteration 10123: total_loss: 0.115660, loss_sup: 0.009615, loss_mps: 0.038831, loss_cps: 0.067215
[13:02:56.632] iteration 10124: total_loss: 0.266916, loss_sup: 0.075397, loss_mps: 0.065630, loss_cps: 0.125889
[13:02:56.780] iteration 10125: total_loss: 0.245461, loss_sup: 0.105545, loss_mps: 0.049527, loss_cps: 0.090389
[13:02:56.927] iteration 10126: total_loss: 0.246018, loss_sup: 0.114514, loss_mps: 0.048692, loss_cps: 0.082812
[13:02:57.073] iteration 10127: total_loss: 0.194469, loss_sup: 0.010641, loss_mps: 0.058143, loss_cps: 0.125685
[13:02:57.218] iteration 10128: total_loss: 0.132693, loss_sup: 0.021272, loss_mps: 0.038280, loss_cps: 0.073140
[13:02:57.364] iteration 10129: total_loss: 0.332660, loss_sup: 0.110925, loss_mps: 0.072504, loss_cps: 0.149231
[13:02:57.510] iteration 10130: total_loss: 0.199450, loss_sup: 0.054946, loss_mps: 0.051465, loss_cps: 0.093039
[13:02:57.656] iteration 10131: total_loss: 0.293695, loss_sup: 0.030347, loss_mps: 0.085097, loss_cps: 0.178251
[13:02:57.803] iteration 10132: total_loss: 0.144965, loss_sup: 0.023118, loss_mps: 0.045554, loss_cps: 0.076292
[13:02:57.949] iteration 10133: total_loss: 0.192052, loss_sup: 0.076578, loss_mps: 0.040505, loss_cps: 0.074968
[13:02:58.095] iteration 10134: total_loss: 0.343297, loss_sup: 0.135407, loss_mps: 0.069922, loss_cps: 0.137968
[13:02:58.243] iteration 10135: total_loss: 0.640968, loss_sup: 0.397100, loss_mps: 0.078569, loss_cps: 0.165299
[13:02:58.390] iteration 10136: total_loss: 0.278988, loss_sup: 0.057465, loss_mps: 0.074386, loss_cps: 0.147137
[13:02:58.538] iteration 10137: total_loss: 0.200762, loss_sup: 0.022289, loss_mps: 0.060439, loss_cps: 0.118033
[13:02:58.687] iteration 10138: total_loss: 0.284622, loss_sup: 0.055851, loss_mps: 0.075641, loss_cps: 0.153130
[13:02:58.833] iteration 10139: total_loss: 0.192263, loss_sup: 0.043094, loss_mps: 0.050724, loss_cps: 0.098445
[13:02:58.982] iteration 10140: total_loss: 0.276239, loss_sup: 0.067303, loss_mps: 0.067293, loss_cps: 0.141643
[13:02:59.128] iteration 10141: total_loss: 0.155369, loss_sup: 0.028936, loss_mps: 0.043903, loss_cps: 0.082530
[13:02:59.273] iteration 10142: total_loss: 0.297640, loss_sup: 0.068414, loss_mps: 0.074595, loss_cps: 0.154631
[13:02:59.419] iteration 10143: total_loss: 0.295419, loss_sup: 0.069841, loss_mps: 0.074609, loss_cps: 0.150968
[13:02:59.564] iteration 10144: total_loss: 0.293020, loss_sup: 0.072946, loss_mps: 0.071395, loss_cps: 0.148679
[13:02:59.710] iteration 10145: total_loss: 0.188154, loss_sup: 0.035031, loss_mps: 0.051299, loss_cps: 0.101824
[13:02:59.855] iteration 10146: total_loss: 0.279137, loss_sup: 0.030369, loss_mps: 0.078881, loss_cps: 0.169886
[13:03:00.001] iteration 10147: total_loss: 0.213383, loss_sup: 0.076550, loss_mps: 0.048378, loss_cps: 0.088455
[13:03:00.148] iteration 10148: total_loss: 0.295442, loss_sup: 0.054869, loss_mps: 0.077568, loss_cps: 0.163004
[13:03:00.295] iteration 10149: total_loss: 0.455263, loss_sup: 0.061381, loss_mps: 0.124072, loss_cps: 0.269811
[13:03:00.440] iteration 10150: total_loss: 0.254104, loss_sup: 0.087708, loss_mps: 0.057157, loss_cps: 0.109238
[13:03:00.586] iteration 10151: total_loss: 0.246473, loss_sup: 0.060389, loss_mps: 0.064446, loss_cps: 0.121638
[13:03:00.732] iteration 10152: total_loss: 0.277100, loss_sup: 0.016404, loss_mps: 0.083775, loss_cps: 0.176921
[13:03:00.886] iteration 10153: total_loss: 0.389031, loss_sup: 0.156101, loss_mps: 0.076215, loss_cps: 0.156715
[13:03:01.032] iteration 10154: total_loss: 0.232842, loss_sup: 0.042530, loss_mps: 0.062184, loss_cps: 0.128128
[13:03:01.179] iteration 10155: total_loss: 0.344829, loss_sup: 0.162998, loss_mps: 0.061550, loss_cps: 0.120281
[13:03:01.326] iteration 10156: total_loss: 0.218615, loss_sup: 0.098839, loss_mps: 0.043602, loss_cps: 0.076174
[13:03:01.474] iteration 10157: total_loss: 0.194207, loss_sup: 0.075226, loss_mps: 0.041877, loss_cps: 0.077104
[13:03:01.621] iteration 10158: total_loss: 0.260641, loss_sup: 0.130035, loss_mps: 0.047888, loss_cps: 0.082717
[13:03:01.767] iteration 10159: total_loss: 0.346136, loss_sup: 0.081650, loss_mps: 0.083675, loss_cps: 0.180811
[13:03:01.913] iteration 10160: total_loss: 0.333986, loss_sup: 0.085451, loss_mps: 0.085241, loss_cps: 0.163294
[13:03:02.061] iteration 10161: total_loss: 0.370667, loss_sup: 0.153509, loss_mps: 0.073162, loss_cps: 0.143996
[13:03:02.207] iteration 10162: total_loss: 0.262360, loss_sup: 0.062605, loss_mps: 0.070399, loss_cps: 0.129356
[13:03:02.353] iteration 10163: total_loss: 0.386306, loss_sup: 0.105287, loss_mps: 0.093904, loss_cps: 0.187115
[13:03:02.499] iteration 10164: total_loss: 0.477349, loss_sup: 0.114225, loss_mps: 0.115605, loss_cps: 0.247519
[13:03:02.645] iteration 10165: total_loss: 0.155025, loss_sup: 0.036727, loss_mps: 0.045095, loss_cps: 0.073203
[13:03:02.791] iteration 10166: total_loss: 0.221784, loss_sup: 0.039157, loss_mps: 0.064884, loss_cps: 0.117743
[13:03:02.938] iteration 10167: total_loss: 0.439963, loss_sup: 0.139168, loss_mps: 0.096067, loss_cps: 0.204729
[13:03:03.085] iteration 10168: total_loss: 0.247061, loss_sup: 0.026735, loss_mps: 0.078876, loss_cps: 0.141450
[13:03:03.231] iteration 10169: total_loss: 0.170313, loss_sup: 0.035965, loss_mps: 0.047568, loss_cps: 0.086779
[13:03:03.379] iteration 10170: total_loss: 0.238086, loss_sup: 0.039677, loss_mps: 0.069145, loss_cps: 0.129263
[13:03:03.525] iteration 10171: total_loss: 0.306795, loss_sup: 0.081734, loss_mps: 0.076702, loss_cps: 0.148359
[13:03:03.671] iteration 10172: total_loss: 0.424696, loss_sup: 0.109267, loss_mps: 0.103228, loss_cps: 0.212201
[13:03:03.817] iteration 10173: total_loss: 0.154308, loss_sup: 0.016956, loss_mps: 0.049026, loss_cps: 0.088327
[13:03:03.965] iteration 10174: total_loss: 0.268545, loss_sup: 0.129986, loss_mps: 0.051858, loss_cps: 0.086701
[13:03:04.112] iteration 10175: total_loss: 0.235325, loss_sup: 0.116075, loss_mps: 0.044554, loss_cps: 0.074696
[13:03:04.259] iteration 10176: total_loss: 0.332159, loss_sup: 0.040707, loss_mps: 0.094259, loss_cps: 0.197193
[13:03:04.406] iteration 10177: total_loss: 0.166291, loss_sup: 0.032340, loss_mps: 0.046427, loss_cps: 0.087524
[13:03:04.554] iteration 10178: total_loss: 0.340554, loss_sup: 0.109477, loss_mps: 0.074694, loss_cps: 0.156382
[13:03:04.700] iteration 10179: total_loss: 0.230058, loss_sup: 0.026130, loss_mps: 0.069179, loss_cps: 0.134749
[13:03:04.846] iteration 10180: total_loss: 0.398894, loss_sup: 0.098099, loss_mps: 0.095224, loss_cps: 0.205571
[13:03:04.993] iteration 10181: total_loss: 0.225256, loss_sup: 0.038376, loss_mps: 0.063950, loss_cps: 0.122929
[13:03:05.139] iteration 10182: total_loss: 0.447759, loss_sup: 0.240025, loss_mps: 0.070656, loss_cps: 0.137078
[13:03:05.286] iteration 10183: total_loss: 0.479239, loss_sup: 0.233676, loss_mps: 0.086391, loss_cps: 0.159172
[13:03:05.432] iteration 10184: total_loss: 0.156734, loss_sup: 0.029499, loss_mps: 0.046950, loss_cps: 0.080284
[13:03:05.577] iteration 10185: total_loss: 0.328363, loss_sup: 0.132516, loss_mps: 0.069312, loss_cps: 0.126536
[13:03:05.724] iteration 10186: total_loss: 0.420598, loss_sup: 0.125163, loss_mps: 0.097727, loss_cps: 0.197708
[13:03:05.870] iteration 10187: total_loss: 0.358887, loss_sup: 0.108827, loss_mps: 0.083728, loss_cps: 0.166332
[13:03:06.018] iteration 10188: total_loss: 0.233060, loss_sup: 0.060575, loss_mps: 0.061935, loss_cps: 0.110550
[13:03:06.163] iteration 10189: total_loss: 0.206275, loss_sup: 0.037800, loss_mps: 0.060437, loss_cps: 0.108037
[13:03:06.309] iteration 10190: total_loss: 0.393294, loss_sup: 0.125531, loss_mps: 0.089339, loss_cps: 0.178425
[13:03:06.455] iteration 10191: total_loss: 0.566938, loss_sup: 0.265447, loss_mps: 0.100282, loss_cps: 0.201209
[13:03:06.602] iteration 10192: total_loss: 0.212547, loss_sup: 0.024077, loss_mps: 0.064133, loss_cps: 0.124337
[13:03:06.749] iteration 10193: total_loss: 0.432569, loss_sup: 0.169214, loss_mps: 0.090219, loss_cps: 0.173136
[13:03:06.900] iteration 10194: total_loss: 0.251324, loss_sup: 0.104072, loss_mps: 0.055241, loss_cps: 0.092011
[13:03:07.048] iteration 10195: total_loss: 0.253481, loss_sup: 0.113084, loss_mps: 0.053641, loss_cps: 0.086756
[13:03:07.194] iteration 10196: total_loss: 0.253873, loss_sup: 0.068204, loss_mps: 0.067409, loss_cps: 0.118260
[13:03:07.341] iteration 10197: total_loss: 0.262347, loss_sup: 0.063654, loss_mps: 0.069185, loss_cps: 0.129508
[13:03:07.487] iteration 10198: total_loss: 0.284620, loss_sup: 0.075342, loss_mps: 0.073028, loss_cps: 0.136249
[13:03:07.633] iteration 10199: total_loss: 0.324441, loss_sup: 0.136245, loss_mps: 0.067297, loss_cps: 0.120899
[13:03:07.780] iteration 10200: total_loss: 0.282862, loss_sup: 0.087024, loss_mps: 0.066398, loss_cps: 0.129440
[13:03:07.780] Evaluation Started ==>
[13:03:19.129] ==> valid iteration 10200: unet metrics: {'dc': 0.6424497611540697, 'jc': 0.5174287659562304, 'pre': 0.7489201690287381, 'hd': 5.9257194698985955}, ynet metrics: {'dc': 0.6158382771441261, 'jc': 0.49534668916699726, 'pre': 0.7803085628178075, 'hd': 5.703972794745558}.
[13:03:19.290] ==> New best valid dice for ynet: 0.615838, at iteration 10200
[13:03:19.292] Evaluation Finished!⏹️
[13:03:19.445] iteration 10201: total_loss: 0.194491, loss_sup: 0.026204, loss_mps: 0.059846, loss_cps: 0.108441
[13:03:19.592] iteration 10202: total_loss: 0.458943, loss_sup: 0.234447, loss_mps: 0.076480, loss_cps: 0.148016
[13:03:19.737] iteration 10203: total_loss: 0.251466, loss_sup: 0.031296, loss_mps: 0.076562, loss_cps: 0.143608
[13:03:19.883] iteration 10204: total_loss: 0.245609, loss_sup: 0.059800, loss_mps: 0.064116, loss_cps: 0.121694
[13:03:20.030] iteration 10205: total_loss: 0.226316, loss_sup: 0.045499, loss_mps: 0.062823, loss_cps: 0.117994
[13:03:20.177] iteration 10206: total_loss: 0.568726, loss_sup: 0.282305, loss_mps: 0.094156, loss_cps: 0.192264
[13:03:20.322] iteration 10207: total_loss: 0.378495, loss_sup: 0.087099, loss_mps: 0.092668, loss_cps: 0.198729
[13:03:20.467] iteration 10208: total_loss: 0.106030, loss_sup: 0.006378, loss_mps: 0.039048, loss_cps: 0.060603
[13:03:20.612] iteration 10209: total_loss: 0.251989, loss_sup: 0.044729, loss_mps: 0.071169, loss_cps: 0.136091
[13:03:20.759] iteration 10210: total_loss: 0.369871, loss_sup: 0.085914, loss_mps: 0.093176, loss_cps: 0.190781
[13:03:20.906] iteration 10211: total_loss: 0.131510, loss_sup: 0.017678, loss_mps: 0.042325, loss_cps: 0.071506
[13:03:21.051] iteration 10212: total_loss: 0.229439, loss_sup: 0.018851, loss_mps: 0.070536, loss_cps: 0.140052
[13:03:21.196] iteration 10213: total_loss: 0.320011, loss_sup: 0.051159, loss_mps: 0.087086, loss_cps: 0.181767
[13:03:21.344] iteration 10214: total_loss: 0.231271, loss_sup: 0.034934, loss_mps: 0.067350, loss_cps: 0.128987
[13:03:21.490] iteration 10215: total_loss: 0.194944, loss_sup: 0.052088, loss_mps: 0.051925, loss_cps: 0.090931
[13:03:21.636] iteration 10216: total_loss: 0.605756, loss_sup: 0.292030, loss_mps: 0.102369, loss_cps: 0.211358
[13:03:21.782] iteration 10217: total_loss: 0.324119, loss_sup: 0.032703, loss_mps: 0.094442, loss_cps: 0.196974
[13:03:21.927] iteration 10218: total_loss: 0.146550, loss_sup: 0.013878, loss_mps: 0.046890, loss_cps: 0.085783
[13:03:22.073] iteration 10219: total_loss: 0.509434, loss_sup: 0.212769, loss_mps: 0.097010, loss_cps: 0.199655
[13:03:22.219] iteration 10220: total_loss: 0.386915, loss_sup: 0.223626, loss_mps: 0.055920, loss_cps: 0.107370
[13:03:22.365] iteration 10221: total_loss: 0.232700, loss_sup: 0.022656, loss_mps: 0.073592, loss_cps: 0.136452
[13:03:22.512] iteration 10222: total_loss: 0.227029, loss_sup: 0.106241, loss_mps: 0.043855, loss_cps: 0.076933
[13:03:22.657] iteration 10223: total_loss: 0.547866, loss_sup: 0.241999, loss_mps: 0.096771, loss_cps: 0.209096
[13:03:22.803] iteration 10224: total_loss: 0.369306, loss_sup: 0.136282, loss_mps: 0.076580, loss_cps: 0.156444
[13:03:22.948] iteration 10225: total_loss: 0.126796, loss_sup: 0.021845, loss_mps: 0.042344, loss_cps: 0.062607
[13:03:23.098] iteration 10226: total_loss: 0.339556, loss_sup: 0.081454, loss_mps: 0.086652, loss_cps: 0.171451
[13:03:23.244] iteration 10227: total_loss: 0.174850, loss_sup: 0.037729, loss_mps: 0.051772, loss_cps: 0.085348
[13:03:23.390] iteration 10228: total_loss: 0.308310, loss_sup: 0.121792, loss_mps: 0.062700, loss_cps: 0.123818
[13:03:23.535] iteration 10229: total_loss: 0.262151, loss_sup: 0.088352, loss_mps: 0.060681, loss_cps: 0.113119
[13:03:23.682] iteration 10230: total_loss: 0.400713, loss_sup: 0.268892, loss_mps: 0.048692, loss_cps: 0.083129
[13:03:23.828] iteration 10231: total_loss: 0.455333, loss_sup: 0.296221, loss_mps: 0.057177, loss_cps: 0.101935
[13:03:23.973] iteration 10232: total_loss: 0.282716, loss_sup: 0.056346, loss_mps: 0.076122, loss_cps: 0.150248
[13:03:24.120] iteration 10233: total_loss: 0.175631, loss_sup: 0.072424, loss_mps: 0.041016, loss_cps: 0.062191
[13:03:24.266] iteration 10234: total_loss: 0.211934, loss_sup: 0.022020, loss_mps: 0.069128, loss_cps: 0.120785
[13:03:24.412] iteration 10235: total_loss: 0.213928, loss_sup: 0.005537, loss_mps: 0.071658, loss_cps: 0.136733
[13:03:24.558] iteration 10236: total_loss: 0.251693, loss_sup: 0.067079, loss_mps: 0.064620, loss_cps: 0.119994
[13:03:24.704] iteration 10237: total_loss: 0.195016, loss_sup: 0.032690, loss_mps: 0.058491, loss_cps: 0.103836
[13:03:24.851] iteration 10238: total_loss: 0.277770, loss_sup: 0.138113, loss_mps: 0.054868, loss_cps: 0.084789
[13:03:24.997] iteration 10239: total_loss: 0.217234, loss_sup: 0.051843, loss_mps: 0.061748, loss_cps: 0.103643
[13:03:25.142] iteration 10240: total_loss: 0.249750, loss_sup: 0.104926, loss_mps: 0.055417, loss_cps: 0.089407
[13:03:25.287] iteration 10241: total_loss: 0.179494, loss_sup: 0.020046, loss_mps: 0.056498, loss_cps: 0.102950
[13:03:25.433] iteration 10242: total_loss: 0.334050, loss_sup: 0.147757, loss_mps: 0.067373, loss_cps: 0.118921
[13:03:25.579] iteration 10243: total_loss: 0.168868, loss_sup: 0.049353, loss_mps: 0.044680, loss_cps: 0.074835
[13:03:25.725] iteration 10244: total_loss: 0.235043, loss_sup: 0.018154, loss_mps: 0.072351, loss_cps: 0.144538
[13:03:25.871] iteration 10245: total_loss: 0.335589, loss_sup: 0.217217, loss_mps: 0.045252, loss_cps: 0.073120
[13:03:26.017] iteration 10246: total_loss: 0.240394, loss_sup: 0.042741, loss_mps: 0.066004, loss_cps: 0.131649
[13:03:26.164] iteration 10247: total_loss: 0.228394, loss_sup: 0.078808, loss_mps: 0.054622, loss_cps: 0.094965
[13:03:26.309] iteration 10248: total_loss: 0.214833, loss_sup: 0.044318, loss_mps: 0.060877, loss_cps: 0.109637
[13:03:26.457] iteration 10249: total_loss: 0.197768, loss_sup: 0.028825, loss_mps: 0.061512, loss_cps: 0.107431
[13:03:26.603] iteration 10250: total_loss: 0.232003, loss_sup: 0.069753, loss_mps: 0.058905, loss_cps: 0.103345
[13:03:26.750] iteration 10251: total_loss: 0.151657, loss_sup: 0.006172, loss_mps: 0.052078, loss_cps: 0.093407
[13:03:26.897] iteration 10252: total_loss: 0.246427, loss_sup: 0.034504, loss_mps: 0.071544, loss_cps: 0.140378
[13:03:27.044] iteration 10253: total_loss: 0.221128, loss_sup: 0.068087, loss_mps: 0.053678, loss_cps: 0.099364
[13:03:27.190] iteration 10254: total_loss: 0.297606, loss_sup: 0.024494, loss_mps: 0.089336, loss_cps: 0.183776
[13:03:27.336] iteration 10255: total_loss: 0.224847, loss_sup: 0.081846, loss_mps: 0.050881, loss_cps: 0.092120
[13:03:27.482] iteration 10256: total_loss: 0.157934, loss_sup: 0.033061, loss_mps: 0.044900, loss_cps: 0.079974
[13:03:27.630] iteration 10257: total_loss: 0.127869, loss_sup: 0.027164, loss_mps: 0.038244, loss_cps: 0.062461
[13:03:27.776] iteration 10258: total_loss: 0.175782, loss_sup: 0.010329, loss_mps: 0.057412, loss_cps: 0.108041
[13:03:27.921] iteration 10259: total_loss: 0.168348, loss_sup: 0.025006, loss_mps: 0.049692, loss_cps: 0.093650
[13:03:28.067] iteration 10260: total_loss: 0.287360, loss_sup: 0.174711, loss_mps: 0.041790, loss_cps: 0.070859
[13:03:28.213] iteration 10261: total_loss: 0.165071, loss_sup: 0.004826, loss_mps: 0.056723, loss_cps: 0.103522
[13:03:28.359] iteration 10262: total_loss: 0.213465, loss_sup: 0.128859, loss_mps: 0.033071, loss_cps: 0.051534
[13:03:28.506] iteration 10263: total_loss: 0.168888, loss_sup: 0.014505, loss_mps: 0.053523, loss_cps: 0.100860
[13:03:28.652] iteration 10264: total_loss: 0.158840, loss_sup: 0.027132, loss_mps: 0.046520, loss_cps: 0.085188
[13:03:28.798] iteration 10265: total_loss: 0.170982, loss_sup: 0.037943, loss_mps: 0.045771, loss_cps: 0.087268
[13:03:28.943] iteration 10266: total_loss: 0.339482, loss_sup: 0.070847, loss_mps: 0.085261, loss_cps: 0.183374
[13:03:29.089] iteration 10267: total_loss: 0.241129, loss_sup: 0.121008, loss_mps: 0.042738, loss_cps: 0.077383
[13:03:29.236] iteration 10268: total_loss: 0.239934, loss_sup: 0.061628, loss_mps: 0.060264, loss_cps: 0.118042
[13:03:29.381] iteration 10269: total_loss: 0.295588, loss_sup: 0.120792, loss_mps: 0.062649, loss_cps: 0.112147
[13:03:29.527] iteration 10270: total_loss: 0.126740, loss_sup: 0.040640, loss_mps: 0.033484, loss_cps: 0.052616
[13:03:29.673] iteration 10271: total_loss: 0.433459, loss_sup: 0.265145, loss_mps: 0.054217, loss_cps: 0.114097
[13:03:29.819] iteration 10272: total_loss: 0.331982, loss_sup: 0.034577, loss_mps: 0.094847, loss_cps: 0.202557
[13:03:29.964] iteration 10273: total_loss: 0.118813, loss_sup: 0.012464, loss_mps: 0.039980, loss_cps: 0.066369
[13:03:30.110] iteration 10274: total_loss: 0.238391, loss_sup: 0.087944, loss_mps: 0.053037, loss_cps: 0.097411
[13:03:30.256] iteration 10275: total_loss: 0.243784, loss_sup: 0.073433, loss_mps: 0.056357, loss_cps: 0.113994
[13:03:30.401] iteration 10276: total_loss: 0.163498, loss_sup: 0.021023, loss_mps: 0.050335, loss_cps: 0.092140
[13:03:30.547] iteration 10277: total_loss: 0.179353, loss_sup: 0.067983, loss_mps: 0.041390, loss_cps: 0.069980
[13:03:30.693] iteration 10278: total_loss: 0.225960, loss_sup: 0.054599, loss_mps: 0.059306, loss_cps: 0.112055
[13:03:30.838] iteration 10279: total_loss: 0.219769, loss_sup: 0.054649, loss_mps: 0.057784, loss_cps: 0.107335
[13:03:30.984] iteration 10280: total_loss: 0.179874, loss_sup: 0.041483, loss_mps: 0.049629, loss_cps: 0.088762
[13:03:31.130] iteration 10281: total_loss: 0.224042, loss_sup: 0.014838, loss_mps: 0.068568, loss_cps: 0.140636
[13:03:31.275] iteration 10282: total_loss: 0.149237, loss_sup: 0.024977, loss_mps: 0.045361, loss_cps: 0.078899
[13:03:31.421] iteration 10283: total_loss: 0.205160, loss_sup: 0.044391, loss_mps: 0.053814, loss_cps: 0.106954
[13:03:31.568] iteration 10284: total_loss: 0.205017, loss_sup: 0.055554, loss_mps: 0.055602, loss_cps: 0.093861
[13:03:31.714] iteration 10285: total_loss: 0.271251, loss_sup: 0.112343, loss_mps: 0.054206, loss_cps: 0.104702
[13:03:31.860] iteration 10286: total_loss: 0.281875, loss_sup: 0.040713, loss_mps: 0.081184, loss_cps: 0.159977
[13:03:32.006] iteration 10287: total_loss: 0.356338, loss_sup: 0.165291, loss_mps: 0.063833, loss_cps: 0.127214
[13:03:32.152] iteration 10288: total_loss: 0.204440, loss_sup: 0.040165, loss_mps: 0.055202, loss_cps: 0.109073
[13:03:32.297] iteration 10289: total_loss: 0.168472, loss_sup: 0.017623, loss_mps: 0.052063, loss_cps: 0.098786
[13:03:32.443] iteration 10290: total_loss: 0.329692, loss_sup: 0.065884, loss_mps: 0.086140, loss_cps: 0.177668
[13:03:32.589] iteration 10291: total_loss: 0.255816, loss_sup: 0.046938, loss_mps: 0.069762, loss_cps: 0.139116
[13:03:32.735] iteration 10292: total_loss: 0.313319, loss_sup: 0.016006, loss_mps: 0.097957, loss_cps: 0.199356
[13:03:32.880] iteration 10293: total_loss: 0.211165, loss_sup: 0.055961, loss_mps: 0.052428, loss_cps: 0.102776
[13:03:33.025] iteration 10294: total_loss: 0.625389, loss_sup: 0.322114, loss_mps: 0.095874, loss_cps: 0.207401
[13:03:33.171] iteration 10295: total_loss: 0.299719, loss_sup: 0.161934, loss_mps: 0.047959, loss_cps: 0.089826
[13:03:33.317] iteration 10296: total_loss: 0.161762, loss_sup: 0.023841, loss_mps: 0.048656, loss_cps: 0.089265
[13:03:33.463] iteration 10297: total_loss: 0.318528, loss_sup: 0.066513, loss_mps: 0.082059, loss_cps: 0.169955
[13:03:33.609] iteration 10298: total_loss: 0.182279, loss_sup: 0.015419, loss_mps: 0.056842, loss_cps: 0.110019
[13:03:33.755] iteration 10299: total_loss: 0.467910, loss_sup: 0.113920, loss_mps: 0.112253, loss_cps: 0.241737
[13:03:33.900] iteration 10300: total_loss: 0.181009, loss_sup: 0.028977, loss_mps: 0.052319, loss_cps: 0.099713
[13:03:33.901] Evaluation Started ==>
[13:03:45.278] ==> valid iteration 10300: unet metrics: {'dc': 0.6010751652234165, 'jc': 0.4777962487395193, 'pre': 0.7275518671648923, 'hd': 5.965704457529611}, ynet metrics: {'dc': 0.5971916027009251, 'jc': 0.4758163133873016, 'pre': 0.7658276517184051, 'hd': 5.823715033310957}.
[13:03:45.280] Evaluation Finished!⏹️
[13:03:45.429] iteration 10301: total_loss: 0.199951, loss_sup: 0.039624, loss_mps: 0.054604, loss_cps: 0.105723
[13:03:45.576] iteration 10302: total_loss: 0.302395, loss_sup: 0.037644, loss_mps: 0.086505, loss_cps: 0.178246
[13:03:45.721] iteration 10303: total_loss: 0.354470, loss_sup: 0.077192, loss_mps: 0.088115, loss_cps: 0.189163
[13:03:45.867] iteration 10304: total_loss: 0.262320, loss_sup: 0.072226, loss_mps: 0.064389, loss_cps: 0.125705
[13:03:46.012] iteration 10305: total_loss: 0.233507, loss_sup: 0.073764, loss_mps: 0.053677, loss_cps: 0.106067
[13:03:46.157] iteration 10306: total_loss: 0.148700, loss_sup: 0.012414, loss_mps: 0.050111, loss_cps: 0.086175
[13:03:46.303] iteration 10307: total_loss: 0.300795, loss_sup: 0.069206, loss_mps: 0.077701, loss_cps: 0.153888
[13:03:46.448] iteration 10308: total_loss: 0.245031, loss_sup: 0.045337, loss_mps: 0.066746, loss_cps: 0.132948
[13:03:46.595] iteration 10309: total_loss: 0.197503, loss_sup: 0.068044, loss_mps: 0.046771, loss_cps: 0.082688
[13:03:46.740] iteration 10310: total_loss: 0.288779, loss_sup: 0.078165, loss_mps: 0.073481, loss_cps: 0.137133
[13:03:46.885] iteration 10311: total_loss: 0.216607, loss_sup: 0.040330, loss_mps: 0.059480, loss_cps: 0.116798
[13:03:47.031] iteration 10312: total_loss: 0.482784, loss_sup: 0.142758, loss_mps: 0.108297, loss_cps: 0.231729
[13:03:47.176] iteration 10313: total_loss: 0.225535, loss_sup: 0.058979, loss_mps: 0.055655, loss_cps: 0.110901
[13:03:47.321] iteration 10314: total_loss: 0.296239, loss_sup: 0.075408, loss_mps: 0.073489, loss_cps: 0.147342
[13:03:47.467] iteration 10315: total_loss: 0.228059, loss_sup: 0.053148, loss_mps: 0.057941, loss_cps: 0.116969
[13:03:47.613] iteration 10316: total_loss: 0.271868, loss_sup: 0.077238, loss_mps: 0.069153, loss_cps: 0.125477
[13:03:47.759] iteration 10317: total_loss: 0.268672, loss_sup: 0.106115, loss_mps: 0.055690, loss_cps: 0.106867
[13:03:47.905] iteration 10318: total_loss: 0.147662, loss_sup: 0.017076, loss_mps: 0.046036, loss_cps: 0.084549
[13:03:48.050] iteration 10319: total_loss: 0.191695, loss_sup: 0.076307, loss_mps: 0.043067, loss_cps: 0.072321
[13:03:48.196] iteration 10320: total_loss: 0.266236, loss_sup: 0.043699, loss_mps: 0.076668, loss_cps: 0.145870
[13:03:48.342] iteration 10321: total_loss: 0.153909, loss_sup: 0.021246, loss_mps: 0.047129, loss_cps: 0.085534
[13:03:48.488] iteration 10322: total_loss: 0.208023, loss_sup: 0.079816, loss_mps: 0.044173, loss_cps: 0.084034
[13:03:48.633] iteration 10323: total_loss: 0.264762, loss_sup: 0.070556, loss_mps: 0.064283, loss_cps: 0.129923
[13:03:48.778] iteration 10324: total_loss: 0.312465, loss_sup: 0.077650, loss_mps: 0.077947, loss_cps: 0.156868
[13:03:48.924] iteration 10325: total_loss: 0.278228, loss_sup: 0.113754, loss_mps: 0.057779, loss_cps: 0.106695
[13:03:49.070] iteration 10326: total_loss: 0.262932, loss_sup: 0.136012, loss_mps: 0.045649, loss_cps: 0.081271
[13:03:49.216] iteration 10327: total_loss: 0.141459, loss_sup: 0.037992, loss_mps: 0.039186, loss_cps: 0.064281
[13:03:49.361] iteration 10328: total_loss: 0.247549, loss_sup: 0.026687, loss_mps: 0.072991, loss_cps: 0.147871
[13:03:49.507] iteration 10329: total_loss: 0.201133, loss_sup: 0.028817, loss_mps: 0.061131, loss_cps: 0.111186
[13:03:49.652] iteration 10330: total_loss: 0.572502, loss_sup: 0.282069, loss_mps: 0.093815, loss_cps: 0.196618
[13:03:49.797] iteration 10331: total_loss: 0.335725, loss_sup: 0.110456, loss_mps: 0.072717, loss_cps: 0.152553
[13:03:49.943] iteration 10332: total_loss: 0.323272, loss_sup: 0.117994, loss_mps: 0.067731, loss_cps: 0.137547
[13:03:50.089] iteration 10333: total_loss: 0.497052, loss_sup: 0.266501, loss_mps: 0.075203, loss_cps: 0.155348
[13:03:50.234] iteration 10334: total_loss: 0.242115, loss_sup: 0.069610, loss_mps: 0.061648, loss_cps: 0.110857
[13:03:50.379] iteration 10335: total_loss: 0.277095, loss_sup: 0.112427, loss_mps: 0.056966, loss_cps: 0.107702
[13:03:50.525] iteration 10336: total_loss: 0.319263, loss_sup: 0.083526, loss_mps: 0.077386, loss_cps: 0.158351
[13:03:50.671] iteration 10337: total_loss: 0.216156, loss_sup: 0.083056, loss_mps: 0.048910, loss_cps: 0.084189
[13:03:50.816] iteration 10338: total_loss: 0.217299, loss_sup: 0.055429, loss_mps: 0.057147, loss_cps: 0.104724
[13:03:50.961] iteration 10339: total_loss: 0.368887, loss_sup: 0.053241, loss_mps: 0.099422, loss_cps: 0.216224
[13:03:51.107] iteration 10340: total_loss: 0.323532, loss_sup: 0.139704, loss_mps: 0.063975, loss_cps: 0.119853
[13:03:51.253] iteration 10341: total_loss: 0.299124, loss_sup: 0.128754, loss_mps: 0.058000, loss_cps: 0.112370
[13:03:51.399] iteration 10342: total_loss: 0.328939, loss_sup: 0.146648, loss_mps: 0.062523, loss_cps: 0.119769
[13:03:51.545] iteration 10343: total_loss: 0.224189, loss_sup: 0.030497, loss_mps: 0.066836, loss_cps: 0.126856
[13:03:51.690] iteration 10344: total_loss: 0.239371, loss_sup: 0.086128, loss_mps: 0.055960, loss_cps: 0.097283
[13:03:51.836] iteration 10345: total_loss: 0.180807, loss_sup: 0.063302, loss_mps: 0.043271, loss_cps: 0.074234
[13:03:51.982] iteration 10346: total_loss: 0.398872, loss_sup: 0.164876, loss_mps: 0.079918, loss_cps: 0.154077
[13:03:52.127] iteration 10347: total_loss: 0.276218, loss_sup: 0.047488, loss_mps: 0.079022, loss_cps: 0.149708
[13:03:52.273] iteration 10348: total_loss: 0.189400, loss_sup: 0.023273, loss_mps: 0.058476, loss_cps: 0.107651
[13:03:52.419] iteration 10349: total_loss: 0.445053, loss_sup: 0.135922, loss_mps: 0.101025, loss_cps: 0.208106
[13:03:52.564] iteration 10350: total_loss: 0.318628, loss_sup: 0.116394, loss_mps: 0.071699, loss_cps: 0.130535
[13:03:52.710] iteration 10351: total_loss: 0.305505, loss_sup: 0.091647, loss_mps: 0.072775, loss_cps: 0.141083
[13:03:52.855] iteration 10352: total_loss: 0.249444, loss_sup: 0.086176, loss_mps: 0.059809, loss_cps: 0.103459
[13:03:53.001] iteration 10353: total_loss: 0.173135, loss_sup: 0.045536, loss_mps: 0.045886, loss_cps: 0.081713
[13:03:53.146] iteration 10354: total_loss: 0.177910, loss_sup: 0.052941, loss_mps: 0.044963, loss_cps: 0.080006
[13:03:53.292] iteration 10355: total_loss: 0.450186, loss_sup: 0.186393, loss_mps: 0.087790, loss_cps: 0.176003
[13:03:53.437] iteration 10356: total_loss: 0.284340, loss_sup: 0.125478, loss_mps: 0.056644, loss_cps: 0.102218
[13:03:53.584] iteration 10357: total_loss: 0.503010, loss_sup: 0.173038, loss_mps: 0.106346, loss_cps: 0.223627
[13:03:53.729] iteration 10358: total_loss: 0.188467, loss_sup: 0.036449, loss_mps: 0.056629, loss_cps: 0.095389
[13:03:53.876] iteration 10359: total_loss: 0.240572, loss_sup: 0.037945, loss_mps: 0.069371, loss_cps: 0.133256
[13:03:54.022] iteration 10360: total_loss: 0.272461, loss_sup: 0.060858, loss_mps: 0.070229, loss_cps: 0.141374
[13:03:54.168] iteration 10361: total_loss: 0.433082, loss_sup: 0.150600, loss_mps: 0.092158, loss_cps: 0.190324
[13:03:54.313] iteration 10362: total_loss: 0.277633, loss_sup: 0.075921, loss_mps: 0.066907, loss_cps: 0.134805
[13:03:54.458] iteration 10363: total_loss: 0.274355, loss_sup: 0.146570, loss_mps: 0.045610, loss_cps: 0.082174
[13:03:54.603] iteration 10364: total_loss: 0.165910, loss_sup: 0.028237, loss_mps: 0.048619, loss_cps: 0.089054
[13:03:54.750] iteration 10365: total_loss: 0.187050, loss_sup: 0.003584, loss_mps: 0.064540, loss_cps: 0.118926
[13:03:54.896] iteration 10366: total_loss: 0.407863, loss_sup: 0.159565, loss_mps: 0.083176, loss_cps: 0.165122
[13:03:55.041] iteration 10367: total_loss: 0.201137, loss_sup: 0.051953, loss_mps: 0.054189, loss_cps: 0.094995
[13:03:55.187] iteration 10368: total_loss: 0.308449, loss_sup: 0.105688, loss_mps: 0.067549, loss_cps: 0.135212
[13:03:55.333] iteration 10369: total_loss: 0.346071, loss_sup: 0.170474, loss_mps: 0.062575, loss_cps: 0.113022
[13:03:55.478] iteration 10370: total_loss: 0.212622, loss_sup: 0.045157, loss_mps: 0.059166, loss_cps: 0.108299
[13:03:55.624] iteration 10371: total_loss: 0.397861, loss_sup: 0.174582, loss_mps: 0.074665, loss_cps: 0.148615
[13:03:55.770] iteration 10372: total_loss: 0.387194, loss_sup: 0.175428, loss_mps: 0.075992, loss_cps: 0.135774
[13:03:55.917] iteration 10373: total_loss: 0.524487, loss_sup: 0.196793, loss_mps: 0.104306, loss_cps: 0.223388
[13:03:56.063] iteration 10374: total_loss: 0.160121, loss_sup: 0.013928, loss_mps: 0.051994, loss_cps: 0.094199
[13:03:56.209] iteration 10375: total_loss: 0.282563, loss_sup: 0.016286, loss_mps: 0.089591, loss_cps: 0.176685
[13:03:56.355] iteration 10376: total_loss: 0.282051, loss_sup: 0.035063, loss_mps: 0.082621, loss_cps: 0.164367
[13:03:56.500] iteration 10377: total_loss: 0.150818, loss_sup: 0.046804, loss_mps: 0.038702, loss_cps: 0.065313
[13:03:56.647] iteration 10378: total_loss: 0.234436, loss_sup: 0.038844, loss_mps: 0.064498, loss_cps: 0.131093
[13:03:56.792] iteration 10379: total_loss: 0.264301, loss_sup: 0.134925, loss_mps: 0.046849, loss_cps: 0.082526
[13:03:56.937] iteration 10380: total_loss: 0.151326, loss_sup: 0.014872, loss_mps: 0.047644, loss_cps: 0.088810
[13:03:57.083] iteration 10381: total_loss: 0.265420, loss_sup: 0.115035, loss_mps: 0.055383, loss_cps: 0.095001
[13:03:57.230] iteration 10382: total_loss: 0.255999, loss_sup: 0.019658, loss_mps: 0.078115, loss_cps: 0.158226
[13:03:57.375] iteration 10383: total_loss: 0.172525, loss_sup: 0.055416, loss_mps: 0.044138, loss_cps: 0.072971
[13:03:57.521] iteration 10384: total_loss: 0.493253, loss_sup: 0.151788, loss_mps: 0.107839, loss_cps: 0.233626
[13:03:57.667] iteration 10385: total_loss: 0.353579, loss_sup: 0.159193, loss_mps: 0.065066, loss_cps: 0.129321
[13:03:57.812] iteration 10386: total_loss: 0.533466, loss_sup: 0.288195, loss_mps: 0.082302, loss_cps: 0.162969
[13:03:57.959] iteration 10387: total_loss: 0.185680, loss_sup: 0.039637, loss_mps: 0.055703, loss_cps: 0.090340
[13:03:58.108] iteration 10388: total_loss: 0.296121, loss_sup: 0.082679, loss_mps: 0.077728, loss_cps: 0.135714
[13:03:58.255] iteration 10389: total_loss: 0.181186, loss_sup: 0.037125, loss_mps: 0.052261, loss_cps: 0.091800
[13:03:58.401] iteration 10390: total_loss: 0.200192, loss_sup: 0.041220, loss_mps: 0.054320, loss_cps: 0.104651
[13:03:58.547] iteration 10391: total_loss: 0.258145, loss_sup: 0.053032, loss_mps: 0.072171, loss_cps: 0.132943
[13:03:58.693] iteration 10392: total_loss: 0.248397, loss_sup: 0.057465, loss_mps: 0.064270, loss_cps: 0.126663
[13:03:58.839] iteration 10393: total_loss: 0.239383, loss_sup: 0.049286, loss_mps: 0.066567, loss_cps: 0.123529
[13:03:58.986] iteration 10394: total_loss: 0.263637, loss_sup: 0.086564, loss_mps: 0.060667, loss_cps: 0.116406
[13:03:59.132] iteration 10395: total_loss: 0.468467, loss_sup: 0.188006, loss_mps: 0.093622, loss_cps: 0.186839
[13:03:59.279] iteration 10396: total_loss: 0.285627, loss_sup: 0.075895, loss_mps: 0.073969, loss_cps: 0.135763
[13:03:59.426] iteration 10397: total_loss: 0.260771, loss_sup: 0.157188, loss_mps: 0.040358, loss_cps: 0.063225
[13:03:59.572] iteration 10398: total_loss: 0.213331, loss_sup: 0.026867, loss_mps: 0.063914, loss_cps: 0.122550
[13:03:59.718] iteration 10399: total_loss: 0.214875, loss_sup: 0.057279, loss_mps: 0.057897, loss_cps: 0.099699
[13:03:59.864] iteration 10400: total_loss: 0.245996, loss_sup: 0.011638, loss_mps: 0.077577, loss_cps: 0.156781
[13:03:59.864] Evaluation Started ==>
[13:04:11.235] ==> valid iteration 10400: unet metrics: {'dc': 0.626522814290095, 'jc': 0.5007019984266761, 'pre': 0.7098512340436904, 'hd': 6.150470060748524}, ynet metrics: {'dc': 0.586336573997312, 'jc': 0.46565876956267527, 'pre': 0.7665188652796415, 'hd': 6.016313108617093}.
[13:04:11.238] Evaluation Finished!⏹️
[13:04:11.389] iteration 10401: total_loss: 0.329263, loss_sup: 0.062113, loss_mps: 0.092991, loss_cps: 0.174160
[13:04:11.538] iteration 10402: total_loss: 0.127005, loss_sup: 0.008780, loss_mps: 0.046543, loss_cps: 0.071681
[13:04:11.684] iteration 10403: total_loss: 0.337068, loss_sup: 0.041863, loss_mps: 0.095323, loss_cps: 0.199882
[13:04:11.832] iteration 10404: total_loss: 0.294416, loss_sup: 0.091245, loss_mps: 0.071951, loss_cps: 0.131220
[13:04:11.978] iteration 10405: total_loss: 0.373677, loss_sup: 0.053510, loss_mps: 0.102649, loss_cps: 0.217518
[13:04:12.124] iteration 10406: total_loss: 0.288895, loss_sup: 0.117693, loss_mps: 0.060791, loss_cps: 0.110411
[13:04:12.270] iteration 10407: total_loss: 0.238367, loss_sup: 0.044553, loss_mps: 0.069882, loss_cps: 0.123932
[13:04:12.422] iteration 10408: total_loss: 0.378576, loss_sup: 0.087580, loss_mps: 0.094833, loss_cps: 0.196163
[13:04:12.570] iteration 10409: total_loss: 0.214670, loss_sup: 0.048326, loss_mps: 0.060987, loss_cps: 0.105357
[13:04:12.716] iteration 10410: total_loss: 0.287010, loss_sup: 0.095688, loss_mps: 0.071334, loss_cps: 0.119988
[13:04:12.863] iteration 10411: total_loss: 0.345371, loss_sup: 0.097462, loss_mps: 0.083799, loss_cps: 0.164110
[13:04:13.009] iteration 10412: total_loss: 0.239659, loss_sup: 0.028085, loss_mps: 0.074296, loss_cps: 0.137278
[13:04:13.158] iteration 10413: total_loss: 0.163163, loss_sup: 0.019540, loss_mps: 0.051348, loss_cps: 0.092275
[13:04:13.305] iteration 10414: total_loss: 0.160237, loss_sup: 0.028668, loss_mps: 0.048061, loss_cps: 0.083508
[13:04:13.450] iteration 10415: total_loss: 0.136616, loss_sup: 0.011373, loss_mps: 0.046291, loss_cps: 0.078952
[13:04:13.596] iteration 10416: total_loss: 0.332407, loss_sup: 0.105827, loss_mps: 0.075612, loss_cps: 0.150968
[13:04:13.742] iteration 10417: total_loss: 0.265147, loss_sup: 0.048378, loss_mps: 0.071691, loss_cps: 0.145078
[13:04:13.887] iteration 10418: total_loss: 0.179468, loss_sup: 0.050358, loss_mps: 0.045286, loss_cps: 0.083824
[13:04:14.032] iteration 10419: total_loss: 0.341847, loss_sup: 0.107334, loss_mps: 0.074748, loss_cps: 0.159765
[13:04:14.178] iteration 10420: total_loss: 0.204006, loss_sup: 0.087849, loss_mps: 0.044042, loss_cps: 0.072114
[13:04:14.323] iteration 10421: total_loss: 0.216750, loss_sup: 0.057427, loss_mps: 0.059727, loss_cps: 0.099596
[13:04:14.468] iteration 10422: total_loss: 0.253590, loss_sup: 0.085457, loss_mps: 0.060044, loss_cps: 0.108089
[13:04:14.615] iteration 10423: total_loss: 0.175379, loss_sup: 0.064273, loss_mps: 0.042969, loss_cps: 0.068137
[13:04:14.760] iteration 10424: total_loss: 0.299242, loss_sup: 0.158633, loss_mps: 0.050315, loss_cps: 0.090293
[13:04:14.906] iteration 10425: total_loss: 0.223854, loss_sup: 0.097340, loss_mps: 0.047203, loss_cps: 0.079311
[13:04:15.051] iteration 10426: total_loss: 0.252739, loss_sup: 0.025895, loss_mps: 0.077390, loss_cps: 0.149453
[13:04:15.196] iteration 10427: total_loss: 0.444706, loss_sup: 0.201617, loss_mps: 0.082744, loss_cps: 0.160345
[13:04:15.342] iteration 10428: total_loss: 0.334382, loss_sup: 0.171831, loss_mps: 0.060663, loss_cps: 0.101888
[13:04:15.487] iteration 10429: total_loss: 0.193710, loss_sup: 0.049158, loss_mps: 0.051197, loss_cps: 0.093354
[13:04:15.633] iteration 10430: total_loss: 0.223913, loss_sup: 0.010651, loss_mps: 0.071453, loss_cps: 0.141809
[13:04:15.779] iteration 10431: total_loss: 0.164670, loss_sup: 0.016838, loss_mps: 0.052233, loss_cps: 0.095598
[13:04:15.924] iteration 10432: total_loss: 0.458615, loss_sup: 0.194293, loss_mps: 0.083616, loss_cps: 0.180706
[13:04:16.071] iteration 10433: total_loss: 0.257466, loss_sup: 0.107385, loss_mps: 0.050591, loss_cps: 0.099490
[13:04:16.217] iteration 10434: total_loss: 0.691521, loss_sup: 0.481752, loss_mps: 0.068939, loss_cps: 0.140830
[13:04:16.362] iteration 10435: total_loss: 0.209236, loss_sup: 0.022866, loss_mps: 0.060043, loss_cps: 0.126326
[13:04:16.508] iteration 10436: total_loss: 0.171027, loss_sup: 0.013662, loss_mps: 0.055887, loss_cps: 0.101478
[13:04:16.655] iteration 10437: total_loss: 0.236325, loss_sup: 0.046482, loss_mps: 0.063653, loss_cps: 0.126190
[13:04:16.801] iteration 10438: total_loss: 0.340961, loss_sup: 0.070925, loss_mps: 0.083919, loss_cps: 0.186116
[13:04:16.947] iteration 10439: total_loss: 0.231122, loss_sup: 0.041180, loss_mps: 0.066082, loss_cps: 0.123860
[13:04:17.092] iteration 10440: total_loss: 0.310637, loss_sup: 0.099913, loss_mps: 0.077411, loss_cps: 0.133314
[13:04:17.238] iteration 10441: total_loss: 0.406123, loss_sup: 0.120010, loss_mps: 0.089751, loss_cps: 0.196362
[13:04:17.383] iteration 10442: total_loss: 0.307945, loss_sup: 0.185246, loss_mps: 0.043493, loss_cps: 0.079207
[13:04:17.528] iteration 10443: total_loss: 0.367779, loss_sup: 0.119659, loss_mps: 0.081367, loss_cps: 0.166754
[13:04:17.675] iteration 10444: total_loss: 0.217520, loss_sup: 0.083236, loss_mps: 0.048629, loss_cps: 0.085656
[13:04:17.821] iteration 10445: total_loss: 0.255053, loss_sup: 0.036203, loss_mps: 0.073480, loss_cps: 0.145369
[13:04:17.967] iteration 10446: total_loss: 0.314957, loss_sup: 0.080423, loss_mps: 0.079670, loss_cps: 0.154863
[13:04:18.112] iteration 10447: total_loss: 0.239132, loss_sup: 0.049596, loss_mps: 0.065580, loss_cps: 0.123957
[13:04:18.258] iteration 10448: total_loss: 0.325943, loss_sup: 0.025888, loss_mps: 0.095248, loss_cps: 0.204807
[13:04:18.403] iteration 10449: total_loss: 0.331998, loss_sup: 0.068920, loss_mps: 0.088828, loss_cps: 0.174249
[13:04:18.473] iteration 10450: total_loss: 0.446164, loss_sup: 0.207972, loss_mps: 0.086879, loss_cps: 0.151313
[13:04:19.669] iteration 10451: total_loss: 0.346580, loss_sup: 0.112550, loss_mps: 0.080818, loss_cps: 0.153212
[13:04:19.818] iteration 10452: total_loss: 0.210504, loss_sup: 0.051931, loss_mps: 0.060626, loss_cps: 0.097947
[13:04:19.964] iteration 10453: total_loss: 0.222558, loss_sup: 0.053610, loss_mps: 0.064122, loss_cps: 0.104826
[13:04:20.112] iteration 10454: total_loss: 0.230907, loss_sup: 0.081696, loss_mps: 0.053425, loss_cps: 0.095786
[13:04:20.261] iteration 10455: total_loss: 0.190725, loss_sup: 0.065058, loss_mps: 0.046752, loss_cps: 0.078915
[13:04:20.410] iteration 10456: total_loss: 0.435207, loss_sup: 0.213931, loss_mps: 0.072862, loss_cps: 0.148413
[13:04:20.556] iteration 10457: total_loss: 0.254233, loss_sup: 0.033380, loss_mps: 0.074723, loss_cps: 0.146131
[13:04:20.704] iteration 10458: total_loss: 0.279598, loss_sup: 0.103609, loss_mps: 0.061781, loss_cps: 0.114208
[13:04:20.850] iteration 10459: total_loss: 0.272737, loss_sup: 0.015009, loss_mps: 0.082066, loss_cps: 0.175662
[13:04:20.996] iteration 10460: total_loss: 0.229217, loss_sup: 0.032906, loss_mps: 0.069567, loss_cps: 0.126743
[13:04:21.142] iteration 10461: total_loss: 0.327754, loss_sup: 0.074467, loss_mps: 0.085578, loss_cps: 0.167709
[13:04:21.289] iteration 10462: total_loss: 0.381688, loss_sup: 0.176381, loss_mps: 0.071143, loss_cps: 0.134164
[13:04:21.435] iteration 10463: total_loss: 0.249467, loss_sup: 0.043371, loss_mps: 0.068502, loss_cps: 0.137594
[13:04:21.582] iteration 10464: total_loss: 0.410262, loss_sup: 0.160040, loss_mps: 0.084899, loss_cps: 0.165323
[13:04:21.728] iteration 10465: total_loss: 0.355447, loss_sup: 0.107936, loss_mps: 0.085691, loss_cps: 0.161820
[13:04:21.875] iteration 10466: total_loss: 0.293487, loss_sup: 0.032078, loss_mps: 0.082350, loss_cps: 0.179059
[13:04:22.021] iteration 10467: total_loss: 0.197476, loss_sup: 0.025270, loss_mps: 0.058388, loss_cps: 0.113818
[13:04:22.167] iteration 10468: total_loss: 0.312085, loss_sup: 0.105927, loss_mps: 0.067791, loss_cps: 0.138367
[13:04:22.313] iteration 10469: total_loss: 0.296633, loss_sup: 0.077225, loss_mps: 0.074607, loss_cps: 0.144802
[13:04:22.459] iteration 10470: total_loss: 0.395387, loss_sup: 0.123379, loss_mps: 0.085809, loss_cps: 0.186199
[13:04:22.607] iteration 10471: total_loss: 0.153146, loss_sup: 0.016061, loss_mps: 0.048441, loss_cps: 0.088645
[13:04:22.754] iteration 10472: total_loss: 0.220002, loss_sup: 0.053930, loss_mps: 0.057692, loss_cps: 0.108380
[13:04:22.900] iteration 10473: total_loss: 0.261647, loss_sup: 0.087586, loss_mps: 0.062192, loss_cps: 0.111868
[13:04:23.047] iteration 10474: total_loss: 0.175733, loss_sup: 0.032704, loss_mps: 0.051173, loss_cps: 0.091856
[13:04:23.194] iteration 10475: total_loss: 0.275969, loss_sup: 0.070355, loss_mps: 0.068624, loss_cps: 0.136990
[13:04:23.340] iteration 10476: total_loss: 0.436453, loss_sup: 0.077408, loss_mps: 0.108586, loss_cps: 0.250459
[13:04:23.486] iteration 10477: total_loss: 0.172318, loss_sup: 0.076596, loss_mps: 0.034726, loss_cps: 0.060996
[13:04:23.632] iteration 10478: total_loss: 0.269768, loss_sup: 0.086862, loss_mps: 0.058384, loss_cps: 0.124523
[13:04:23.778] iteration 10479: total_loss: 0.415391, loss_sup: 0.235813, loss_mps: 0.058399, loss_cps: 0.121178
[13:04:23.924] iteration 10480: total_loss: 0.304313, loss_sup: 0.099235, loss_mps: 0.068415, loss_cps: 0.136663
[13:04:24.070] iteration 10481: total_loss: 0.254923, loss_sup: 0.070816, loss_mps: 0.063883, loss_cps: 0.120224
[13:04:24.218] iteration 10482: total_loss: 0.227383, loss_sup: 0.019453, loss_mps: 0.068101, loss_cps: 0.139829
[13:04:24.364] iteration 10483: total_loss: 0.271732, loss_sup: 0.058411, loss_mps: 0.073106, loss_cps: 0.140214
[13:04:24.510] iteration 10484: total_loss: 0.161721, loss_sup: 0.046145, loss_mps: 0.043679, loss_cps: 0.071897
[13:04:24.657] iteration 10485: total_loss: 0.398510, loss_sup: 0.158395, loss_mps: 0.081495, loss_cps: 0.158621
[13:04:24.803] iteration 10486: total_loss: 0.168307, loss_sup: 0.034708, loss_mps: 0.050598, loss_cps: 0.083001
[13:04:24.949] iteration 10487: total_loss: 0.299080, loss_sup: 0.074699, loss_mps: 0.077049, loss_cps: 0.147332
[13:04:25.097] iteration 10488: total_loss: 0.326170, loss_sup: 0.067033, loss_mps: 0.084395, loss_cps: 0.174743
[13:04:25.243] iteration 10489: total_loss: 0.308335, loss_sup: 0.101553, loss_mps: 0.069788, loss_cps: 0.136994
[13:04:25.389] iteration 10490: total_loss: 0.227238, loss_sup: 0.039133, loss_mps: 0.066248, loss_cps: 0.121857
[13:04:25.536] iteration 10491: total_loss: 0.330271, loss_sup: 0.047795, loss_mps: 0.094921, loss_cps: 0.187556
[13:04:25.687] iteration 10492: total_loss: 0.273057, loss_sup: 0.131426, loss_mps: 0.052168, loss_cps: 0.089463
[13:04:25.834] iteration 10493: total_loss: 0.211308, loss_sup: 0.024036, loss_mps: 0.066325, loss_cps: 0.120947
[13:04:25.980] iteration 10494: total_loss: 0.267542, loss_sup: 0.083960, loss_mps: 0.062091, loss_cps: 0.121491
[13:04:26.125] iteration 10495: total_loss: 0.554163, loss_sup: 0.250977, loss_mps: 0.099596, loss_cps: 0.203590
[13:04:26.271] iteration 10496: total_loss: 0.225918, loss_sup: 0.030177, loss_mps: 0.067818, loss_cps: 0.127923
[13:04:26.417] iteration 10497: total_loss: 0.273858, loss_sup: 0.057554, loss_mps: 0.078046, loss_cps: 0.138258
[13:04:26.568] iteration 10498: total_loss: 0.329946, loss_sup: 0.046596, loss_mps: 0.093805, loss_cps: 0.189545
[13:04:26.715] iteration 10499: total_loss: 0.330245, loss_sup: 0.062612, loss_mps: 0.090646, loss_cps: 0.176987
[13:04:26.861] iteration 10500: total_loss: 0.282395, loss_sup: 0.060017, loss_mps: 0.073600, loss_cps: 0.148778
[13:04:26.861] Evaluation Started ==>
[13:04:38.268] ==> valid iteration 10500: unet metrics: {'dc': 0.6303630195435165, 'jc': 0.5083423140674912, 'pre': 0.7281541905070652, 'hd': 5.857481296432204}, ynet metrics: {'dc': 0.5780293793521265, 'jc': 0.4647992152857581, 'pre': 0.7425520131138271, 'hd': 5.986368581287338}.
[13:04:38.269] Evaluation Finished!⏹️
[13:04:38.419] iteration 10501: total_loss: 0.199034, loss_sup: 0.052207, loss_mps: 0.056044, loss_cps: 0.090782
[13:04:38.567] iteration 10502: total_loss: 0.316074, loss_sup: 0.087347, loss_mps: 0.078006, loss_cps: 0.150721
[13:04:38.713] iteration 10503: total_loss: 0.228315, loss_sup: 0.027023, loss_mps: 0.071736, loss_cps: 0.129556
[13:04:38.859] iteration 10504: total_loss: 0.385629, loss_sup: 0.115703, loss_mps: 0.088385, loss_cps: 0.181542
[13:04:39.007] iteration 10505: total_loss: 0.380280, loss_sup: 0.059710, loss_mps: 0.106524, loss_cps: 0.214046
[13:04:39.153] iteration 10506: total_loss: 0.213708, loss_sup: 0.099531, loss_mps: 0.044613, loss_cps: 0.069564
[13:04:39.300] iteration 10507: total_loss: 0.221015, loss_sup: 0.035594, loss_mps: 0.066023, loss_cps: 0.119398
[13:04:39.446] iteration 10508: total_loss: 0.100512, loss_sup: 0.007242, loss_mps: 0.036852, loss_cps: 0.056417
[13:04:39.591] iteration 10509: total_loss: 0.189523, loss_sup: 0.026704, loss_mps: 0.056244, loss_cps: 0.106575
[13:04:39.737] iteration 10510: total_loss: 0.348641, loss_sup: 0.051446, loss_mps: 0.095279, loss_cps: 0.201916
[13:04:39.882] iteration 10511: total_loss: 0.266763, loss_sup: 0.086254, loss_mps: 0.064425, loss_cps: 0.116083
[13:04:40.030] iteration 10512: total_loss: 0.386302, loss_sup: 0.202293, loss_mps: 0.064857, loss_cps: 0.119152
[13:04:40.175] iteration 10513: total_loss: 0.403268, loss_sup: 0.138682, loss_mps: 0.082590, loss_cps: 0.181996
[13:04:40.321] iteration 10514: total_loss: 0.211414, loss_sup: 0.041110, loss_mps: 0.058408, loss_cps: 0.111896
[13:04:40.469] iteration 10515: total_loss: 0.267595, loss_sup: 0.088469, loss_mps: 0.060670, loss_cps: 0.118457
[13:04:40.615] iteration 10516: total_loss: 0.299127, loss_sup: 0.127270, loss_mps: 0.060984, loss_cps: 0.110872
[13:04:40.762] iteration 10517: total_loss: 0.140720, loss_sup: 0.011188, loss_mps: 0.046368, loss_cps: 0.083165
[13:04:40.908] iteration 10518: total_loss: 0.239981, loss_sup: 0.060602, loss_mps: 0.060973, loss_cps: 0.118406
[13:04:41.054] iteration 10519: total_loss: 0.489311, loss_sup: 0.271929, loss_mps: 0.073379, loss_cps: 0.144002
[13:04:41.201] iteration 10520: total_loss: 0.382140, loss_sup: 0.133204, loss_mps: 0.086793, loss_cps: 0.162143
[13:04:41.347] iteration 10521: total_loss: 0.431057, loss_sup: 0.221114, loss_mps: 0.069726, loss_cps: 0.140216
[13:04:41.493] iteration 10522: total_loss: 0.315724, loss_sup: 0.094291, loss_mps: 0.070423, loss_cps: 0.151010
[13:04:41.638] iteration 10523: total_loss: 0.385400, loss_sup: 0.062284, loss_mps: 0.107712, loss_cps: 0.215405
[13:04:41.784] iteration 10524: total_loss: 0.402272, loss_sup: 0.190323, loss_mps: 0.071563, loss_cps: 0.140386
[13:04:41.930] iteration 10525: total_loss: 0.223393, loss_sup: 0.059855, loss_mps: 0.060366, loss_cps: 0.103172
[13:04:42.077] iteration 10526: total_loss: 0.156221, loss_sup: 0.010584, loss_mps: 0.059339, loss_cps: 0.086297
[13:04:42.222] iteration 10527: total_loss: 0.374144, loss_sup: 0.209482, loss_mps: 0.058491, loss_cps: 0.106170
[13:04:42.368] iteration 10528: total_loss: 0.332161, loss_sup: 0.040319, loss_mps: 0.100314, loss_cps: 0.191528
[13:04:42.516] iteration 10529: total_loss: 0.340090, loss_sup: 0.085173, loss_mps: 0.085904, loss_cps: 0.169012
[13:04:42.664] iteration 10530: total_loss: 0.341031, loss_sup: 0.096056, loss_mps: 0.083217, loss_cps: 0.161759
[13:04:42.811] iteration 10531: total_loss: 0.195132, loss_sup: 0.016025, loss_mps: 0.062794, loss_cps: 0.116314
[13:04:42.957] iteration 10532: total_loss: 0.344382, loss_sup: 0.141748, loss_mps: 0.075856, loss_cps: 0.126779
[13:04:43.105] iteration 10533: total_loss: 0.657622, loss_sup: 0.445181, loss_mps: 0.077295, loss_cps: 0.135146
[13:04:43.252] iteration 10534: total_loss: 0.187718, loss_sup: 0.061151, loss_mps: 0.051361, loss_cps: 0.075206
[13:04:43.398] iteration 10535: total_loss: 0.222086, loss_sup: 0.055483, loss_mps: 0.061091, loss_cps: 0.105512
[13:04:43.544] iteration 10536: total_loss: 0.258669, loss_sup: 0.052525, loss_mps: 0.072525, loss_cps: 0.133618
[13:04:43.689] iteration 10537: total_loss: 0.199063, loss_sup: 0.049826, loss_mps: 0.055168, loss_cps: 0.094070
[13:04:43.835] iteration 10538: total_loss: 0.317965, loss_sup: 0.059784, loss_mps: 0.088665, loss_cps: 0.169515
[13:04:43.981] iteration 10539: total_loss: 0.555057, loss_sup: 0.188058, loss_mps: 0.119476, loss_cps: 0.247523
[13:04:44.127] iteration 10540: total_loss: 0.265423, loss_sup: 0.060516, loss_mps: 0.070817, loss_cps: 0.134090
[13:04:44.273] iteration 10541: total_loss: 0.151935, loss_sup: 0.019918, loss_mps: 0.049587, loss_cps: 0.082429
[13:04:44.420] iteration 10542: total_loss: 0.356412, loss_sup: 0.129848, loss_mps: 0.078207, loss_cps: 0.148357
[13:04:44.565] iteration 10543: total_loss: 0.314898, loss_sup: 0.034285, loss_mps: 0.092036, loss_cps: 0.188577
[13:04:44.712] iteration 10544: total_loss: 0.175461, loss_sup: 0.016309, loss_mps: 0.056921, loss_cps: 0.102231
[13:04:44.857] iteration 10545: total_loss: 0.156289, loss_sup: 0.015828, loss_mps: 0.051749, loss_cps: 0.088712
[13:04:45.006] iteration 10546: total_loss: 0.319296, loss_sup: 0.104841, loss_mps: 0.073881, loss_cps: 0.140574
[13:04:45.154] iteration 10547: total_loss: 0.146082, loss_sup: 0.023285, loss_mps: 0.046543, loss_cps: 0.076254
[13:04:45.299] iteration 10548: total_loss: 0.204641, loss_sup: 0.060307, loss_mps: 0.054256, loss_cps: 0.090078
[13:04:45.445] iteration 10549: total_loss: 0.385795, loss_sup: 0.154719, loss_mps: 0.080481, loss_cps: 0.150594
[13:04:45.592] iteration 10550: total_loss: 0.266229, loss_sup: 0.036236, loss_mps: 0.079345, loss_cps: 0.150648
[13:04:45.737] iteration 10551: total_loss: 0.275876, loss_sup: 0.025355, loss_mps: 0.082834, loss_cps: 0.167688
[13:04:45.885] iteration 10552: total_loss: 0.249385, loss_sup: 0.097712, loss_mps: 0.053136, loss_cps: 0.098537
[13:04:46.033] iteration 10553: total_loss: 0.153654, loss_sup: 0.020389, loss_mps: 0.046538, loss_cps: 0.086727
[13:04:46.180] iteration 10554: total_loss: 0.310686, loss_sup: 0.121566, loss_mps: 0.064676, loss_cps: 0.124445
[13:04:46.327] iteration 10555: total_loss: 0.236218, loss_sup: 0.079391, loss_mps: 0.052995, loss_cps: 0.103832
[13:04:46.474] iteration 10556: total_loss: 0.269607, loss_sup: 0.072717, loss_mps: 0.067572, loss_cps: 0.129319
[13:04:46.620] iteration 10557: total_loss: 0.162489, loss_sup: 0.034966, loss_mps: 0.048790, loss_cps: 0.078732
[13:04:46.767] iteration 10558: total_loss: 0.158852, loss_sup: 0.034036, loss_mps: 0.048458, loss_cps: 0.076357
[13:04:46.913] iteration 10559: total_loss: 0.528531, loss_sup: 0.294329, loss_mps: 0.077900, loss_cps: 0.156302
[13:04:47.059] iteration 10560: total_loss: 0.170803, loss_sup: 0.019693, loss_mps: 0.054084, loss_cps: 0.097026
[13:04:47.206] iteration 10561: total_loss: 0.282833, loss_sup: 0.045963, loss_mps: 0.076657, loss_cps: 0.160213
[13:04:47.353] iteration 10562: total_loss: 0.498774, loss_sup: 0.348997, loss_mps: 0.054134, loss_cps: 0.095642
[13:04:47.500] iteration 10563: total_loss: 0.227348, loss_sup: 0.078750, loss_mps: 0.054093, loss_cps: 0.094505
[13:04:47.649] iteration 10564: total_loss: 0.095136, loss_sup: 0.011111, loss_mps: 0.033157, loss_cps: 0.050868
[13:04:47.795] iteration 10565: total_loss: 0.234278, loss_sup: 0.070835, loss_mps: 0.058089, loss_cps: 0.105354
[13:04:47.945] iteration 10566: total_loss: 0.159848, loss_sup: 0.047836, loss_mps: 0.043007, loss_cps: 0.069005
[13:04:48.091] iteration 10567: total_loss: 0.415466, loss_sup: 0.125581, loss_mps: 0.093071, loss_cps: 0.196814
[13:04:48.237] iteration 10568: total_loss: 0.194907, loss_sup: 0.025895, loss_mps: 0.061102, loss_cps: 0.107910
[13:04:48.385] iteration 10569: total_loss: 0.364552, loss_sup: 0.160476, loss_mps: 0.071654, loss_cps: 0.132421
[13:04:48.531] iteration 10570: total_loss: 0.495518, loss_sup: 0.263944, loss_mps: 0.077991, loss_cps: 0.153583
[13:04:48.678] iteration 10571: total_loss: 0.407269, loss_sup: 0.117125, loss_mps: 0.096923, loss_cps: 0.193221
[13:04:48.825] iteration 10572: total_loss: 0.451988, loss_sup: 0.201284, loss_mps: 0.082411, loss_cps: 0.168293
[13:04:48.972] iteration 10573: total_loss: 0.249612, loss_sup: 0.036830, loss_mps: 0.071921, loss_cps: 0.140860
[13:04:49.118] iteration 10574: total_loss: 0.324133, loss_sup: 0.144007, loss_mps: 0.065425, loss_cps: 0.114702
[13:04:49.265] iteration 10575: total_loss: 0.240120, loss_sup: 0.060698, loss_mps: 0.065593, loss_cps: 0.113828
[13:04:49.411] iteration 10576: total_loss: 0.330231, loss_sup: 0.089560, loss_mps: 0.082670, loss_cps: 0.158001
[13:04:49.557] iteration 10577: total_loss: 0.576605, loss_sup: 0.276804, loss_mps: 0.100222, loss_cps: 0.199579
[13:04:49.703] iteration 10578: total_loss: 0.425141, loss_sup: 0.195894, loss_mps: 0.078423, loss_cps: 0.150824
[13:04:49.850] iteration 10579: total_loss: 0.239196, loss_sup: 0.030422, loss_mps: 0.071413, loss_cps: 0.137361
[13:04:49.996] iteration 10580: total_loss: 0.210985, loss_sup: 0.063484, loss_mps: 0.054785, loss_cps: 0.092717
[13:04:50.144] iteration 10581: total_loss: 0.168427, loss_sup: 0.037484, loss_mps: 0.049768, loss_cps: 0.081175
[13:04:50.290] iteration 10582: total_loss: 0.356805, loss_sup: 0.049662, loss_mps: 0.103138, loss_cps: 0.204005
[13:04:50.436] iteration 10583: total_loss: 0.264718, loss_sup: 0.100581, loss_mps: 0.060367, loss_cps: 0.103770
[13:04:50.582] iteration 10584: total_loss: 0.342594, loss_sup: 0.151795, loss_mps: 0.067781, loss_cps: 0.123019
[13:04:50.728] iteration 10585: total_loss: 0.259979, loss_sup: 0.013164, loss_mps: 0.083906, loss_cps: 0.162908
[13:04:50.875] iteration 10586: total_loss: 0.239187, loss_sup: 0.071656, loss_mps: 0.059319, loss_cps: 0.108212
[13:04:51.022] iteration 10587: total_loss: 0.212759, loss_sup: 0.025866, loss_mps: 0.064067, loss_cps: 0.122826
[13:04:51.168] iteration 10588: total_loss: 0.142622, loss_sup: 0.033022, loss_mps: 0.040867, loss_cps: 0.068733
[13:04:51.316] iteration 10589: total_loss: 0.231765, loss_sup: 0.105825, loss_mps: 0.045370, loss_cps: 0.080569
[13:04:51.462] iteration 10590: total_loss: 0.490258, loss_sup: 0.307898, loss_mps: 0.063395, loss_cps: 0.118965
[13:04:51.608] iteration 10591: total_loss: 0.298075, loss_sup: 0.158971, loss_mps: 0.052600, loss_cps: 0.086505
[13:04:51.758] iteration 10592: total_loss: 0.140081, loss_sup: 0.006848, loss_mps: 0.050235, loss_cps: 0.082998
[13:04:51.905] iteration 10593: total_loss: 0.154217, loss_sup: 0.018679, loss_mps: 0.049660, loss_cps: 0.085878
[13:04:52.052] iteration 10594: total_loss: 0.447104, loss_sup: 0.106164, loss_mps: 0.107626, loss_cps: 0.233314
[13:04:52.198] iteration 10595: total_loss: 0.173344, loss_sup: 0.031536, loss_mps: 0.052006, loss_cps: 0.089802
[13:04:52.345] iteration 10596: total_loss: 0.390665, loss_sup: 0.204739, loss_mps: 0.065079, loss_cps: 0.120847
[13:04:52.491] iteration 10597: total_loss: 0.112119, loss_sup: 0.016438, loss_mps: 0.036908, loss_cps: 0.058772
[13:04:52.639] iteration 10598: total_loss: 0.458441, loss_sup: 0.194676, loss_mps: 0.088392, loss_cps: 0.175373
[13:04:52.785] iteration 10599: total_loss: 0.240998, loss_sup: 0.070570, loss_mps: 0.059844, loss_cps: 0.110584
[13:04:52.931] iteration 10600: total_loss: 0.169051, loss_sup: 0.047082, loss_mps: 0.047114, loss_cps: 0.074855
[13:04:52.932] Evaluation Started ==>
[13:05:04.305] ==> valid iteration 10600: unet metrics: {'dc': 0.6440089539671502, 'jc': 0.5199756202903907, 'pre': 0.7488384369653519, 'hd': 5.876762441886676}, ynet metrics: {'dc': 0.6093043042105487, 'jc': 0.4908660046927014, 'pre': 0.7855947065024544, 'hd': 5.8016061296323125}.
[13:05:04.308] Evaluation Finished!⏹️
[13:05:04.461] iteration 10601: total_loss: 0.483576, loss_sup: 0.269690, loss_mps: 0.073985, loss_cps: 0.139901
[13:05:04.612] iteration 10602: total_loss: 0.144383, loss_sup: 0.015174, loss_mps: 0.045574, loss_cps: 0.083636
[13:05:04.758] iteration 10603: total_loss: 0.169723, loss_sup: 0.038951, loss_mps: 0.046171, loss_cps: 0.084601
[13:05:04.903] iteration 10604: total_loss: 0.223613, loss_sup: 0.065198, loss_mps: 0.057818, loss_cps: 0.100597
[13:05:05.048] iteration 10605: total_loss: 0.190236, loss_sup: 0.010758, loss_mps: 0.061881, loss_cps: 0.117597
[13:05:05.194] iteration 10606: total_loss: 0.445434, loss_sup: 0.223048, loss_mps: 0.074337, loss_cps: 0.148048
[13:05:05.341] iteration 10607: total_loss: 0.179305, loss_sup: 0.035943, loss_mps: 0.052832, loss_cps: 0.090529
[13:05:05.487] iteration 10608: total_loss: 0.233229, loss_sup: 0.090743, loss_mps: 0.050649, loss_cps: 0.091837
[13:05:05.632] iteration 10609: total_loss: 0.278786, loss_sup: 0.101526, loss_mps: 0.060867, loss_cps: 0.116392
[13:05:05.778] iteration 10610: total_loss: 0.221707, loss_sup: 0.043140, loss_mps: 0.063061, loss_cps: 0.115506
[13:05:05.923] iteration 10611: total_loss: 0.260942, loss_sup: 0.050421, loss_mps: 0.074932, loss_cps: 0.135590
[13:05:06.071] iteration 10612: total_loss: 0.400281, loss_sup: 0.136677, loss_mps: 0.088014, loss_cps: 0.175589
[13:05:06.217] iteration 10613: total_loss: 0.239034, loss_sup: 0.051187, loss_mps: 0.067652, loss_cps: 0.120194
[13:05:06.363] iteration 10614: total_loss: 0.293568, loss_sup: 0.093443, loss_mps: 0.067413, loss_cps: 0.132711
[13:05:06.508] iteration 10615: total_loss: 0.195304, loss_sup: 0.022128, loss_mps: 0.059313, loss_cps: 0.113863
[13:05:06.653] iteration 10616: total_loss: 0.260905, loss_sup: 0.097359, loss_mps: 0.058616, loss_cps: 0.104930
[13:05:06.799] iteration 10617: total_loss: 0.225237, loss_sup: 0.066752, loss_mps: 0.056793, loss_cps: 0.101692
[13:05:06.945] iteration 10618: total_loss: 0.393297, loss_sup: 0.209262, loss_mps: 0.066187, loss_cps: 0.117849
[13:05:07.091] iteration 10619: total_loss: 0.239275, loss_sup: 0.063375, loss_mps: 0.061141, loss_cps: 0.114759
[13:05:07.236] iteration 10620: total_loss: 0.204432, loss_sup: 0.044170, loss_mps: 0.058195, loss_cps: 0.102067
[13:05:07.382] iteration 10621: total_loss: 0.335730, loss_sup: 0.184710, loss_mps: 0.054645, loss_cps: 0.096375
[13:05:07.532] iteration 10622: total_loss: 0.205889, loss_sup: 0.023670, loss_mps: 0.063723, loss_cps: 0.118495
[13:05:07.677] iteration 10623: total_loss: 0.190800, loss_sup: 0.024962, loss_mps: 0.059272, loss_cps: 0.106566
[13:05:07.823] iteration 10624: total_loss: 0.274848, loss_sup: 0.075662, loss_mps: 0.068833, loss_cps: 0.130353
[13:05:07.970] iteration 10625: total_loss: 0.261474, loss_sup: 0.061639, loss_mps: 0.068160, loss_cps: 0.131675
[13:05:08.117] iteration 10626: total_loss: 0.156621, loss_sup: 0.022660, loss_mps: 0.049659, loss_cps: 0.084301
[13:05:08.263] iteration 10627: total_loss: 0.213782, loss_sup: 0.081000, loss_mps: 0.052849, loss_cps: 0.079933
[13:05:08.409] iteration 10628: total_loss: 0.395177, loss_sup: 0.087433, loss_mps: 0.101494, loss_cps: 0.206250
[13:05:08.554] iteration 10629: total_loss: 0.304114, loss_sup: 0.069078, loss_mps: 0.078815, loss_cps: 0.156221
[13:05:08.703] iteration 10630: total_loss: 0.225639, loss_sup: 0.041888, loss_mps: 0.065439, loss_cps: 0.118312
[13:05:08.848] iteration 10631: total_loss: 0.229982, loss_sup: 0.055859, loss_mps: 0.066817, loss_cps: 0.107306
[13:05:08.993] iteration 10632: total_loss: 0.216865, loss_sup: 0.038229, loss_mps: 0.062814, loss_cps: 0.115822
[13:05:09.139] iteration 10633: total_loss: 0.304356, loss_sup: 0.109731, loss_mps: 0.067146, loss_cps: 0.127479
[13:05:09.287] iteration 10634: total_loss: 0.268531, loss_sup: 0.130453, loss_mps: 0.048719, loss_cps: 0.089359
[13:05:09.433] iteration 10635: total_loss: 0.182111, loss_sup: 0.033686, loss_mps: 0.054265, loss_cps: 0.094160
[13:05:09.579] iteration 10636: total_loss: 0.188876, loss_sup: 0.031180, loss_mps: 0.056119, loss_cps: 0.101577
[13:05:09.725] iteration 10637: total_loss: 0.325093, loss_sup: 0.053784, loss_mps: 0.085599, loss_cps: 0.185710
[13:05:09.872] iteration 10638: total_loss: 0.197084, loss_sup: 0.071366, loss_mps: 0.049581, loss_cps: 0.076137
[13:05:10.018] iteration 10639: total_loss: 0.451656, loss_sup: 0.290995, loss_mps: 0.056604, loss_cps: 0.104057
[13:05:10.165] iteration 10640: total_loss: 0.134855, loss_sup: 0.018369, loss_mps: 0.043535, loss_cps: 0.072951
[13:05:10.310] iteration 10641: total_loss: 0.269235, loss_sup: 0.110060, loss_mps: 0.053519, loss_cps: 0.105656
[13:05:10.460] iteration 10642: total_loss: 0.442589, loss_sup: 0.168088, loss_mps: 0.092265, loss_cps: 0.182237
[13:05:10.606] iteration 10643: total_loss: 0.303215, loss_sup: 0.074277, loss_mps: 0.074307, loss_cps: 0.154630
[13:05:10.754] iteration 10644: total_loss: 0.410227, loss_sup: 0.090415, loss_mps: 0.103100, loss_cps: 0.216712
[13:05:10.900] iteration 10645: total_loss: 0.150792, loss_sup: 0.014696, loss_mps: 0.048498, loss_cps: 0.087598
[13:05:11.047] iteration 10646: total_loss: 0.336664, loss_sup: 0.115794, loss_mps: 0.073527, loss_cps: 0.147343
[13:05:11.194] iteration 10647: total_loss: 0.114735, loss_sup: 0.008062, loss_mps: 0.041455, loss_cps: 0.065217
[13:05:11.339] iteration 10648: total_loss: 0.349816, loss_sup: 0.169344, loss_mps: 0.061993, loss_cps: 0.118479
[13:05:11.485] iteration 10649: total_loss: 0.311628, loss_sup: 0.105906, loss_mps: 0.067765, loss_cps: 0.137957
[13:05:11.631] iteration 10650: total_loss: 0.423913, loss_sup: 0.245144, loss_mps: 0.060122, loss_cps: 0.118647
[13:05:11.780] iteration 10651: total_loss: 0.666443, loss_sup: 0.236135, loss_mps: 0.131498, loss_cps: 0.298810
[13:05:11.926] iteration 10652: total_loss: 0.491643, loss_sup: 0.068783, loss_mps: 0.129216, loss_cps: 0.293644
[13:05:12.072] iteration 10653: total_loss: 0.406922, loss_sup: 0.143686, loss_mps: 0.089111, loss_cps: 0.174125
[13:05:12.219] iteration 10654: total_loss: 0.143124, loss_sup: 0.025667, loss_mps: 0.042403, loss_cps: 0.075054
[13:05:12.365] iteration 10655: total_loss: 0.219663, loss_sup: 0.068337, loss_mps: 0.055417, loss_cps: 0.095910
[13:05:12.511] iteration 10656: total_loss: 0.264096, loss_sup: 0.098233, loss_mps: 0.057940, loss_cps: 0.107923
[13:05:12.657] iteration 10657: total_loss: 0.348535, loss_sup: 0.024348, loss_mps: 0.101492, loss_cps: 0.222695
[13:05:12.805] iteration 10658: total_loss: 0.317028, loss_sup: 0.111012, loss_mps: 0.073251, loss_cps: 0.132765
[13:05:12.951] iteration 10659: total_loss: 0.278696, loss_sup: 0.055085, loss_mps: 0.075050, loss_cps: 0.148561
[13:05:13.097] iteration 10660: total_loss: 0.432972, loss_sup: 0.239531, loss_mps: 0.066111, loss_cps: 0.127331
[13:05:13.242] iteration 10661: total_loss: 0.322024, loss_sup: 0.107174, loss_mps: 0.073605, loss_cps: 0.141244
[13:05:13.388] iteration 10662: total_loss: 0.218482, loss_sup: 0.029222, loss_mps: 0.065834, loss_cps: 0.123427
[13:05:13.534] iteration 10663: total_loss: 0.306509, loss_sup: 0.071450, loss_mps: 0.082623, loss_cps: 0.152436
[13:05:13.682] iteration 10664: total_loss: 0.426952, loss_sup: 0.143863, loss_mps: 0.093420, loss_cps: 0.189669
[13:05:13.830] iteration 10665: total_loss: 0.243893, loss_sup: 0.023510, loss_mps: 0.077102, loss_cps: 0.143281
[13:05:13.976] iteration 10666: total_loss: 0.284995, loss_sup: 0.046096, loss_mps: 0.081355, loss_cps: 0.157544
[13:05:14.122] iteration 10667: total_loss: 0.294501, loss_sup: 0.041737, loss_mps: 0.085579, loss_cps: 0.167184
[13:05:14.270] iteration 10668: total_loss: 0.270524, loss_sup: 0.109580, loss_mps: 0.060814, loss_cps: 0.100130
[13:05:14.419] iteration 10669: total_loss: 0.266458, loss_sup: 0.088103, loss_mps: 0.063222, loss_cps: 0.115133
[13:05:14.564] iteration 10670: total_loss: 0.231258, loss_sup: 0.008930, loss_mps: 0.076107, loss_cps: 0.146222
[13:05:14.710] iteration 10671: total_loss: 0.181749, loss_sup: 0.029568, loss_mps: 0.055517, loss_cps: 0.096664
[13:05:14.857] iteration 10672: total_loss: 0.187810, loss_sup: 0.020434, loss_mps: 0.059714, loss_cps: 0.107662
[13:05:15.003] iteration 10673: total_loss: 0.289262, loss_sup: 0.125434, loss_mps: 0.058341, loss_cps: 0.105487
[13:05:15.149] iteration 10674: total_loss: 0.366822, loss_sup: 0.155176, loss_mps: 0.072230, loss_cps: 0.139416
[13:05:15.295] iteration 10675: total_loss: 0.283605, loss_sup: 0.091974, loss_mps: 0.069067, loss_cps: 0.122565
[13:05:15.442] iteration 10676: total_loss: 0.195419, loss_sup: 0.029974, loss_mps: 0.058444, loss_cps: 0.107000
[13:05:15.588] iteration 10677: total_loss: 0.234539, loss_sup: 0.024194, loss_mps: 0.074947, loss_cps: 0.135397
[13:05:15.734] iteration 10678: total_loss: 0.240625, loss_sup: 0.060518, loss_mps: 0.063205, loss_cps: 0.116901
[13:05:15.880] iteration 10679: total_loss: 0.376319, loss_sup: 0.152065, loss_mps: 0.078634, loss_cps: 0.145620
[13:05:16.027] iteration 10680: total_loss: 0.280158, loss_sup: 0.112407, loss_mps: 0.059694, loss_cps: 0.108057
[13:05:16.172] iteration 10681: total_loss: 0.189494, loss_sup: 0.038939, loss_mps: 0.055327, loss_cps: 0.095228
[13:05:16.318] iteration 10682: total_loss: 0.192136, loss_sup: 0.054237, loss_mps: 0.048983, loss_cps: 0.088916
[13:05:16.464] iteration 10683: total_loss: 0.320030, loss_sup: 0.134495, loss_mps: 0.067913, loss_cps: 0.117622
[13:05:16.610] iteration 10684: total_loss: 0.315236, loss_sup: 0.146679, loss_mps: 0.057983, loss_cps: 0.110574
[13:05:16.756] iteration 10685: total_loss: 0.224759, loss_sup: 0.014735, loss_mps: 0.073068, loss_cps: 0.136956
[13:05:16.903] iteration 10686: total_loss: 0.270484, loss_sup: 0.077640, loss_mps: 0.064685, loss_cps: 0.128160
[13:05:17.050] iteration 10687: total_loss: 0.378567, loss_sup: 0.153583, loss_mps: 0.076419, loss_cps: 0.148565
[13:05:17.196] iteration 10688: total_loss: 0.440930, loss_sup: 0.110333, loss_mps: 0.104698, loss_cps: 0.225899
[13:05:17.343] iteration 10689: total_loss: 0.393861, loss_sup: 0.121298, loss_mps: 0.091928, loss_cps: 0.180635
[13:05:17.489] iteration 10690: total_loss: 0.192618, loss_sup: 0.012116, loss_mps: 0.061967, loss_cps: 0.118535
[13:05:17.636] iteration 10691: total_loss: 0.402768, loss_sup: 0.159699, loss_mps: 0.080955, loss_cps: 0.162115
[13:05:17.784] iteration 10692: total_loss: 0.192709, loss_sup: 0.054282, loss_mps: 0.052079, loss_cps: 0.086349
[13:05:17.930] iteration 10693: total_loss: 0.269439, loss_sup: 0.117586, loss_mps: 0.055615, loss_cps: 0.096239
[13:05:18.076] iteration 10694: total_loss: 0.420311, loss_sup: 0.251409, loss_mps: 0.062274, loss_cps: 0.106628
[13:05:18.222] iteration 10695: total_loss: 0.232732, loss_sup: 0.085391, loss_mps: 0.052542, loss_cps: 0.094798
[13:05:18.368] iteration 10696: total_loss: 0.184169, loss_sup: 0.053728, loss_mps: 0.048182, loss_cps: 0.082259
[13:05:18.514] iteration 10697: total_loss: 0.178497, loss_sup: 0.023339, loss_mps: 0.056613, loss_cps: 0.098544
[13:05:18.662] iteration 10698: total_loss: 0.320480, loss_sup: 0.097416, loss_mps: 0.074231, loss_cps: 0.148834
[13:05:18.810] iteration 10699: total_loss: 0.398086, loss_sup: 0.247052, loss_mps: 0.054786, loss_cps: 0.096248
[13:05:18.957] iteration 10700: total_loss: 0.270420, loss_sup: 0.093355, loss_mps: 0.061920, loss_cps: 0.115146
[13:05:18.957] Evaluation Started ==>
[13:05:30.338] ==> valid iteration 10700: unet metrics: {'dc': 0.6517862667642935, 'jc': 0.5271772789826055, 'pre': 0.7427717400806588, 'hd': 5.832526890813687}, ynet metrics: {'dc': 0.5902471930443116, 'jc': 0.47083588642178725, 'pre': 0.7496408785756266, 'hd': 5.9417421731727025}.
[13:05:30.340] Evaluation Finished!⏹️
[13:05:30.493] iteration 10701: total_loss: 0.232050, loss_sup: 0.054327, loss_mps: 0.063028, loss_cps: 0.114695
[13:05:30.643] iteration 10702: total_loss: 0.476112, loss_sup: 0.075045, loss_mps: 0.118095, loss_cps: 0.282972
[13:05:30.788] iteration 10703: total_loss: 0.277099, loss_sup: 0.086972, loss_mps: 0.066245, loss_cps: 0.123882
[13:05:30.940] iteration 10704: total_loss: 0.229588, loss_sup: 0.054468, loss_mps: 0.058705, loss_cps: 0.116416
[13:05:31.086] iteration 10705: total_loss: 0.188268, loss_sup: 0.037616, loss_mps: 0.053365, loss_cps: 0.097287
[13:05:31.232] iteration 10706: total_loss: 0.288977, loss_sup: 0.076264, loss_mps: 0.073462, loss_cps: 0.139251
[13:05:31.378] iteration 10707: total_loss: 0.328184, loss_sup: 0.163493, loss_mps: 0.056953, loss_cps: 0.107739
[13:05:31.527] iteration 10708: total_loss: 0.213269, loss_sup: 0.061201, loss_mps: 0.052434, loss_cps: 0.099633
[13:05:31.673] iteration 10709: total_loss: 0.335956, loss_sup: 0.105322, loss_mps: 0.079451, loss_cps: 0.151182
[13:05:31.819] iteration 10710: total_loss: 0.254912, loss_sup: 0.010782, loss_mps: 0.083006, loss_cps: 0.161124
[13:05:31.967] iteration 10711: total_loss: 0.154362, loss_sup: 0.020178, loss_mps: 0.052490, loss_cps: 0.081695
[13:05:32.113] iteration 10712: total_loss: 0.228024, loss_sup: 0.038967, loss_mps: 0.065745, loss_cps: 0.123313
[13:05:32.258] iteration 10713: total_loss: 0.194674, loss_sup: 0.044928, loss_mps: 0.055769, loss_cps: 0.093976
[13:05:32.404] iteration 10714: total_loss: 0.144039, loss_sup: 0.017440, loss_mps: 0.046028, loss_cps: 0.080571
[13:05:32.551] iteration 10715: total_loss: 0.260780, loss_sup: 0.053141, loss_mps: 0.070928, loss_cps: 0.136710
[13:05:32.697] iteration 10716: total_loss: 0.209005, loss_sup: 0.067498, loss_mps: 0.051983, loss_cps: 0.089524
[13:05:32.843] iteration 10717: total_loss: 0.384681, loss_sup: 0.138831, loss_mps: 0.083591, loss_cps: 0.162259
[13:05:32.989] iteration 10718: total_loss: 0.258750, loss_sup: 0.077686, loss_mps: 0.063063, loss_cps: 0.118002
[13:05:33.134] iteration 10719: total_loss: 0.261222, loss_sup: 0.087872, loss_mps: 0.058232, loss_cps: 0.115118
[13:05:33.280] iteration 10720: total_loss: 0.579216, loss_sup: 0.218998, loss_mps: 0.113901, loss_cps: 0.246317
[13:05:33.426] iteration 10721: total_loss: 0.170003, loss_sup: 0.021812, loss_mps: 0.053203, loss_cps: 0.094989
[13:05:33.572] iteration 10722: total_loss: 0.207180, loss_sup: 0.092115, loss_mps: 0.044871, loss_cps: 0.070194
[13:05:33.719] iteration 10723: total_loss: 0.227532, loss_sup: 0.025889, loss_mps: 0.072383, loss_cps: 0.129260
[13:05:33.864] iteration 10724: total_loss: 0.240717, loss_sup: 0.009328, loss_mps: 0.078578, loss_cps: 0.152811
[13:05:34.010] iteration 10725: total_loss: 0.265413, loss_sup: 0.048136, loss_mps: 0.075780, loss_cps: 0.141497
[13:05:34.157] iteration 10726: total_loss: 0.254358, loss_sup: 0.041287, loss_mps: 0.072550, loss_cps: 0.140520
[13:05:34.303] iteration 10727: total_loss: 0.485861, loss_sup: 0.200204, loss_mps: 0.092660, loss_cps: 0.192997
[13:05:34.449] iteration 10728: total_loss: 0.288862, loss_sup: 0.017304, loss_mps: 0.094067, loss_cps: 0.177490
[13:05:34.595] iteration 10729: total_loss: 0.342266, loss_sup: 0.151620, loss_mps: 0.070058, loss_cps: 0.120589
[13:05:34.742] iteration 10730: total_loss: 0.361070, loss_sup: 0.025371, loss_mps: 0.110251, loss_cps: 0.225448
[13:05:34.888] iteration 10731: total_loss: 0.214246, loss_sup: 0.058085, loss_mps: 0.056930, loss_cps: 0.099231
[13:05:35.034] iteration 10732: total_loss: 0.328638, loss_sup: 0.160316, loss_mps: 0.060872, loss_cps: 0.107449
[13:05:35.179] iteration 10733: total_loss: 0.188987, loss_sup: 0.026571, loss_mps: 0.060578, loss_cps: 0.101838
[13:05:35.327] iteration 10734: total_loss: 0.199848, loss_sup: 0.015771, loss_mps: 0.060578, loss_cps: 0.123499
[13:05:35.473] iteration 10735: total_loss: 0.269550, loss_sup: 0.030673, loss_mps: 0.080587, loss_cps: 0.158290
[13:05:35.619] iteration 10736: total_loss: 0.195072, loss_sup: 0.050804, loss_mps: 0.050583, loss_cps: 0.093686
[13:05:35.764] iteration 10737: total_loss: 0.160610, loss_sup: 0.042762, loss_mps: 0.044451, loss_cps: 0.073397
[13:05:35.910] iteration 10738: total_loss: 0.320042, loss_sup: 0.129788, loss_mps: 0.065181, loss_cps: 0.125074
[13:05:36.058] iteration 10739: total_loss: 0.315220, loss_sup: 0.149502, loss_mps: 0.055812, loss_cps: 0.109906
[13:05:36.204] iteration 10740: total_loss: 0.423638, loss_sup: 0.098863, loss_mps: 0.107303, loss_cps: 0.217472
[13:05:36.355] iteration 10741: total_loss: 0.188999, loss_sup: 0.044389, loss_mps: 0.051401, loss_cps: 0.093209
[13:05:36.501] iteration 10742: total_loss: 0.190563, loss_sup: 0.050713, loss_mps: 0.051681, loss_cps: 0.088169
[13:05:36.647] iteration 10743: total_loss: 0.168551, loss_sup: 0.005555, loss_mps: 0.056334, loss_cps: 0.106662
[13:05:36.793] iteration 10744: total_loss: 0.105520, loss_sup: 0.008576, loss_mps: 0.036486, loss_cps: 0.060458
[13:05:36.939] iteration 10745: total_loss: 0.451066, loss_sup: 0.062041, loss_mps: 0.118024, loss_cps: 0.271001
[13:05:37.085] iteration 10746: total_loss: 0.304562, loss_sup: 0.051486, loss_mps: 0.083656, loss_cps: 0.169419
[13:05:37.237] iteration 10747: total_loss: 0.202061, loss_sup: 0.027574, loss_mps: 0.060761, loss_cps: 0.113726
[13:05:37.383] iteration 10748: total_loss: 0.172181, loss_sup: 0.034394, loss_mps: 0.047731, loss_cps: 0.090056
[13:05:37.531] iteration 10749: total_loss: 0.208441, loss_sup: 0.009767, loss_mps: 0.065772, loss_cps: 0.132901
[13:05:37.677] iteration 10750: total_loss: 0.422355, loss_sup: 0.172567, loss_mps: 0.078334, loss_cps: 0.171454
[13:05:37.823] iteration 10751: total_loss: 0.329505, loss_sup: 0.134892, loss_mps: 0.065499, loss_cps: 0.129114
[13:05:37.969] iteration 10752: total_loss: 0.089778, loss_sup: 0.007591, loss_mps: 0.030941, loss_cps: 0.051246
[13:05:38.115] iteration 10753: total_loss: 0.250555, loss_sup: 0.093049, loss_mps: 0.050767, loss_cps: 0.106738
[13:05:38.261] iteration 10754: total_loss: 0.211020, loss_sup: 0.055477, loss_mps: 0.053342, loss_cps: 0.102200
[13:05:38.408] iteration 10755: total_loss: 0.463910, loss_sup: 0.110785, loss_mps: 0.108086, loss_cps: 0.245039
[13:05:38.556] iteration 10756: total_loss: 0.112908, loss_sup: 0.024843, loss_mps: 0.035075, loss_cps: 0.052990
[13:05:38.705] iteration 10757: total_loss: 0.362813, loss_sup: 0.141308, loss_mps: 0.072630, loss_cps: 0.148875
[13:05:38.851] iteration 10758: total_loss: 0.231813, loss_sup: 0.027372, loss_mps: 0.065759, loss_cps: 0.138682
[13:05:38.997] iteration 10759: total_loss: 0.164975, loss_sup: 0.012986, loss_mps: 0.053131, loss_cps: 0.098858
[13:05:39.143] iteration 10760: total_loss: 0.267196, loss_sup: 0.077066, loss_mps: 0.063367, loss_cps: 0.126762
[13:05:39.290] iteration 10761: total_loss: 0.267852, loss_sup: 0.060725, loss_mps: 0.070786, loss_cps: 0.136340
[13:05:39.436] iteration 10762: total_loss: 0.289234, loss_sup: 0.050399, loss_mps: 0.086230, loss_cps: 0.152606
[13:05:39.586] iteration 10763: total_loss: 0.250418, loss_sup: 0.080498, loss_mps: 0.059510, loss_cps: 0.110410
[13:05:39.732] iteration 10764: total_loss: 0.382478, loss_sup: 0.133613, loss_mps: 0.084573, loss_cps: 0.164292
[13:05:39.879] iteration 10765: total_loss: 0.269939, loss_sup: 0.102195, loss_mps: 0.060417, loss_cps: 0.107327
[13:05:40.026] iteration 10766: total_loss: 0.207115, loss_sup: 0.040546, loss_mps: 0.057507, loss_cps: 0.109062
[13:05:40.175] iteration 10767: total_loss: 0.566144, loss_sup: 0.220225, loss_mps: 0.112620, loss_cps: 0.233299
[13:05:40.322] iteration 10768: total_loss: 0.336421, loss_sup: 0.059361, loss_mps: 0.089920, loss_cps: 0.187141
[13:05:40.469] iteration 10769: total_loss: 0.297902, loss_sup: 0.107211, loss_mps: 0.065857, loss_cps: 0.124834
[13:05:40.620] iteration 10770: total_loss: 0.287403, loss_sup: 0.136346, loss_mps: 0.053234, loss_cps: 0.097823
[13:05:40.768] iteration 10771: total_loss: 0.269698, loss_sup: 0.024310, loss_mps: 0.081798, loss_cps: 0.163590
[13:05:40.917] iteration 10772: total_loss: 0.443245, loss_sup: 0.159229, loss_mps: 0.090083, loss_cps: 0.193933
[13:05:41.063] iteration 10773: total_loss: 0.245043, loss_sup: 0.071658, loss_mps: 0.057190, loss_cps: 0.116194
[13:05:41.209] iteration 10774: total_loss: 0.256321, loss_sup: 0.033217, loss_mps: 0.074339, loss_cps: 0.148765
[13:05:41.359] iteration 10775: total_loss: 0.301438, loss_sup: 0.037417, loss_mps: 0.082971, loss_cps: 0.181050
[13:05:41.508] iteration 10776: total_loss: 0.305642, loss_sup: 0.044001, loss_mps: 0.084901, loss_cps: 0.176740
[13:05:41.657] iteration 10777: total_loss: 0.299970, loss_sup: 0.090065, loss_mps: 0.068157, loss_cps: 0.141748
[13:05:41.803] iteration 10778: total_loss: 0.388934, loss_sup: 0.097612, loss_mps: 0.094596, loss_cps: 0.196727
[13:05:41.949] iteration 10779: total_loss: 0.239603, loss_sup: 0.056752, loss_mps: 0.064522, loss_cps: 0.118330
[13:05:42.096] iteration 10780: total_loss: 0.276428, loss_sup: 0.042422, loss_mps: 0.077851, loss_cps: 0.156155
[13:05:42.242] iteration 10781: total_loss: 0.369428, loss_sup: 0.022519, loss_mps: 0.109084, loss_cps: 0.237826
[13:05:42.388] iteration 10782: total_loss: 0.360860, loss_sup: 0.100383, loss_mps: 0.083898, loss_cps: 0.176580
[13:05:42.535] iteration 10783: total_loss: 0.196548, loss_sup: 0.016987, loss_mps: 0.064336, loss_cps: 0.115225
[13:05:42.683] iteration 10784: total_loss: 0.342421, loss_sup: 0.063176, loss_mps: 0.095030, loss_cps: 0.184215
[13:05:42.831] iteration 10785: total_loss: 0.166465, loss_sup: 0.004455, loss_mps: 0.057713, loss_cps: 0.104297
[13:05:42.976] iteration 10786: total_loss: 0.266513, loss_sup: 0.038696, loss_mps: 0.078892, loss_cps: 0.148925
[13:05:43.123] iteration 10787: total_loss: 0.351982, loss_sup: 0.093930, loss_mps: 0.086325, loss_cps: 0.171726
[13:05:43.269] iteration 10788: total_loss: 0.390682, loss_sup: 0.103244, loss_mps: 0.094882, loss_cps: 0.192556
[13:05:43.415] iteration 10789: total_loss: 0.259035, loss_sup: 0.048108, loss_mps: 0.073134, loss_cps: 0.137793
[13:05:43.561] iteration 10790: total_loss: 0.182713, loss_sup: 0.015115, loss_mps: 0.060856, loss_cps: 0.106742
[13:05:43.708] iteration 10791: total_loss: 0.222641, loss_sup: 0.067046, loss_mps: 0.055993, loss_cps: 0.099602
[13:05:43.854] iteration 10792: total_loss: 0.400521, loss_sup: 0.172503, loss_mps: 0.079250, loss_cps: 0.148768
[13:05:44.002] iteration 10793: total_loss: 0.521469, loss_sup: 0.266831, loss_mps: 0.085424, loss_cps: 0.169214
[13:05:44.149] iteration 10794: total_loss: 0.172393, loss_sup: 0.026713, loss_mps: 0.051852, loss_cps: 0.093827
[13:05:44.294] iteration 10795: total_loss: 0.398228, loss_sup: 0.038082, loss_mps: 0.115116, loss_cps: 0.245030
[13:05:44.441] iteration 10796: total_loss: 0.230557, loss_sup: 0.046097, loss_mps: 0.064376, loss_cps: 0.120083
[13:05:44.587] iteration 10797: total_loss: 0.208344, loss_sup: 0.038307, loss_mps: 0.058374, loss_cps: 0.111663
[13:05:44.733] iteration 10798: total_loss: 0.338507, loss_sup: 0.060369, loss_mps: 0.093903, loss_cps: 0.184235
[13:05:44.880] iteration 10799: total_loss: 0.557719, loss_sup: 0.211677, loss_mps: 0.110982, loss_cps: 0.235060
[13:05:45.026] iteration 10800: total_loss: 0.377414, loss_sup: 0.201725, loss_mps: 0.063226, loss_cps: 0.112463
[13:05:45.026] Evaluation Started ==>
[13:05:56.388] ==> valid iteration 10800: unet metrics: {'dc': 0.6685254219829251, 'jc': 0.5445304809276639, 'pre': 0.7233141923314453, 'hd': 5.942649130684992}, ynet metrics: {'dc': 0.6250952864286772, 'jc': 0.5035471728621241, 'pre': 0.7646159123378178, 'hd': 5.811310806984742}.
[13:05:56.450] ==> New best valid dice for unet: 0.668525, at iteration 10800
[13:05:56.608] ==> New best valid dice for ynet: 0.625095, at iteration 10800
[13:05:56.611] Evaluation Finished!⏹️
[13:05:56.762] iteration 10801: total_loss: 0.280186, loss_sup: 0.047189, loss_mps: 0.080265, loss_cps: 0.152732
[13:05:56.911] iteration 10802: total_loss: 0.313753, loss_sup: 0.020687, loss_mps: 0.095137, loss_cps: 0.197929
[13:05:57.059] iteration 10803: total_loss: 0.423396, loss_sup: 0.136841, loss_mps: 0.099714, loss_cps: 0.186841
[13:05:57.205] iteration 10804: total_loss: 0.208631, loss_sup: 0.029859, loss_mps: 0.060575, loss_cps: 0.118197
[13:05:57.352] iteration 10805: total_loss: 0.302117, loss_sup: 0.131786, loss_mps: 0.060960, loss_cps: 0.109372
[13:05:57.497] iteration 10806: total_loss: 0.355268, loss_sup: 0.101798, loss_mps: 0.084439, loss_cps: 0.169030
[13:05:57.643] iteration 10807: total_loss: 0.256804, loss_sup: 0.099968, loss_mps: 0.056205, loss_cps: 0.100631
[13:05:57.789] iteration 10808: total_loss: 0.245986, loss_sup: 0.065797, loss_mps: 0.061785, loss_cps: 0.118404
[13:05:57.934] iteration 10809: total_loss: 0.161271, loss_sup: 0.017738, loss_mps: 0.051331, loss_cps: 0.092202
[13:05:58.084] iteration 10810: total_loss: 0.236272, loss_sup: 0.053033, loss_mps: 0.060397, loss_cps: 0.122842
[13:05:58.231] iteration 10811: total_loss: 0.505509, loss_sup: 0.227685, loss_mps: 0.091344, loss_cps: 0.186481
[13:05:58.377] iteration 10812: total_loss: 0.217136, loss_sup: 0.012648, loss_mps: 0.065699, loss_cps: 0.138789
[13:05:58.523] iteration 10813: total_loss: 0.177575, loss_sup: 0.016908, loss_mps: 0.057472, loss_cps: 0.103196
[13:05:58.669] iteration 10814: total_loss: 0.288378, loss_sup: 0.067550, loss_mps: 0.073164, loss_cps: 0.147663
[13:05:58.815] iteration 10815: total_loss: 0.198571, loss_sup: 0.039694, loss_mps: 0.057510, loss_cps: 0.101367
[13:05:58.961] iteration 10816: total_loss: 0.165529, loss_sup: 0.041090, loss_mps: 0.047121, loss_cps: 0.077318
[13:05:59.107] iteration 10817: total_loss: 0.173671, loss_sup: 0.030157, loss_mps: 0.052234, loss_cps: 0.091280
[13:05:59.254] iteration 10818: total_loss: 0.115849, loss_sup: 0.013453, loss_mps: 0.039335, loss_cps: 0.063060
[13:05:59.400] iteration 10819: total_loss: 0.262211, loss_sup: 0.066670, loss_mps: 0.066530, loss_cps: 0.129011
[13:05:59.547] iteration 10820: total_loss: 0.354962, loss_sup: 0.125377, loss_mps: 0.079033, loss_cps: 0.150552
[13:05:59.694] iteration 10821: total_loss: 0.214526, loss_sup: 0.047118, loss_mps: 0.058951, loss_cps: 0.108458
[13:05:59.841] iteration 10822: total_loss: 0.177120, loss_sup: 0.024634, loss_mps: 0.056887, loss_cps: 0.095599
[13:05:59.987] iteration 10823: total_loss: 0.303161, loss_sup: 0.183929, loss_mps: 0.045041, loss_cps: 0.074191
[13:06:00.134] iteration 10824: total_loss: 0.442131, loss_sup: 0.200066, loss_mps: 0.081457, loss_cps: 0.160608
[13:06:00.279] iteration 10825: total_loss: 0.130255, loss_sup: 0.028867, loss_mps: 0.038979, loss_cps: 0.062409
[13:06:00.426] iteration 10826: total_loss: 0.269778, loss_sup: 0.082617, loss_mps: 0.065414, loss_cps: 0.121746
[13:06:00.572] iteration 10827: total_loss: 0.279905, loss_sup: 0.083131, loss_mps: 0.065700, loss_cps: 0.131074
[13:06:00.717] iteration 10828: total_loss: 0.315392, loss_sup: 0.042435, loss_mps: 0.086411, loss_cps: 0.186546
[13:06:00.863] iteration 10829: total_loss: 0.276325, loss_sup: 0.046132, loss_mps: 0.076790, loss_cps: 0.153403
[13:06:01.011] iteration 10830: total_loss: 0.231753, loss_sup: 0.044966, loss_mps: 0.070346, loss_cps: 0.116440
[13:06:01.157] iteration 10831: total_loss: 0.220782, loss_sup: 0.018104, loss_mps: 0.068951, loss_cps: 0.133727
[13:06:01.305] iteration 10832: total_loss: 0.144044, loss_sup: 0.023414, loss_mps: 0.044214, loss_cps: 0.076416
[13:06:01.450] iteration 10833: total_loss: 0.449615, loss_sup: 0.231660, loss_mps: 0.074463, loss_cps: 0.143492
[13:06:01.596] iteration 10834: total_loss: 0.237073, loss_sup: 0.053446, loss_mps: 0.061155, loss_cps: 0.122472
[13:06:01.742] iteration 10835: total_loss: 0.215381, loss_sup: 0.068739, loss_mps: 0.053788, loss_cps: 0.092854
[13:06:01.889] iteration 10836: total_loss: 0.264526, loss_sup: 0.105035, loss_mps: 0.056227, loss_cps: 0.103264
[13:06:02.035] iteration 10837: total_loss: 0.346253, loss_sup: 0.141913, loss_mps: 0.068501, loss_cps: 0.135839
[13:06:02.181] iteration 10838: total_loss: 0.191295, loss_sup: 0.029556, loss_mps: 0.057291, loss_cps: 0.104447
[13:06:02.327] iteration 10839: total_loss: 0.430730, loss_sup: 0.116287, loss_mps: 0.101631, loss_cps: 0.212813
[13:06:02.474] iteration 10840: total_loss: 0.481981, loss_sup: 0.097781, loss_mps: 0.124762, loss_cps: 0.259438
[13:06:02.621] iteration 10841: total_loss: 0.245962, loss_sup: 0.061321, loss_mps: 0.064195, loss_cps: 0.120446
[13:06:02.767] iteration 10842: total_loss: 0.294889, loss_sup: 0.049365, loss_mps: 0.082023, loss_cps: 0.163501
[13:06:02.914] iteration 10843: total_loss: 0.313707, loss_sup: 0.119400, loss_mps: 0.067563, loss_cps: 0.126744
[13:06:03.060] iteration 10844: total_loss: 0.278942, loss_sup: 0.069772, loss_mps: 0.074292, loss_cps: 0.134878
[13:06:03.211] iteration 10845: total_loss: 0.279166, loss_sup: 0.055391, loss_mps: 0.075924, loss_cps: 0.147851
[13:06:03.361] iteration 10846: total_loss: 0.710823, loss_sup: 0.379324, loss_mps: 0.108747, loss_cps: 0.222752
[13:06:03.508] iteration 10847: total_loss: 0.451192, loss_sup: 0.155937, loss_mps: 0.096976, loss_cps: 0.198279
[13:06:03.654] iteration 10848: total_loss: 0.311768, loss_sup: 0.153902, loss_mps: 0.055691, loss_cps: 0.102175
[13:06:03.801] iteration 10849: total_loss: 0.282049, loss_sup: 0.070347, loss_mps: 0.074037, loss_cps: 0.137665
[13:06:03.949] iteration 10850: total_loss: 0.308807, loss_sup: 0.109004, loss_mps: 0.070049, loss_cps: 0.129754
[13:06:04.095] iteration 10851: total_loss: 0.234013, loss_sup: 0.071700, loss_mps: 0.061805, loss_cps: 0.100508
[13:06:04.242] iteration 10852: total_loss: 0.359381, loss_sup: 0.044910, loss_mps: 0.102211, loss_cps: 0.212260
[13:06:04.389] iteration 10853: total_loss: 0.200438, loss_sup: 0.080003, loss_mps: 0.045592, loss_cps: 0.074843
[13:06:04.537] iteration 10854: total_loss: 0.420110, loss_sup: 0.264138, loss_mps: 0.058818, loss_cps: 0.097154
[13:06:04.682] iteration 10855: total_loss: 0.221524, loss_sup: 0.069497, loss_mps: 0.056303, loss_cps: 0.095723
[13:06:04.828] iteration 10856: total_loss: 0.561050, loss_sup: 0.185480, loss_mps: 0.118584, loss_cps: 0.256986
[13:06:04.974] iteration 10857: total_loss: 0.242808, loss_sup: 0.069822, loss_mps: 0.063456, loss_cps: 0.109530
[13:06:05.120] iteration 10858: total_loss: 0.265612, loss_sup: 0.047018, loss_mps: 0.073053, loss_cps: 0.145541
[13:06:05.267] iteration 10859: total_loss: 0.326383, loss_sup: 0.118185, loss_mps: 0.069501, loss_cps: 0.138697
[13:06:05.413] iteration 10860: total_loss: 0.470894, loss_sup: 0.217827, loss_mps: 0.083980, loss_cps: 0.169087
[13:06:05.559] iteration 10861: total_loss: 0.284692, loss_sup: 0.068691, loss_mps: 0.072154, loss_cps: 0.143848
[13:06:05.705] iteration 10862: total_loss: 0.332359, loss_sup: 0.103271, loss_mps: 0.079552, loss_cps: 0.149536
[13:06:05.851] iteration 10863: total_loss: 0.298629, loss_sup: 0.075480, loss_mps: 0.077620, loss_cps: 0.145529
[13:06:05.997] iteration 10864: total_loss: 0.446104, loss_sup: 0.141268, loss_mps: 0.098889, loss_cps: 0.205947
[13:06:06.143] iteration 10865: total_loss: 0.262188, loss_sup: 0.029250, loss_mps: 0.081610, loss_cps: 0.151328
[13:06:06.291] iteration 10866: total_loss: 0.307222, loss_sup: 0.146628, loss_mps: 0.056867, loss_cps: 0.103727
[13:06:06.437] iteration 10867: total_loss: 0.338724, loss_sup: 0.067778, loss_mps: 0.092308, loss_cps: 0.178638
[13:06:06.498] iteration 10868: total_loss: 1.065243, loss_sup: 0.498103, loss_mps: 0.178817, loss_cps: 0.388323
[13:06:07.714] iteration 10869: total_loss: 0.235322, loss_sup: 0.058176, loss_mps: 0.065887, loss_cps: 0.111260
[13:06:07.863] iteration 10870: total_loss: 0.241772, loss_sup: 0.077447, loss_mps: 0.057220, loss_cps: 0.107105
[13:06:08.009] iteration 10871: total_loss: 0.337168, loss_sup: 0.029493, loss_mps: 0.097415, loss_cps: 0.210260
[13:06:08.156] iteration 10872: total_loss: 0.651701, loss_sup: 0.223829, loss_mps: 0.130161, loss_cps: 0.297711
[13:06:08.302] iteration 10873: total_loss: 0.350886, loss_sup: 0.125860, loss_mps: 0.075329, loss_cps: 0.149697
[13:06:08.448] iteration 10874: total_loss: 0.366668, loss_sup: 0.057218, loss_mps: 0.100440, loss_cps: 0.209009
[13:06:08.594] iteration 10875: total_loss: 0.519861, loss_sup: 0.226647, loss_mps: 0.094150, loss_cps: 0.199063
[13:06:08.746] iteration 10876: total_loss: 0.342980, loss_sup: 0.107241, loss_mps: 0.081885, loss_cps: 0.153854
[13:06:08.893] iteration 10877: total_loss: 0.423896, loss_sup: 0.121542, loss_mps: 0.099821, loss_cps: 0.202533
[13:06:09.039] iteration 10878: total_loss: 0.725345, loss_sup: 0.112271, loss_mps: 0.187494, loss_cps: 0.425581
[13:06:09.188] iteration 10879: total_loss: 0.589295, loss_sup: 0.120737, loss_mps: 0.147017, loss_cps: 0.321541
[13:06:09.334] iteration 10880: total_loss: 0.438033, loss_sup: 0.123419, loss_mps: 0.100775, loss_cps: 0.213840
[13:06:09.480] iteration 10881: total_loss: 0.515833, loss_sup: 0.084043, loss_mps: 0.138961, loss_cps: 0.292829
[13:06:09.626] iteration 10882: total_loss: 0.338657, loss_sup: 0.058130, loss_mps: 0.097026, loss_cps: 0.183501
[13:06:09.772] iteration 10883: total_loss: 0.450078, loss_sup: 0.078006, loss_mps: 0.119725, loss_cps: 0.252346
[13:06:09.920] iteration 10884: total_loss: 0.422841, loss_sup: 0.133117, loss_mps: 0.093130, loss_cps: 0.196594
[13:06:10.066] iteration 10885: total_loss: 0.280482, loss_sup: 0.076542, loss_mps: 0.073504, loss_cps: 0.130436
[13:06:10.213] iteration 10886: total_loss: 0.403652, loss_sup: 0.092160, loss_mps: 0.101058, loss_cps: 0.210434
[13:06:10.359] iteration 10887: total_loss: 0.326532, loss_sup: 0.119165, loss_mps: 0.072411, loss_cps: 0.134957
[13:06:10.507] iteration 10888: total_loss: 0.249771, loss_sup: 0.062804, loss_mps: 0.068519, loss_cps: 0.118447
[13:06:10.654] iteration 10889: total_loss: 0.256860, loss_sup: 0.024475, loss_mps: 0.079605, loss_cps: 0.152780
[13:06:10.800] iteration 10890: total_loss: 0.540598, loss_sup: 0.188490, loss_mps: 0.118653, loss_cps: 0.233455
[13:06:10.948] iteration 10891: total_loss: 0.317721, loss_sup: 0.074078, loss_mps: 0.082523, loss_cps: 0.161121
[13:06:11.094] iteration 10892: total_loss: 0.478189, loss_sup: 0.210297, loss_mps: 0.087638, loss_cps: 0.180254
[13:06:11.240] iteration 10893: total_loss: 0.501692, loss_sup: 0.103218, loss_mps: 0.130315, loss_cps: 0.268158
[13:06:11.386] iteration 10894: total_loss: 0.616098, loss_sup: 0.241557, loss_mps: 0.119200, loss_cps: 0.255341
[13:06:11.537] iteration 10895: total_loss: 0.324933, loss_sup: 0.087889, loss_mps: 0.081764, loss_cps: 0.155280
[13:06:11.683] iteration 10896: total_loss: 0.353121, loss_sup: 0.108940, loss_mps: 0.084366, loss_cps: 0.159815
[13:06:11.829] iteration 10897: total_loss: 0.218140, loss_sup: 0.009575, loss_mps: 0.073224, loss_cps: 0.135342
[13:06:11.975] iteration 10898: total_loss: 0.223964, loss_sup: 0.057158, loss_mps: 0.063583, loss_cps: 0.103224
[13:06:12.121] iteration 10899: total_loss: 0.210843, loss_sup: 0.035003, loss_mps: 0.066935, loss_cps: 0.108905
[13:06:12.267] iteration 10900: total_loss: 0.322433, loss_sup: 0.108421, loss_mps: 0.076684, loss_cps: 0.137329
[13:06:12.268] Evaluation Started ==>
[13:06:23.768] ==> valid iteration 10900: unet metrics: {'dc': 0.6170402857715934, 'jc': 0.49066479528844154, 'pre': 0.7330141788585365, 'hd': 5.889880164096193}, ynet metrics: {'dc': 0.5677591151034436, 'jc': 0.45161145818324056, 'pre': 0.7560722051781174, 'hd': 6.033776664091925}.
[13:06:23.770] Evaluation Finished!⏹️
[13:06:23.923] iteration 10901: total_loss: 0.371983, loss_sup: 0.151364, loss_mps: 0.078372, loss_cps: 0.142246
[13:06:24.072] iteration 10902: total_loss: 0.453383, loss_sup: 0.138779, loss_mps: 0.104170, loss_cps: 0.210434
[13:06:24.219] iteration 10903: total_loss: 0.319338, loss_sup: 0.110430, loss_mps: 0.074161, loss_cps: 0.134747
[13:06:24.364] iteration 10904: total_loss: 0.587162, loss_sup: 0.036199, loss_mps: 0.169342, loss_cps: 0.381621
[13:06:24.512] iteration 10905: total_loss: 0.298314, loss_sup: 0.039009, loss_mps: 0.089526, loss_cps: 0.169779
[13:06:24.658] iteration 10906: total_loss: 0.447246, loss_sup: 0.199490, loss_mps: 0.088376, loss_cps: 0.159379
[13:06:24.804] iteration 10907: total_loss: 0.313103, loss_sup: 0.084375, loss_mps: 0.081587, loss_cps: 0.147141
[13:06:24.950] iteration 10908: total_loss: 0.635930, loss_sup: 0.326108, loss_mps: 0.103145, loss_cps: 0.206676
[13:06:25.096] iteration 10909: total_loss: 0.353883, loss_sup: 0.113567, loss_mps: 0.082262, loss_cps: 0.158054
[13:06:25.242] iteration 10910: total_loss: 0.307290, loss_sup: 0.096224, loss_mps: 0.072138, loss_cps: 0.138928
[13:06:25.387] iteration 10911: total_loss: 0.495926, loss_sup: 0.086044, loss_mps: 0.132068, loss_cps: 0.277814
[13:06:25.533] iteration 10912: total_loss: 0.386679, loss_sup: 0.076125, loss_mps: 0.103472, loss_cps: 0.207083
[13:06:25.679] iteration 10913: total_loss: 0.293078, loss_sup: 0.124611, loss_mps: 0.062071, loss_cps: 0.106396
[13:06:25.825] iteration 10914: total_loss: 0.346480, loss_sup: 0.101694, loss_mps: 0.085845, loss_cps: 0.158942
[13:06:25.971] iteration 10915: total_loss: 0.413618, loss_sup: 0.179108, loss_mps: 0.080728, loss_cps: 0.153781
[13:06:26.117] iteration 10916: total_loss: 0.223996, loss_sup: 0.028706, loss_mps: 0.071293, loss_cps: 0.123997
[13:06:26.262] iteration 10917: total_loss: 0.438810, loss_sup: 0.185328, loss_mps: 0.088428, loss_cps: 0.165054
[13:06:26.408] iteration 10918: total_loss: 0.278710, loss_sup: 0.084256, loss_mps: 0.067867, loss_cps: 0.126586
[13:06:26.556] iteration 10919: total_loss: 0.262656, loss_sup: 0.105035, loss_mps: 0.059649, loss_cps: 0.097972
[13:06:26.702] iteration 10920: total_loss: 0.305831, loss_sup: 0.043067, loss_mps: 0.087332, loss_cps: 0.175432
[13:06:26.847] iteration 10921: total_loss: 0.273347, loss_sup: 0.048167, loss_mps: 0.076087, loss_cps: 0.149092
[13:06:26.993] iteration 10922: total_loss: 0.357959, loss_sup: 0.075782, loss_mps: 0.095198, loss_cps: 0.186978
[13:06:27.139] iteration 10923: total_loss: 0.290751, loss_sup: 0.107818, loss_mps: 0.065456, loss_cps: 0.117477
[13:06:27.285] iteration 10924: total_loss: 0.358505, loss_sup: 0.080326, loss_mps: 0.095789, loss_cps: 0.182389
[13:06:27.431] iteration 10925: total_loss: 0.270074, loss_sup: 0.037700, loss_mps: 0.081649, loss_cps: 0.150726
[13:06:27.577] iteration 10926: total_loss: 0.279282, loss_sup: 0.025718, loss_mps: 0.086581, loss_cps: 0.166983
[13:06:27.723] iteration 10927: total_loss: 0.288669, loss_sup: 0.042684, loss_mps: 0.083188, loss_cps: 0.162797
[13:06:27.869] iteration 10928: total_loss: 0.382316, loss_sup: 0.220494, loss_mps: 0.061977, loss_cps: 0.099845
[13:06:28.015] iteration 10929: total_loss: 0.355529, loss_sup: 0.153720, loss_mps: 0.071046, loss_cps: 0.130763
[13:06:28.161] iteration 10930: total_loss: 0.241745, loss_sup: 0.009848, loss_mps: 0.078274, loss_cps: 0.153623
[13:06:28.307] iteration 10931: total_loss: 0.205464, loss_sup: 0.035383, loss_mps: 0.060027, loss_cps: 0.110054
[13:06:28.453] iteration 10932: total_loss: 0.253533, loss_sup: 0.050466, loss_mps: 0.070147, loss_cps: 0.132920
[13:06:28.599] iteration 10933: total_loss: 0.383150, loss_sup: 0.151369, loss_mps: 0.080522, loss_cps: 0.151259
[13:06:28.747] iteration 10934: total_loss: 0.375300, loss_sup: 0.078854, loss_mps: 0.095130, loss_cps: 0.201315
[13:06:28.894] iteration 10935: total_loss: 0.192116, loss_sup: 0.011716, loss_mps: 0.062978, loss_cps: 0.117421
[13:06:29.040] iteration 10936: total_loss: 0.289379, loss_sup: 0.044846, loss_mps: 0.079661, loss_cps: 0.164872
[13:06:29.187] iteration 10937: total_loss: 0.232515, loss_sup: 0.054002, loss_mps: 0.061216, loss_cps: 0.117297
[13:06:29.333] iteration 10938: total_loss: 0.594959, loss_sup: 0.359084, loss_mps: 0.076714, loss_cps: 0.159160
[13:06:29.480] iteration 10939: total_loss: 0.228911, loss_sup: 0.049241, loss_mps: 0.064920, loss_cps: 0.114750
[13:06:29.626] iteration 10940: total_loss: 0.197221, loss_sup: 0.048601, loss_mps: 0.055782, loss_cps: 0.092838
[13:06:29.773] iteration 10941: total_loss: 0.315167, loss_sup: 0.082318, loss_mps: 0.078441, loss_cps: 0.154407
[13:06:29.919] iteration 10942: total_loss: 0.347178, loss_sup: 0.125934, loss_mps: 0.075835, loss_cps: 0.145409
[13:06:30.066] iteration 10943: total_loss: 0.308773, loss_sup: 0.109819, loss_mps: 0.068712, loss_cps: 0.130242
[13:06:30.212] iteration 10944: total_loss: 0.555652, loss_sup: 0.193372, loss_mps: 0.113877, loss_cps: 0.248403
[13:06:30.362] iteration 10945: total_loss: 0.382340, loss_sup: 0.073005, loss_mps: 0.104102, loss_cps: 0.205233
[13:06:30.508] iteration 10946: total_loss: 0.308958, loss_sup: 0.108671, loss_mps: 0.067600, loss_cps: 0.132687
[13:06:30.654] iteration 10947: total_loss: 0.401733, loss_sup: 0.080512, loss_mps: 0.101259, loss_cps: 0.219963
[13:06:30.801] iteration 10948: total_loss: 0.339594, loss_sup: 0.148290, loss_mps: 0.068463, loss_cps: 0.122841
[13:06:30.947] iteration 10949: total_loss: 0.384014, loss_sup: 0.072243, loss_mps: 0.096970, loss_cps: 0.214802
[13:06:31.096] iteration 10950: total_loss: 0.241168, loss_sup: 0.093496, loss_mps: 0.054132, loss_cps: 0.093539
[13:06:31.246] iteration 10951: total_loss: 0.299620, loss_sup: 0.067090, loss_mps: 0.085145, loss_cps: 0.147385
[13:06:31.392] iteration 10952: total_loss: 0.274104, loss_sup: 0.069188, loss_mps: 0.069257, loss_cps: 0.135659
[13:06:31.538] iteration 10953: total_loss: 0.177050, loss_sup: 0.015090, loss_mps: 0.057195, loss_cps: 0.104765
[13:06:31.684] iteration 10954: total_loss: 0.142149, loss_sup: 0.021117, loss_mps: 0.046409, loss_cps: 0.074623
[13:06:31.829] iteration 10955: total_loss: 0.529977, loss_sup: 0.121058, loss_mps: 0.136538, loss_cps: 0.272380
[13:06:31.975] iteration 10956: total_loss: 0.717148, loss_sup: 0.372293, loss_mps: 0.108154, loss_cps: 0.236701
[13:06:32.121] iteration 10957: total_loss: 0.377389, loss_sup: 0.113496, loss_mps: 0.086673, loss_cps: 0.177221
[13:06:32.268] iteration 10958: total_loss: 0.194533, loss_sup: 0.062821, loss_mps: 0.051193, loss_cps: 0.080519
[13:06:32.414] iteration 10959: total_loss: 0.482791, loss_sup: 0.208132, loss_mps: 0.088345, loss_cps: 0.186314
[13:06:32.559] iteration 10960: total_loss: 0.290919, loss_sup: 0.031944, loss_mps: 0.086537, loss_cps: 0.172437
[13:06:32.705] iteration 10961: total_loss: 0.380285, loss_sup: 0.123796, loss_mps: 0.089482, loss_cps: 0.167007
[13:06:32.851] iteration 10962: total_loss: 0.234442, loss_sup: 0.046750, loss_mps: 0.066203, loss_cps: 0.121489
[13:06:32.997] iteration 10963: total_loss: 0.408878, loss_sup: 0.202228, loss_mps: 0.070018, loss_cps: 0.136632
[13:06:33.143] iteration 10964: total_loss: 0.358059, loss_sup: 0.108288, loss_mps: 0.084490, loss_cps: 0.165281
[13:06:33.289] iteration 10965: total_loss: 0.398185, loss_sup: 0.192256, loss_mps: 0.070577, loss_cps: 0.135352
[13:06:33.438] iteration 10966: total_loss: 0.447163, loss_sup: 0.155897, loss_mps: 0.095838, loss_cps: 0.195427
[13:06:33.591] iteration 10967: total_loss: 0.179634, loss_sup: 0.018403, loss_mps: 0.059381, loss_cps: 0.101851
[13:06:33.737] iteration 10968: total_loss: 0.173519, loss_sup: 0.039070, loss_mps: 0.051691, loss_cps: 0.082758
[13:06:33.884] iteration 10969: total_loss: 0.202539, loss_sup: 0.028419, loss_mps: 0.065849, loss_cps: 0.108270
[13:06:34.030] iteration 10970: total_loss: 0.224143, loss_sup: 0.042422, loss_mps: 0.068274, loss_cps: 0.113446
[13:06:34.179] iteration 10971: total_loss: 0.384354, loss_sup: 0.160108, loss_mps: 0.078865, loss_cps: 0.145380
[13:06:34.326] iteration 10972: total_loss: 0.374276, loss_sup: 0.104733, loss_mps: 0.093603, loss_cps: 0.175940
[13:06:34.472] iteration 10973: total_loss: 0.380813, loss_sup: 0.129548, loss_mps: 0.090445, loss_cps: 0.160820
[13:06:34.618] iteration 10974: total_loss: 0.339282, loss_sup: 0.124357, loss_mps: 0.080202, loss_cps: 0.134723
[13:06:34.765] iteration 10975: total_loss: 0.240722, loss_sup: 0.086316, loss_mps: 0.056848, loss_cps: 0.097558
[13:06:34.912] iteration 10976: total_loss: 0.279857, loss_sup: 0.040138, loss_mps: 0.086480, loss_cps: 0.153239
[13:06:35.059] iteration 10977: total_loss: 0.372585, loss_sup: 0.159091, loss_mps: 0.075134, loss_cps: 0.138361
[13:06:35.205] iteration 10978: total_loss: 0.248480, loss_sup: 0.019792, loss_mps: 0.081772, loss_cps: 0.146917
[13:06:35.356] iteration 10979: total_loss: 0.402494, loss_sup: 0.102545, loss_mps: 0.101828, loss_cps: 0.198121
[13:06:35.503] iteration 10980: total_loss: 0.284188, loss_sup: 0.138629, loss_mps: 0.056335, loss_cps: 0.089224
[13:06:35.649] iteration 10981: total_loss: 0.272588, loss_sup: 0.093282, loss_mps: 0.067258, loss_cps: 0.112047
[13:06:35.797] iteration 10982: total_loss: 0.343684, loss_sup: 0.117651, loss_mps: 0.078934, loss_cps: 0.147099
[13:06:35.948] iteration 10983: total_loss: 0.315659, loss_sup: 0.119617, loss_mps: 0.071843, loss_cps: 0.124199
[13:06:36.095] iteration 10984: total_loss: 0.213923, loss_sup: 0.049098, loss_mps: 0.059909, loss_cps: 0.104915
[13:06:36.244] iteration 10985: total_loss: 0.272593, loss_sup: 0.073567, loss_mps: 0.068826, loss_cps: 0.130200
[13:06:36.390] iteration 10986: total_loss: 0.164636, loss_sup: 0.023253, loss_mps: 0.053485, loss_cps: 0.087898
[13:06:36.537] iteration 10987: total_loss: 0.196323, loss_sup: 0.058014, loss_mps: 0.050420, loss_cps: 0.087890
[13:06:36.684] iteration 10988: total_loss: 0.264766, loss_sup: 0.063425, loss_mps: 0.068046, loss_cps: 0.133295
[13:06:36.831] iteration 10989: total_loss: 0.224705, loss_sup: 0.020717, loss_mps: 0.070826, loss_cps: 0.133162
[13:06:36.977] iteration 10990: total_loss: 0.170441, loss_sup: 0.031466, loss_mps: 0.051905, loss_cps: 0.087071
[13:06:37.123] iteration 10991: total_loss: 0.192382, loss_sup: 0.043679, loss_mps: 0.053413, loss_cps: 0.095291
[13:06:37.269] iteration 10992: total_loss: 0.396882, loss_sup: 0.167668, loss_mps: 0.079140, loss_cps: 0.150073
[13:06:37.414] iteration 10993: total_loss: 0.181575, loss_sup: 0.027323, loss_mps: 0.054812, loss_cps: 0.099441
[13:06:37.560] iteration 10994: total_loss: 0.293082, loss_sup: 0.072495, loss_mps: 0.075453, loss_cps: 0.145135
[13:06:37.706] iteration 10995: total_loss: 0.595565, loss_sup: 0.264392, loss_mps: 0.109365, loss_cps: 0.221808
[13:06:37.855] iteration 10996: total_loss: 0.306734, loss_sup: 0.039225, loss_mps: 0.090151, loss_cps: 0.177358
[13:06:38.001] iteration 10997: total_loss: 0.280527, loss_sup: 0.067114, loss_mps: 0.072784, loss_cps: 0.140630
[13:06:38.147] iteration 10998: total_loss: 0.236464, loss_sup: 0.050611, loss_mps: 0.066238, loss_cps: 0.119615
[13:06:38.293] iteration 10999: total_loss: 0.239546, loss_sup: 0.064625, loss_mps: 0.060876, loss_cps: 0.114046
[13:06:38.439] iteration 11000: total_loss: 0.328216, loss_sup: 0.053040, loss_mps: 0.087936, loss_cps: 0.187240
[13:06:38.439] Evaluation Started ==>
[13:06:49.938] ==> valid iteration 11000: unet metrics: {'dc': 0.6609614375175141, 'jc': 0.5368486465933819, 'pre': 0.7405254840814275, 'hd': 5.901291891000661}, ynet metrics: {'dc': 0.6337205860460409, 'jc': 0.5074081834705352, 'pre': 0.7603050177713845, 'hd': 5.914474585180054}.
[13:06:50.099] ==> New best valid dice for ynet: 0.633721, at iteration 11000
[13:06:50.101] Evaluation Finished!⏹️
[13:06:50.253] iteration 11001: total_loss: 0.192238, loss_sup: 0.038646, loss_mps: 0.051967, loss_cps: 0.101626
[13:06:50.402] iteration 11002: total_loss: 0.289901, loss_sup: 0.102694, loss_mps: 0.066797, loss_cps: 0.120409
[13:06:50.548] iteration 11003: total_loss: 0.252924, loss_sup: 0.017775, loss_mps: 0.082664, loss_cps: 0.152485
[13:06:50.695] iteration 11004: total_loss: 0.192148, loss_sup: 0.035054, loss_mps: 0.054266, loss_cps: 0.102828
[13:06:50.843] iteration 11005: total_loss: 0.237299, loss_sup: 0.096214, loss_mps: 0.050118, loss_cps: 0.090967
[13:06:50.990] iteration 11006: total_loss: 0.306337, loss_sup: 0.052516, loss_mps: 0.082004, loss_cps: 0.171816
[13:06:51.139] iteration 11007: total_loss: 0.263973, loss_sup: 0.025495, loss_mps: 0.081416, loss_cps: 0.157061
[13:06:51.285] iteration 11008: total_loss: 0.362860, loss_sup: 0.067810, loss_mps: 0.096701, loss_cps: 0.198349
[13:06:51.431] iteration 11009: total_loss: 0.292775, loss_sup: 0.054779, loss_mps: 0.077142, loss_cps: 0.160854
[13:06:51.576] iteration 11010: total_loss: 0.304545, loss_sup: 0.079408, loss_mps: 0.075103, loss_cps: 0.150034
[13:06:51.721] iteration 11011: total_loss: 0.195134, loss_sup: 0.031916, loss_mps: 0.060343, loss_cps: 0.102875
[13:06:51.867] iteration 11012: total_loss: 0.366213, loss_sup: 0.143829, loss_mps: 0.077595, loss_cps: 0.144789
[13:06:52.013] iteration 11013: total_loss: 0.417581, loss_sup: 0.169671, loss_mps: 0.084641, loss_cps: 0.163268
[13:06:52.158] iteration 11014: total_loss: 0.294368, loss_sup: 0.107754, loss_mps: 0.062742, loss_cps: 0.123872
[13:06:52.303] iteration 11015: total_loss: 0.310590, loss_sup: 0.058124, loss_mps: 0.079160, loss_cps: 0.173306
[13:06:52.448] iteration 11016: total_loss: 0.284914, loss_sup: 0.051126, loss_mps: 0.072079, loss_cps: 0.161709
[13:06:52.594] iteration 11017: total_loss: 0.257933, loss_sup: 0.056922, loss_mps: 0.069271, loss_cps: 0.131740
[13:06:52.739] iteration 11018: total_loss: 0.299054, loss_sup: 0.018958, loss_mps: 0.090165, loss_cps: 0.189931
[13:06:52.886] iteration 11019: total_loss: 0.230259, loss_sup: 0.020012, loss_mps: 0.070420, loss_cps: 0.139826
[13:06:53.033] iteration 11020: total_loss: 0.231525, loss_sup: 0.030870, loss_mps: 0.066475, loss_cps: 0.134181
[13:06:53.178] iteration 11021: total_loss: 0.282045, loss_sup: 0.081544, loss_mps: 0.070236, loss_cps: 0.130264
[13:06:53.323] iteration 11022: total_loss: 0.437119, loss_sup: 0.148955, loss_mps: 0.089061, loss_cps: 0.199102
[13:06:53.468] iteration 11023: total_loss: 0.138430, loss_sup: 0.042802, loss_mps: 0.036634, loss_cps: 0.058994
[13:06:53.613] iteration 11024: total_loss: 0.520628, loss_sup: 0.167803, loss_mps: 0.111963, loss_cps: 0.240862
[13:06:53.758] iteration 11025: total_loss: 0.363542, loss_sup: 0.172124, loss_mps: 0.065488, loss_cps: 0.125930
[13:06:53.904] iteration 11026: total_loss: 0.241402, loss_sup: 0.041216, loss_mps: 0.066509, loss_cps: 0.133677
[13:06:54.053] iteration 11027: total_loss: 0.222991, loss_sup: 0.040101, loss_mps: 0.062915, loss_cps: 0.119975
[13:06:54.198] iteration 11028: total_loss: 0.289471, loss_sup: 0.027600, loss_mps: 0.081098, loss_cps: 0.180774
[13:06:54.344] iteration 11029: total_loss: 0.447744, loss_sup: 0.140829, loss_mps: 0.100067, loss_cps: 0.206848
[13:06:54.492] iteration 11030: total_loss: 0.241539, loss_sup: 0.037753, loss_mps: 0.070454, loss_cps: 0.133333
[13:06:54.638] iteration 11031: total_loss: 0.178865, loss_sup: 0.037891, loss_mps: 0.051549, loss_cps: 0.089424
[13:06:54.784] iteration 11032: total_loss: 0.198388, loss_sup: 0.017348, loss_mps: 0.060918, loss_cps: 0.120122
[13:06:54.929] iteration 11033: total_loss: 0.350163, loss_sup: 0.072956, loss_mps: 0.089583, loss_cps: 0.187624
[13:06:55.078] iteration 11034: total_loss: 0.203282, loss_sup: 0.043515, loss_mps: 0.057635, loss_cps: 0.102132
[13:06:55.225] iteration 11035: total_loss: 0.372036, loss_sup: 0.131862, loss_mps: 0.080754, loss_cps: 0.159420
[13:06:55.371] iteration 11036: total_loss: 0.293357, loss_sup: 0.151075, loss_mps: 0.053657, loss_cps: 0.088625
[13:06:55.517] iteration 11037: total_loss: 0.360449, loss_sup: 0.040154, loss_mps: 0.106188, loss_cps: 0.214107
[13:06:55.662] iteration 11038: total_loss: 0.258661, loss_sup: 0.052704, loss_mps: 0.069919, loss_cps: 0.136038
[13:06:55.808] iteration 11039: total_loss: 0.165971, loss_sup: 0.011133, loss_mps: 0.055549, loss_cps: 0.099289
[13:06:55.953] iteration 11040: total_loss: 0.368361, loss_sup: 0.129726, loss_mps: 0.084215, loss_cps: 0.154419
[13:06:56.098] iteration 11041: total_loss: 0.308806, loss_sup: 0.104521, loss_mps: 0.070426, loss_cps: 0.133859
[13:06:56.243] iteration 11042: total_loss: 0.403743, loss_sup: 0.071825, loss_mps: 0.104508, loss_cps: 0.227411
[13:06:56.389] iteration 11043: total_loss: 0.145446, loss_sup: 0.033104, loss_mps: 0.043003, loss_cps: 0.069339
[13:06:56.535] iteration 11044: total_loss: 0.338956, loss_sup: 0.015442, loss_mps: 0.107668, loss_cps: 0.215846
[13:06:56.681] iteration 11045: total_loss: 0.690192, loss_sup: 0.326201, loss_mps: 0.116371, loss_cps: 0.247620
[13:06:56.826] iteration 11046: total_loss: 0.561684, loss_sup: 0.367596, loss_mps: 0.072430, loss_cps: 0.121659
[13:06:56.972] iteration 11047: total_loss: 0.303876, loss_sup: 0.105197, loss_mps: 0.065687, loss_cps: 0.132992
[13:06:57.118] iteration 11048: total_loss: 0.293391, loss_sup: 0.068340, loss_mps: 0.076495, loss_cps: 0.148556
[13:06:57.263] iteration 11049: total_loss: 0.280086, loss_sup: 0.079251, loss_mps: 0.068134, loss_cps: 0.132701
[13:06:57.409] iteration 11050: total_loss: 0.179582, loss_sup: 0.007012, loss_mps: 0.061665, loss_cps: 0.110905
[13:06:57.554] iteration 11051: total_loss: 0.287835, loss_sup: 0.080915, loss_mps: 0.070933, loss_cps: 0.135987
[13:06:57.700] iteration 11052: total_loss: 0.319131, loss_sup: 0.114043, loss_mps: 0.071682, loss_cps: 0.133407
[13:06:57.847] iteration 11053: total_loss: 0.134720, loss_sup: 0.017964, loss_mps: 0.045081, loss_cps: 0.071675
[13:06:57.993] iteration 11054: total_loss: 0.353780, loss_sup: 0.116538, loss_mps: 0.079407, loss_cps: 0.157835
[13:06:58.139] iteration 11055: total_loss: 0.233993, loss_sup: 0.010092, loss_mps: 0.075502, loss_cps: 0.148400
[13:06:58.285] iteration 11056: total_loss: 0.351156, loss_sup: 0.090597, loss_mps: 0.086468, loss_cps: 0.174090
[13:06:58.431] iteration 11057: total_loss: 0.422469, loss_sup: 0.121023, loss_mps: 0.102171, loss_cps: 0.199276
[13:06:58.576] iteration 11058: total_loss: 0.224831, loss_sup: 0.035503, loss_mps: 0.067976, loss_cps: 0.121352
[13:06:58.724] iteration 11059: total_loss: 0.207079, loss_sup: 0.019673, loss_mps: 0.070450, loss_cps: 0.116955
[13:06:58.870] iteration 11060: total_loss: 0.232790, loss_sup: 0.072720, loss_mps: 0.058999, loss_cps: 0.101071
[13:06:59.016] iteration 11061: total_loss: 0.226509, loss_sup: 0.027335, loss_mps: 0.070793, loss_cps: 0.128381
[13:06:59.162] iteration 11062: total_loss: 0.410205, loss_sup: 0.203418, loss_mps: 0.072689, loss_cps: 0.134097
[13:06:59.309] iteration 11063: total_loss: 0.469799, loss_sup: 0.235669, loss_mps: 0.083479, loss_cps: 0.150651
[13:06:59.459] iteration 11064: total_loss: 0.414484, loss_sup: 0.122492, loss_mps: 0.100096, loss_cps: 0.191897
[13:06:59.606] iteration 11065: total_loss: 0.302271, loss_sup: 0.083147, loss_mps: 0.078456, loss_cps: 0.140668
[13:06:59.752] iteration 11066: total_loss: 0.300391, loss_sup: 0.096524, loss_mps: 0.069559, loss_cps: 0.134308
[13:06:59.898] iteration 11067: total_loss: 0.337998, loss_sup: 0.055152, loss_mps: 0.093892, loss_cps: 0.188954
[13:07:00.044] iteration 11068: total_loss: 0.184063, loss_sup: 0.005886, loss_mps: 0.064222, loss_cps: 0.113955
[13:07:00.190] iteration 11069: total_loss: 0.399130, loss_sup: 0.117228, loss_mps: 0.094112, loss_cps: 0.187789
[13:07:00.336] iteration 11070: total_loss: 0.191596, loss_sup: 0.015388, loss_mps: 0.063288, loss_cps: 0.112921
[13:07:00.485] iteration 11071: total_loss: 0.531710, loss_sup: 0.267969, loss_mps: 0.086691, loss_cps: 0.177050
[13:07:00.633] iteration 11072: total_loss: 0.355836, loss_sup: 0.120425, loss_mps: 0.079985, loss_cps: 0.155427
[13:07:00.779] iteration 11073: total_loss: 0.213715, loss_sup: 0.046709, loss_mps: 0.064235, loss_cps: 0.102771
[13:07:00.925] iteration 11074: total_loss: 0.317192, loss_sup: 0.111157, loss_mps: 0.073275, loss_cps: 0.132760
[13:07:01.072] iteration 11075: total_loss: 0.218694, loss_sup: 0.054736, loss_mps: 0.057018, loss_cps: 0.106941
[13:07:01.218] iteration 11076: total_loss: 0.208644, loss_sup: 0.062152, loss_mps: 0.054799, loss_cps: 0.091693
[13:07:01.367] iteration 11077: total_loss: 0.180612, loss_sup: 0.040332, loss_mps: 0.051993, loss_cps: 0.088286
[13:07:01.514] iteration 11078: total_loss: 0.257381, loss_sup: 0.039574, loss_mps: 0.077242, loss_cps: 0.140565
[13:07:01.662] iteration 11079: total_loss: 0.519888, loss_sup: 0.173986, loss_mps: 0.112404, loss_cps: 0.233498
[13:07:01.809] iteration 11080: total_loss: 0.437938, loss_sup: 0.104451, loss_mps: 0.108403, loss_cps: 0.225084
[13:07:01.955] iteration 11081: total_loss: 0.422921, loss_sup: 0.150332, loss_mps: 0.089843, loss_cps: 0.182746
[13:07:02.101] iteration 11082: total_loss: 0.366089, loss_sup: 0.112358, loss_mps: 0.086227, loss_cps: 0.167504
[13:07:02.247] iteration 11083: total_loss: 0.446321, loss_sup: 0.164247, loss_mps: 0.094608, loss_cps: 0.187466
[13:07:02.393] iteration 11084: total_loss: 0.560707, loss_sup: 0.141361, loss_mps: 0.130764, loss_cps: 0.288582
[13:07:02.539] iteration 11085: total_loss: 0.316553, loss_sup: 0.042904, loss_mps: 0.095446, loss_cps: 0.178203
[13:07:02.685] iteration 11086: total_loss: 0.577226, loss_sup: 0.133149, loss_mps: 0.140460, loss_cps: 0.303617
[13:07:02.831] iteration 11087: total_loss: 0.206577, loss_sup: 0.070262, loss_mps: 0.051884, loss_cps: 0.084431
[13:07:02.977] iteration 11088: total_loss: 0.206643, loss_sup: 0.023714, loss_mps: 0.063952, loss_cps: 0.118977
[13:07:03.126] iteration 11089: total_loss: 0.195982, loss_sup: 0.021769, loss_mps: 0.063177, loss_cps: 0.111036
[13:07:03.271] iteration 11090: total_loss: 0.420902, loss_sup: 0.115215, loss_mps: 0.101394, loss_cps: 0.204293
[13:07:03.417] iteration 11091: total_loss: 0.813974, loss_sup: 0.345182, loss_mps: 0.147166, loss_cps: 0.321626
[13:07:03.565] iteration 11092: total_loss: 0.428338, loss_sup: 0.143339, loss_mps: 0.093495, loss_cps: 0.191504
[13:07:03.711] iteration 11093: total_loss: 0.304018, loss_sup: 0.053863, loss_mps: 0.080862, loss_cps: 0.169292
[13:07:03.856] iteration 11094: total_loss: 0.225693, loss_sup: 0.049506, loss_mps: 0.063582, loss_cps: 0.112606
[13:07:04.002] iteration 11095: total_loss: 0.287821, loss_sup: 0.081669, loss_mps: 0.072802, loss_cps: 0.133349
[13:07:04.149] iteration 11096: total_loss: 0.343557, loss_sup: 0.035666, loss_mps: 0.103657, loss_cps: 0.204234
[13:07:04.294] iteration 11097: total_loss: 0.348404, loss_sup: 0.070408, loss_mps: 0.099570, loss_cps: 0.178426
[13:07:04.442] iteration 11098: total_loss: 0.418646, loss_sup: 0.151574, loss_mps: 0.088607, loss_cps: 0.178464
[13:07:04.588] iteration 11099: total_loss: 0.287050, loss_sup: 0.021289, loss_mps: 0.090993, loss_cps: 0.174768
[13:07:04.735] iteration 11100: total_loss: 0.229542, loss_sup: 0.047463, loss_mps: 0.068155, loss_cps: 0.113924
[13:07:04.736] Evaluation Started ==>
[13:07:16.143] ==> valid iteration 11100: unet metrics: {'dc': 0.6475531059050439, 'jc': 0.5273219412577129, 'pre': 0.7472512874377933, 'hd': 5.738665737573301}, ynet metrics: {'dc': 0.5161297299399682, 'jc': 0.40385926454164245, 'pre': 0.7448084381601826, 'hd': 5.92277095492103}.
[13:07:16.145] Evaluation Finished!⏹️
[13:07:16.295] iteration 11101: total_loss: 0.479289, loss_sup: 0.098332, loss_mps: 0.123572, loss_cps: 0.257385
[13:07:16.443] iteration 11102: total_loss: 0.340326, loss_sup: 0.039181, loss_mps: 0.096363, loss_cps: 0.204782
[13:07:16.588] iteration 11103: total_loss: 0.437771, loss_sup: 0.123686, loss_mps: 0.103632, loss_cps: 0.210454
[13:07:16.733] iteration 11104: total_loss: 0.323007, loss_sup: 0.113166, loss_mps: 0.077691, loss_cps: 0.132150
[13:07:16.878] iteration 11105: total_loss: 0.219133, loss_sup: 0.056256, loss_mps: 0.059924, loss_cps: 0.102953
[13:07:17.026] iteration 11106: total_loss: 0.272055, loss_sup: 0.060674, loss_mps: 0.072582, loss_cps: 0.138799
[13:07:17.173] iteration 11107: total_loss: 0.403349, loss_sup: 0.083491, loss_mps: 0.112562, loss_cps: 0.207296
[13:07:17.319] iteration 11108: total_loss: 0.262229, loss_sup: 0.045321, loss_mps: 0.072194, loss_cps: 0.144714
[13:07:17.465] iteration 11109: total_loss: 0.339590, loss_sup: 0.138502, loss_mps: 0.073016, loss_cps: 0.128071
[13:07:17.614] iteration 11110: total_loss: 0.326407, loss_sup: 0.103410, loss_mps: 0.076782, loss_cps: 0.146215
[13:07:17.761] iteration 11111: total_loss: 0.220039, loss_sup: 0.067172, loss_mps: 0.054072, loss_cps: 0.098795
[13:07:17.909] iteration 11112: total_loss: 0.338167, loss_sup: 0.088014, loss_mps: 0.085879, loss_cps: 0.164274
[13:07:18.055] iteration 11113: total_loss: 0.416561, loss_sup: 0.101496, loss_mps: 0.101312, loss_cps: 0.213752
[13:07:18.203] iteration 11114: total_loss: 0.371955, loss_sup: 0.131736, loss_mps: 0.081593, loss_cps: 0.158625
[13:07:18.350] iteration 11115: total_loss: 0.235767, loss_sup: 0.083987, loss_mps: 0.055045, loss_cps: 0.096735
[13:07:18.496] iteration 11116: total_loss: 0.601820, loss_sup: 0.223021, loss_mps: 0.119810, loss_cps: 0.258989
[13:07:18.642] iteration 11117: total_loss: 0.187632, loss_sup: 0.015953, loss_mps: 0.059003, loss_cps: 0.112675
[13:07:18.789] iteration 11118: total_loss: 0.455827, loss_sup: 0.190676, loss_mps: 0.085699, loss_cps: 0.179453
[13:07:18.936] iteration 11119: total_loss: 0.430473, loss_sup: 0.183721, loss_mps: 0.082332, loss_cps: 0.164421
[13:07:19.085] iteration 11120: total_loss: 0.368779, loss_sup: 0.093547, loss_mps: 0.092400, loss_cps: 0.182832
[13:07:19.231] iteration 11121: total_loss: 0.398621, loss_sup: 0.197054, loss_mps: 0.070361, loss_cps: 0.131206
[13:07:19.377] iteration 11122: total_loss: 0.241315, loss_sup: 0.070619, loss_mps: 0.065025, loss_cps: 0.105670
[13:07:19.523] iteration 11123: total_loss: 0.367813, loss_sup: 0.085167, loss_mps: 0.093556, loss_cps: 0.189090
[13:07:19.673] iteration 11124: total_loss: 0.471807, loss_sup: 0.215625, loss_mps: 0.086143, loss_cps: 0.170039
[13:07:19.819] iteration 11125: total_loss: 0.277944, loss_sup: 0.049958, loss_mps: 0.074740, loss_cps: 0.153247
[13:07:19.964] iteration 11126: total_loss: 0.376166, loss_sup: 0.060254, loss_mps: 0.107324, loss_cps: 0.208588
[13:07:20.111] iteration 11127: total_loss: 0.239012, loss_sup: 0.006776, loss_mps: 0.079734, loss_cps: 0.152502
[13:07:20.257] iteration 11128: total_loss: 0.197717, loss_sup: 0.018687, loss_mps: 0.062616, loss_cps: 0.116415
[13:07:20.403] iteration 11129: total_loss: 0.161340, loss_sup: 0.019195, loss_mps: 0.049718, loss_cps: 0.092427
[13:07:20.548] iteration 11130: total_loss: 0.276218, loss_sup: 0.069912, loss_mps: 0.070585, loss_cps: 0.135720
[13:07:20.695] iteration 11131: total_loss: 0.297470, loss_sup: 0.074984, loss_mps: 0.075557, loss_cps: 0.146928
[13:07:20.841] iteration 11132: total_loss: 0.392012, loss_sup: 0.102666, loss_mps: 0.093783, loss_cps: 0.195564
[13:07:20.987] iteration 11133: total_loss: 0.499110, loss_sup: 0.203997, loss_mps: 0.098956, loss_cps: 0.196157
[13:07:21.132] iteration 11134: total_loss: 0.161598, loss_sup: 0.014017, loss_mps: 0.054065, loss_cps: 0.093516
[13:07:21.278] iteration 11135: total_loss: 0.466584, loss_sup: 0.127793, loss_mps: 0.110882, loss_cps: 0.227909
[13:07:21.424] iteration 11136: total_loss: 0.261573, loss_sup: 0.047968, loss_mps: 0.072788, loss_cps: 0.140816
[13:07:21.570] iteration 11137: total_loss: 0.346844, loss_sup: 0.065150, loss_mps: 0.097544, loss_cps: 0.184150
[13:07:21.717] iteration 11138: total_loss: 0.321879, loss_sup: 0.040507, loss_mps: 0.096136, loss_cps: 0.185235
[13:07:21.863] iteration 11139: total_loss: 0.244856, loss_sup: 0.101831, loss_mps: 0.051621, loss_cps: 0.091404
[13:07:22.008] iteration 11140: total_loss: 0.263681, loss_sup: 0.024378, loss_mps: 0.082217, loss_cps: 0.157086
[13:07:22.157] iteration 11141: total_loss: 0.403171, loss_sup: 0.059800, loss_mps: 0.111866, loss_cps: 0.231504
[13:07:22.303] iteration 11142: total_loss: 0.320308, loss_sup: 0.052701, loss_mps: 0.089433, loss_cps: 0.178174
[13:07:22.449] iteration 11143: total_loss: 0.277267, loss_sup: 0.051641, loss_mps: 0.074445, loss_cps: 0.151181
[13:07:22.595] iteration 11144: total_loss: 0.300150, loss_sup: 0.082808, loss_mps: 0.079189, loss_cps: 0.138154
[13:07:22.742] iteration 11145: total_loss: 0.212521, loss_sup: 0.034381, loss_mps: 0.065936, loss_cps: 0.112204
[13:07:22.889] iteration 11146: total_loss: 0.413878, loss_sup: 0.047095, loss_mps: 0.116875, loss_cps: 0.249907
[13:07:23.035] iteration 11147: total_loss: 0.277681, loss_sup: 0.072407, loss_mps: 0.070458, loss_cps: 0.134815
[13:07:23.182] iteration 11148: total_loss: 0.234396, loss_sup: 0.025266, loss_mps: 0.073576, loss_cps: 0.135554
[13:07:23.328] iteration 11149: total_loss: 0.358763, loss_sup: 0.118118, loss_mps: 0.083625, loss_cps: 0.157020
[13:07:23.479] iteration 11150: total_loss: 0.364442, loss_sup: 0.137307, loss_mps: 0.081199, loss_cps: 0.145936
[13:07:23.626] iteration 11151: total_loss: 0.382039, loss_sup: 0.123787, loss_mps: 0.086366, loss_cps: 0.171887
[13:07:23.772] iteration 11152: total_loss: 0.238901, loss_sup: 0.042932, loss_mps: 0.065612, loss_cps: 0.130357
[13:07:23.918] iteration 11153: total_loss: 0.245922, loss_sup: 0.045904, loss_mps: 0.071308, loss_cps: 0.128710
[13:07:24.064] iteration 11154: total_loss: 0.196531, loss_sup: 0.029943, loss_mps: 0.062317, loss_cps: 0.104272
[13:07:24.211] iteration 11155: total_loss: 0.201816, loss_sup: 0.064605, loss_mps: 0.050401, loss_cps: 0.086810
[13:07:24.356] iteration 11156: total_loss: 0.326418, loss_sup: 0.071573, loss_mps: 0.088709, loss_cps: 0.166136
[13:07:24.501] iteration 11157: total_loss: 0.568122, loss_sup: 0.188210, loss_mps: 0.124400, loss_cps: 0.255512
[13:07:24.647] iteration 11158: total_loss: 0.259449, loss_sup: 0.074331, loss_mps: 0.063988, loss_cps: 0.121131
[13:07:24.793] iteration 11159: total_loss: 0.295401, loss_sup: 0.113723, loss_mps: 0.065318, loss_cps: 0.116359
[13:07:24.942] iteration 11160: total_loss: 0.124085, loss_sup: 0.031452, loss_mps: 0.038700, loss_cps: 0.053933
[13:07:25.092] iteration 11161: total_loss: 0.204142, loss_sup: 0.048046, loss_mps: 0.058405, loss_cps: 0.097691
[13:07:25.242] iteration 11162: total_loss: 0.253353, loss_sup: 0.049391, loss_mps: 0.068221, loss_cps: 0.135741
[13:07:25.389] iteration 11163: total_loss: 0.363803, loss_sup: 0.058394, loss_mps: 0.103548, loss_cps: 0.201862
[13:07:25.536] iteration 11164: total_loss: 0.300848, loss_sup: 0.035974, loss_mps: 0.091669, loss_cps: 0.173205
[13:07:25.683] iteration 11165: total_loss: 0.357418, loss_sup: 0.130402, loss_mps: 0.078672, loss_cps: 0.148344
[13:07:25.829] iteration 11166: total_loss: 0.439565, loss_sup: 0.173917, loss_mps: 0.091944, loss_cps: 0.173704
[13:07:25.976] iteration 11167: total_loss: 0.771465, loss_sup: 0.251138, loss_mps: 0.156983, loss_cps: 0.363344
[13:07:26.122] iteration 11168: total_loss: 0.689486, loss_sup: 0.338161, loss_mps: 0.113061, loss_cps: 0.238265
[13:07:26.270] iteration 11169: total_loss: 0.312780, loss_sup: 0.018662, loss_mps: 0.092103, loss_cps: 0.202014
[13:07:26.419] iteration 11170: total_loss: 0.175661, loss_sup: 0.009406, loss_mps: 0.060335, loss_cps: 0.105920
[13:07:26.565] iteration 11171: total_loss: 0.404320, loss_sup: 0.121540, loss_mps: 0.091081, loss_cps: 0.191699
[13:07:26.711] iteration 11172: total_loss: 0.421702, loss_sup: 0.139322, loss_mps: 0.087438, loss_cps: 0.194942
[13:07:26.856] iteration 11173: total_loss: 0.335485, loss_sup: 0.139828, loss_mps: 0.066758, loss_cps: 0.128900
[13:07:27.002] iteration 11174: total_loss: 0.212371, loss_sup: 0.031181, loss_mps: 0.062705, loss_cps: 0.118485
[13:07:27.149] iteration 11175: total_loss: 0.279489, loss_sup: 0.100337, loss_mps: 0.062689, loss_cps: 0.116462
[13:07:27.294] iteration 11176: total_loss: 0.295888, loss_sup: 0.082834, loss_mps: 0.071982, loss_cps: 0.141072
[13:07:27.440] iteration 11177: total_loss: 0.190870, loss_sup: 0.029420, loss_mps: 0.058969, loss_cps: 0.102481
[13:07:27.585] iteration 11178: total_loss: 0.465815, loss_sup: 0.314675, loss_mps: 0.054396, loss_cps: 0.096744
[13:07:27.730] iteration 11179: total_loss: 0.334897, loss_sup: 0.111377, loss_mps: 0.074494, loss_cps: 0.149025
[13:07:27.876] iteration 11180: total_loss: 0.348277, loss_sup: 0.137516, loss_mps: 0.073353, loss_cps: 0.137408
[13:07:28.022] iteration 11181: total_loss: 0.348134, loss_sup: 0.138962, loss_mps: 0.073207, loss_cps: 0.135966
[13:07:28.168] iteration 11182: total_loss: 0.596555, loss_sup: 0.182348, loss_mps: 0.136417, loss_cps: 0.277791
[13:07:28.313] iteration 11183: total_loss: 0.291681, loss_sup: 0.037952, loss_mps: 0.085620, loss_cps: 0.168110
[13:07:28.459] iteration 11184: total_loss: 0.357400, loss_sup: 0.192280, loss_mps: 0.057921, loss_cps: 0.107198
[13:07:28.604] iteration 11185: total_loss: 0.210773, loss_sup: 0.055191, loss_mps: 0.058487, loss_cps: 0.097095
[13:07:28.750] iteration 11186: total_loss: 0.252627, loss_sup: 0.043631, loss_mps: 0.073935, loss_cps: 0.135060
[13:07:28.896] iteration 11187: total_loss: 0.250125, loss_sup: 0.025314, loss_mps: 0.077143, loss_cps: 0.147667
[13:07:29.042] iteration 11188: total_loss: 0.434664, loss_sup: 0.121550, loss_mps: 0.104482, loss_cps: 0.208632
[13:07:29.188] iteration 11189: total_loss: 0.337997, loss_sup: 0.045527, loss_mps: 0.097969, loss_cps: 0.194501
[13:07:29.334] iteration 11190: total_loss: 0.183209, loss_sup: 0.019060, loss_mps: 0.059507, loss_cps: 0.104642
[13:07:29.481] iteration 11191: total_loss: 0.370459, loss_sup: 0.108518, loss_mps: 0.090275, loss_cps: 0.171666
[13:07:29.627] iteration 11192: total_loss: 0.402819, loss_sup: 0.077450, loss_mps: 0.107850, loss_cps: 0.217518
[13:07:29.772] iteration 11193: total_loss: 0.221868, loss_sup: 0.028966, loss_mps: 0.069897, loss_cps: 0.123005
[13:07:29.918] iteration 11194: total_loss: 0.255918, loss_sup: 0.031681, loss_mps: 0.079815, loss_cps: 0.144422
[13:07:30.064] iteration 11195: total_loss: 0.448174, loss_sup: 0.155461, loss_mps: 0.102956, loss_cps: 0.189757
[13:07:30.210] iteration 11196: total_loss: 0.203055, loss_sup: 0.025380, loss_mps: 0.065758, loss_cps: 0.111917
[13:07:30.356] iteration 11197: total_loss: 0.201513, loss_sup: 0.015646, loss_mps: 0.069169, loss_cps: 0.116698
[13:07:30.501] iteration 11198: total_loss: 0.440352, loss_sup: 0.228981, loss_mps: 0.072996, loss_cps: 0.138375
[13:07:30.647] iteration 11199: total_loss: 0.322227, loss_sup: 0.187212, loss_mps: 0.051895, loss_cps: 0.083121
[13:07:30.792] iteration 11200: total_loss: 0.223322, loss_sup: 0.029987, loss_mps: 0.064916, loss_cps: 0.128420
[13:07:30.793] Evaluation Started ==>
[13:07:42.281] ==> valid iteration 11200: unet metrics: {'dc': 0.6682067206703424, 'jc': 0.5432821881036373, 'pre': 0.7577967066048072, 'hd': 5.845525994518172}, ynet metrics: {'dc': 0.5529580796334338, 'jc': 0.4358473354183779, 'pre': 0.7568797182241976, 'hd': 5.921931238488518}.
[13:07:42.283] Evaluation Finished!⏹️
[13:07:42.436] iteration 11201: total_loss: 0.338662, loss_sup: 0.054324, loss_mps: 0.094645, loss_cps: 0.189693
[13:07:42.586] iteration 11202: total_loss: 0.483348, loss_sup: 0.303120, loss_mps: 0.065641, loss_cps: 0.114587
[13:07:42.731] iteration 11203: total_loss: 0.335143, loss_sup: 0.104870, loss_mps: 0.077762, loss_cps: 0.152512
[13:07:42.877] iteration 11204: total_loss: 0.232419, loss_sup: 0.038196, loss_mps: 0.069274, loss_cps: 0.124948
[13:07:43.022] iteration 11205: total_loss: 0.271242, loss_sup: 0.023079, loss_mps: 0.083111, loss_cps: 0.165052
[13:07:43.167] iteration 11206: total_loss: 0.277825, loss_sup: 0.068865, loss_mps: 0.073864, loss_cps: 0.135096
[13:07:43.314] iteration 11207: total_loss: 0.271541, loss_sup: 0.061890, loss_mps: 0.070563, loss_cps: 0.139087
[13:07:43.460] iteration 11208: total_loss: 0.336499, loss_sup: 0.182746, loss_mps: 0.058747, loss_cps: 0.095006
[13:07:43.607] iteration 11209: total_loss: 0.359558, loss_sup: 0.039064, loss_mps: 0.104501, loss_cps: 0.215993
[13:07:43.755] iteration 11210: total_loss: 0.434916, loss_sup: 0.156063, loss_mps: 0.092196, loss_cps: 0.186657
[13:07:43.901] iteration 11211: total_loss: 0.319997, loss_sup: 0.040366, loss_mps: 0.095411, loss_cps: 0.184220
[13:07:44.046] iteration 11212: total_loss: 0.241423, loss_sup: 0.059338, loss_mps: 0.067737, loss_cps: 0.114349
[13:07:44.192] iteration 11213: total_loss: 0.417177, loss_sup: 0.193119, loss_mps: 0.076325, loss_cps: 0.147733
[13:07:44.338] iteration 11214: total_loss: 0.254875, loss_sup: 0.070403, loss_mps: 0.062436, loss_cps: 0.122035
[13:07:44.483] iteration 11215: total_loss: 0.397617, loss_sup: 0.148676, loss_mps: 0.080642, loss_cps: 0.168299
[13:07:44.631] iteration 11216: total_loss: 0.237664, loss_sup: 0.073008, loss_mps: 0.057823, loss_cps: 0.106833
[13:07:44.777] iteration 11217: total_loss: 0.224233, loss_sup: 0.051032, loss_mps: 0.062373, loss_cps: 0.110828
[13:07:44.923] iteration 11218: total_loss: 0.251564, loss_sup: 0.024299, loss_mps: 0.074801, loss_cps: 0.152465
[13:07:45.072] iteration 11219: total_loss: 0.507232, loss_sup: 0.261082, loss_mps: 0.084258, loss_cps: 0.161892
[13:07:45.217] iteration 11220: total_loss: 0.358531, loss_sup: 0.097586, loss_mps: 0.086105, loss_cps: 0.174840
[13:07:45.365] iteration 11221: total_loss: 0.230485, loss_sup: 0.022444, loss_mps: 0.073218, loss_cps: 0.134823
[13:07:45.510] iteration 11222: total_loss: 0.261032, loss_sup: 0.026623, loss_mps: 0.077147, loss_cps: 0.157262
[13:07:45.657] iteration 11223: total_loss: 0.174846, loss_sup: 0.012076, loss_mps: 0.057178, loss_cps: 0.105592
[13:07:45.802] iteration 11224: total_loss: 0.183296, loss_sup: 0.032551, loss_mps: 0.054701, loss_cps: 0.096044
[13:07:45.947] iteration 11225: total_loss: 0.375644, loss_sup: 0.103911, loss_mps: 0.090119, loss_cps: 0.181614
[13:07:46.093] iteration 11226: total_loss: 0.293647, loss_sup: 0.089358, loss_mps: 0.072504, loss_cps: 0.131786
[13:07:46.239] iteration 11227: total_loss: 0.286883, loss_sup: 0.119178, loss_mps: 0.056745, loss_cps: 0.110960
[13:07:46.385] iteration 11228: total_loss: 0.366491, loss_sup: 0.075229, loss_mps: 0.093731, loss_cps: 0.197531
[13:07:46.532] iteration 11229: total_loss: 0.332046, loss_sup: 0.093515, loss_mps: 0.078695, loss_cps: 0.159836
[13:07:46.678] iteration 11230: total_loss: 0.224582, loss_sup: 0.039694, loss_mps: 0.065208, loss_cps: 0.119680
[13:07:46.823] iteration 11231: total_loss: 0.227463, loss_sup: 0.013655, loss_mps: 0.076465, loss_cps: 0.137343
[13:07:46.970] iteration 11232: total_loss: 0.246308, loss_sup: 0.039238, loss_mps: 0.072799, loss_cps: 0.134271
[13:07:47.116] iteration 11233: total_loss: 0.247545, loss_sup: 0.049547, loss_mps: 0.064763, loss_cps: 0.133235
[13:07:47.262] iteration 11234: total_loss: 0.270578, loss_sup: 0.008243, loss_mps: 0.085224, loss_cps: 0.177111
[13:07:47.409] iteration 11235: total_loss: 0.261525, loss_sup: 0.056123, loss_mps: 0.068183, loss_cps: 0.137219
[13:07:47.555] iteration 11236: total_loss: 0.250073, loss_sup: 0.017096, loss_mps: 0.078509, loss_cps: 0.154467
[13:07:47.700] iteration 11237: total_loss: 0.350475, loss_sup: 0.013100, loss_mps: 0.105822, loss_cps: 0.231553
[13:07:47.846] iteration 11238: total_loss: 0.298680, loss_sup: 0.074588, loss_mps: 0.073593, loss_cps: 0.150498
[13:07:47.992] iteration 11239: total_loss: 0.188107, loss_sup: 0.048734, loss_mps: 0.049102, loss_cps: 0.090270
[13:07:48.138] iteration 11240: total_loss: 0.293525, loss_sup: 0.095776, loss_mps: 0.070465, loss_cps: 0.127284
[13:07:48.284] iteration 11241: total_loss: 0.288910, loss_sup: 0.088582, loss_mps: 0.066106, loss_cps: 0.134222
[13:07:48.430] iteration 11242: total_loss: 0.357128, loss_sup: 0.204341, loss_mps: 0.056631, loss_cps: 0.096155
[13:07:48.576] iteration 11243: total_loss: 0.203321, loss_sup: 0.016255, loss_mps: 0.068949, loss_cps: 0.118118
[13:07:48.722] iteration 11244: total_loss: 0.131957, loss_sup: 0.005349, loss_mps: 0.046403, loss_cps: 0.080204
[13:07:48.869] iteration 11245: total_loss: 0.266481, loss_sup: 0.053942, loss_mps: 0.073159, loss_cps: 0.139380
[13:07:49.015] iteration 11246: total_loss: 0.268467, loss_sup: 0.055149, loss_mps: 0.071623, loss_cps: 0.141694
[13:07:49.161] iteration 11247: total_loss: 0.465247, loss_sup: 0.299481, loss_mps: 0.058325, loss_cps: 0.107440
[13:07:49.311] iteration 11248: total_loss: 0.273951, loss_sup: 0.011231, loss_mps: 0.083947, loss_cps: 0.178773
[13:07:49.457] iteration 11249: total_loss: 0.160787, loss_sup: 0.054895, loss_mps: 0.039833, loss_cps: 0.066059
[13:07:49.602] iteration 11250: total_loss: 0.297208, loss_sup: 0.166926, loss_mps: 0.047981, loss_cps: 0.082301
[13:07:49.748] iteration 11251: total_loss: 0.153017, loss_sup: 0.025492, loss_mps: 0.047896, loss_cps: 0.079629
[13:07:49.893] iteration 11252: total_loss: 0.360171, loss_sup: 0.162948, loss_mps: 0.066935, loss_cps: 0.130288
[13:07:50.039] iteration 11253: total_loss: 0.308061, loss_sup: 0.137502, loss_mps: 0.060499, loss_cps: 0.110060
[13:07:50.185] iteration 11254: total_loss: 0.311420, loss_sup: 0.033386, loss_mps: 0.088345, loss_cps: 0.189690
[13:07:50.330] iteration 11255: total_loss: 0.528346, loss_sup: 0.223775, loss_mps: 0.100626, loss_cps: 0.203944
[13:07:50.476] iteration 11256: total_loss: 0.574964, loss_sup: 0.317069, loss_mps: 0.084470, loss_cps: 0.173425
[13:07:50.622] iteration 11257: total_loss: 0.290432, loss_sup: 0.075508, loss_mps: 0.076029, loss_cps: 0.138895
[13:07:50.769] iteration 11258: total_loss: 0.438465, loss_sup: 0.238355, loss_mps: 0.071028, loss_cps: 0.129083
[13:07:50.915] iteration 11259: total_loss: 0.306408, loss_sup: 0.037108, loss_mps: 0.090832, loss_cps: 0.178468
[13:07:51.062] iteration 11260: total_loss: 0.230105, loss_sup: 0.066299, loss_mps: 0.058315, loss_cps: 0.105492
[13:07:51.208] iteration 11261: total_loss: 0.309416, loss_sup: 0.090466, loss_mps: 0.076994, loss_cps: 0.141956
[13:07:51.355] iteration 11262: total_loss: 0.370062, loss_sup: 0.086963, loss_mps: 0.093539, loss_cps: 0.189560
[13:07:51.501] iteration 11263: total_loss: 0.259932, loss_sup: 0.027351, loss_mps: 0.075640, loss_cps: 0.156941
[13:07:51.646] iteration 11264: total_loss: 0.432883, loss_sup: 0.196190, loss_mps: 0.080454, loss_cps: 0.156240
[13:07:51.792] iteration 11265: total_loss: 0.435789, loss_sup: 0.125294, loss_mps: 0.098696, loss_cps: 0.211799
[13:07:51.939] iteration 11266: total_loss: 0.407307, loss_sup: 0.086554, loss_mps: 0.105555, loss_cps: 0.215197
[13:07:52.085] iteration 11267: total_loss: 0.612746, loss_sup: 0.268276, loss_mps: 0.116758, loss_cps: 0.227711
[13:07:52.231] iteration 11268: total_loss: 0.227108, loss_sup: 0.105037, loss_mps: 0.048482, loss_cps: 0.073589
[13:07:52.376] iteration 11269: total_loss: 0.244401, loss_sup: 0.070708, loss_mps: 0.061948, loss_cps: 0.111745
[13:07:52.522] iteration 11270: total_loss: 0.341293, loss_sup: 0.139157, loss_mps: 0.071923, loss_cps: 0.130213
[13:07:52.668] iteration 11271: total_loss: 0.364027, loss_sup: 0.075954, loss_mps: 0.096530, loss_cps: 0.191543
[13:07:52.814] iteration 11272: total_loss: 0.210291, loss_sup: 0.009608, loss_mps: 0.073435, loss_cps: 0.127248
[13:07:52.960] iteration 11273: total_loss: 0.376052, loss_sup: 0.095364, loss_mps: 0.094476, loss_cps: 0.186212
[13:07:53.106] iteration 11274: total_loss: 0.290684, loss_sup: 0.026005, loss_mps: 0.088234, loss_cps: 0.176446
[13:07:53.252] iteration 11275: total_loss: 0.347771, loss_sup: 0.041036, loss_mps: 0.104010, loss_cps: 0.202724
[13:07:53.397] iteration 11276: total_loss: 0.441077, loss_sup: 0.058562, loss_mps: 0.118307, loss_cps: 0.264208
[13:07:53.543] iteration 11277: total_loss: 0.355318, loss_sup: 0.108923, loss_mps: 0.085793, loss_cps: 0.160603
[13:07:53.688] iteration 11278: total_loss: 0.423036, loss_sup: 0.067410, loss_mps: 0.118067, loss_cps: 0.237559
[13:07:53.834] iteration 11279: total_loss: 0.154032, loss_sup: 0.035997, loss_mps: 0.047348, loss_cps: 0.070687
[13:07:53.980] iteration 11280: total_loss: 0.737844, loss_sup: 0.415718, loss_mps: 0.105655, loss_cps: 0.216471
[13:07:54.125] iteration 11281: total_loss: 0.292306, loss_sup: 0.052448, loss_mps: 0.085614, loss_cps: 0.154244
[13:07:54.271] iteration 11282: total_loss: 0.200005, loss_sup: 0.017252, loss_mps: 0.065575, loss_cps: 0.117178
[13:07:54.416] iteration 11283: total_loss: 0.257319, loss_sup: 0.026989, loss_mps: 0.077938, loss_cps: 0.152392
[13:07:54.562] iteration 11284: total_loss: 0.214041, loss_sup: 0.047540, loss_mps: 0.058404, loss_cps: 0.108098
[13:07:54.709] iteration 11285: total_loss: 0.468018, loss_sup: 0.246687, loss_mps: 0.079252, loss_cps: 0.142080
[13:07:54.770] iteration 11286: total_loss: 0.303331, loss_sup: 0.015268, loss_mps: 0.098723, loss_cps: 0.189340
[13:07:55.981] iteration 11287: total_loss: 0.346593, loss_sup: 0.112193, loss_mps: 0.081036, loss_cps: 0.153364
[13:07:56.130] iteration 11288: total_loss: 0.158550, loss_sup: 0.025684, loss_mps: 0.051517, loss_cps: 0.081349
[13:07:56.278] iteration 11289: total_loss: 0.266147, loss_sup: 0.047765, loss_mps: 0.075907, loss_cps: 0.142475
[13:07:56.424] iteration 11290: total_loss: 0.308683, loss_sup: 0.019694, loss_mps: 0.099807, loss_cps: 0.189182
[13:07:56.573] iteration 11291: total_loss: 0.395856, loss_sup: 0.135320, loss_mps: 0.089918, loss_cps: 0.170619
[13:07:56.720] iteration 11292: total_loss: 0.548942, loss_sup: 0.235404, loss_mps: 0.103000, loss_cps: 0.210538
[13:07:56.867] iteration 11293: total_loss: 0.359562, loss_sup: 0.152838, loss_mps: 0.071483, loss_cps: 0.135241
[13:07:57.015] iteration 11294: total_loss: 0.371978, loss_sup: 0.092227, loss_mps: 0.092756, loss_cps: 0.186995
[13:07:57.162] iteration 11295: total_loss: 0.333010, loss_sup: 0.051076, loss_mps: 0.094332, loss_cps: 0.187602
[13:07:57.308] iteration 11296: total_loss: 0.277498, loss_sup: 0.069766, loss_mps: 0.075001, loss_cps: 0.132731
[13:07:57.455] iteration 11297: total_loss: 0.296678, loss_sup: 0.062032, loss_mps: 0.079865, loss_cps: 0.154781
[13:07:57.602] iteration 11298: total_loss: 0.356692, loss_sup: 0.084243, loss_mps: 0.094858, loss_cps: 0.177592
[13:07:57.749] iteration 11299: total_loss: 0.370635, loss_sup: 0.096526, loss_mps: 0.092610, loss_cps: 0.181498
[13:07:57.895] iteration 11300: total_loss: 0.405224, loss_sup: 0.098897, loss_mps: 0.104377, loss_cps: 0.201950
[13:07:57.895] Evaluation Started ==>
[13:08:09.435] ==> valid iteration 11300: unet metrics: {'dc': 0.6337248837171096, 'jc': 0.5095562277798232, 'pre': 0.7232484723942602, 'hd': 5.971862023887616}, ynet metrics: {'dc': 0.5387300119703865, 'jc': 0.4234221931621683, 'pre': 0.7439277134039136, 'hd': 6.054314914621795}.
[13:08:09.437] Evaluation Finished!⏹️
[13:08:09.591] iteration 11301: total_loss: 0.146189, loss_sup: 0.006654, loss_mps: 0.050953, loss_cps: 0.088583
[13:08:09.742] iteration 11302: total_loss: 0.224409, loss_sup: 0.035607, loss_mps: 0.068638, loss_cps: 0.120164
[13:08:09.887] iteration 11303: total_loss: 0.412797, loss_sup: 0.114032, loss_mps: 0.093939, loss_cps: 0.204826
[13:08:10.033] iteration 11304: total_loss: 0.261027, loss_sup: 0.075691, loss_mps: 0.066853, loss_cps: 0.118483
[13:08:10.178] iteration 11305: total_loss: 0.211458, loss_sup: 0.057751, loss_mps: 0.058025, loss_cps: 0.095682
[13:08:10.324] iteration 11306: total_loss: 0.166101, loss_sup: 0.020357, loss_mps: 0.054252, loss_cps: 0.091492
[13:08:10.469] iteration 11307: total_loss: 0.266816, loss_sup: 0.050538, loss_mps: 0.074412, loss_cps: 0.141866
[13:08:10.616] iteration 11308: total_loss: 0.304006, loss_sup: 0.081158, loss_mps: 0.076165, loss_cps: 0.146683
[13:08:10.761] iteration 11309: total_loss: 0.249436, loss_sup: 0.070074, loss_mps: 0.063035, loss_cps: 0.116327
[13:08:10.907] iteration 11310: total_loss: 0.222145, loss_sup: 0.007684, loss_mps: 0.075678, loss_cps: 0.138783
[13:08:11.052] iteration 11311: total_loss: 0.295199, loss_sup: 0.067701, loss_mps: 0.077467, loss_cps: 0.150031
[13:08:11.198] iteration 11312: total_loss: 0.223790, loss_sup: 0.090747, loss_mps: 0.051754, loss_cps: 0.081288
[13:08:11.344] iteration 11313: total_loss: 0.263631, loss_sup: 0.086636, loss_mps: 0.059790, loss_cps: 0.117205
[13:08:11.491] iteration 11314: total_loss: 0.235589, loss_sup: 0.071299, loss_mps: 0.057322, loss_cps: 0.106968
[13:08:11.641] iteration 11315: total_loss: 0.285999, loss_sup: 0.043494, loss_mps: 0.080864, loss_cps: 0.161641
[13:08:11.791] iteration 11316: total_loss: 0.211941, loss_sup: 0.040024, loss_mps: 0.060617, loss_cps: 0.111300
[13:08:11.939] iteration 11317: total_loss: 0.286058, loss_sup: 0.141750, loss_mps: 0.051221, loss_cps: 0.093087
[13:08:12.087] iteration 11318: total_loss: 0.347242, loss_sup: 0.068701, loss_mps: 0.089513, loss_cps: 0.189028
[13:08:12.232] iteration 11319: total_loss: 0.269829, loss_sup: 0.064730, loss_mps: 0.069939, loss_cps: 0.135160
[13:08:12.378] iteration 11320: total_loss: 0.361501, loss_sup: 0.086358, loss_mps: 0.093684, loss_cps: 0.181459
[13:08:12.524] iteration 11321: total_loss: 0.305816, loss_sup: 0.047346, loss_mps: 0.087559, loss_cps: 0.170911
[13:08:12.669] iteration 11322: total_loss: 0.257875, loss_sup: 0.013556, loss_mps: 0.081620, loss_cps: 0.162698
[13:08:12.814] iteration 11323: total_loss: 0.434022, loss_sup: 0.257306, loss_mps: 0.061353, loss_cps: 0.115364
[13:08:12.960] iteration 11324: total_loss: 0.258524, loss_sup: 0.034623, loss_mps: 0.078898, loss_cps: 0.145004
[13:08:13.106] iteration 11325: total_loss: 0.180043, loss_sup: 0.036484, loss_mps: 0.050828, loss_cps: 0.092731
[13:08:13.252] iteration 11326: total_loss: 0.223939, loss_sup: 0.021111, loss_mps: 0.070051, loss_cps: 0.132778
[13:08:13.400] iteration 11327: total_loss: 0.488120, loss_sup: 0.113580, loss_mps: 0.119292, loss_cps: 0.255249
[13:08:13.545] iteration 11328: total_loss: 0.144444, loss_sup: 0.016903, loss_mps: 0.046271, loss_cps: 0.081270
[13:08:13.691] iteration 11329: total_loss: 0.157805, loss_sup: 0.041162, loss_mps: 0.046363, loss_cps: 0.070280
[13:08:13.836] iteration 11330: total_loss: 0.486947, loss_sup: 0.252561, loss_mps: 0.079953, loss_cps: 0.154433
[13:08:13.982] iteration 11331: total_loss: 0.227777, loss_sup: 0.100257, loss_mps: 0.047963, loss_cps: 0.079556
[13:08:14.128] iteration 11332: total_loss: 0.737415, loss_sup: 0.138593, loss_mps: 0.182617, loss_cps: 0.416206
[13:08:14.277] iteration 11333: total_loss: 0.300928, loss_sup: 0.046750, loss_mps: 0.084306, loss_cps: 0.169872
[13:08:14.426] iteration 11334: total_loss: 0.247513, loss_sup: 0.042174, loss_mps: 0.069118, loss_cps: 0.136220
[13:08:14.573] iteration 11335: total_loss: 0.675782, loss_sup: 0.332857, loss_mps: 0.111445, loss_cps: 0.231481
[13:08:14.721] iteration 11336: total_loss: 0.279445, loss_sup: 0.101276, loss_mps: 0.064335, loss_cps: 0.113834
[13:08:14.869] iteration 11337: total_loss: 0.155922, loss_sup: 0.011897, loss_mps: 0.052553, loss_cps: 0.091472
[13:08:15.014] iteration 11338: total_loss: 0.234903, loss_sup: 0.027553, loss_mps: 0.073636, loss_cps: 0.133713
[13:08:15.160] iteration 11339: total_loss: 0.348582, loss_sup: 0.126244, loss_mps: 0.078395, loss_cps: 0.143944
[13:08:15.306] iteration 11340: total_loss: 0.204734, loss_sup: 0.025603, loss_mps: 0.066627, loss_cps: 0.112504
[13:08:15.451] iteration 11341: total_loss: 0.354346, loss_sup: 0.108238, loss_mps: 0.079702, loss_cps: 0.166407
[13:08:15.598] iteration 11342: total_loss: 0.254826, loss_sup: 0.027098, loss_mps: 0.075125, loss_cps: 0.152602
[13:08:15.745] iteration 11343: total_loss: 0.309633, loss_sup: 0.080117, loss_mps: 0.079308, loss_cps: 0.150209
[13:08:15.893] iteration 11344: total_loss: 0.271638, loss_sup: 0.033086, loss_mps: 0.083896, loss_cps: 0.154656
[13:08:16.040] iteration 11345: total_loss: 0.169098, loss_sup: 0.031725, loss_mps: 0.048901, loss_cps: 0.088472
[13:08:16.188] iteration 11346: total_loss: 0.301568, loss_sup: 0.063718, loss_mps: 0.084034, loss_cps: 0.153817
[13:08:16.335] iteration 11347: total_loss: 0.484896, loss_sup: 0.125493, loss_mps: 0.113378, loss_cps: 0.246025
[13:08:16.480] iteration 11348: total_loss: 0.306565, loss_sup: 0.157264, loss_mps: 0.053562, loss_cps: 0.095739
[13:08:16.627] iteration 11349: total_loss: 0.272137, loss_sup: 0.068875, loss_mps: 0.070829, loss_cps: 0.132433
[13:08:16.772] iteration 11350: total_loss: 0.249080, loss_sup: 0.094056, loss_mps: 0.057146, loss_cps: 0.097878
[13:08:16.919] iteration 11351: total_loss: 0.247026, loss_sup: 0.046223, loss_mps: 0.071010, loss_cps: 0.129793
[13:08:17.066] iteration 11352: total_loss: 0.278697, loss_sup: 0.051642, loss_mps: 0.079463, loss_cps: 0.147592
[13:08:17.212] iteration 11353: total_loss: 0.442508, loss_sup: 0.184134, loss_mps: 0.091158, loss_cps: 0.167215
[13:08:17.359] iteration 11354: total_loss: 0.348921, loss_sup: 0.031071, loss_mps: 0.106093, loss_cps: 0.211756
[13:08:17.504] iteration 11355: total_loss: 0.205972, loss_sup: 0.010900, loss_mps: 0.067934, loss_cps: 0.127139
[13:08:17.651] iteration 11356: total_loss: 0.365507, loss_sup: 0.104889, loss_mps: 0.087614, loss_cps: 0.173004
[13:08:17.799] iteration 11357: total_loss: 0.545462, loss_sup: 0.374135, loss_mps: 0.062352, loss_cps: 0.108975
[13:08:17.945] iteration 11358: total_loss: 0.150319, loss_sup: 0.003627, loss_mps: 0.052958, loss_cps: 0.093734
[13:08:18.090] iteration 11359: total_loss: 0.217459, loss_sup: 0.049483, loss_mps: 0.059067, loss_cps: 0.108910
[13:08:18.237] iteration 11360: total_loss: 0.245113, loss_sup: 0.040495, loss_mps: 0.071135, loss_cps: 0.133482
[13:08:18.383] iteration 11361: total_loss: 0.491078, loss_sup: 0.205214, loss_mps: 0.095374, loss_cps: 0.190490
[13:08:18.528] iteration 11362: total_loss: 0.371188, loss_sup: 0.121177, loss_mps: 0.083551, loss_cps: 0.166460
[13:08:18.673] iteration 11363: total_loss: 0.257418, loss_sup: 0.083121, loss_mps: 0.062404, loss_cps: 0.111894
[13:08:18.821] iteration 11364: total_loss: 0.363395, loss_sup: 0.150609, loss_mps: 0.072954, loss_cps: 0.139833
[13:08:18.972] iteration 11365: total_loss: 0.300932, loss_sup: 0.155954, loss_mps: 0.049242, loss_cps: 0.095736
[13:08:19.118] iteration 11366: total_loss: 0.277549, loss_sup: 0.030249, loss_mps: 0.082684, loss_cps: 0.164616
[13:08:19.264] iteration 11367: total_loss: 0.397325, loss_sup: 0.183650, loss_mps: 0.071402, loss_cps: 0.142274
[13:08:19.410] iteration 11368: total_loss: 0.232246, loss_sup: 0.036756, loss_mps: 0.070407, loss_cps: 0.125083
[13:08:19.558] iteration 11369: total_loss: 0.147325, loss_sup: 0.015993, loss_mps: 0.048645, loss_cps: 0.082688
[13:08:19.704] iteration 11370: total_loss: 0.360310, loss_sup: 0.098602, loss_mps: 0.087874, loss_cps: 0.173835
[13:08:19.852] iteration 11371: total_loss: 0.210291, loss_sup: 0.010525, loss_mps: 0.071894, loss_cps: 0.127872
[13:08:19.998] iteration 11372: total_loss: 0.199656, loss_sup: 0.044238, loss_mps: 0.058016, loss_cps: 0.097402
[13:08:20.144] iteration 11373: total_loss: 0.357818, loss_sup: 0.069763, loss_mps: 0.096191, loss_cps: 0.191864
[13:08:20.290] iteration 11374: total_loss: 0.229675, loss_sup: 0.012512, loss_mps: 0.072620, loss_cps: 0.144543
[13:08:20.436] iteration 11375: total_loss: 0.274328, loss_sup: 0.076077, loss_mps: 0.071355, loss_cps: 0.126896
[13:08:20.582] iteration 11376: total_loss: 0.280176, loss_sup: 0.045679, loss_mps: 0.084752, loss_cps: 0.149746
[13:08:20.730] iteration 11377: total_loss: 0.429686, loss_sup: 0.230432, loss_mps: 0.069746, loss_cps: 0.129508
[13:08:20.876] iteration 11378: total_loss: 0.377359, loss_sup: 0.034321, loss_mps: 0.110512, loss_cps: 0.232526
[13:08:21.024] iteration 11379: total_loss: 0.532266, loss_sup: 0.097639, loss_mps: 0.141330, loss_cps: 0.293297
[13:08:21.170] iteration 11380: total_loss: 0.298626, loss_sup: 0.016448, loss_mps: 0.095542, loss_cps: 0.186636
[13:08:21.318] iteration 11381: total_loss: 0.335272, loss_sup: 0.032467, loss_mps: 0.098796, loss_cps: 0.204009
[13:08:21.466] iteration 11382: total_loss: 0.391596, loss_sup: 0.034850, loss_mps: 0.116915, loss_cps: 0.239831
[13:08:21.612] iteration 11383: total_loss: 0.260306, loss_sup: 0.015284, loss_mps: 0.081124, loss_cps: 0.163899
[13:08:21.759] iteration 11384: total_loss: 0.243557, loss_sup: 0.060077, loss_mps: 0.063017, loss_cps: 0.120463
[13:08:21.907] iteration 11385: total_loss: 0.363262, loss_sup: 0.102554, loss_mps: 0.092598, loss_cps: 0.168110
[13:08:22.053] iteration 11386: total_loss: 0.341162, loss_sup: 0.083358, loss_mps: 0.089398, loss_cps: 0.168406
[13:08:22.199] iteration 11387: total_loss: 0.160951, loss_sup: 0.028525, loss_mps: 0.048473, loss_cps: 0.083953
[13:08:22.344] iteration 11388: total_loss: 0.265923, loss_sup: 0.071113, loss_mps: 0.069294, loss_cps: 0.125516
[13:08:22.490] iteration 11389: total_loss: 0.557555, loss_sup: 0.154313, loss_mps: 0.125716, loss_cps: 0.277525
[13:08:22.636] iteration 11390: total_loss: 0.316403, loss_sup: 0.155670, loss_mps: 0.061880, loss_cps: 0.098853
[13:08:22.783] iteration 11391: total_loss: 0.282506, loss_sup: 0.080319, loss_mps: 0.075351, loss_cps: 0.126836
[13:08:22.929] iteration 11392: total_loss: 0.417252, loss_sup: 0.097100, loss_mps: 0.098713, loss_cps: 0.221440
[13:08:23.075] iteration 11393: total_loss: 0.194166, loss_sup: 0.022809, loss_mps: 0.060590, loss_cps: 0.110767
[13:08:23.222] iteration 11394: total_loss: 0.231386, loss_sup: 0.082633, loss_mps: 0.054367, loss_cps: 0.094386
[13:08:23.372] iteration 11395: total_loss: 0.229599, loss_sup: 0.007174, loss_mps: 0.075857, loss_cps: 0.146568
[13:08:23.521] iteration 11396: total_loss: 0.143225, loss_sup: 0.012619, loss_mps: 0.050033, loss_cps: 0.080572
[13:08:23.670] iteration 11397: total_loss: 0.331504, loss_sup: 0.055553, loss_mps: 0.092304, loss_cps: 0.183647
[13:08:23.817] iteration 11398: total_loss: 0.183079, loss_sup: 0.030568, loss_mps: 0.054866, loss_cps: 0.097645
[13:08:23.962] iteration 11399: total_loss: 0.357093, loss_sup: 0.036407, loss_mps: 0.106784, loss_cps: 0.213902
[13:08:24.108] iteration 11400: total_loss: 0.274051, loss_sup: 0.034161, loss_mps: 0.076634, loss_cps: 0.163256
[13:08:24.109] Evaluation Started ==>
[13:08:35.550] ==> valid iteration 11400: unet metrics: {'dc': 0.6328315782681796, 'jc': 0.5130730660664634, 'pre': 0.7682134816403009, 'hd': 5.638558892151694}, ynet metrics: {'dc': 0.6057892286523447, 'jc': 0.48537531953960805, 'pre': 0.7298544319042702, 'hd': 6.039062354758905}.
[13:08:35.552] Evaluation Finished!⏹️
[13:08:35.702] iteration 11401: total_loss: 0.401438, loss_sup: 0.060141, loss_mps: 0.103606, loss_cps: 0.237691
[13:08:35.849] iteration 11402: total_loss: 0.443111, loss_sup: 0.196028, loss_mps: 0.086772, loss_cps: 0.160311
[13:08:35.995] iteration 11403: total_loss: 0.129378, loss_sup: 0.012168, loss_mps: 0.043500, loss_cps: 0.073710
[13:08:36.141] iteration 11404: total_loss: 0.360063, loss_sup: 0.149992, loss_mps: 0.073550, loss_cps: 0.136521
[13:08:36.286] iteration 11405: total_loss: 0.285557, loss_sup: 0.053175, loss_mps: 0.079371, loss_cps: 0.153012
[13:08:36.432] iteration 11406: total_loss: 0.344884, loss_sup: 0.087506, loss_mps: 0.089568, loss_cps: 0.167810
[13:08:36.579] iteration 11407: total_loss: 0.512732, loss_sup: 0.217060, loss_mps: 0.096415, loss_cps: 0.199257
[13:08:36.724] iteration 11408: total_loss: 0.111192, loss_sup: 0.006178, loss_mps: 0.039157, loss_cps: 0.065857
[13:08:36.869] iteration 11409: total_loss: 0.492637, loss_sup: 0.290952, loss_mps: 0.068136, loss_cps: 0.133549
[13:08:37.015] iteration 11410: total_loss: 0.195564, loss_sup: 0.039675, loss_mps: 0.058667, loss_cps: 0.097223
[13:08:37.161] iteration 11411: total_loss: 0.313234, loss_sup: 0.169289, loss_mps: 0.053928, loss_cps: 0.090017
[13:08:37.306] iteration 11412: total_loss: 0.388610, loss_sup: 0.154821, loss_mps: 0.079648, loss_cps: 0.154141
[13:08:37.452] iteration 11413: total_loss: 0.355808, loss_sup: 0.073058, loss_mps: 0.096291, loss_cps: 0.186459
[13:08:37.597] iteration 11414: total_loss: 0.488512, loss_sup: 0.252860, loss_mps: 0.081392, loss_cps: 0.154259
[13:08:37.744] iteration 11415: total_loss: 0.343909, loss_sup: 0.179159, loss_mps: 0.057403, loss_cps: 0.107347
[13:08:37.890] iteration 11416: total_loss: 0.417123, loss_sup: 0.053663, loss_mps: 0.116539, loss_cps: 0.246922
[13:08:38.036] iteration 11417: total_loss: 0.362873, loss_sup: 0.026302, loss_mps: 0.110147, loss_cps: 0.226424
[13:08:38.182] iteration 11418: total_loss: 0.317496, loss_sup: 0.135152, loss_mps: 0.068288, loss_cps: 0.114056
[13:08:38.329] iteration 11419: total_loss: 0.496188, loss_sup: 0.226709, loss_mps: 0.091596, loss_cps: 0.177882
[13:08:38.475] iteration 11420: total_loss: 0.248850, loss_sup: 0.040224, loss_mps: 0.073286, loss_cps: 0.135340
[13:08:38.621] iteration 11421: total_loss: 0.263085, loss_sup: 0.031159, loss_mps: 0.084074, loss_cps: 0.147853
[13:08:38.766] iteration 11422: total_loss: 0.315494, loss_sup: 0.061072, loss_mps: 0.086240, loss_cps: 0.168182
[13:08:38.911] iteration 11423: total_loss: 0.247455, loss_sup: 0.054298, loss_mps: 0.066949, loss_cps: 0.126208
[13:08:39.058] iteration 11424: total_loss: 0.224856, loss_sup: 0.021369, loss_mps: 0.071148, loss_cps: 0.132339
[13:08:39.203] iteration 11425: total_loss: 0.236420, loss_sup: 0.100035, loss_mps: 0.054046, loss_cps: 0.082338
[13:08:39.350] iteration 11426: total_loss: 0.294657, loss_sup: 0.068701, loss_mps: 0.078870, loss_cps: 0.147086
[13:08:39.496] iteration 11427: total_loss: 0.147112, loss_sup: 0.030907, loss_mps: 0.044096, loss_cps: 0.072108
[13:08:39.641] iteration 11428: total_loss: 0.402901, loss_sup: 0.212432, loss_mps: 0.067357, loss_cps: 0.123112
[13:08:39.789] iteration 11429: total_loss: 0.286075, loss_sup: 0.074314, loss_mps: 0.074723, loss_cps: 0.137038
[13:08:39.934] iteration 11430: total_loss: 0.325634, loss_sup: 0.052944, loss_mps: 0.088164, loss_cps: 0.184526
[13:08:40.080] iteration 11431: total_loss: 0.313895, loss_sup: 0.072959, loss_mps: 0.080949, loss_cps: 0.159987
[13:08:40.227] iteration 11432: total_loss: 0.272573, loss_sup: 0.020384, loss_mps: 0.087505, loss_cps: 0.164684
[13:08:40.372] iteration 11433: total_loss: 0.216855, loss_sup: 0.024739, loss_mps: 0.065739, loss_cps: 0.126376
[13:08:40.519] iteration 11434: total_loss: 0.184231, loss_sup: 0.031482, loss_mps: 0.054928, loss_cps: 0.097821
[13:08:40.665] iteration 11435: total_loss: 0.350132, loss_sup: 0.113620, loss_mps: 0.081183, loss_cps: 0.155328
[13:08:40.812] iteration 11436: total_loss: 0.357625, loss_sup: 0.112061, loss_mps: 0.083234, loss_cps: 0.162330
[13:08:40.959] iteration 11437: total_loss: 0.248207, loss_sup: 0.072229, loss_mps: 0.062131, loss_cps: 0.113847
[13:08:41.104] iteration 11438: total_loss: 0.263743, loss_sup: 0.059295, loss_mps: 0.068919, loss_cps: 0.135530
[13:08:41.250] iteration 11439: total_loss: 0.381439, loss_sup: 0.077738, loss_mps: 0.100417, loss_cps: 0.203285
[13:08:41.399] iteration 11440: total_loss: 0.331464, loss_sup: 0.091026, loss_mps: 0.079811, loss_cps: 0.160627
[13:08:41.544] iteration 11441: total_loss: 0.278022, loss_sup: 0.106311, loss_mps: 0.060735, loss_cps: 0.110976
[13:08:41.690] iteration 11442: total_loss: 0.332748, loss_sup: 0.076136, loss_mps: 0.085405, loss_cps: 0.171207
[13:08:41.835] iteration 11443: total_loss: 0.230502, loss_sup: 0.045117, loss_mps: 0.065098, loss_cps: 0.120287
[13:08:41.981] iteration 11444: total_loss: 0.239769, loss_sup: 0.029341, loss_mps: 0.075011, loss_cps: 0.135418
[13:08:42.127] iteration 11445: total_loss: 0.429662, loss_sup: 0.117603, loss_mps: 0.102810, loss_cps: 0.209249
[13:08:42.272] iteration 11446: total_loss: 0.320844, loss_sup: 0.104979, loss_mps: 0.075431, loss_cps: 0.140435
[13:08:42.419] iteration 11447: total_loss: 0.208605, loss_sup: 0.041953, loss_mps: 0.058753, loss_cps: 0.107899
[13:08:42.565] iteration 11448: total_loss: 0.416905, loss_sup: 0.176670, loss_mps: 0.078551, loss_cps: 0.161685
[13:08:42.714] iteration 11449: total_loss: 0.230485, loss_sup: 0.095042, loss_mps: 0.050172, loss_cps: 0.085271
[13:08:42.860] iteration 11450: total_loss: 0.351440, loss_sup: 0.016149, loss_mps: 0.107542, loss_cps: 0.227748
[13:08:43.008] iteration 11451: total_loss: 0.215100, loss_sup: 0.030676, loss_mps: 0.066841, loss_cps: 0.117583
[13:08:43.154] iteration 11452: total_loss: 0.283231, loss_sup: 0.072674, loss_mps: 0.074627, loss_cps: 0.135930
[13:08:43.299] iteration 11453: total_loss: 0.548599, loss_sup: 0.191126, loss_mps: 0.116134, loss_cps: 0.241339
[13:08:43.445] iteration 11454: total_loss: 0.416261, loss_sup: 0.211493, loss_mps: 0.072420, loss_cps: 0.132348
[13:08:43.591] iteration 11455: total_loss: 0.548457, loss_sup: 0.275934, loss_mps: 0.089940, loss_cps: 0.182583
[13:08:43.737] iteration 11456: total_loss: 0.317695, loss_sup: 0.134815, loss_mps: 0.064689, loss_cps: 0.118191
[13:08:43.884] iteration 11457: total_loss: 0.553717, loss_sup: 0.224223, loss_mps: 0.103243, loss_cps: 0.226251
[13:08:44.030] iteration 11458: total_loss: 0.307593, loss_sup: 0.094741, loss_mps: 0.076433, loss_cps: 0.136418
[13:08:44.176] iteration 11459: total_loss: 0.290204, loss_sup: 0.053516, loss_mps: 0.085685, loss_cps: 0.151003
[13:08:44.322] iteration 11460: total_loss: 0.296094, loss_sup: 0.076838, loss_mps: 0.080276, loss_cps: 0.138980
[13:08:44.474] iteration 11461: total_loss: 0.221008, loss_sup: 0.103666, loss_mps: 0.045593, loss_cps: 0.071749
[13:08:44.620] iteration 11462: total_loss: 0.181408, loss_sup: 0.022153, loss_mps: 0.062373, loss_cps: 0.096882
[13:08:44.765] iteration 11463: total_loss: 0.257236, loss_sup: 0.066728, loss_mps: 0.067596, loss_cps: 0.122913
[13:08:44.911] iteration 11464: total_loss: 0.199590, loss_sup: 0.048306, loss_mps: 0.056033, loss_cps: 0.095251
[13:08:45.057] iteration 11465: total_loss: 0.451364, loss_sup: 0.239049, loss_mps: 0.072366, loss_cps: 0.139949
[13:08:45.205] iteration 11466: total_loss: 0.205086, loss_sup: 0.043190, loss_mps: 0.066647, loss_cps: 0.095249
[13:08:45.351] iteration 11467: total_loss: 0.246175, loss_sup: 0.073354, loss_mps: 0.066898, loss_cps: 0.105923
[13:08:45.497] iteration 11468: total_loss: 0.255651, loss_sup: 0.010034, loss_mps: 0.085019, loss_cps: 0.160598
[13:08:45.643] iteration 11469: total_loss: 0.167873, loss_sup: 0.017260, loss_mps: 0.056882, loss_cps: 0.093732
[13:08:45.790] iteration 11470: total_loss: 0.217967, loss_sup: 0.058713, loss_mps: 0.059369, loss_cps: 0.099884
[13:08:45.935] iteration 11471: total_loss: 0.306219, loss_sup: 0.068968, loss_mps: 0.084350, loss_cps: 0.152901
[13:08:46.082] iteration 11472: total_loss: 0.287542, loss_sup: 0.034224, loss_mps: 0.085386, loss_cps: 0.167932
[13:08:46.229] iteration 11473: total_loss: 0.290682, loss_sup: 0.098509, loss_mps: 0.069507, loss_cps: 0.122665
[13:08:46.375] iteration 11474: total_loss: 0.374317, loss_sup: 0.094419, loss_mps: 0.098445, loss_cps: 0.181454
[13:08:46.521] iteration 11475: total_loss: 0.315871, loss_sup: 0.153329, loss_mps: 0.063752, loss_cps: 0.098790
[13:08:46.672] iteration 11476: total_loss: 0.229746, loss_sup: 0.059502, loss_mps: 0.060541, loss_cps: 0.109703
[13:08:46.820] iteration 11477: total_loss: 0.349251, loss_sup: 0.075257, loss_mps: 0.091631, loss_cps: 0.182363
[13:08:46.966] iteration 11478: total_loss: 0.302035, loss_sup: 0.089987, loss_mps: 0.074541, loss_cps: 0.137507
[13:08:47.113] iteration 11479: total_loss: 0.241407, loss_sup: 0.057395, loss_mps: 0.064886, loss_cps: 0.119126
[13:08:47.260] iteration 11480: total_loss: 0.234097, loss_sup: 0.059458, loss_mps: 0.067360, loss_cps: 0.107279
[13:08:47.408] iteration 11481: total_loss: 0.230858, loss_sup: 0.035037, loss_mps: 0.067601, loss_cps: 0.128220
[13:08:47.555] iteration 11482: total_loss: 0.398618, loss_sup: 0.129559, loss_mps: 0.089299, loss_cps: 0.179761
[13:08:47.702] iteration 11483: total_loss: 0.405025, loss_sup: 0.178059, loss_mps: 0.073451, loss_cps: 0.153515
[13:08:47.850] iteration 11484: total_loss: 0.436712, loss_sup: 0.219925, loss_mps: 0.073605, loss_cps: 0.143182
[13:08:47.996] iteration 11485: total_loss: 0.483522, loss_sup: 0.279877, loss_mps: 0.067030, loss_cps: 0.136614
[13:08:48.141] iteration 11486: total_loss: 0.392696, loss_sup: 0.040715, loss_mps: 0.108960, loss_cps: 0.243021
[13:08:48.288] iteration 11487: total_loss: 0.195681, loss_sup: 0.057271, loss_mps: 0.051120, loss_cps: 0.087289
[13:08:48.442] iteration 11488: total_loss: 0.242800, loss_sup: 0.050184, loss_mps: 0.069516, loss_cps: 0.123099
[13:08:48.589] iteration 11489: total_loss: 0.502204, loss_sup: 0.055783, loss_mps: 0.135512, loss_cps: 0.310909
[13:08:48.736] iteration 11490: total_loss: 0.342856, loss_sup: 0.133835, loss_mps: 0.075416, loss_cps: 0.133605
[13:08:48.882] iteration 11491: total_loss: 0.332815, loss_sup: 0.040006, loss_mps: 0.094496, loss_cps: 0.198314
[13:08:49.028] iteration 11492: total_loss: 0.832739, loss_sup: 0.239886, loss_mps: 0.183844, loss_cps: 0.409009
[13:08:49.174] iteration 11493: total_loss: 0.282372, loss_sup: 0.011707, loss_mps: 0.089818, loss_cps: 0.180848
[13:08:49.320] iteration 11494: total_loss: 0.222221, loss_sup: 0.078065, loss_mps: 0.054686, loss_cps: 0.089470
[13:08:49.468] iteration 11495: total_loss: 0.491790, loss_sup: 0.164260, loss_mps: 0.110315, loss_cps: 0.217216
[13:08:49.614] iteration 11496: total_loss: 0.185110, loss_sup: 0.005512, loss_mps: 0.067032, loss_cps: 0.112566
[13:08:49.759] iteration 11497: total_loss: 0.235809, loss_sup: 0.019895, loss_mps: 0.074849, loss_cps: 0.141065
[13:08:49.905] iteration 11498: total_loss: 0.308915, loss_sup: 0.110266, loss_mps: 0.069911, loss_cps: 0.128738
[13:08:50.055] iteration 11499: total_loss: 0.160908, loss_sup: 0.013060, loss_mps: 0.053640, loss_cps: 0.094208
[13:08:50.201] iteration 11500: total_loss: 0.342532, loss_sup: 0.079292, loss_mps: 0.088917, loss_cps: 0.174323
[13:08:50.201] Evaluation Started ==>
[13:09:01.661] ==> valid iteration 11500: unet metrics: {'dc': 0.6688854826525658, 'jc': 0.5456138020626271, 'pre': 0.7492547874559422, 'hd': 5.816818216830894}, ynet metrics: {'dc': 0.5569715367244972, 'jc': 0.4428274337484116, 'pre': 0.7739062384688561, 'hd': 5.779463691552355}.
[13:09:01.723] ==> New best valid dice for unet: 0.668885, at iteration 11500
[13:09:01.725] Evaluation Finished!⏹️
[13:09:01.877] iteration 11501: total_loss: 0.648991, loss_sup: 0.466725, loss_mps: 0.066060, loss_cps: 0.116207
[13:09:02.027] iteration 11502: total_loss: 0.259755, loss_sup: 0.023896, loss_mps: 0.084051, loss_cps: 0.151808
[13:09:02.171] iteration 11503: total_loss: 0.348447, loss_sup: 0.095983, loss_mps: 0.086543, loss_cps: 0.165922
[13:09:02.316] iteration 11504: total_loss: 0.191207, loss_sup: 0.021186, loss_mps: 0.065473, loss_cps: 0.104548
[13:09:02.461] iteration 11505: total_loss: 0.274671, loss_sup: 0.053127, loss_mps: 0.070525, loss_cps: 0.151019
[13:09:02.607] iteration 11506: total_loss: 0.334943, loss_sup: 0.120560, loss_mps: 0.071981, loss_cps: 0.142403
[13:09:02.752] iteration 11507: total_loss: 0.198656, loss_sup: 0.037223, loss_mps: 0.059804, loss_cps: 0.101628
[13:09:02.897] iteration 11508: total_loss: 0.331798, loss_sup: 0.177513, loss_mps: 0.055488, loss_cps: 0.098797
[13:09:03.043] iteration 11509: total_loss: 0.667545, loss_sup: 0.268866, loss_mps: 0.127928, loss_cps: 0.270750
[13:09:03.190] iteration 11510: total_loss: 0.369781, loss_sup: 0.106190, loss_mps: 0.088584, loss_cps: 0.175006
[13:09:03.335] iteration 11511: total_loss: 0.359918, loss_sup: 0.120721, loss_mps: 0.079855, loss_cps: 0.159343
[13:09:03.480] iteration 11512: total_loss: 0.303037, loss_sup: 0.092158, loss_mps: 0.074732, loss_cps: 0.136146
[13:09:03.626] iteration 11513: total_loss: 0.169816, loss_sup: 0.016144, loss_mps: 0.061366, loss_cps: 0.092306
[13:09:03.772] iteration 11514: total_loss: 0.579952, loss_sup: 0.171132, loss_mps: 0.136145, loss_cps: 0.272675
[13:09:03.917] iteration 11515: total_loss: 0.407628, loss_sup: 0.172074, loss_mps: 0.081442, loss_cps: 0.154112
[13:09:04.062] iteration 11516: total_loss: 0.350860, loss_sup: 0.126115, loss_mps: 0.079199, loss_cps: 0.145546
[13:09:04.208] iteration 11517: total_loss: 0.337967, loss_sup: 0.032497, loss_mps: 0.098432, loss_cps: 0.207039
[13:09:04.356] iteration 11518: total_loss: 0.321181, loss_sup: 0.107683, loss_mps: 0.074548, loss_cps: 0.138950
[13:09:04.501] iteration 11519: total_loss: 0.443399, loss_sup: 0.058904, loss_mps: 0.121475, loss_cps: 0.263020
[13:09:04.647] iteration 11520: total_loss: 0.300808, loss_sup: 0.165820, loss_mps: 0.050309, loss_cps: 0.084679
[13:09:04.792] iteration 11521: total_loss: 0.541424, loss_sup: 0.252200, loss_mps: 0.094660, loss_cps: 0.194563
[13:09:04.937] iteration 11522: total_loss: 0.318360, loss_sup: 0.075083, loss_mps: 0.081531, loss_cps: 0.161745
[13:09:05.082] iteration 11523: total_loss: 0.294623, loss_sup: 0.042175, loss_mps: 0.085082, loss_cps: 0.167367
[13:09:05.228] iteration 11524: total_loss: 0.303162, loss_sup: 0.094329, loss_mps: 0.077358, loss_cps: 0.131475
[13:09:05.375] iteration 11525: total_loss: 0.267515, loss_sup: 0.089229, loss_mps: 0.065578, loss_cps: 0.112707
[13:09:05.522] iteration 11526: total_loss: 0.242728, loss_sup: 0.084997, loss_mps: 0.059165, loss_cps: 0.098566
[13:09:05.668] iteration 11527: total_loss: 0.225429, loss_sup: 0.053274, loss_mps: 0.063444, loss_cps: 0.108711
[13:09:05.813] iteration 11528: total_loss: 0.303081, loss_sup: 0.035621, loss_mps: 0.093432, loss_cps: 0.174028
[13:09:05.960] iteration 11529: total_loss: 0.246609, loss_sup: 0.077286, loss_mps: 0.066303, loss_cps: 0.103020
[13:09:06.106] iteration 11530: total_loss: 0.397893, loss_sup: 0.176468, loss_mps: 0.080545, loss_cps: 0.140880
[13:09:06.251] iteration 11531: total_loss: 0.455414, loss_sup: 0.062841, loss_mps: 0.124431, loss_cps: 0.268142
[13:09:06.396] iteration 11532: total_loss: 0.356837, loss_sup: 0.030571, loss_mps: 0.112755, loss_cps: 0.213511
[13:09:06.543] iteration 11533: total_loss: 0.444153, loss_sup: 0.120394, loss_mps: 0.113829, loss_cps: 0.209929
[13:09:06.691] iteration 11534: total_loss: 0.267537, loss_sup: 0.057142, loss_mps: 0.075878, loss_cps: 0.134517
[13:09:06.838] iteration 11535: total_loss: 0.358361, loss_sup: 0.133978, loss_mps: 0.077464, loss_cps: 0.146919
[13:09:06.984] iteration 11536: total_loss: 0.280581, loss_sup: 0.026314, loss_mps: 0.083629, loss_cps: 0.170637
[13:09:07.131] iteration 11537: total_loss: 0.311140, loss_sup: 0.150859, loss_mps: 0.056261, loss_cps: 0.104020
[13:09:07.277] iteration 11538: total_loss: 0.294587, loss_sup: 0.040458, loss_mps: 0.088718, loss_cps: 0.165411
[13:09:07.423] iteration 11539: total_loss: 0.355109, loss_sup: 0.051314, loss_mps: 0.097010, loss_cps: 0.206785
[13:09:07.568] iteration 11540: total_loss: 0.385758, loss_sup: 0.150368, loss_mps: 0.085901, loss_cps: 0.149489
[13:09:07.715] iteration 11541: total_loss: 0.452614, loss_sup: 0.105473, loss_mps: 0.112760, loss_cps: 0.234381
[13:09:07.863] iteration 11542: total_loss: 0.202771, loss_sup: 0.025875, loss_mps: 0.062758, loss_cps: 0.114138
[13:09:08.008] iteration 11543: total_loss: 0.386383, loss_sup: 0.075400, loss_mps: 0.105969, loss_cps: 0.205015
[13:09:08.154] iteration 11544: total_loss: 0.293736, loss_sup: 0.025548, loss_mps: 0.089521, loss_cps: 0.178666
[13:09:08.300] iteration 11545: total_loss: 0.302488, loss_sup: 0.030720, loss_mps: 0.093271, loss_cps: 0.178497
[13:09:08.446] iteration 11546: total_loss: 0.162402, loss_sup: 0.013987, loss_mps: 0.055129, loss_cps: 0.093286
[13:09:08.592] iteration 11547: total_loss: 0.328085, loss_sup: 0.059560, loss_mps: 0.086872, loss_cps: 0.181653
[13:09:08.739] iteration 11548: total_loss: 0.364404, loss_sup: 0.127278, loss_mps: 0.080023, loss_cps: 0.157104
[13:09:08.885] iteration 11549: total_loss: 0.191858, loss_sup: 0.025489, loss_mps: 0.062451, loss_cps: 0.103918
[13:09:09.031] iteration 11550: total_loss: 0.277870, loss_sup: 0.066480, loss_mps: 0.068169, loss_cps: 0.143220
[13:09:09.178] iteration 11551: total_loss: 0.348679, loss_sup: 0.164345, loss_mps: 0.064066, loss_cps: 0.120269
[13:09:09.324] iteration 11552: total_loss: 0.428159, loss_sup: 0.073331, loss_mps: 0.110210, loss_cps: 0.244618
[13:09:09.472] iteration 11553: total_loss: 0.441425, loss_sup: 0.059107, loss_mps: 0.119699, loss_cps: 0.262619
[13:09:09.621] iteration 11554: total_loss: 0.315750, loss_sup: 0.136796, loss_mps: 0.063085, loss_cps: 0.115870
[13:09:09.768] iteration 11555: total_loss: 0.268541, loss_sup: 0.023243, loss_mps: 0.082540, loss_cps: 0.162758
[13:09:09.915] iteration 11556: total_loss: 0.336920, loss_sup: 0.035728, loss_mps: 0.094998, loss_cps: 0.206195
[13:09:10.061] iteration 11557: total_loss: 0.437305, loss_sup: 0.160068, loss_mps: 0.091102, loss_cps: 0.186134
[13:09:10.208] iteration 11558: total_loss: 0.352245, loss_sup: 0.169352, loss_mps: 0.063322, loss_cps: 0.119571
[13:09:10.356] iteration 11559: total_loss: 0.376986, loss_sup: 0.074570, loss_mps: 0.096351, loss_cps: 0.206065
[13:09:10.503] iteration 11560: total_loss: 0.438218, loss_sup: 0.101349, loss_mps: 0.111520, loss_cps: 0.225349
[13:09:10.650] iteration 11561: total_loss: 0.416413, loss_sup: 0.117380, loss_mps: 0.104827, loss_cps: 0.194206
[13:09:10.795] iteration 11562: total_loss: 0.373477, loss_sup: 0.062035, loss_mps: 0.099181, loss_cps: 0.212262
[13:09:10.942] iteration 11563: total_loss: 0.244488, loss_sup: 0.079219, loss_mps: 0.059122, loss_cps: 0.106147
[13:09:11.089] iteration 11564: total_loss: 0.534478, loss_sup: 0.123149, loss_mps: 0.134903, loss_cps: 0.276426
[13:09:11.240] iteration 11565: total_loss: 0.328712, loss_sup: 0.070276, loss_mps: 0.090523, loss_cps: 0.167913
[13:09:11.386] iteration 11566: total_loss: 0.576523, loss_sup: 0.172044, loss_mps: 0.129125, loss_cps: 0.275353
[13:09:11.533] iteration 11567: total_loss: 0.402362, loss_sup: 0.073901, loss_mps: 0.106786, loss_cps: 0.221675
[13:09:11.679] iteration 11568: total_loss: 0.365642, loss_sup: 0.031987, loss_mps: 0.112141, loss_cps: 0.221514
[13:09:11.826] iteration 11569: total_loss: 0.267648, loss_sup: 0.036693, loss_mps: 0.077910, loss_cps: 0.153046
[13:09:11.971] iteration 11570: total_loss: 0.308199, loss_sup: 0.052949, loss_mps: 0.084765, loss_cps: 0.170485
[13:09:12.118] iteration 11571: total_loss: 0.224420, loss_sup: 0.084070, loss_mps: 0.051496, loss_cps: 0.088854
[13:09:12.266] iteration 11572: total_loss: 0.367593, loss_sup: 0.189521, loss_mps: 0.063364, loss_cps: 0.114709
[13:09:12.412] iteration 11573: total_loss: 0.612890, loss_sup: 0.237002, loss_mps: 0.124291, loss_cps: 0.251598
[13:09:12.558] iteration 11574: total_loss: 0.211598, loss_sup: 0.024628, loss_mps: 0.065505, loss_cps: 0.121465
[13:09:12.706] iteration 11575: total_loss: 0.307751, loss_sup: 0.104964, loss_mps: 0.069049, loss_cps: 0.133739
[13:09:12.854] iteration 11576: total_loss: 0.169225, loss_sup: 0.009854, loss_mps: 0.055090, loss_cps: 0.104281
[13:09:13.001] iteration 11577: total_loss: 0.245277, loss_sup: 0.069893, loss_mps: 0.062053, loss_cps: 0.113332
[13:09:13.147] iteration 11578: total_loss: 0.235948, loss_sup: 0.036023, loss_mps: 0.066935, loss_cps: 0.132990
[13:09:13.294] iteration 11579: total_loss: 0.441053, loss_sup: 0.034912, loss_mps: 0.125686, loss_cps: 0.280455
[13:09:13.444] iteration 11580: total_loss: 0.276812, loss_sup: 0.061565, loss_mps: 0.070281, loss_cps: 0.144965
[13:09:13.591] iteration 11581: total_loss: 0.499788, loss_sup: 0.107980, loss_mps: 0.119258, loss_cps: 0.272549
[13:09:13.742] iteration 11582: total_loss: 0.174205, loss_sup: 0.034767, loss_mps: 0.053292, loss_cps: 0.086146
[13:09:13.890] iteration 11583: total_loss: 0.271302, loss_sup: 0.124188, loss_mps: 0.053149, loss_cps: 0.093965
[13:09:14.036] iteration 11584: total_loss: 0.306951, loss_sup: 0.088747, loss_mps: 0.077895, loss_cps: 0.140309
[13:09:14.182] iteration 11585: total_loss: 0.745721, loss_sup: 0.288609, loss_mps: 0.139053, loss_cps: 0.318060
[13:09:14.332] iteration 11586: total_loss: 0.224075, loss_sup: 0.041689, loss_mps: 0.064006, loss_cps: 0.118379
[13:09:14.478] iteration 11587: total_loss: 0.213120, loss_sup: 0.059368, loss_mps: 0.056897, loss_cps: 0.096854
[13:09:14.628] iteration 11588: total_loss: 0.474612, loss_sup: 0.071459, loss_mps: 0.129865, loss_cps: 0.273288
[13:09:14.774] iteration 11589: total_loss: 0.473251, loss_sup: 0.124490, loss_mps: 0.113459, loss_cps: 0.235301
[13:09:14.921] iteration 11590: total_loss: 0.322883, loss_sup: 0.057330, loss_mps: 0.092122, loss_cps: 0.173431
[13:09:15.066] iteration 11591: total_loss: 0.288968, loss_sup: 0.082492, loss_mps: 0.072605, loss_cps: 0.133870
[13:09:15.213] iteration 11592: total_loss: 0.351904, loss_sup: 0.094325, loss_mps: 0.092493, loss_cps: 0.165086
[13:09:15.360] iteration 11593: total_loss: 0.473368, loss_sup: 0.135658, loss_mps: 0.108267, loss_cps: 0.229442
[13:09:15.507] iteration 11594: total_loss: 0.372618, loss_sup: 0.157821, loss_mps: 0.077602, loss_cps: 0.137195
[13:09:15.653] iteration 11595: total_loss: 0.203795, loss_sup: 0.018903, loss_mps: 0.070403, loss_cps: 0.114489
[13:09:15.800] iteration 11596: total_loss: 0.450128, loss_sup: 0.086877, loss_mps: 0.118548, loss_cps: 0.244702
[13:09:15.948] iteration 11597: total_loss: 0.536488, loss_sup: 0.292391, loss_mps: 0.085339, loss_cps: 0.158759
[13:09:16.096] iteration 11598: total_loss: 0.209516, loss_sup: 0.067324, loss_mps: 0.054311, loss_cps: 0.087882
[13:09:16.242] iteration 11599: total_loss: 0.318740, loss_sup: 0.102216, loss_mps: 0.075342, loss_cps: 0.141182
[13:09:16.389] iteration 11600: total_loss: 0.316680, loss_sup: 0.064620, loss_mps: 0.092093, loss_cps: 0.159968
[13:09:16.389] Evaluation Started ==>
[13:09:27.802] ==> valid iteration 11600: unet metrics: {'dc': 0.6080891315113072, 'jc': 0.491848029063396, 'pre': 0.7194462383852123, 'hd': 5.909650894180895}, ynet metrics: {'dc': 0.538063397057446, 'jc': 0.426171111843624, 'pre': 0.7278938022949796, 'hd': 5.861968547490378}.
[13:09:27.803] Evaluation Finished!⏹️
[13:09:27.954] iteration 11601: total_loss: 0.226060, loss_sup: 0.025996, loss_mps: 0.071448, loss_cps: 0.128615
[13:09:28.102] iteration 11602: total_loss: 0.239328, loss_sup: 0.044531, loss_mps: 0.067432, loss_cps: 0.127366
[13:09:28.249] iteration 11603: total_loss: 0.213915, loss_sup: 0.038034, loss_mps: 0.064213, loss_cps: 0.111669
[13:09:28.395] iteration 11604: total_loss: 0.267396, loss_sup: 0.044416, loss_mps: 0.081648, loss_cps: 0.141332
[13:09:28.540] iteration 11605: total_loss: 0.365375, loss_sup: 0.041166, loss_mps: 0.112114, loss_cps: 0.212095
[13:09:28.687] iteration 11606: total_loss: 0.255458, loss_sup: 0.036620, loss_mps: 0.078562, loss_cps: 0.140276
[13:09:28.832] iteration 11607: total_loss: 0.305857, loss_sup: 0.072490, loss_mps: 0.081962, loss_cps: 0.151405
[13:09:28.978] iteration 11608: total_loss: 0.183759, loss_sup: 0.044817, loss_mps: 0.052142, loss_cps: 0.086800
[13:09:29.123] iteration 11609: total_loss: 0.304567, loss_sup: 0.031394, loss_mps: 0.093101, loss_cps: 0.180071
[13:09:29.269] iteration 11610: total_loss: 0.248885, loss_sup: 0.021318, loss_mps: 0.078806, loss_cps: 0.148761
[13:09:29.415] iteration 11611: total_loss: 0.399341, loss_sup: 0.007231, loss_mps: 0.125270, loss_cps: 0.266840
[13:09:29.560] iteration 11612: total_loss: 0.496639, loss_sup: 0.284238, loss_mps: 0.076082, loss_cps: 0.136320
[13:09:29.708] iteration 11613: total_loss: 0.326937, loss_sup: 0.115063, loss_mps: 0.076250, loss_cps: 0.135624
[13:09:29.855] iteration 11614: total_loss: 0.191609, loss_sup: 0.025369, loss_mps: 0.058100, loss_cps: 0.108140
[13:09:30.001] iteration 11615: total_loss: 0.416098, loss_sup: 0.067519, loss_mps: 0.110490, loss_cps: 0.238090
[13:09:30.148] iteration 11616: total_loss: 0.229310, loss_sup: 0.043127, loss_mps: 0.066014, loss_cps: 0.120169
[13:09:30.294] iteration 11617: total_loss: 0.267368, loss_sup: 0.025066, loss_mps: 0.081560, loss_cps: 0.160741
[13:09:30.439] iteration 11618: total_loss: 0.327563, loss_sup: 0.170874, loss_mps: 0.055591, loss_cps: 0.101098
[13:09:30.585] iteration 11619: total_loss: 0.586528, loss_sup: 0.369797, loss_mps: 0.073172, loss_cps: 0.143559
[13:09:30.730] iteration 11620: total_loss: 0.497467, loss_sup: 0.251537, loss_mps: 0.085055, loss_cps: 0.160875
[13:09:30.876] iteration 11621: total_loss: 0.259244, loss_sup: 0.068842, loss_mps: 0.069025, loss_cps: 0.121378
[13:09:31.024] iteration 11622: total_loss: 0.380865, loss_sup: 0.080831, loss_mps: 0.096624, loss_cps: 0.203410
[13:09:31.169] iteration 11623: total_loss: 0.318609, loss_sup: 0.062481, loss_mps: 0.089153, loss_cps: 0.166975
[13:09:31.315] iteration 11624: total_loss: 0.229088, loss_sup: 0.109785, loss_mps: 0.049839, loss_cps: 0.069464
[13:09:31.460] iteration 11625: total_loss: 0.291350, loss_sup: 0.101183, loss_mps: 0.068377, loss_cps: 0.121790
[13:09:31.606] iteration 11626: total_loss: 0.484949, loss_sup: 0.141960, loss_mps: 0.112072, loss_cps: 0.230917
[13:09:31.752] iteration 11627: total_loss: 0.252561, loss_sup: 0.098235, loss_mps: 0.054872, loss_cps: 0.099454
[13:09:31.897] iteration 11628: total_loss: 0.378311, loss_sup: 0.131924, loss_mps: 0.082036, loss_cps: 0.164351
[13:09:32.042] iteration 11629: total_loss: 0.205675, loss_sup: 0.021375, loss_mps: 0.066151, loss_cps: 0.118149
[13:09:32.188] iteration 11630: total_loss: 0.363098, loss_sup: 0.132899, loss_mps: 0.084320, loss_cps: 0.145879
[13:09:32.333] iteration 11631: total_loss: 0.271653, loss_sup: 0.058164, loss_mps: 0.076339, loss_cps: 0.137150
[13:09:32.480] iteration 11632: total_loss: 0.227634, loss_sup: 0.069644, loss_mps: 0.058754, loss_cps: 0.099236
[13:09:32.626] iteration 11633: total_loss: 0.245980, loss_sup: 0.043450, loss_mps: 0.071496, loss_cps: 0.131033
[13:09:32.772] iteration 11634: total_loss: 0.283188, loss_sup: 0.036024, loss_mps: 0.083400, loss_cps: 0.163764
[13:09:32.921] iteration 11635: total_loss: 0.328257, loss_sup: 0.049803, loss_mps: 0.098369, loss_cps: 0.180085
[13:09:33.066] iteration 11636: total_loss: 0.487299, loss_sup: 0.265428, loss_mps: 0.081798, loss_cps: 0.140073
[13:09:33.211] iteration 11637: total_loss: 0.242357, loss_sup: 0.015001, loss_mps: 0.077212, loss_cps: 0.150144
[13:09:33.357] iteration 11638: total_loss: 0.263112, loss_sup: 0.065339, loss_mps: 0.068448, loss_cps: 0.129325
[13:09:33.503] iteration 11639: total_loss: 0.337281, loss_sup: 0.166755, loss_mps: 0.063231, loss_cps: 0.107294
[13:09:33.648] iteration 11640: total_loss: 0.364622, loss_sup: 0.144677, loss_mps: 0.079481, loss_cps: 0.140464
[13:09:33.794] iteration 11641: total_loss: 0.432379, loss_sup: 0.104304, loss_mps: 0.108043, loss_cps: 0.220032
[13:09:33.940] iteration 11642: total_loss: 0.166246, loss_sup: 0.060107, loss_mps: 0.041265, loss_cps: 0.064874
[13:09:34.085] iteration 11643: total_loss: 0.440710, loss_sup: 0.088210, loss_mps: 0.109777, loss_cps: 0.242723
[13:09:34.231] iteration 11644: total_loss: 0.505864, loss_sup: 0.087812, loss_mps: 0.133115, loss_cps: 0.284937
[13:09:34.377] iteration 11645: total_loss: 0.224471, loss_sup: 0.067077, loss_mps: 0.060156, loss_cps: 0.097237
[13:09:34.524] iteration 11646: total_loss: 0.282311, loss_sup: 0.022610, loss_mps: 0.084674, loss_cps: 0.175026
[13:09:34.670] iteration 11647: total_loss: 0.397112, loss_sup: 0.143676, loss_mps: 0.084312, loss_cps: 0.169124
[13:09:34.818] iteration 11648: total_loss: 0.309012, loss_sup: 0.037844, loss_mps: 0.087390, loss_cps: 0.183778
[13:09:34.964] iteration 11649: total_loss: 0.290273, loss_sup: 0.015877, loss_mps: 0.088760, loss_cps: 0.185636
[13:09:35.109] iteration 11650: total_loss: 0.420712, loss_sup: 0.142780, loss_mps: 0.091153, loss_cps: 0.186779
[13:09:35.256] iteration 11651: total_loss: 0.310606, loss_sup: 0.069084, loss_mps: 0.083487, loss_cps: 0.158035
[13:09:35.402] iteration 11652: total_loss: 0.542278, loss_sup: 0.281694, loss_mps: 0.088105, loss_cps: 0.172479
[13:09:35.549] iteration 11653: total_loss: 0.288154, loss_sup: 0.036181, loss_mps: 0.082096, loss_cps: 0.169878
[13:09:35.696] iteration 11654: total_loss: 0.436329, loss_sup: 0.026385, loss_mps: 0.127795, loss_cps: 0.282150
[13:09:35.842] iteration 11655: total_loss: 0.362369, loss_sup: 0.068470, loss_mps: 0.097515, loss_cps: 0.196383
[13:09:35.988] iteration 11656: total_loss: 0.324062, loss_sup: 0.104739, loss_mps: 0.075829, loss_cps: 0.143494
[13:09:36.134] iteration 11657: total_loss: 0.431714, loss_sup: 0.170161, loss_mps: 0.087423, loss_cps: 0.174130
[13:09:36.280] iteration 11658: total_loss: 0.381683, loss_sup: 0.113021, loss_mps: 0.095311, loss_cps: 0.173351
[13:09:36.426] iteration 11659: total_loss: 0.379709, loss_sup: 0.072464, loss_mps: 0.098306, loss_cps: 0.208939
[13:09:36.572] iteration 11660: total_loss: 0.580468, loss_sup: 0.072890, loss_mps: 0.164497, loss_cps: 0.343081
[13:09:36.722] iteration 11661: total_loss: 0.414189, loss_sup: 0.146303, loss_mps: 0.090731, loss_cps: 0.177155
[13:09:36.867] iteration 11662: total_loss: 0.244240, loss_sup: 0.030709, loss_mps: 0.077889, loss_cps: 0.135641
[13:09:37.014] iteration 11663: total_loss: 0.581061, loss_sup: 0.151845, loss_mps: 0.138543, loss_cps: 0.290672
[13:09:37.161] iteration 11664: total_loss: 0.222836, loss_sup: 0.026376, loss_mps: 0.068821, loss_cps: 0.127639
[13:09:37.307] iteration 11665: total_loss: 0.254906, loss_sup: 0.120698, loss_mps: 0.050388, loss_cps: 0.083820
[13:09:37.454] iteration 11666: total_loss: 0.327350, loss_sup: 0.108732, loss_mps: 0.078746, loss_cps: 0.139872
[13:09:37.603] iteration 11667: total_loss: 0.333532, loss_sup: 0.171262, loss_mps: 0.059997, loss_cps: 0.102273
[13:09:37.748] iteration 11668: total_loss: 0.261100, loss_sup: 0.037937, loss_mps: 0.078919, loss_cps: 0.144244
[13:09:37.896] iteration 11669: total_loss: 0.319734, loss_sup: 0.056607, loss_mps: 0.093224, loss_cps: 0.169904
[13:09:38.041] iteration 11670: total_loss: 0.398573, loss_sup: 0.183733, loss_mps: 0.076131, loss_cps: 0.138710
[13:09:38.187] iteration 11671: total_loss: 0.281648, loss_sup: 0.121752, loss_mps: 0.061660, loss_cps: 0.098236
[13:09:38.334] iteration 11672: total_loss: 0.394969, loss_sup: 0.151281, loss_mps: 0.085034, loss_cps: 0.158653
[13:09:38.480] iteration 11673: total_loss: 0.193193, loss_sup: 0.013507, loss_mps: 0.062574, loss_cps: 0.117112
[13:09:38.626] iteration 11674: total_loss: 0.526725, loss_sup: 0.122595, loss_mps: 0.131895, loss_cps: 0.272235
[13:09:38.771] iteration 11675: total_loss: 0.264514, loss_sup: 0.050572, loss_mps: 0.075030, loss_cps: 0.138911
[13:09:38.917] iteration 11676: total_loss: 0.205600, loss_sup: 0.018773, loss_mps: 0.072882, loss_cps: 0.113946
[13:09:39.065] iteration 11677: total_loss: 0.496218, loss_sup: 0.217780, loss_mps: 0.095342, loss_cps: 0.183097
[13:09:39.211] iteration 11678: total_loss: 0.412140, loss_sup: 0.070255, loss_mps: 0.111433, loss_cps: 0.230452
[13:09:39.358] iteration 11679: total_loss: 0.374327, loss_sup: 0.092651, loss_mps: 0.094897, loss_cps: 0.186779
[13:09:39.504] iteration 11680: total_loss: 0.187441, loss_sup: 0.036907, loss_mps: 0.057423, loss_cps: 0.093111
[13:09:39.651] iteration 11681: total_loss: 0.176404, loss_sup: 0.012695, loss_mps: 0.064419, loss_cps: 0.099289
[13:09:39.800] iteration 11682: total_loss: 0.409812, loss_sup: 0.143526, loss_mps: 0.088593, loss_cps: 0.177693
[13:09:39.946] iteration 11683: total_loss: 0.375121, loss_sup: 0.083880, loss_mps: 0.098889, loss_cps: 0.192351
[13:09:40.092] iteration 11684: total_loss: 0.570167, loss_sup: 0.191143, loss_mps: 0.125503, loss_cps: 0.253521
[13:09:40.238] iteration 11685: total_loss: 0.410605, loss_sup: 0.104077, loss_mps: 0.108838, loss_cps: 0.197689
[13:09:40.384] iteration 11686: total_loss: 0.276309, loss_sup: 0.088987, loss_mps: 0.068056, loss_cps: 0.119265
[13:09:40.532] iteration 11687: total_loss: 0.455366, loss_sup: 0.082118, loss_mps: 0.124211, loss_cps: 0.249038
[13:09:40.678] iteration 11688: total_loss: 0.496647, loss_sup: 0.308720, loss_mps: 0.068827, loss_cps: 0.119100
[13:09:40.825] iteration 11689: total_loss: 0.322354, loss_sup: 0.137726, loss_mps: 0.066332, loss_cps: 0.118295
[13:09:40.971] iteration 11690: total_loss: 0.357532, loss_sup: 0.038560, loss_mps: 0.105668, loss_cps: 0.213304
[13:09:41.117] iteration 11691: total_loss: 0.325225, loss_sup: 0.039988, loss_mps: 0.101389, loss_cps: 0.183848
[13:09:41.263] iteration 11692: total_loss: 0.474452, loss_sup: 0.064137, loss_mps: 0.133533, loss_cps: 0.276782
[13:09:41.409] iteration 11693: total_loss: 0.289070, loss_sup: 0.067091, loss_mps: 0.078031, loss_cps: 0.143947
[13:09:41.555] iteration 11694: total_loss: 0.309585, loss_sup: 0.021672, loss_mps: 0.096771, loss_cps: 0.191142
[13:09:41.700] iteration 11695: total_loss: 0.195399, loss_sup: 0.032842, loss_mps: 0.062001, loss_cps: 0.100555
[13:09:41.845] iteration 11696: total_loss: 0.231396, loss_sup: 0.014831, loss_mps: 0.077506, loss_cps: 0.139059
[13:09:41.991] iteration 11697: total_loss: 0.174186, loss_sup: 0.044202, loss_mps: 0.051075, loss_cps: 0.078909
[13:09:42.136] iteration 11698: total_loss: 0.408544, loss_sup: 0.174921, loss_mps: 0.080472, loss_cps: 0.153150
[13:09:42.282] iteration 11699: total_loss: 0.317041, loss_sup: 0.106753, loss_mps: 0.073264, loss_cps: 0.137025
[13:09:42.427] iteration 11700: total_loss: 0.309926, loss_sup: 0.046902, loss_mps: 0.086893, loss_cps: 0.176132
[13:09:42.427] Evaluation Started ==>
[13:09:53.864] ==> valid iteration 11700: unet metrics: {'dc': 0.652467645523585, 'jc': 0.5297373852697235, 'pre': 0.768147392946172, 'hd': 5.756209463437841}, ynet metrics: {'dc': 0.610891213104842, 'jc': 0.4902600244022625, 'pre': 0.7840690493579114, 'hd': 5.732028057828198}.
[13:09:53.866] Evaluation Finished!⏹️
[13:09:54.016] iteration 11701: total_loss: 0.197851, loss_sup: 0.028959, loss_mps: 0.063297, loss_cps: 0.105595
[13:09:54.163] iteration 11702: total_loss: 0.300757, loss_sup: 0.055652, loss_mps: 0.085339, loss_cps: 0.159766
[13:09:54.308] iteration 11703: total_loss: 0.217436, loss_sup: 0.062317, loss_mps: 0.054618, loss_cps: 0.100501
[13:09:54.370] iteration 11704: total_loss: 0.168153, loss_sup: 0.021730, loss_mps: 0.054316, loss_cps: 0.092108
[13:09:55.735] iteration 11705: total_loss: 0.348481, loss_sup: 0.036288, loss_mps: 0.104275, loss_cps: 0.207918
[13:09:55.884] iteration 11706: total_loss: 0.369578, loss_sup: 0.058799, loss_mps: 0.101852, loss_cps: 0.208927
[13:09:56.030] iteration 11707: total_loss: 0.257809, loss_sup: 0.086767, loss_mps: 0.062664, loss_cps: 0.108378
[13:09:56.178] iteration 11708: total_loss: 0.325568, loss_sup: 0.136988, loss_mps: 0.065190, loss_cps: 0.123390
[13:09:56.326] iteration 11709: total_loss: 0.531425, loss_sup: 0.170780, loss_mps: 0.113036, loss_cps: 0.247610
[13:09:56.473] iteration 11710: total_loss: 0.321889, loss_sup: 0.034825, loss_mps: 0.093563, loss_cps: 0.193501
[13:09:56.619] iteration 11711: total_loss: 0.142108, loss_sup: 0.026508, loss_mps: 0.043386, loss_cps: 0.072214
[13:09:56.767] iteration 11712: total_loss: 0.356766, loss_sup: 0.110496, loss_mps: 0.087128, loss_cps: 0.159142
[13:09:56.914] iteration 11713: total_loss: 0.250351, loss_sup: 0.050692, loss_mps: 0.074503, loss_cps: 0.125155
[13:09:57.061] iteration 11714: total_loss: 0.206033, loss_sup: 0.036333, loss_mps: 0.060188, loss_cps: 0.109512
[13:09:57.206] iteration 11715: total_loss: 0.215899, loss_sup: 0.044248, loss_mps: 0.064041, loss_cps: 0.107611
[13:09:57.352] iteration 11716: total_loss: 0.222248, loss_sup: 0.088199, loss_mps: 0.051267, loss_cps: 0.082782
[13:09:57.499] iteration 11717: total_loss: 0.282059, loss_sup: 0.054197, loss_mps: 0.077798, loss_cps: 0.150064
[13:09:57.645] iteration 11718: total_loss: 0.197803, loss_sup: 0.031951, loss_mps: 0.056536, loss_cps: 0.109317
[13:09:57.792] iteration 11719: total_loss: 0.244083, loss_sup: 0.025024, loss_mps: 0.075807, loss_cps: 0.143252
[13:09:57.939] iteration 11720: total_loss: 0.191497, loss_sup: 0.003561, loss_mps: 0.062836, loss_cps: 0.125100
[13:09:58.086] iteration 11721: total_loss: 0.325237, loss_sup: 0.062536, loss_mps: 0.084252, loss_cps: 0.178449
[13:09:58.233] iteration 11722: total_loss: 0.237572, loss_sup: 0.094993, loss_mps: 0.052029, loss_cps: 0.090550
[13:09:58.379] iteration 11723: total_loss: 0.241455, loss_sup: 0.054522, loss_mps: 0.065698, loss_cps: 0.121235
[13:09:58.525] iteration 11724: total_loss: 0.258794, loss_sup: 0.072862, loss_mps: 0.063924, loss_cps: 0.122008
[13:09:58.671] iteration 11725: total_loss: 0.201246, loss_sup: 0.024142, loss_mps: 0.061396, loss_cps: 0.115707
[13:09:58.817] iteration 11726: total_loss: 0.346868, loss_sup: 0.056184, loss_mps: 0.094280, loss_cps: 0.196405
[13:09:58.963] iteration 11727: total_loss: 0.235335, loss_sup: 0.008264, loss_mps: 0.073554, loss_cps: 0.153517
[13:09:59.109] iteration 11728: total_loss: 0.222068, loss_sup: 0.027442, loss_mps: 0.070050, loss_cps: 0.124576
[13:09:59.255] iteration 11729: total_loss: 0.185443, loss_sup: 0.028295, loss_mps: 0.058866, loss_cps: 0.098282
[13:09:59.400] iteration 11730: total_loss: 0.228064, loss_sup: 0.017143, loss_mps: 0.070340, loss_cps: 0.140581
[13:09:59.546] iteration 11731: total_loss: 0.306141, loss_sup: 0.095273, loss_mps: 0.073874, loss_cps: 0.136994
[13:09:59.693] iteration 11732: total_loss: 0.299556, loss_sup: 0.037043, loss_mps: 0.084718, loss_cps: 0.177795
[13:09:59.839] iteration 11733: total_loss: 0.264746, loss_sup: 0.048373, loss_mps: 0.068932, loss_cps: 0.147441
[13:09:59.984] iteration 11734: total_loss: 0.242782, loss_sup: 0.038507, loss_mps: 0.069727, loss_cps: 0.134547
[13:10:00.132] iteration 11735: total_loss: 0.214456, loss_sup: 0.042036, loss_mps: 0.057272, loss_cps: 0.115149
[13:10:00.277] iteration 11736: total_loss: 0.178963, loss_sup: 0.012759, loss_mps: 0.057177, loss_cps: 0.109028
[13:10:00.423] iteration 11737: total_loss: 0.301673, loss_sup: 0.036036, loss_mps: 0.089392, loss_cps: 0.176245
[13:10:00.569] iteration 11738: total_loss: 0.401244, loss_sup: 0.157807, loss_mps: 0.079676, loss_cps: 0.163760
[13:10:00.715] iteration 11739: total_loss: 0.264487, loss_sup: 0.034232, loss_mps: 0.074804, loss_cps: 0.155450
[13:10:00.861] iteration 11740: total_loss: 0.198415, loss_sup: 0.019664, loss_mps: 0.060289, loss_cps: 0.118462
[13:10:01.008] iteration 11741: total_loss: 0.379487, loss_sup: 0.188495, loss_mps: 0.064550, loss_cps: 0.126442
[13:10:01.154] iteration 11742: total_loss: 0.325586, loss_sup: 0.077857, loss_mps: 0.080111, loss_cps: 0.167618
[13:10:01.299] iteration 11743: total_loss: 0.333037, loss_sup: 0.023325, loss_mps: 0.095703, loss_cps: 0.214010
[13:10:01.445] iteration 11744: total_loss: 0.479047, loss_sup: 0.105219, loss_mps: 0.116675, loss_cps: 0.257153
[13:10:01.591] iteration 11745: total_loss: 0.304340, loss_sup: 0.142111, loss_mps: 0.058412, loss_cps: 0.103816
[13:10:01.737] iteration 11746: total_loss: 0.336709, loss_sup: 0.062480, loss_mps: 0.084472, loss_cps: 0.189757
[13:10:01.883] iteration 11747: total_loss: 0.314825, loss_sup: 0.088138, loss_mps: 0.077737, loss_cps: 0.148950
[13:10:02.029] iteration 11748: total_loss: 0.402378, loss_sup: 0.035099, loss_mps: 0.121195, loss_cps: 0.246085
[13:10:02.174] iteration 11749: total_loss: 0.186586, loss_sup: 0.023467, loss_mps: 0.056454, loss_cps: 0.106664
[13:10:02.320] iteration 11750: total_loss: 0.222080, loss_sup: 0.009262, loss_mps: 0.071861, loss_cps: 0.140957
[13:10:02.466] iteration 11751: total_loss: 0.325606, loss_sup: 0.101947, loss_mps: 0.078105, loss_cps: 0.145554
[13:10:02.612] iteration 11752: total_loss: 0.461745, loss_sup: 0.227912, loss_mps: 0.077805, loss_cps: 0.156028
[13:10:02.758] iteration 11753: total_loss: 0.377909, loss_sup: 0.051998, loss_mps: 0.102512, loss_cps: 0.223399
[13:10:02.904] iteration 11754: total_loss: 0.268381, loss_sup: 0.024020, loss_mps: 0.077037, loss_cps: 0.167324
[13:10:03.050] iteration 11755: total_loss: 0.453688, loss_sup: 0.076124, loss_mps: 0.118110, loss_cps: 0.259454
[13:10:03.196] iteration 11756: total_loss: 0.241116, loss_sup: 0.090259, loss_mps: 0.051925, loss_cps: 0.098932
[13:10:03.341] iteration 11757: total_loss: 0.431010, loss_sup: 0.191416, loss_mps: 0.082165, loss_cps: 0.157429
[13:10:03.488] iteration 11758: total_loss: 0.374231, loss_sup: 0.100356, loss_mps: 0.090974, loss_cps: 0.182902
[13:10:03.633] iteration 11759: total_loss: 0.304673, loss_sup: 0.033679, loss_mps: 0.090426, loss_cps: 0.180568
[13:10:03.779] iteration 11760: total_loss: 0.238208, loss_sup: 0.046175, loss_mps: 0.070242, loss_cps: 0.121791
[13:10:03.926] iteration 11761: total_loss: 0.253815, loss_sup: 0.038397, loss_mps: 0.076757, loss_cps: 0.138661
[13:10:04.079] iteration 11762: total_loss: 0.172227, loss_sup: 0.016175, loss_mps: 0.055340, loss_cps: 0.100711
[13:10:04.226] iteration 11763: total_loss: 0.351579, loss_sup: 0.072212, loss_mps: 0.091179, loss_cps: 0.188187
[13:10:04.372] iteration 11764: total_loss: 0.179788, loss_sup: 0.017453, loss_mps: 0.057248, loss_cps: 0.105087
[13:10:04.519] iteration 11765: total_loss: 0.252210, loss_sup: 0.047630, loss_mps: 0.071553, loss_cps: 0.133028
[13:10:04.666] iteration 11766: total_loss: 0.430994, loss_sup: 0.107609, loss_mps: 0.104242, loss_cps: 0.219142
[13:10:04.813] iteration 11767: total_loss: 0.385206, loss_sup: 0.140131, loss_mps: 0.084714, loss_cps: 0.160361
[13:10:04.959] iteration 11768: total_loss: 0.456895, loss_sup: 0.097611, loss_mps: 0.115650, loss_cps: 0.243634
[13:10:05.106] iteration 11769: total_loss: 0.327118, loss_sup: 0.032043, loss_mps: 0.098466, loss_cps: 0.196609
[13:10:05.252] iteration 11770: total_loss: 0.252677, loss_sup: 0.048358, loss_mps: 0.072220, loss_cps: 0.132099
[13:10:05.398] iteration 11771: total_loss: 0.507894, loss_sup: 0.227089, loss_mps: 0.093088, loss_cps: 0.187718
[13:10:05.545] iteration 11772: total_loss: 0.394164, loss_sup: 0.035666, loss_mps: 0.116240, loss_cps: 0.242258
[13:10:05.691] iteration 11773: total_loss: 0.299561, loss_sup: 0.125273, loss_mps: 0.058902, loss_cps: 0.115386
[13:10:05.837] iteration 11774: total_loss: 0.300238, loss_sup: 0.103301, loss_mps: 0.068280, loss_cps: 0.128657
[13:10:05.983] iteration 11775: total_loss: 0.187592, loss_sup: 0.024369, loss_mps: 0.061377, loss_cps: 0.101846
[13:10:06.131] iteration 11776: total_loss: 0.402560, loss_sup: 0.114884, loss_mps: 0.089619, loss_cps: 0.198056
[13:10:06.277] iteration 11777: total_loss: 0.449949, loss_sup: 0.137220, loss_mps: 0.103108, loss_cps: 0.209622
[13:10:06.423] iteration 11778: total_loss: 0.231839, loss_sup: 0.035482, loss_mps: 0.067567, loss_cps: 0.128790
[13:10:06.569] iteration 11779: total_loss: 0.298148, loss_sup: 0.077286, loss_mps: 0.078025, loss_cps: 0.142837
[13:10:06.718] iteration 11780: total_loss: 0.300240, loss_sup: 0.091782, loss_mps: 0.069705, loss_cps: 0.138754
[13:10:06.864] iteration 11781: total_loss: 0.325081, loss_sup: 0.091558, loss_mps: 0.079522, loss_cps: 0.154001
[13:10:07.012] iteration 11782: total_loss: 0.271289, loss_sup: 0.034230, loss_mps: 0.077854, loss_cps: 0.159204
[13:10:07.158] iteration 11783: total_loss: 0.307103, loss_sup: 0.111805, loss_mps: 0.070895, loss_cps: 0.124404
[13:10:07.305] iteration 11784: total_loss: 0.509879, loss_sup: 0.218272, loss_mps: 0.098212, loss_cps: 0.193395
[13:10:07.451] iteration 11785: total_loss: 0.216018, loss_sup: 0.043049, loss_mps: 0.063478, loss_cps: 0.109490
[13:10:07.599] iteration 11786: total_loss: 0.333712, loss_sup: 0.074931, loss_mps: 0.090727, loss_cps: 0.168054
[13:10:07.746] iteration 11787: total_loss: 0.351551, loss_sup: 0.096391, loss_mps: 0.086287, loss_cps: 0.168872
[13:10:07.896] iteration 11788: total_loss: 0.190248, loss_sup: 0.029351, loss_mps: 0.059458, loss_cps: 0.101440
[13:10:08.043] iteration 11789: total_loss: 0.413356, loss_sup: 0.099548, loss_mps: 0.103914, loss_cps: 0.209894
[13:10:08.189] iteration 11790: total_loss: 0.295519, loss_sup: 0.057532, loss_mps: 0.081447, loss_cps: 0.156540
[13:10:08.335] iteration 11791: total_loss: 0.324129, loss_sup: 0.040932, loss_mps: 0.093162, loss_cps: 0.190034
[13:10:08.481] iteration 11792: total_loss: 0.404082, loss_sup: 0.106816, loss_mps: 0.099242, loss_cps: 0.198023
[13:10:08.631] iteration 11793: total_loss: 0.421329, loss_sup: 0.126417, loss_mps: 0.100384, loss_cps: 0.194529
[13:10:08.777] iteration 11794: total_loss: 0.218499, loss_sup: 0.036782, loss_mps: 0.067918, loss_cps: 0.113798
[13:10:08.923] iteration 11795: total_loss: 0.332586, loss_sup: 0.095647, loss_mps: 0.082775, loss_cps: 0.154164
[13:10:09.069] iteration 11796: total_loss: 0.224726, loss_sup: 0.016655, loss_mps: 0.072752, loss_cps: 0.135319
[13:10:09.216] iteration 11797: total_loss: 0.522689, loss_sup: 0.028604, loss_mps: 0.154962, loss_cps: 0.339123
[13:10:09.362] iteration 11798: total_loss: 0.283858, loss_sup: 0.043030, loss_mps: 0.083726, loss_cps: 0.157102
[13:10:09.508] iteration 11799: total_loss: 0.391526, loss_sup: 0.124693, loss_mps: 0.085585, loss_cps: 0.181249
[13:10:09.657] iteration 11800: total_loss: 0.225295, loss_sup: 0.006829, loss_mps: 0.076314, loss_cps: 0.142153
[13:10:09.657] Evaluation Started ==>
[13:10:21.033] ==> valid iteration 11800: unet metrics: {'dc': 0.6315870704047934, 'jc': 0.5101541109650812, 'pre': 0.7275539204299698, 'hd': 5.862352675402052}, ynet metrics: {'dc': 0.6513487282729944, 'jc': 0.5350932194998024, 'pre': 0.766697157266093, 'hd': 5.57788031843251}.
[13:10:21.194] ==> New best valid dice for ynet: 0.651349, at iteration 11800
[13:10:21.197] Evaluation Finished!⏹️
[13:10:21.351] iteration 11801: total_loss: 0.206430, loss_sup: 0.036431, loss_mps: 0.059228, loss_cps: 0.110771
[13:10:21.500] iteration 11802: total_loss: 0.366575, loss_sup: 0.078034, loss_mps: 0.096685, loss_cps: 0.191856
[13:10:21.647] iteration 11803: total_loss: 0.193472, loss_sup: 0.024018, loss_mps: 0.060445, loss_cps: 0.109009
[13:10:21.792] iteration 11804: total_loss: 0.318630, loss_sup: 0.064468, loss_mps: 0.082186, loss_cps: 0.171976
[13:10:21.937] iteration 11805: total_loss: 0.344983, loss_sup: 0.106058, loss_mps: 0.080546, loss_cps: 0.158379
[13:10:22.081] iteration 11806: total_loss: 0.318936, loss_sup: 0.070416, loss_mps: 0.080743, loss_cps: 0.167777
[13:10:22.231] iteration 11807: total_loss: 0.201125, loss_sup: 0.022595, loss_mps: 0.063235, loss_cps: 0.115295
[13:10:22.378] iteration 11808: total_loss: 0.305267, loss_sup: 0.035172, loss_mps: 0.090048, loss_cps: 0.180047
[13:10:22.524] iteration 11809: total_loss: 0.479886, loss_sup: 0.206709, loss_mps: 0.087638, loss_cps: 0.185539
[13:10:22.670] iteration 11810: total_loss: 0.339924, loss_sup: 0.084494, loss_mps: 0.084964, loss_cps: 0.170465
[13:10:22.816] iteration 11811: total_loss: 0.244094, loss_sup: 0.084509, loss_mps: 0.054987, loss_cps: 0.104597
[13:10:22.962] iteration 11812: total_loss: 0.318623, loss_sup: 0.066783, loss_mps: 0.085563, loss_cps: 0.166277
[13:10:23.107] iteration 11813: total_loss: 0.173684, loss_sup: 0.030929, loss_mps: 0.052543, loss_cps: 0.090213
[13:10:23.253] iteration 11814: total_loss: 0.222240, loss_sup: 0.002022, loss_mps: 0.074998, loss_cps: 0.145220
[13:10:23.399] iteration 11815: total_loss: 0.320874, loss_sup: 0.077547, loss_mps: 0.083613, loss_cps: 0.159713
[13:10:23.549] iteration 11816: total_loss: 0.337558, loss_sup: 0.063480, loss_mps: 0.096275, loss_cps: 0.177803
[13:10:23.696] iteration 11817: total_loss: 0.430768, loss_sup: 0.160217, loss_mps: 0.087041, loss_cps: 0.183510
[13:10:23.842] iteration 11818: total_loss: 0.273783, loss_sup: 0.025291, loss_mps: 0.084251, loss_cps: 0.164242
[13:10:23.987] iteration 11819: total_loss: 0.313264, loss_sup: 0.042016, loss_mps: 0.089443, loss_cps: 0.181805
[13:10:24.133] iteration 11820: total_loss: 0.443942, loss_sup: 0.095833, loss_mps: 0.110344, loss_cps: 0.237765
[13:10:24.278] iteration 11821: total_loss: 0.200451, loss_sup: 0.058316, loss_mps: 0.051375, loss_cps: 0.090760
[13:10:24.425] iteration 11822: total_loss: 0.385587, loss_sup: 0.060148, loss_mps: 0.108496, loss_cps: 0.216943
[13:10:24.571] iteration 11823: total_loss: 0.641866, loss_sup: 0.318871, loss_mps: 0.107584, loss_cps: 0.215410
[13:10:24.716] iteration 11824: total_loss: 0.407498, loss_sup: 0.039125, loss_mps: 0.123744, loss_cps: 0.244628
[13:10:24.861] iteration 11825: total_loss: 0.281741, loss_sup: 0.076152, loss_mps: 0.072592, loss_cps: 0.132997
[13:10:25.007] iteration 11826: total_loss: 0.291525, loss_sup: 0.050927, loss_mps: 0.085736, loss_cps: 0.154862
[13:10:25.152] iteration 11827: total_loss: 0.222284, loss_sup: 0.017245, loss_mps: 0.070315, loss_cps: 0.134724
[13:10:25.298] iteration 11828: total_loss: 0.340649, loss_sup: 0.057249, loss_mps: 0.093808, loss_cps: 0.189592
[13:10:25.445] iteration 11829: total_loss: 0.194370, loss_sup: 0.061122, loss_mps: 0.049138, loss_cps: 0.084110
[13:10:25.590] iteration 11830: total_loss: 0.334358, loss_sup: 0.016143, loss_mps: 0.103286, loss_cps: 0.214930
[13:10:25.736] iteration 11831: total_loss: 0.270963, loss_sup: 0.034051, loss_mps: 0.084932, loss_cps: 0.151980
[13:10:25.882] iteration 11832: total_loss: 0.223026, loss_sup: 0.062372, loss_mps: 0.058676, loss_cps: 0.101977
[13:10:26.028] iteration 11833: total_loss: 0.225676, loss_sup: 0.019099, loss_mps: 0.068816, loss_cps: 0.137761
[13:10:26.173] iteration 11834: total_loss: 0.219949, loss_sup: 0.092417, loss_mps: 0.047412, loss_cps: 0.080120
[13:10:26.318] iteration 11835: total_loss: 0.186307, loss_sup: 0.009288, loss_mps: 0.061412, loss_cps: 0.115608
[13:10:26.465] iteration 11836: total_loss: 0.500965, loss_sup: 0.136913, loss_mps: 0.111607, loss_cps: 0.252446
[13:10:26.613] iteration 11837: total_loss: 0.214533, loss_sup: 0.021363, loss_mps: 0.071090, loss_cps: 0.122080
[13:10:26.759] iteration 11838: total_loss: 0.272590, loss_sup: 0.037403, loss_mps: 0.081406, loss_cps: 0.153782
[13:10:26.905] iteration 11839: total_loss: 0.566532, loss_sup: 0.325097, loss_mps: 0.081105, loss_cps: 0.160331
[13:10:27.051] iteration 11840: total_loss: 0.142490, loss_sup: 0.024682, loss_mps: 0.043480, loss_cps: 0.074328
[13:10:27.196] iteration 11841: total_loss: 0.218277, loss_sup: 0.047567, loss_mps: 0.059131, loss_cps: 0.111580
[13:10:27.342] iteration 11842: total_loss: 0.242487, loss_sup: 0.033698, loss_mps: 0.070945, loss_cps: 0.137843
[13:10:27.488] iteration 11843: total_loss: 0.246880, loss_sup: 0.019976, loss_mps: 0.080664, loss_cps: 0.146240
[13:10:27.634] iteration 11844: total_loss: 0.275520, loss_sup: 0.078263, loss_mps: 0.069361, loss_cps: 0.127896
[13:10:27.780] iteration 11845: total_loss: 0.274193, loss_sup: 0.068973, loss_mps: 0.070096, loss_cps: 0.135125
[13:10:27.926] iteration 11846: total_loss: 0.241315, loss_sup: 0.045437, loss_mps: 0.067318, loss_cps: 0.128560
[13:10:28.071] iteration 11847: total_loss: 0.229049, loss_sup: 0.023224, loss_mps: 0.071175, loss_cps: 0.134651
[13:10:28.217] iteration 11848: total_loss: 0.440271, loss_sup: 0.166029, loss_mps: 0.097645, loss_cps: 0.176596
[13:10:28.364] iteration 11849: total_loss: 0.613655, loss_sup: 0.164080, loss_mps: 0.144471, loss_cps: 0.305104
[13:10:28.509] iteration 11850: total_loss: 0.318571, loss_sup: 0.056309, loss_mps: 0.083256, loss_cps: 0.179006
[13:10:28.656] iteration 11851: total_loss: 0.566635, loss_sup: 0.289301, loss_mps: 0.091320, loss_cps: 0.186015
[13:10:28.802] iteration 11852: total_loss: 0.266593, loss_sup: 0.102488, loss_mps: 0.056597, loss_cps: 0.107508
[13:10:28.952] iteration 11853: total_loss: 0.427260, loss_sup: 0.164884, loss_mps: 0.086438, loss_cps: 0.175938
[13:10:29.100] iteration 11854: total_loss: 0.245835, loss_sup: 0.056443, loss_mps: 0.068667, loss_cps: 0.120725
[13:10:29.247] iteration 11855: total_loss: 0.598185, loss_sup: 0.348337, loss_mps: 0.086329, loss_cps: 0.163520
[13:10:29.393] iteration 11856: total_loss: 0.197334, loss_sup: 0.030289, loss_mps: 0.059735, loss_cps: 0.107310
[13:10:29.539] iteration 11857: total_loss: 0.314641, loss_sup: 0.104210, loss_mps: 0.074409, loss_cps: 0.136022
[13:10:29.685] iteration 11858: total_loss: 0.322562, loss_sup: 0.068216, loss_mps: 0.087761, loss_cps: 0.166585
[13:10:29.832] iteration 11859: total_loss: 0.276836, loss_sup: 0.061269, loss_mps: 0.076517, loss_cps: 0.139051
[13:10:29.978] iteration 11860: total_loss: 0.365947, loss_sup: 0.130446, loss_mps: 0.079981, loss_cps: 0.155520
[13:10:30.125] iteration 11861: total_loss: 0.241648, loss_sup: 0.076055, loss_mps: 0.062112, loss_cps: 0.103480
[13:10:30.270] iteration 11862: total_loss: 0.379390, loss_sup: 0.112607, loss_mps: 0.092072, loss_cps: 0.174711
[13:10:30.417] iteration 11863: total_loss: 0.320867, loss_sup: 0.021909, loss_mps: 0.099177, loss_cps: 0.199781
[13:10:30.565] iteration 11864: total_loss: 0.448554, loss_sup: 0.093946, loss_mps: 0.115947, loss_cps: 0.238661
[13:10:30.711] iteration 11865: total_loss: 0.300648, loss_sup: 0.063472, loss_mps: 0.079619, loss_cps: 0.157556
[13:10:30.857] iteration 11866: total_loss: 0.334302, loss_sup: 0.019487, loss_mps: 0.102648, loss_cps: 0.212167
[13:10:31.003] iteration 11867: total_loss: 0.276418, loss_sup: 0.019085, loss_mps: 0.087234, loss_cps: 0.170100
[13:10:31.149] iteration 11868: total_loss: 0.362025, loss_sup: 0.176399, loss_mps: 0.068310, loss_cps: 0.117316
[13:10:31.295] iteration 11869: total_loss: 0.266567, loss_sup: 0.094374, loss_mps: 0.066990, loss_cps: 0.105203
[13:10:31.443] iteration 11870: total_loss: 0.205605, loss_sup: 0.040438, loss_mps: 0.061629, loss_cps: 0.103538
[13:10:31.590] iteration 11871: total_loss: 0.383179, loss_sup: 0.069071, loss_mps: 0.106770, loss_cps: 0.207338
[13:10:31.740] iteration 11872: total_loss: 0.351935, loss_sup: 0.153622, loss_mps: 0.075298, loss_cps: 0.123015
[13:10:31.888] iteration 11873: total_loss: 0.256096, loss_sup: 0.057112, loss_mps: 0.070413, loss_cps: 0.128572
[13:10:32.036] iteration 11874: total_loss: 0.276907, loss_sup: 0.033372, loss_mps: 0.082452, loss_cps: 0.161083
[13:10:32.183] iteration 11875: total_loss: 0.421447, loss_sup: 0.154825, loss_mps: 0.089489, loss_cps: 0.177133
[13:10:32.332] iteration 11876: total_loss: 0.224421, loss_sup: 0.032229, loss_mps: 0.068449, loss_cps: 0.123743
[13:10:32.479] iteration 11877: total_loss: 0.409929, loss_sup: 0.053999, loss_mps: 0.119886, loss_cps: 0.236044
[13:10:32.627] iteration 11878: total_loss: 0.209792, loss_sup: 0.030843, loss_mps: 0.065883, loss_cps: 0.113067
[13:10:32.776] iteration 11879: total_loss: 0.161781, loss_sup: 0.028200, loss_mps: 0.049875, loss_cps: 0.083706
[13:10:32.924] iteration 11880: total_loss: 0.272407, loss_sup: 0.037416, loss_mps: 0.086404, loss_cps: 0.148587
[13:10:33.070] iteration 11881: total_loss: 0.265639, loss_sup: 0.061227, loss_mps: 0.075419, loss_cps: 0.128993
[13:10:33.218] iteration 11882: total_loss: 0.490887, loss_sup: 0.234430, loss_mps: 0.089596, loss_cps: 0.166862
[13:10:33.364] iteration 11883: total_loss: 0.285310, loss_sup: 0.147884, loss_mps: 0.050171, loss_cps: 0.087255
[13:10:33.510] iteration 11884: total_loss: 0.358036, loss_sup: 0.029032, loss_mps: 0.102911, loss_cps: 0.226092
[13:10:33.657] iteration 11885: total_loss: 0.517035, loss_sup: 0.271008, loss_mps: 0.083617, loss_cps: 0.162409
[13:10:33.804] iteration 11886: total_loss: 0.268172, loss_sup: 0.026906, loss_mps: 0.080950, loss_cps: 0.160316
[13:10:33.950] iteration 11887: total_loss: 0.276269, loss_sup: 0.095535, loss_mps: 0.065474, loss_cps: 0.115260
[13:10:34.097] iteration 11888: total_loss: 0.226005, loss_sup: 0.043784, loss_mps: 0.065286, loss_cps: 0.116935
[13:10:34.243] iteration 11889: total_loss: 0.265942, loss_sup: 0.050003, loss_mps: 0.072941, loss_cps: 0.142998
[13:10:34.389] iteration 11890: total_loss: 0.352643, loss_sup: 0.098053, loss_mps: 0.093320, loss_cps: 0.161269
[13:10:34.535] iteration 11891: total_loss: 0.278390, loss_sup: 0.058567, loss_mps: 0.078818, loss_cps: 0.141005
[13:10:34.681] iteration 11892: total_loss: 0.278529, loss_sup: 0.067680, loss_mps: 0.073186, loss_cps: 0.137663
[13:10:34.829] iteration 11893: total_loss: 0.434557, loss_sup: 0.020918, loss_mps: 0.133861, loss_cps: 0.279778
[13:10:34.976] iteration 11894: total_loss: 0.481643, loss_sup: 0.183947, loss_mps: 0.101137, loss_cps: 0.196558
[13:10:35.123] iteration 11895: total_loss: 0.281360, loss_sup: 0.100951, loss_mps: 0.067445, loss_cps: 0.112964
[13:10:35.269] iteration 11896: total_loss: 0.200375, loss_sup: 0.051635, loss_mps: 0.057097, loss_cps: 0.091643
[13:10:35.416] iteration 11897: total_loss: 0.269873, loss_sup: 0.054499, loss_mps: 0.076443, loss_cps: 0.138932
[13:10:35.562] iteration 11898: total_loss: 0.315239, loss_sup: 0.070616, loss_mps: 0.080858, loss_cps: 0.163766
[13:10:35.708] iteration 11899: total_loss: 0.303325, loss_sup: 0.112047, loss_mps: 0.070012, loss_cps: 0.121266
[13:10:35.854] iteration 11900: total_loss: 0.271253, loss_sup: 0.134923, loss_mps: 0.047924, loss_cps: 0.088406
[13:10:35.854] Evaluation Started ==>
[13:10:47.275] ==> valid iteration 11900: unet metrics: {'dc': 0.6688680112254677, 'jc': 0.5444961993261785, 'pre': 0.7379401670581414, 'hd': 5.833917969670113}, ynet metrics: {'dc': 0.5967735926240708, 'jc': 0.4804571791448245, 'pre': 0.7554244079346184, 'hd': 5.831193029915536}.
[13:10:47.276] Evaluation Finished!⏹️
[13:10:47.427] iteration 11901: total_loss: 0.510176, loss_sup: 0.237133, loss_mps: 0.091346, loss_cps: 0.181697
[13:10:47.574] iteration 11902: total_loss: 0.174185, loss_sup: 0.019468, loss_mps: 0.055977, loss_cps: 0.098739
[13:10:47.720] iteration 11903: total_loss: 0.381404, loss_sup: 0.130448, loss_mps: 0.081965, loss_cps: 0.168991
[13:10:47.864] iteration 11904: total_loss: 0.232573, loss_sup: 0.046619, loss_mps: 0.063567, loss_cps: 0.122387
[13:10:48.012] iteration 11905: total_loss: 0.437055, loss_sup: 0.191414, loss_mps: 0.086769, loss_cps: 0.158871
[13:10:48.157] iteration 11906: total_loss: 0.249375, loss_sup: 0.077612, loss_mps: 0.063791, loss_cps: 0.107973
[13:10:48.303] iteration 11907: total_loss: 0.206458, loss_sup: 0.032971, loss_mps: 0.059691, loss_cps: 0.113796
[13:10:48.448] iteration 11908: total_loss: 0.634076, loss_sup: 0.218756, loss_mps: 0.135421, loss_cps: 0.279900
[13:10:48.595] iteration 11909: total_loss: 0.430998, loss_sup: 0.161508, loss_mps: 0.088146, loss_cps: 0.181343
[13:10:48.740] iteration 11910: total_loss: 0.412845, loss_sup: 0.052098, loss_mps: 0.119970, loss_cps: 0.240776
[13:10:48.885] iteration 11911: total_loss: 0.679226, loss_sup: 0.210724, loss_mps: 0.150765, loss_cps: 0.317737
[13:10:49.030] iteration 11912: total_loss: 0.422470, loss_sup: 0.231128, loss_mps: 0.067571, loss_cps: 0.123770
[13:10:49.178] iteration 11913: total_loss: 0.260222, loss_sup: 0.079681, loss_mps: 0.068440, loss_cps: 0.112101
[13:10:49.325] iteration 11914: total_loss: 0.293875, loss_sup: 0.063999, loss_mps: 0.082521, loss_cps: 0.147356
[13:10:49.471] iteration 11915: total_loss: 0.253124, loss_sup: 0.068689, loss_mps: 0.065299, loss_cps: 0.119136
[13:10:49.617] iteration 11916: total_loss: 0.253760, loss_sup: 0.090797, loss_mps: 0.058693, loss_cps: 0.104270
[13:10:49.764] iteration 11917: total_loss: 0.254282, loss_sup: 0.041172, loss_mps: 0.078559, loss_cps: 0.134550
[13:10:49.910] iteration 11918: total_loss: 0.361930, loss_sup: 0.036474, loss_mps: 0.108661, loss_cps: 0.216795
[13:10:50.057] iteration 11919: total_loss: 0.266620, loss_sup: 0.064188, loss_mps: 0.073121, loss_cps: 0.129312
[13:10:50.202] iteration 11920: total_loss: 0.280361, loss_sup: 0.024026, loss_mps: 0.086808, loss_cps: 0.169527
[13:10:50.348] iteration 11921: total_loss: 0.597676, loss_sup: 0.137999, loss_mps: 0.145194, loss_cps: 0.314483
[13:10:50.493] iteration 11922: total_loss: 0.402872, loss_sup: 0.045887, loss_mps: 0.118028, loss_cps: 0.238957
[13:10:50.640] iteration 11923: total_loss: 0.270212, loss_sup: 0.023941, loss_mps: 0.082791, loss_cps: 0.163480
[13:10:50.785] iteration 11924: total_loss: 0.302828, loss_sup: 0.040796, loss_mps: 0.090467, loss_cps: 0.171565
[13:10:50.935] iteration 11925: total_loss: 0.523606, loss_sup: 0.222581, loss_mps: 0.100836, loss_cps: 0.200189
[13:10:51.083] iteration 11926: total_loss: 0.230661, loss_sup: 0.026565, loss_mps: 0.074046, loss_cps: 0.130050
[13:10:51.230] iteration 11927: total_loss: 0.474487, loss_sup: 0.209238, loss_mps: 0.091258, loss_cps: 0.173992
[13:10:51.378] iteration 11928: total_loss: 0.343248, loss_sup: 0.161387, loss_mps: 0.065251, loss_cps: 0.116611
[13:10:51.524] iteration 11929: total_loss: 0.350611, loss_sup: 0.095620, loss_mps: 0.084300, loss_cps: 0.170690
[13:10:51.670] iteration 11930: total_loss: 0.345092, loss_sup: 0.188355, loss_mps: 0.057534, loss_cps: 0.099202
[13:10:51.816] iteration 11931: total_loss: 0.408965, loss_sup: 0.108019, loss_mps: 0.098783, loss_cps: 0.202163
[13:10:51.961] iteration 11932: total_loss: 0.309944, loss_sup: 0.031628, loss_mps: 0.096169, loss_cps: 0.182147
[13:10:52.111] iteration 11933: total_loss: 0.247710, loss_sup: 0.106013, loss_mps: 0.052145, loss_cps: 0.089553
[13:10:52.258] iteration 11934: total_loss: 0.278616, loss_sup: 0.088387, loss_mps: 0.071572, loss_cps: 0.118657
[13:10:52.404] iteration 11935: total_loss: 0.301310, loss_sup: 0.021434, loss_mps: 0.095563, loss_cps: 0.184313
[13:10:52.550] iteration 11936: total_loss: 0.291037, loss_sup: 0.085990, loss_mps: 0.070199, loss_cps: 0.134849
[13:10:52.696] iteration 11937: total_loss: 0.466494, loss_sup: 0.163381, loss_mps: 0.102485, loss_cps: 0.200628
[13:10:52.842] iteration 11938: total_loss: 0.229098, loss_sup: 0.035796, loss_mps: 0.068338, loss_cps: 0.124965
[13:10:52.987] iteration 11939: total_loss: 0.271340, loss_sup: 0.059936, loss_mps: 0.075738, loss_cps: 0.135666
[13:10:53.133] iteration 11940: total_loss: 0.101853, loss_sup: 0.005733, loss_mps: 0.037050, loss_cps: 0.059070
[13:10:53.280] iteration 11941: total_loss: 0.341590, loss_sup: 0.160079, loss_mps: 0.066159, loss_cps: 0.115351
[13:10:53.426] iteration 11942: total_loss: 0.438652, loss_sup: 0.040857, loss_mps: 0.125967, loss_cps: 0.271828
[13:10:53.573] iteration 11943: total_loss: 0.336840, loss_sup: 0.048556, loss_mps: 0.094394, loss_cps: 0.193889
[13:10:53.719] iteration 11944: total_loss: 0.479016, loss_sup: 0.291759, loss_mps: 0.068355, loss_cps: 0.118902
[13:10:53.866] iteration 11945: total_loss: 0.268820, loss_sup: 0.109926, loss_mps: 0.057684, loss_cps: 0.101211
[13:10:54.011] iteration 11946: total_loss: 0.502974, loss_sup: 0.183351, loss_mps: 0.104232, loss_cps: 0.215391
[13:10:54.156] iteration 11947: total_loss: 0.300204, loss_sup: 0.091495, loss_mps: 0.075755, loss_cps: 0.132953
[13:10:54.306] iteration 11948: total_loss: 0.247124, loss_sup: 0.059575, loss_mps: 0.066934, loss_cps: 0.120614
[13:10:54.453] iteration 11949: total_loss: 0.406484, loss_sup: 0.179490, loss_mps: 0.079286, loss_cps: 0.147709
[13:10:54.599] iteration 11950: total_loss: 0.265601, loss_sup: 0.036519, loss_mps: 0.082848, loss_cps: 0.146233
[13:10:54.745] iteration 11951: total_loss: 0.243295, loss_sup: 0.039352, loss_mps: 0.073105, loss_cps: 0.130838
[13:10:54.891] iteration 11952: total_loss: 0.286887, loss_sup: 0.074548, loss_mps: 0.078151, loss_cps: 0.134189
[13:10:55.036] iteration 11953: total_loss: 0.301067, loss_sup: 0.153465, loss_mps: 0.054909, loss_cps: 0.092694
[13:10:55.182] iteration 11954: total_loss: 0.200245, loss_sup: 0.055015, loss_mps: 0.053932, loss_cps: 0.091299
[13:10:55.327] iteration 11955: total_loss: 0.321291, loss_sup: 0.091580, loss_mps: 0.081954, loss_cps: 0.147757
[13:10:55.473] iteration 11956: total_loss: 0.466388, loss_sup: 0.236553, loss_mps: 0.080653, loss_cps: 0.149182
[13:10:55.619] iteration 11957: total_loss: 0.277028, loss_sup: 0.095041, loss_mps: 0.065753, loss_cps: 0.116233
[13:10:55.765] iteration 11958: total_loss: 0.212146, loss_sup: 0.053707, loss_mps: 0.062730, loss_cps: 0.095710
[13:10:55.910] iteration 11959: total_loss: 0.271630, loss_sup: 0.060923, loss_mps: 0.072462, loss_cps: 0.138245
[13:10:56.056] iteration 11960: total_loss: 0.164303, loss_sup: 0.008945, loss_mps: 0.055851, loss_cps: 0.099507
[13:10:56.203] iteration 11961: total_loss: 0.373323, loss_sup: 0.066631, loss_mps: 0.102249, loss_cps: 0.204444
[13:10:56.349] iteration 11962: total_loss: 0.162844, loss_sup: 0.006223, loss_mps: 0.056914, loss_cps: 0.099706
[13:10:56.494] iteration 11963: total_loss: 0.369137, loss_sup: 0.059553, loss_mps: 0.104262, loss_cps: 0.205322
[13:10:56.640] iteration 11964: total_loss: 0.237039, loss_sup: 0.080347, loss_mps: 0.058444, loss_cps: 0.098248
[13:10:56.786] iteration 11965: total_loss: 0.193924, loss_sup: 0.017358, loss_mps: 0.067558, loss_cps: 0.109009
[13:10:56.932] iteration 11966: total_loss: 0.303983, loss_sup: 0.020674, loss_mps: 0.095416, loss_cps: 0.187893
[13:10:57.077] iteration 11967: total_loss: 0.520199, loss_sup: 0.322875, loss_mps: 0.068117, loss_cps: 0.129207
[13:10:57.223] iteration 11968: total_loss: 0.291772, loss_sup: 0.060210, loss_mps: 0.082125, loss_cps: 0.149437
[13:10:57.369] iteration 11969: total_loss: 0.220921, loss_sup: 0.024141, loss_mps: 0.069394, loss_cps: 0.127387
[13:10:57.514] iteration 11970: total_loss: 0.390535, loss_sup: 0.178595, loss_mps: 0.074173, loss_cps: 0.137767
[13:10:57.661] iteration 11971: total_loss: 0.296346, loss_sup: 0.018848, loss_mps: 0.094257, loss_cps: 0.183241
[13:10:57.807] iteration 11972: total_loss: 0.173007, loss_sup: 0.030392, loss_mps: 0.055365, loss_cps: 0.087250
[13:10:57.953] iteration 11973: total_loss: 0.370622, loss_sup: 0.064290, loss_mps: 0.106692, loss_cps: 0.199640
[13:10:58.099] iteration 11974: total_loss: 0.215921, loss_sup: 0.044550, loss_mps: 0.065034, loss_cps: 0.106337
[13:10:58.244] iteration 11975: total_loss: 0.350116, loss_sup: 0.068778, loss_mps: 0.093832, loss_cps: 0.187507
[13:10:58.391] iteration 11976: total_loss: 0.234406, loss_sup: 0.034093, loss_mps: 0.070221, loss_cps: 0.130092
[13:10:58.538] iteration 11977: total_loss: 0.237742, loss_sup: 0.004275, loss_mps: 0.076642, loss_cps: 0.156825
[13:10:58.684] iteration 11978: total_loss: 0.212631, loss_sup: 0.019916, loss_mps: 0.070890, loss_cps: 0.121826
[13:10:58.830] iteration 11979: total_loss: 0.227157, loss_sup: 0.081392, loss_mps: 0.054834, loss_cps: 0.090931
[13:10:58.975] iteration 11980: total_loss: 0.516678, loss_sup: 0.195092, loss_mps: 0.111107, loss_cps: 0.210479
[13:10:59.125] iteration 11981: total_loss: 0.390917, loss_sup: 0.094091, loss_mps: 0.099436, loss_cps: 0.197390
[13:10:59.272] iteration 11982: total_loss: 0.228271, loss_sup: 0.029920, loss_mps: 0.068188, loss_cps: 0.130162
[13:10:59.419] iteration 11983: total_loss: 0.228699, loss_sup: 0.033295, loss_mps: 0.069247, loss_cps: 0.126157
[13:10:59.565] iteration 11984: total_loss: 0.256467, loss_sup: 0.056348, loss_mps: 0.075558, loss_cps: 0.124561
[13:10:59.713] iteration 11985: total_loss: 0.258138, loss_sup: 0.016436, loss_mps: 0.084709, loss_cps: 0.156993
[13:10:59.859] iteration 11986: total_loss: 0.212381, loss_sup: 0.042525, loss_mps: 0.059453, loss_cps: 0.110403
[13:11:00.004] iteration 11987: total_loss: 0.366850, loss_sup: 0.052050, loss_mps: 0.110744, loss_cps: 0.204056
[13:11:00.150] iteration 11988: total_loss: 0.145696, loss_sup: 0.015156, loss_mps: 0.046481, loss_cps: 0.084059
[13:11:00.297] iteration 11989: total_loss: 0.232687, loss_sup: 0.089899, loss_mps: 0.052730, loss_cps: 0.090058
[13:11:00.444] iteration 11990: total_loss: 0.212224, loss_sup: 0.032665, loss_mps: 0.062013, loss_cps: 0.117546
[13:11:00.590] iteration 11991: total_loss: 0.521999, loss_sup: 0.168030, loss_mps: 0.114210, loss_cps: 0.239759
[13:11:00.737] iteration 11992: total_loss: 0.452803, loss_sup: 0.213468, loss_mps: 0.080874, loss_cps: 0.158461
[13:11:00.883] iteration 11993: total_loss: 0.285461, loss_sup: 0.074083, loss_mps: 0.077308, loss_cps: 0.134070
[13:11:01.028] iteration 11994: total_loss: 0.163762, loss_sup: 0.042227, loss_mps: 0.046150, loss_cps: 0.075384
[13:11:01.174] iteration 11995: total_loss: 0.321846, loss_sup: 0.139934, loss_mps: 0.065564, loss_cps: 0.116348
[13:11:01.321] iteration 11996: total_loss: 0.192692, loss_sup: 0.014372, loss_mps: 0.063941, loss_cps: 0.114379
[13:11:01.469] iteration 11997: total_loss: 0.247101, loss_sup: 0.062568, loss_mps: 0.064875, loss_cps: 0.119658
[13:11:01.616] iteration 11998: total_loss: 0.323387, loss_sup: 0.107656, loss_mps: 0.074988, loss_cps: 0.140744
[13:11:01.761] iteration 11999: total_loss: 0.252573, loss_sup: 0.032252, loss_mps: 0.076879, loss_cps: 0.143441
[13:11:01.907] iteration 12000: total_loss: 0.495479, loss_sup: 0.307390, loss_mps: 0.064531, loss_cps: 0.123558
[13:11:01.911] Evaluation Started ==>
[13:11:13.329] ==> valid iteration 12000: unet metrics: {'dc': 0.6417566816084656, 'jc': 0.5246374465647831, 'pre': 0.7664740830926378, 'hd': 5.708849114262463}, ynet metrics: {'dc': 0.6031673233070217, 'jc': 0.482299871627375, 'pre': 0.7689553004144701, 'hd': 5.840997793361486}.
[13:11:13.331] Evaluation Finished!⏹️
[13:11:13.483] iteration 12001: total_loss: 0.311273, loss_sup: 0.126681, loss_mps: 0.064190, loss_cps: 0.120402
[13:11:13.629] iteration 12002: total_loss: 0.398825, loss_sup: 0.115685, loss_mps: 0.091502, loss_cps: 0.191638
[13:11:13.774] iteration 12003: total_loss: 0.291459, loss_sup: 0.055263, loss_mps: 0.081345, loss_cps: 0.154851
[13:11:13.920] iteration 12004: total_loss: 0.295483, loss_sup: 0.047471, loss_mps: 0.081517, loss_cps: 0.166495
[13:11:14.069] iteration 12005: total_loss: 0.284759, loss_sup: 0.076321, loss_mps: 0.074319, loss_cps: 0.134119
[13:11:14.214] iteration 12006: total_loss: 0.391186, loss_sup: 0.129336, loss_mps: 0.089028, loss_cps: 0.172822
[13:11:14.360] iteration 12007: total_loss: 0.386236, loss_sup: 0.054980, loss_mps: 0.102801, loss_cps: 0.228455
[13:11:14.506] iteration 12008: total_loss: 0.403813, loss_sup: 0.168285, loss_mps: 0.076940, loss_cps: 0.158589
[13:11:14.656] iteration 12009: total_loss: 0.375722, loss_sup: 0.091892, loss_mps: 0.094370, loss_cps: 0.189460
[13:11:14.802] iteration 12010: total_loss: 0.309396, loss_sup: 0.045392, loss_mps: 0.089284, loss_cps: 0.174721
[13:11:14.947] iteration 12011: total_loss: 0.281252, loss_sup: 0.071260, loss_mps: 0.075831, loss_cps: 0.134161
[13:11:15.093] iteration 12012: total_loss: 0.256344, loss_sup: 0.034739, loss_mps: 0.076694, loss_cps: 0.144911
[13:11:15.240] iteration 12013: total_loss: 0.222600, loss_sup: 0.026235, loss_mps: 0.067367, loss_cps: 0.128997
[13:11:15.387] iteration 12014: total_loss: 0.236783, loss_sup: 0.033014, loss_mps: 0.072767, loss_cps: 0.131002
[13:11:15.533] iteration 12015: total_loss: 0.247397, loss_sup: 0.060722, loss_mps: 0.067791, loss_cps: 0.118884
[13:11:15.679] iteration 12016: total_loss: 0.422981, loss_sup: 0.110657, loss_mps: 0.104643, loss_cps: 0.207681
[13:11:15.828] iteration 12017: total_loss: 0.386787, loss_sup: 0.094671, loss_mps: 0.094946, loss_cps: 0.197170
[13:11:15.974] iteration 12018: total_loss: 0.250620, loss_sup: 0.027137, loss_mps: 0.082247, loss_cps: 0.141236
[13:11:16.119] iteration 12019: total_loss: 0.231828, loss_sup: 0.062249, loss_mps: 0.061346, loss_cps: 0.108233
[13:11:16.265] iteration 12020: total_loss: 0.168887, loss_sup: 0.007672, loss_mps: 0.058857, loss_cps: 0.102357
[13:11:16.414] iteration 12021: total_loss: 0.226299, loss_sup: 0.025307, loss_mps: 0.071878, loss_cps: 0.129114
[13:11:16.562] iteration 12022: total_loss: 0.629000, loss_sup: 0.142231, loss_mps: 0.153310, loss_cps: 0.333459
[13:11:16.708] iteration 12023: total_loss: 0.389495, loss_sup: 0.118983, loss_mps: 0.088564, loss_cps: 0.181948
[13:11:16.854] iteration 12024: total_loss: 0.357813, loss_sup: 0.037925, loss_mps: 0.101478, loss_cps: 0.218409
[13:11:17.000] iteration 12025: total_loss: 0.324567, loss_sup: 0.048451, loss_mps: 0.088070, loss_cps: 0.188046
[13:11:17.146] iteration 12026: total_loss: 0.319508, loss_sup: 0.022206, loss_mps: 0.098246, loss_cps: 0.199056
[13:11:17.291] iteration 12027: total_loss: 0.329036, loss_sup: 0.009459, loss_mps: 0.101973, loss_cps: 0.217604
[13:11:17.437] iteration 12028: total_loss: 0.293582, loss_sup: 0.012543, loss_mps: 0.092288, loss_cps: 0.188751
[13:11:17.583] iteration 12029: total_loss: 0.467027, loss_sup: 0.026186, loss_mps: 0.138632, loss_cps: 0.302209
[13:11:17.732] iteration 12030: total_loss: 0.520636, loss_sup: 0.170332, loss_mps: 0.111883, loss_cps: 0.238421
[13:11:17.877] iteration 12031: total_loss: 0.481348, loss_sup: 0.062736, loss_mps: 0.129050, loss_cps: 0.289562
[13:11:18.023] iteration 12032: total_loss: 0.260888, loss_sup: 0.005842, loss_mps: 0.085357, loss_cps: 0.169689
[13:11:18.170] iteration 12033: total_loss: 0.344582, loss_sup: 0.151978, loss_mps: 0.069327, loss_cps: 0.123277
[13:11:18.316] iteration 12034: total_loss: 0.378061, loss_sup: 0.040585, loss_mps: 0.106991, loss_cps: 0.230485
[13:11:18.468] iteration 12035: total_loss: 0.411675, loss_sup: 0.100819, loss_mps: 0.104289, loss_cps: 0.206567
[13:11:18.613] iteration 12036: total_loss: 0.152017, loss_sup: 0.013983, loss_mps: 0.050676, loss_cps: 0.087358
[13:11:18.761] iteration 12037: total_loss: 0.436483, loss_sup: 0.099218, loss_mps: 0.097637, loss_cps: 0.239628
[13:11:18.910] iteration 12038: total_loss: 0.622539, loss_sup: 0.174540, loss_mps: 0.139166, loss_cps: 0.308832
[13:11:19.056] iteration 12039: total_loss: 0.559280, loss_sup: 0.078194, loss_mps: 0.153834, loss_cps: 0.327253
[13:11:19.202] iteration 12040: total_loss: 0.153270, loss_sup: 0.021706, loss_mps: 0.048580, loss_cps: 0.082985
[13:11:19.348] iteration 12041: total_loss: 0.201153, loss_sup: 0.039791, loss_mps: 0.059435, loss_cps: 0.101927
[13:11:19.495] iteration 12042: total_loss: 0.364995, loss_sup: 0.073059, loss_mps: 0.095885, loss_cps: 0.196051
[13:11:19.641] iteration 12043: total_loss: 0.181420, loss_sup: 0.020684, loss_mps: 0.061211, loss_cps: 0.099526
[13:11:19.787] iteration 12044: total_loss: 0.409978, loss_sup: 0.153276, loss_mps: 0.087103, loss_cps: 0.169599
[13:11:19.933] iteration 12045: total_loss: 0.369705, loss_sup: 0.055095, loss_mps: 0.103877, loss_cps: 0.210734
[13:11:20.083] iteration 12046: total_loss: 0.428673, loss_sup: 0.156399, loss_mps: 0.095599, loss_cps: 0.176675
[13:11:20.229] iteration 12047: total_loss: 0.307063, loss_sup: 0.075985, loss_mps: 0.083286, loss_cps: 0.147792
[13:11:20.374] iteration 12048: total_loss: 0.522043, loss_sup: 0.083833, loss_mps: 0.136593, loss_cps: 0.301616
[13:11:20.521] iteration 12049: total_loss: 0.263959, loss_sup: 0.038694, loss_mps: 0.078014, loss_cps: 0.147250
[13:11:20.667] iteration 12050: total_loss: 0.275434, loss_sup: 0.039519, loss_mps: 0.080461, loss_cps: 0.155454
[13:11:20.813] iteration 12051: total_loss: 0.531493, loss_sup: 0.256048, loss_mps: 0.088181, loss_cps: 0.187263
[13:11:20.959] iteration 12052: total_loss: 0.378397, loss_sup: 0.051713, loss_mps: 0.109491, loss_cps: 0.217193
[13:11:21.105] iteration 12053: total_loss: 0.493389, loss_sup: 0.149281, loss_mps: 0.116219, loss_cps: 0.227889
[13:11:21.253] iteration 12054: total_loss: 0.259951, loss_sup: 0.023306, loss_mps: 0.081569, loss_cps: 0.155076
[13:11:21.399] iteration 12055: total_loss: 0.380321, loss_sup: 0.053367, loss_mps: 0.103519, loss_cps: 0.223435
[13:11:21.545] iteration 12056: total_loss: 0.434137, loss_sup: 0.085961, loss_mps: 0.115958, loss_cps: 0.232218
[13:11:21.691] iteration 12057: total_loss: 0.282675, loss_sup: 0.029331, loss_mps: 0.089606, loss_cps: 0.163738
[13:11:21.839] iteration 12058: total_loss: 0.302672, loss_sup: 0.025934, loss_mps: 0.092289, loss_cps: 0.184449
[13:11:21.985] iteration 12059: total_loss: 0.548347, loss_sup: 0.233875, loss_mps: 0.100391, loss_cps: 0.214080
[13:11:22.131] iteration 12060: total_loss: 0.391812, loss_sup: 0.078130, loss_mps: 0.103702, loss_cps: 0.209980
[13:11:22.277] iteration 12061: total_loss: 0.329948, loss_sup: 0.071773, loss_mps: 0.090226, loss_cps: 0.167949
[13:11:22.424] iteration 12062: total_loss: 0.301150, loss_sup: 0.031892, loss_mps: 0.094460, loss_cps: 0.174798
[13:11:22.570] iteration 12063: total_loss: 0.272913, loss_sup: 0.053732, loss_mps: 0.077486, loss_cps: 0.141694
[13:11:22.716] iteration 12064: total_loss: 0.262980, loss_sup: 0.114117, loss_mps: 0.059297, loss_cps: 0.089566
[13:11:22.862] iteration 12065: total_loss: 0.219104, loss_sup: 0.012930, loss_mps: 0.071086, loss_cps: 0.135087
[13:11:23.009] iteration 12066: total_loss: 0.462109, loss_sup: 0.054412, loss_mps: 0.125554, loss_cps: 0.282143
[13:11:23.154] iteration 12067: total_loss: 0.177600, loss_sup: 0.008716, loss_mps: 0.059687, loss_cps: 0.109196
[13:11:23.301] iteration 12068: total_loss: 0.435754, loss_sup: 0.026409, loss_mps: 0.132536, loss_cps: 0.276809
[13:11:23.447] iteration 12069: total_loss: 0.277079, loss_sup: 0.063462, loss_mps: 0.076380, loss_cps: 0.137238
[13:11:23.593] iteration 12070: total_loss: 0.214209, loss_sup: 0.019311, loss_mps: 0.069793, loss_cps: 0.125105
[13:11:23.739] iteration 12071: total_loss: 0.202200, loss_sup: 0.022175, loss_mps: 0.063667, loss_cps: 0.116358
[13:11:23.885] iteration 12072: total_loss: 0.474699, loss_sup: 0.167921, loss_mps: 0.103128, loss_cps: 0.203650
[13:11:24.031] iteration 12073: total_loss: 0.210212, loss_sup: 0.016091, loss_mps: 0.066726, loss_cps: 0.127396
[13:11:24.177] iteration 12074: total_loss: 0.484526, loss_sup: 0.223804, loss_mps: 0.089537, loss_cps: 0.171185
[13:11:24.323] iteration 12075: total_loss: 0.685336, loss_sup: 0.199465, loss_mps: 0.145384, loss_cps: 0.340486
[13:11:24.468] iteration 12076: total_loss: 0.197677, loss_sup: 0.023626, loss_mps: 0.061702, loss_cps: 0.112350
[13:11:24.614] iteration 12077: total_loss: 0.468611, loss_sup: 0.118833, loss_mps: 0.117540, loss_cps: 0.232238
[13:11:24.760] iteration 12078: total_loss: 0.440767, loss_sup: 0.141358, loss_mps: 0.101643, loss_cps: 0.197765
[13:11:24.908] iteration 12079: total_loss: 0.588379, loss_sup: 0.213347, loss_mps: 0.122193, loss_cps: 0.252839
[13:11:25.053] iteration 12080: total_loss: 0.208686, loss_sup: 0.075192, loss_mps: 0.048061, loss_cps: 0.085434
[13:11:25.199] iteration 12081: total_loss: 0.320034, loss_sup: 0.059363, loss_mps: 0.091841, loss_cps: 0.168830
[13:11:25.344] iteration 12082: total_loss: 0.166845, loss_sup: 0.010686, loss_mps: 0.057816, loss_cps: 0.098343
[13:11:25.490] iteration 12083: total_loss: 0.346396, loss_sup: 0.099387, loss_mps: 0.089124, loss_cps: 0.157885
[13:11:25.635] iteration 12084: total_loss: 0.568252, loss_sup: 0.150355, loss_mps: 0.137454, loss_cps: 0.280443
[13:11:25.782] iteration 12085: total_loss: 0.734051, loss_sup: 0.425128, loss_mps: 0.104850, loss_cps: 0.204073
[13:11:25.932] iteration 12086: total_loss: 0.649329, loss_sup: 0.168394, loss_mps: 0.153997, loss_cps: 0.326938
[13:11:26.081] iteration 12087: total_loss: 0.809808, loss_sup: 0.574207, loss_mps: 0.084024, loss_cps: 0.151577
[13:11:26.227] iteration 12088: total_loss: 0.499811, loss_sup: 0.150052, loss_mps: 0.119139, loss_cps: 0.230620
[13:11:26.373] iteration 12089: total_loss: 0.347570, loss_sup: 0.088997, loss_mps: 0.092929, loss_cps: 0.165644
[13:11:26.519] iteration 12090: total_loss: 0.372974, loss_sup: 0.057223, loss_mps: 0.111355, loss_cps: 0.204396
[13:11:26.666] iteration 12091: total_loss: 0.287489, loss_sup: 0.070828, loss_mps: 0.078051, loss_cps: 0.138609
[13:11:26.812] iteration 12092: total_loss: 0.237506, loss_sup: 0.015122, loss_mps: 0.077626, loss_cps: 0.144758
[13:11:26.958] iteration 12093: total_loss: 0.279103, loss_sup: 0.037075, loss_mps: 0.094829, loss_cps: 0.147199
[13:11:27.104] iteration 12094: total_loss: 1.014222, loss_sup: 0.623985, loss_mps: 0.122287, loss_cps: 0.267950
[13:11:27.251] iteration 12095: total_loss: 0.530615, loss_sup: 0.159429, loss_mps: 0.122808, loss_cps: 0.248378
[13:11:27.397] iteration 12096: total_loss: 0.683706, loss_sup: 0.098945, loss_mps: 0.192579, loss_cps: 0.392182
[13:11:27.544] iteration 12097: total_loss: 0.364171, loss_sup: 0.046453, loss_mps: 0.105927, loss_cps: 0.211790
[13:11:27.689] iteration 12098: total_loss: 0.251605, loss_sup: 0.012103, loss_mps: 0.091072, loss_cps: 0.148429
[13:11:27.835] iteration 12099: total_loss: 0.325775, loss_sup: 0.061512, loss_mps: 0.091704, loss_cps: 0.172558
[13:11:27.981] iteration 12100: total_loss: 0.629878, loss_sup: 0.293960, loss_mps: 0.117719, loss_cps: 0.218200
[13:11:27.981] Evaluation Started ==>
[13:11:39.428] ==> valid iteration 12100: unet metrics: {'dc': 0.643498801531913, 'jc': 0.523968291270382, 'pre': 0.7439292502730698, 'hd': 5.7261037570764906}, ynet metrics: {'dc': 0.5538645684530679, 'jc': 0.4425494650605556, 'pre': 0.7586095819817786, 'hd': 5.5306131795303175}.
[13:11:39.430] Evaluation Finished!⏹️
[13:11:39.585] iteration 12101: total_loss: 0.342899, loss_sup: 0.073250, loss_mps: 0.099579, loss_cps: 0.170070
[13:11:39.734] iteration 12102: total_loss: 0.292007, loss_sup: 0.023186, loss_mps: 0.100185, loss_cps: 0.168636
[13:11:39.880] iteration 12103: total_loss: 0.250443, loss_sup: 0.063401, loss_mps: 0.072892, loss_cps: 0.114149
[13:11:40.025] iteration 12104: total_loss: 0.295142, loss_sup: 0.096236, loss_mps: 0.074247, loss_cps: 0.124658
[13:11:40.170] iteration 12105: total_loss: 0.390573, loss_sup: 0.137183, loss_mps: 0.096325, loss_cps: 0.157066
[13:11:40.316] iteration 12106: total_loss: 0.420278, loss_sup: 0.083723, loss_mps: 0.115074, loss_cps: 0.221482
[13:11:40.463] iteration 12107: total_loss: 0.415665, loss_sup: 0.155911, loss_mps: 0.093776, loss_cps: 0.165978
[13:11:40.608] iteration 12108: total_loss: 0.239202, loss_sup: 0.045706, loss_mps: 0.077983, loss_cps: 0.115512
[13:11:40.753] iteration 12109: total_loss: 0.319291, loss_sup: 0.083734, loss_mps: 0.087771, loss_cps: 0.147786
[13:11:40.899] iteration 12110: total_loss: 0.350925, loss_sup: 0.090206, loss_mps: 0.091501, loss_cps: 0.169218
[13:11:41.044] iteration 12111: total_loss: 0.300241, loss_sup: 0.065660, loss_mps: 0.085346, loss_cps: 0.149235
[13:11:41.190] iteration 12112: total_loss: 0.426610, loss_sup: 0.205587, loss_mps: 0.079203, loss_cps: 0.141819
[13:11:41.336] iteration 12113: total_loss: 0.365703, loss_sup: 0.091208, loss_mps: 0.090229, loss_cps: 0.184266
[13:11:41.481] iteration 12114: total_loss: 0.486402, loss_sup: 0.048268, loss_mps: 0.145503, loss_cps: 0.292631
[13:11:41.626] iteration 12115: total_loss: 0.178482, loss_sup: 0.014978, loss_mps: 0.060743, loss_cps: 0.102761
[13:11:41.773] iteration 12116: total_loss: 0.629787, loss_sup: 0.153851, loss_mps: 0.145137, loss_cps: 0.330799
[13:11:41.920] iteration 12117: total_loss: 0.416830, loss_sup: 0.091363, loss_mps: 0.109497, loss_cps: 0.215970
[13:11:42.067] iteration 12118: total_loss: 0.483803, loss_sup: 0.151227, loss_mps: 0.114597, loss_cps: 0.217979
[13:11:42.214] iteration 12119: total_loss: 0.193665, loss_sup: 0.031934, loss_mps: 0.060784, loss_cps: 0.100947
[13:11:42.360] iteration 12120: total_loss: 0.451732, loss_sup: 0.200231, loss_mps: 0.091087, loss_cps: 0.160413
[13:11:42.507] iteration 12121: total_loss: 0.286591, loss_sup: 0.023600, loss_mps: 0.092481, loss_cps: 0.170510
[13:11:42.573] iteration 12122: total_loss: 0.210288, loss_sup: 0.099297, loss_mps: 0.041370, loss_cps: 0.069622
[13:11:43.802] iteration 12123: total_loss: 0.282925, loss_sup: 0.079473, loss_mps: 0.072787, loss_cps: 0.130665
[13:11:43.951] iteration 12124: total_loss: 0.229331, loss_sup: 0.049411, loss_mps: 0.065090, loss_cps: 0.114829
[13:11:44.098] iteration 12125: total_loss: 0.584579, loss_sup: 0.058637, loss_mps: 0.162932, loss_cps: 0.363011
[13:11:44.244] iteration 12126: total_loss: 0.285748, loss_sup: 0.078958, loss_mps: 0.074140, loss_cps: 0.132650
[13:11:44.390] iteration 12127: total_loss: 0.378584, loss_sup: 0.087432, loss_mps: 0.091065, loss_cps: 0.200087
[13:11:44.535] iteration 12128: total_loss: 0.277085, loss_sup: 0.054902, loss_mps: 0.074281, loss_cps: 0.147902
[13:11:44.682] iteration 12129: total_loss: 0.340200, loss_sup: 0.022774, loss_mps: 0.103258, loss_cps: 0.214168
[13:11:44.829] iteration 12130: total_loss: 0.544127, loss_sup: 0.021861, loss_mps: 0.160460, loss_cps: 0.361807
[13:11:44.975] iteration 12131: total_loss: 0.303738, loss_sup: 0.044377, loss_mps: 0.089611, loss_cps: 0.169750
[13:11:45.121] iteration 12132: total_loss: 0.240032, loss_sup: 0.060972, loss_mps: 0.062109, loss_cps: 0.116951
[13:11:45.267] iteration 12133: total_loss: 0.279599, loss_sup: 0.062986, loss_mps: 0.077019, loss_cps: 0.139595
[13:11:45.413] iteration 12134: total_loss: 0.306588, loss_sup: 0.057247, loss_mps: 0.083085, loss_cps: 0.166255
[13:11:45.578] iteration 12135: total_loss: 0.377939, loss_sup: 0.113555, loss_mps: 0.088456, loss_cps: 0.175928
[13:11:45.735] iteration 12136: total_loss: 0.227461, loss_sup: 0.028652, loss_mps: 0.067213, loss_cps: 0.131596
[13:11:45.881] iteration 12137: total_loss: 0.228780, loss_sup: 0.051733, loss_mps: 0.064541, loss_cps: 0.112505
[13:11:46.029] iteration 12138: total_loss: 0.282236, loss_sup: 0.111232, loss_mps: 0.061773, loss_cps: 0.109232
[13:11:46.175] iteration 12139: total_loss: 0.321458, loss_sup: 0.056191, loss_mps: 0.087138, loss_cps: 0.178129
[13:11:46.321] iteration 12140: total_loss: 0.493288, loss_sup: 0.188687, loss_mps: 0.101473, loss_cps: 0.203128
[13:11:46.467] iteration 12141: total_loss: 0.320695, loss_sup: 0.111513, loss_mps: 0.072298, loss_cps: 0.136883
[13:11:46.616] iteration 12142: total_loss: 0.327228, loss_sup: 0.007170, loss_mps: 0.105424, loss_cps: 0.214633
[13:11:46.763] iteration 12143: total_loss: 0.240074, loss_sup: 0.053345, loss_mps: 0.065235, loss_cps: 0.121494
[13:11:46.909] iteration 12144: total_loss: 0.293960, loss_sup: 0.029631, loss_mps: 0.086628, loss_cps: 0.177701
[13:11:47.055] iteration 12145: total_loss: 0.325260, loss_sup: 0.007372, loss_mps: 0.099387, loss_cps: 0.218501
[13:11:47.204] iteration 12146: total_loss: 0.227991, loss_sup: 0.065240, loss_mps: 0.060652, loss_cps: 0.102099
[13:11:47.351] iteration 12147: total_loss: 0.377968, loss_sup: 0.046123, loss_mps: 0.108663, loss_cps: 0.223182
[13:11:47.499] iteration 12148: total_loss: 0.225281, loss_sup: 0.013571, loss_mps: 0.070945, loss_cps: 0.140766
[13:11:47.645] iteration 12149: total_loss: 0.303179, loss_sup: 0.043474, loss_mps: 0.084429, loss_cps: 0.175277
[13:11:47.791] iteration 12150: total_loss: 0.279520, loss_sup: 0.084199, loss_mps: 0.068625, loss_cps: 0.126696
[13:11:47.938] iteration 12151: total_loss: 0.539143, loss_sup: 0.127372, loss_mps: 0.129652, loss_cps: 0.282119
[13:11:48.083] iteration 12152: total_loss: 0.362410, loss_sup: 0.088073, loss_mps: 0.090366, loss_cps: 0.183971
[13:11:48.229] iteration 12153: total_loss: 0.364382, loss_sup: 0.062188, loss_mps: 0.101041, loss_cps: 0.201153
[13:11:48.376] iteration 12154: total_loss: 0.179300, loss_sup: 0.027699, loss_mps: 0.053531, loss_cps: 0.098070
[13:11:48.522] iteration 12155: total_loss: 0.502617, loss_sup: 0.102928, loss_mps: 0.126627, loss_cps: 0.273063
[13:11:48.668] iteration 12156: total_loss: 0.500087, loss_sup: 0.103425, loss_mps: 0.128374, loss_cps: 0.268288
[13:11:48.815] iteration 12157: total_loss: 0.351615, loss_sup: 0.147267, loss_mps: 0.069707, loss_cps: 0.134640
[13:11:48.964] iteration 12158: total_loss: 0.294411, loss_sup: 0.074940, loss_mps: 0.072597, loss_cps: 0.146875
[13:11:49.110] iteration 12159: total_loss: 0.403212, loss_sup: 0.167026, loss_mps: 0.080190, loss_cps: 0.155996
[13:11:49.256] iteration 12160: total_loss: 0.365911, loss_sup: 0.100503, loss_mps: 0.090998, loss_cps: 0.174410
[13:11:49.402] iteration 12161: total_loss: 0.469043, loss_sup: 0.203927, loss_mps: 0.094187, loss_cps: 0.170929
[13:11:49.548] iteration 12162: total_loss: 0.227233, loss_sup: 0.014650, loss_mps: 0.070447, loss_cps: 0.142136
[13:11:49.694] iteration 12163: total_loss: 0.255305, loss_sup: 0.052992, loss_mps: 0.072358, loss_cps: 0.129955
[13:11:49.841] iteration 12164: total_loss: 0.395205, loss_sup: 0.138546, loss_mps: 0.090689, loss_cps: 0.165969
[13:11:49.987] iteration 12165: total_loss: 0.213660, loss_sup: 0.056270, loss_mps: 0.058275, loss_cps: 0.099116
[13:11:50.133] iteration 12166: total_loss: 0.205064, loss_sup: 0.009208, loss_mps: 0.068275, loss_cps: 0.127582
[13:11:50.279] iteration 12167: total_loss: 0.148337, loss_sup: 0.006590, loss_mps: 0.054863, loss_cps: 0.086883
[13:11:50.426] iteration 12168: total_loss: 0.234673, loss_sup: 0.080245, loss_mps: 0.057206, loss_cps: 0.097222
[13:11:50.572] iteration 12169: total_loss: 0.439579, loss_sup: 0.121647, loss_mps: 0.105265, loss_cps: 0.212668
[13:11:50.719] iteration 12170: total_loss: 0.267247, loss_sup: 0.078670, loss_mps: 0.068103, loss_cps: 0.120474
[13:11:50.865] iteration 12171: total_loss: 0.239258, loss_sup: 0.072387, loss_mps: 0.062558, loss_cps: 0.104313
[13:11:51.011] iteration 12172: total_loss: 0.391182, loss_sup: 0.070612, loss_mps: 0.104759, loss_cps: 0.215811
[13:11:51.158] iteration 12173: total_loss: 0.303232, loss_sup: 0.044180, loss_mps: 0.089525, loss_cps: 0.169526
[13:11:51.307] iteration 12174: total_loss: 0.225960, loss_sup: 0.043654, loss_mps: 0.068239, loss_cps: 0.114067
[13:11:51.453] iteration 12175: total_loss: 0.660504, loss_sup: 0.133368, loss_mps: 0.168398, loss_cps: 0.358737
[13:11:51.600] iteration 12176: total_loss: 0.212324, loss_sup: 0.023929, loss_mps: 0.066940, loss_cps: 0.121455
[13:11:51.752] iteration 12177: total_loss: 0.150305, loss_sup: 0.014880, loss_mps: 0.050246, loss_cps: 0.085178
[13:11:51.899] iteration 12178: total_loss: 0.390426, loss_sup: 0.097073, loss_mps: 0.099218, loss_cps: 0.194135
[13:11:52.045] iteration 12179: total_loss: 0.408822, loss_sup: 0.037046, loss_mps: 0.118078, loss_cps: 0.253698
[13:11:52.192] iteration 12180: total_loss: 0.378618, loss_sup: 0.129441, loss_mps: 0.087609, loss_cps: 0.161568
[13:11:52.341] iteration 12181: total_loss: 0.283678, loss_sup: 0.060084, loss_mps: 0.082130, loss_cps: 0.141465
[13:11:52.487] iteration 12182: total_loss: 0.293873, loss_sup: 0.088067, loss_mps: 0.071294, loss_cps: 0.134512
[13:11:52.633] iteration 12183: total_loss: 0.302058, loss_sup: 0.036533, loss_mps: 0.095398, loss_cps: 0.170126
[13:11:52.779] iteration 12184: total_loss: 0.647334, loss_sup: 0.256192, loss_mps: 0.123299, loss_cps: 0.267843
[13:11:52.925] iteration 12185: total_loss: 0.308531, loss_sup: 0.162201, loss_mps: 0.053726, loss_cps: 0.092605
[13:11:53.072] iteration 12186: total_loss: 0.398244, loss_sup: 0.169254, loss_mps: 0.084047, loss_cps: 0.144944
[13:11:53.220] iteration 12187: total_loss: 0.249872, loss_sup: 0.063506, loss_mps: 0.067524, loss_cps: 0.118843
[13:11:53.366] iteration 12188: total_loss: 0.239173, loss_sup: 0.040729, loss_mps: 0.069202, loss_cps: 0.129242
[13:11:53.512] iteration 12189: total_loss: 0.268588, loss_sup: 0.039592, loss_mps: 0.082346, loss_cps: 0.146649
[13:11:53.659] iteration 12190: total_loss: 0.574037, loss_sup: 0.157753, loss_mps: 0.130174, loss_cps: 0.286109
[13:11:53.805] iteration 12191: total_loss: 0.226492, loss_sup: 0.067071, loss_mps: 0.056025, loss_cps: 0.103396
[13:11:53.951] iteration 12192: total_loss: 0.195075, loss_sup: 0.043993, loss_mps: 0.055551, loss_cps: 0.095530
[13:11:54.098] iteration 12193: total_loss: 0.284641, loss_sup: 0.031687, loss_mps: 0.084237, loss_cps: 0.168717
[13:11:54.244] iteration 12194: total_loss: 0.143190, loss_sup: 0.005612, loss_mps: 0.048499, loss_cps: 0.089078
[13:11:54.390] iteration 12195: total_loss: 0.277627, loss_sup: 0.073173, loss_mps: 0.071940, loss_cps: 0.132515
[13:11:54.538] iteration 12196: total_loss: 0.462539, loss_sup: 0.103625, loss_mps: 0.116823, loss_cps: 0.242092
[13:11:54.684] iteration 12197: total_loss: 0.220454, loss_sup: 0.054613, loss_mps: 0.061438, loss_cps: 0.104403
[13:11:54.830] iteration 12198: total_loss: 0.230134, loss_sup: 0.035389, loss_mps: 0.065826, loss_cps: 0.128919
[13:11:54.977] iteration 12199: total_loss: 0.352781, loss_sup: 0.039464, loss_mps: 0.104330, loss_cps: 0.208988
[13:11:55.123] iteration 12200: total_loss: 0.620186, loss_sup: 0.314318, loss_mps: 0.105553, loss_cps: 0.200315
[13:11:55.124] Evaluation Started ==>
[13:12:06.430] ==> valid iteration 12200: unet metrics: {'dc': 0.6069551449970678, 'jc': 0.4906858058830629, 'pre': 0.7606278401975649, 'hd': 5.6748248382471145}, ynet metrics: {'dc': 0.5899338877332098, 'jc': 0.47332457950447887, 'pre': 0.787512789820452, 'hd': 5.723744288143172}.
[13:12:06.432] Evaluation Finished!⏹️
[13:12:06.582] iteration 12201: total_loss: 0.268929, loss_sup: 0.020172, loss_mps: 0.086696, loss_cps: 0.162061
[13:12:06.730] iteration 12202: total_loss: 0.288090, loss_sup: 0.114926, loss_mps: 0.062064, loss_cps: 0.111100
[13:12:06.876] iteration 12203: total_loss: 0.334974, loss_sup: 0.051297, loss_mps: 0.090034, loss_cps: 0.193644
[13:12:07.021] iteration 12204: total_loss: 0.389677, loss_sup: 0.065828, loss_mps: 0.106933, loss_cps: 0.216916
[13:12:07.166] iteration 12205: total_loss: 0.295628, loss_sup: 0.032504, loss_mps: 0.085569, loss_cps: 0.177555
[13:12:07.312] iteration 12206: total_loss: 0.378915, loss_sup: 0.134838, loss_mps: 0.081937, loss_cps: 0.162139
[13:12:07.457] iteration 12207: total_loss: 0.244267, loss_sup: 0.051990, loss_mps: 0.072511, loss_cps: 0.119765
[13:12:07.604] iteration 12208: total_loss: 0.615940, loss_sup: 0.489487, loss_mps: 0.049409, loss_cps: 0.077044
[13:12:07.752] iteration 12209: total_loss: 0.246902, loss_sup: 0.021553, loss_mps: 0.078243, loss_cps: 0.147106
[13:12:07.897] iteration 12210: total_loss: 0.219750, loss_sup: 0.034367, loss_mps: 0.066802, loss_cps: 0.118581
[13:12:08.043] iteration 12211: total_loss: 0.161220, loss_sup: 0.006059, loss_mps: 0.060800, loss_cps: 0.094361
[13:12:08.188] iteration 12212: total_loss: 0.279038, loss_sup: 0.079275, loss_mps: 0.071607, loss_cps: 0.128156
[13:12:08.333] iteration 12213: total_loss: 0.401877, loss_sup: 0.033463, loss_mps: 0.120597, loss_cps: 0.247817
[13:12:08.478] iteration 12214: total_loss: 0.407653, loss_sup: 0.152034, loss_mps: 0.091410, loss_cps: 0.164209
[13:12:08.626] iteration 12215: total_loss: 0.232170, loss_sup: 0.084390, loss_mps: 0.056222, loss_cps: 0.091558
[13:12:08.772] iteration 12216: total_loss: 0.673158, loss_sup: 0.215206, loss_mps: 0.152080, loss_cps: 0.305873
[13:12:08.918] iteration 12217: total_loss: 0.231064, loss_sup: 0.046533, loss_mps: 0.068410, loss_cps: 0.116120
[13:12:09.064] iteration 12218: total_loss: 0.213037, loss_sup: 0.048911, loss_mps: 0.061873, loss_cps: 0.102253
[13:12:09.211] iteration 12219: total_loss: 0.192009, loss_sup: 0.033242, loss_mps: 0.055955, loss_cps: 0.102811
[13:12:09.365] iteration 12220: total_loss: 0.367847, loss_sup: 0.113782, loss_mps: 0.088876, loss_cps: 0.165188
[13:12:09.511] iteration 12221: total_loss: 0.270883, loss_sup: 0.033567, loss_mps: 0.084404, loss_cps: 0.152912
[13:12:09.657] iteration 12222: total_loss: 0.386335, loss_sup: 0.148729, loss_mps: 0.082171, loss_cps: 0.155436
[13:12:09.807] iteration 12223: total_loss: 0.204169, loss_sup: 0.021853, loss_mps: 0.065322, loss_cps: 0.116994
[13:12:09.957] iteration 12224: total_loss: 0.201436, loss_sup: 0.052577, loss_mps: 0.056805, loss_cps: 0.092054
[13:12:10.103] iteration 12225: total_loss: 0.368737, loss_sup: 0.133042, loss_mps: 0.082599, loss_cps: 0.153095
[13:12:10.249] iteration 12226: total_loss: 0.347553, loss_sup: 0.006266, loss_mps: 0.117382, loss_cps: 0.223905
[13:12:10.395] iteration 12227: total_loss: 0.425486, loss_sup: 0.104976, loss_mps: 0.108252, loss_cps: 0.212258
[13:12:10.541] iteration 12228: total_loss: 0.497261, loss_sup: 0.054986, loss_mps: 0.142366, loss_cps: 0.299910
[13:12:10.687] iteration 12229: total_loss: 0.272452, loss_sup: 0.032289, loss_mps: 0.083761, loss_cps: 0.156402
[13:12:10.832] iteration 12230: total_loss: 0.397092, loss_sup: 0.156301, loss_mps: 0.085479, loss_cps: 0.155312
[13:12:10.979] iteration 12231: total_loss: 0.303523, loss_sup: 0.077137, loss_mps: 0.078832, loss_cps: 0.147555
[13:12:11.129] iteration 12232: total_loss: 0.358944, loss_sup: 0.081626, loss_mps: 0.093523, loss_cps: 0.183795
[13:12:11.274] iteration 12233: total_loss: 0.266319, loss_sup: 0.085844, loss_mps: 0.065156, loss_cps: 0.115319
[13:12:11.420] iteration 12234: total_loss: 0.330535, loss_sup: 0.077942, loss_mps: 0.084887, loss_cps: 0.167706
[13:12:11.566] iteration 12235: total_loss: 0.313526, loss_sup: 0.063663, loss_mps: 0.088835, loss_cps: 0.161029
[13:12:11.711] iteration 12236: total_loss: 0.183132, loss_sup: 0.014412, loss_mps: 0.061189, loss_cps: 0.107531
[13:12:11.857] iteration 12237: total_loss: 0.578898, loss_sup: 0.205194, loss_mps: 0.117486, loss_cps: 0.256219
[13:12:12.003] iteration 12238: total_loss: 0.438921, loss_sup: 0.094084, loss_mps: 0.113573, loss_cps: 0.231264
[13:12:12.148] iteration 12239: total_loss: 0.585517, loss_sup: 0.207629, loss_mps: 0.121316, loss_cps: 0.256572
[13:12:12.294] iteration 12240: total_loss: 0.367713, loss_sup: 0.104176, loss_mps: 0.087349, loss_cps: 0.176188
[13:12:12.439] iteration 12241: total_loss: 0.553277, loss_sup: 0.139592, loss_mps: 0.133532, loss_cps: 0.280154
[13:12:12.585] iteration 12242: total_loss: 0.280286, loss_sup: 0.079952, loss_mps: 0.070140, loss_cps: 0.130194
[13:12:12.731] iteration 12243: total_loss: 0.511150, loss_sup: 0.241314, loss_mps: 0.088430, loss_cps: 0.181407
[13:12:12.876] iteration 12244: total_loss: 0.262112, loss_sup: 0.069457, loss_mps: 0.071385, loss_cps: 0.121270
[13:12:13.025] iteration 12245: total_loss: 0.544738, loss_sup: 0.100762, loss_mps: 0.136832, loss_cps: 0.307144
[13:12:13.170] iteration 12246: total_loss: 0.537919, loss_sup: 0.111655, loss_mps: 0.135613, loss_cps: 0.290651
[13:12:13.315] iteration 12247: total_loss: 0.370414, loss_sup: 0.128392, loss_mps: 0.084111, loss_cps: 0.157912
[13:12:13.461] iteration 12248: total_loss: 0.243677, loss_sup: 0.061374, loss_mps: 0.065522, loss_cps: 0.116782
[13:12:13.607] iteration 12249: total_loss: 0.333710, loss_sup: 0.079164, loss_mps: 0.087180, loss_cps: 0.167367
[13:12:13.754] iteration 12250: total_loss: 0.265290, loss_sup: 0.049085, loss_mps: 0.076660, loss_cps: 0.139545
[13:12:13.900] iteration 12251: total_loss: 0.272993, loss_sup: 0.060085, loss_mps: 0.077423, loss_cps: 0.135485
[13:12:14.045] iteration 12252: total_loss: 0.668810, loss_sup: 0.202122, loss_mps: 0.149154, loss_cps: 0.317534
[13:12:14.191] iteration 12253: total_loss: 0.351467, loss_sup: 0.103761, loss_mps: 0.088187, loss_cps: 0.159519
[13:12:14.336] iteration 12254: total_loss: 0.448124, loss_sup: 0.137691, loss_mps: 0.102391, loss_cps: 0.208041
[13:12:14.482] iteration 12255: total_loss: 0.257421, loss_sup: 0.040218, loss_mps: 0.075640, loss_cps: 0.141563
[13:12:14.627] iteration 12256: total_loss: 0.302935, loss_sup: 0.035431, loss_mps: 0.092311, loss_cps: 0.175193
[13:12:14.772] iteration 12257: total_loss: 0.243063, loss_sup: 0.040499, loss_mps: 0.072784, loss_cps: 0.129780
[13:12:14.918] iteration 12258: total_loss: 0.315071, loss_sup: 0.125548, loss_mps: 0.069638, loss_cps: 0.119885
[13:12:15.064] iteration 12259: total_loss: 0.205164, loss_sup: 0.021623, loss_mps: 0.070487, loss_cps: 0.113055
[13:12:15.210] iteration 12260: total_loss: 0.234900, loss_sup: 0.025488, loss_mps: 0.078126, loss_cps: 0.131286
[13:12:15.359] iteration 12261: total_loss: 0.416545, loss_sup: 0.115377, loss_mps: 0.105682, loss_cps: 0.195486
[13:12:15.505] iteration 12262: total_loss: 0.316394, loss_sup: 0.110058, loss_mps: 0.072421, loss_cps: 0.133915
[13:12:15.650] iteration 12263: total_loss: 0.393529, loss_sup: 0.064724, loss_mps: 0.105654, loss_cps: 0.223151
[13:12:15.795] iteration 12264: total_loss: 0.444855, loss_sup: 0.061711, loss_mps: 0.130235, loss_cps: 0.252909
[13:12:15.941] iteration 12265: total_loss: 0.361109, loss_sup: 0.063261, loss_mps: 0.101212, loss_cps: 0.196635
[13:12:16.088] iteration 12266: total_loss: 0.389853, loss_sup: 0.063728, loss_mps: 0.104444, loss_cps: 0.221681
[13:12:16.233] iteration 12267: total_loss: 0.414956, loss_sup: 0.056764, loss_mps: 0.113511, loss_cps: 0.244681
[13:12:16.379] iteration 12268: total_loss: 0.434004, loss_sup: 0.113369, loss_mps: 0.111033, loss_cps: 0.209602
[13:12:16.526] iteration 12269: total_loss: 0.249971, loss_sup: 0.035026, loss_mps: 0.074927, loss_cps: 0.140018
[13:12:16.672] iteration 12270: total_loss: 0.519980, loss_sup: 0.118174, loss_mps: 0.133038, loss_cps: 0.268768
[13:12:16.818] iteration 12271: total_loss: 0.570479, loss_sup: 0.111010, loss_mps: 0.149145, loss_cps: 0.310325
[13:12:16.963] iteration 12272: total_loss: 0.378380, loss_sup: 0.181701, loss_mps: 0.069688, loss_cps: 0.126991
[13:12:17.110] iteration 12273: total_loss: 0.214612, loss_sup: 0.028565, loss_mps: 0.068485, loss_cps: 0.117561
[13:12:17.257] iteration 12274: total_loss: 0.302341, loss_sup: 0.040910, loss_mps: 0.088891, loss_cps: 0.172540
[13:12:17.403] iteration 12275: total_loss: 0.171389, loss_sup: 0.015267, loss_mps: 0.058159, loss_cps: 0.097963
[13:12:17.549] iteration 12276: total_loss: 0.332283, loss_sup: 0.057978, loss_mps: 0.091555, loss_cps: 0.182750
[13:12:17.695] iteration 12277: total_loss: 0.544169, loss_sup: 0.133575, loss_mps: 0.131987, loss_cps: 0.278607
[13:12:17.845] iteration 12278: total_loss: 0.251145, loss_sup: 0.039762, loss_mps: 0.078459, loss_cps: 0.132924
[13:12:17.991] iteration 12279: total_loss: 0.323591, loss_sup: 0.048791, loss_mps: 0.092694, loss_cps: 0.182106
[13:12:18.143] iteration 12280: total_loss: 0.425097, loss_sup: 0.148581, loss_mps: 0.092309, loss_cps: 0.184206
[13:12:18.302] iteration 12281: total_loss: 0.221415, loss_sup: 0.044991, loss_mps: 0.063099, loss_cps: 0.113324
[13:12:18.449] iteration 12282: total_loss: 0.503414, loss_sup: 0.090633, loss_mps: 0.128413, loss_cps: 0.284368
[13:12:18.594] iteration 12283: total_loss: 0.215370, loss_sup: 0.013346, loss_mps: 0.071544, loss_cps: 0.130481
[13:12:18.740] iteration 12284: total_loss: 0.426521, loss_sup: 0.162191, loss_mps: 0.087201, loss_cps: 0.177129
[13:12:18.889] iteration 12285: total_loss: 0.201205, loss_sup: 0.027532, loss_mps: 0.059403, loss_cps: 0.114270
[13:12:19.044] iteration 12286: total_loss: 0.504821, loss_sup: 0.147294, loss_mps: 0.118003, loss_cps: 0.239524
[13:12:19.190] iteration 12287: total_loss: 0.273652, loss_sup: 0.054044, loss_mps: 0.078698, loss_cps: 0.140910
[13:12:19.337] iteration 12288: total_loss: 0.418213, loss_sup: 0.077379, loss_mps: 0.105487, loss_cps: 0.235348
[13:12:19.484] iteration 12289: total_loss: 0.194868, loss_sup: 0.021412, loss_mps: 0.063423, loss_cps: 0.110033
[13:12:19.635] iteration 12290: total_loss: 0.321533, loss_sup: 0.059293, loss_mps: 0.088434, loss_cps: 0.173805
[13:12:19.781] iteration 12291: total_loss: 0.438608, loss_sup: 0.148988, loss_mps: 0.097415, loss_cps: 0.192204
[13:12:19.926] iteration 12292: total_loss: 0.323308, loss_sup: 0.021568, loss_mps: 0.100693, loss_cps: 0.201047
[13:12:20.072] iteration 12293: total_loss: 0.231995, loss_sup: 0.030356, loss_mps: 0.067134, loss_cps: 0.134506
[13:12:20.220] iteration 12294: total_loss: 0.541738, loss_sup: 0.207196, loss_mps: 0.106410, loss_cps: 0.228132
[13:12:20.366] iteration 12295: total_loss: 0.314804, loss_sup: 0.082115, loss_mps: 0.075131, loss_cps: 0.157558
[13:12:20.513] iteration 12296: total_loss: 0.408253, loss_sup: 0.231024, loss_mps: 0.063409, loss_cps: 0.113820
[13:12:20.659] iteration 12297: total_loss: 0.181554, loss_sup: 0.032231, loss_mps: 0.054323, loss_cps: 0.094999
[13:12:20.805] iteration 12298: total_loss: 0.336640, loss_sup: 0.124636, loss_mps: 0.071780, loss_cps: 0.140225
[13:12:20.950] iteration 12299: total_loss: 0.418175, loss_sup: 0.066242, loss_mps: 0.119303, loss_cps: 0.232630
[13:12:21.099] iteration 12300: total_loss: 0.325118, loss_sup: 0.069440, loss_mps: 0.090070, loss_cps: 0.165608
[13:12:21.099] Evaluation Started ==>
[13:12:32.462] ==> valid iteration 12300: unet metrics: {'dc': 0.6444478167245334, 'jc': 0.5211727753466143, 'pre': 0.7077132628208985, 'hd': 6.021166599850348}, ynet metrics: {'dc': 0.6254646039244688, 'jc': 0.5034659627084948, 'pre': 0.7865870945715904, 'hd': 5.783277803994919}.
[13:12:32.464] Evaluation Finished!⏹️
[13:12:32.615] iteration 12301: total_loss: 0.408432, loss_sup: 0.127452, loss_mps: 0.099946, loss_cps: 0.181034
[13:12:32.762] iteration 12302: total_loss: 0.335482, loss_sup: 0.123220, loss_mps: 0.074740, loss_cps: 0.137522
[13:12:32.908] iteration 12303: total_loss: 0.244055, loss_sup: 0.044311, loss_mps: 0.072565, loss_cps: 0.127180
[13:12:33.053] iteration 12304: total_loss: 0.214133, loss_sup: 0.014867, loss_mps: 0.073227, loss_cps: 0.126039
[13:12:33.198] iteration 12305: total_loss: 0.394493, loss_sup: 0.088570, loss_mps: 0.109522, loss_cps: 0.196401
[13:12:33.344] iteration 12306: total_loss: 0.319708, loss_sup: 0.034870, loss_mps: 0.096680, loss_cps: 0.188157
[13:12:33.490] iteration 12307: total_loss: 0.430153, loss_sup: 0.168008, loss_mps: 0.090195, loss_cps: 0.171950
[13:12:33.641] iteration 12308: total_loss: 0.269492, loss_sup: 0.042775, loss_mps: 0.078316, loss_cps: 0.148401
[13:12:33.787] iteration 12309: total_loss: 0.439991, loss_sup: 0.083938, loss_mps: 0.116671, loss_cps: 0.239382
[13:12:33.933] iteration 12310: total_loss: 0.232197, loss_sup: 0.022264, loss_mps: 0.075675, loss_cps: 0.134257
[13:12:34.078] iteration 12311: total_loss: 0.229392, loss_sup: 0.026686, loss_mps: 0.071401, loss_cps: 0.131305
[13:12:34.224] iteration 12312: total_loss: 0.196814, loss_sup: 0.033628, loss_mps: 0.057585, loss_cps: 0.105602
[13:12:34.369] iteration 12313: total_loss: 0.708649, loss_sup: 0.071436, loss_mps: 0.196565, loss_cps: 0.440648
[13:12:34.515] iteration 12314: total_loss: 0.329598, loss_sup: 0.114548, loss_mps: 0.076565, loss_cps: 0.138485
[13:12:34.662] iteration 12315: total_loss: 0.344135, loss_sup: 0.120231, loss_mps: 0.077169, loss_cps: 0.146736
[13:12:34.808] iteration 12316: total_loss: 0.384121, loss_sup: 0.024628, loss_mps: 0.118622, loss_cps: 0.240870
[13:12:34.956] iteration 12317: total_loss: 0.302923, loss_sup: 0.080151, loss_mps: 0.078751, loss_cps: 0.144021
[13:12:35.102] iteration 12318: total_loss: 0.320054, loss_sup: 0.099003, loss_mps: 0.078543, loss_cps: 0.142508
[13:12:35.248] iteration 12319: total_loss: 0.670556, loss_sup: 0.305425, loss_mps: 0.112523, loss_cps: 0.252608
[13:12:35.394] iteration 12320: total_loss: 0.302577, loss_sup: 0.076113, loss_mps: 0.076785, loss_cps: 0.149679
[13:12:35.539] iteration 12321: total_loss: 0.826429, loss_sup: 0.459164, loss_mps: 0.117934, loss_cps: 0.249330
[13:12:35.686] iteration 12322: total_loss: 0.384706, loss_sup: 0.182571, loss_mps: 0.071540, loss_cps: 0.130595
[13:12:35.831] iteration 12323: total_loss: 0.199685, loss_sup: 0.024997, loss_mps: 0.063650, loss_cps: 0.111038
[13:12:35.976] iteration 12324: total_loss: 0.572301, loss_sup: 0.175673, loss_mps: 0.123757, loss_cps: 0.272871
[13:12:36.123] iteration 12325: total_loss: 0.300886, loss_sup: 0.040307, loss_mps: 0.091325, loss_cps: 0.169254
[13:12:36.269] iteration 12326: total_loss: 0.334002, loss_sup: 0.093426, loss_mps: 0.085979, loss_cps: 0.154597
[13:12:36.414] iteration 12327: total_loss: 0.246294, loss_sup: 0.064241, loss_mps: 0.066231, loss_cps: 0.115822
[13:12:36.564] iteration 12328: total_loss: 0.737087, loss_sup: 0.144750, loss_mps: 0.191193, loss_cps: 0.401144
[13:12:36.711] iteration 12329: total_loss: 0.533807, loss_sup: 0.084469, loss_mps: 0.147760, loss_cps: 0.301578
[13:12:36.857] iteration 12330: total_loss: 0.297251, loss_sup: 0.086343, loss_mps: 0.075598, loss_cps: 0.135309
[13:12:37.004] iteration 12331: total_loss: 0.235858, loss_sup: 0.060806, loss_mps: 0.067416, loss_cps: 0.107635
[13:12:37.150] iteration 12332: total_loss: 0.541327, loss_sup: 0.176215, loss_mps: 0.117005, loss_cps: 0.248107
[13:12:37.297] iteration 12333: total_loss: 0.448752, loss_sup: 0.026676, loss_mps: 0.130027, loss_cps: 0.292049
[13:12:37.443] iteration 12334: total_loss: 0.407105, loss_sup: 0.131920, loss_mps: 0.095115, loss_cps: 0.180070
[13:12:37.589] iteration 12335: total_loss: 0.149342, loss_sup: 0.013744, loss_mps: 0.052368, loss_cps: 0.083230
[13:12:37.736] iteration 12336: total_loss: 0.336322, loss_sup: 0.057109, loss_mps: 0.095872, loss_cps: 0.183341
[13:12:37.882] iteration 12337: total_loss: 0.324900, loss_sup: 0.071819, loss_mps: 0.087914, loss_cps: 0.165168
[13:12:38.030] iteration 12338: total_loss: 0.166346, loss_sup: 0.011618, loss_mps: 0.058348, loss_cps: 0.096380
[13:12:38.175] iteration 12339: total_loss: 0.210566, loss_sup: 0.050994, loss_mps: 0.061090, loss_cps: 0.098482
[13:12:38.321] iteration 12340: total_loss: 0.632914, loss_sup: 0.302070, loss_mps: 0.107051, loss_cps: 0.223794
[13:12:38.466] iteration 12341: total_loss: 0.733021, loss_sup: 0.373735, loss_mps: 0.122089, loss_cps: 0.237197
[13:12:38.612] iteration 12342: total_loss: 0.319155, loss_sup: 0.028011, loss_mps: 0.095938, loss_cps: 0.195206
[13:12:38.759] iteration 12343: total_loss: 0.622665, loss_sup: 0.263862, loss_mps: 0.119341, loss_cps: 0.239462
[13:12:38.905] iteration 12344: total_loss: 0.263239, loss_sup: 0.114354, loss_mps: 0.057341, loss_cps: 0.091545
[13:12:39.052] iteration 12345: total_loss: 0.366639, loss_sup: 0.047462, loss_mps: 0.103375, loss_cps: 0.215802
[13:12:39.198] iteration 12346: total_loss: 0.449902, loss_sup: 0.125165, loss_mps: 0.103521, loss_cps: 0.221216
[13:12:39.343] iteration 12347: total_loss: 0.321303, loss_sup: 0.022827, loss_mps: 0.103679, loss_cps: 0.194797
[13:12:39.489] iteration 12348: total_loss: 0.200030, loss_sup: 0.039099, loss_mps: 0.062130, loss_cps: 0.098801
[13:12:39.635] iteration 12349: total_loss: 0.363114, loss_sup: 0.032174, loss_mps: 0.117873, loss_cps: 0.213067
[13:12:39.782] iteration 12350: total_loss: 0.222774, loss_sup: 0.022891, loss_mps: 0.072846, loss_cps: 0.127037
[13:12:39.929] iteration 12351: total_loss: 0.536214, loss_sup: 0.183236, loss_mps: 0.117283, loss_cps: 0.235695
[13:12:40.075] iteration 12352: total_loss: 0.472750, loss_sup: 0.046448, loss_mps: 0.131799, loss_cps: 0.294503
[13:12:40.223] iteration 12353: total_loss: 0.354561, loss_sup: 0.031190, loss_mps: 0.111365, loss_cps: 0.212006
[13:12:40.369] iteration 12354: total_loss: 0.254158, loss_sup: 0.026922, loss_mps: 0.080580, loss_cps: 0.146656
[13:12:40.516] iteration 12355: total_loss: 0.236067, loss_sup: 0.010627, loss_mps: 0.081719, loss_cps: 0.143720
[13:12:40.666] iteration 12356: total_loss: 0.373905, loss_sup: 0.122154, loss_mps: 0.084248, loss_cps: 0.167503
[13:12:40.813] iteration 12357: total_loss: 0.279310, loss_sup: 0.044168, loss_mps: 0.084457, loss_cps: 0.150685
[13:12:40.959] iteration 12358: total_loss: 0.212873, loss_sup: 0.014813, loss_mps: 0.074405, loss_cps: 0.123655
[13:12:41.104] iteration 12359: total_loss: 0.507563, loss_sup: 0.215289, loss_mps: 0.100232, loss_cps: 0.192042
[13:12:41.254] iteration 12360: total_loss: 0.362754, loss_sup: 0.117627, loss_mps: 0.084370, loss_cps: 0.160757
[13:12:41.401] iteration 12361: total_loss: 0.265632, loss_sup: 0.030850, loss_mps: 0.082102, loss_cps: 0.152680
[13:12:41.550] iteration 12362: total_loss: 0.333760, loss_sup: 0.115046, loss_mps: 0.078775, loss_cps: 0.139939
[13:12:41.697] iteration 12363: total_loss: 0.334618, loss_sup: 0.022723, loss_mps: 0.100186, loss_cps: 0.211709
[13:12:41.848] iteration 12364: total_loss: 0.251702, loss_sup: 0.076858, loss_mps: 0.060427, loss_cps: 0.114417
[13:12:41.994] iteration 12365: total_loss: 0.169234, loss_sup: 0.016378, loss_mps: 0.058176, loss_cps: 0.094680
[13:12:42.141] iteration 12366: total_loss: 0.354322, loss_sup: 0.095776, loss_mps: 0.088684, loss_cps: 0.169862
[13:12:42.289] iteration 12367: total_loss: 0.422359, loss_sup: 0.135098, loss_mps: 0.096387, loss_cps: 0.190875
[13:12:42.435] iteration 12368: total_loss: 0.370348, loss_sup: 0.034213, loss_mps: 0.110403, loss_cps: 0.225732
[13:12:42.581] iteration 12369: total_loss: 0.246942, loss_sup: 0.060560, loss_mps: 0.066867, loss_cps: 0.119514
[13:12:42.727] iteration 12370: total_loss: 0.375503, loss_sup: 0.089503, loss_mps: 0.095103, loss_cps: 0.190897
[13:12:42.874] iteration 12371: total_loss: 0.597649, loss_sup: 0.327304, loss_mps: 0.089744, loss_cps: 0.180602
[13:12:43.021] iteration 12372: total_loss: 0.276183, loss_sup: 0.033717, loss_mps: 0.082617, loss_cps: 0.159849
[13:12:43.167] iteration 12373: total_loss: 0.345388, loss_sup: 0.033446, loss_mps: 0.102662, loss_cps: 0.209280
[13:12:43.313] iteration 12374: total_loss: 0.232073, loss_sup: 0.016224, loss_mps: 0.075538, loss_cps: 0.140311
[13:12:43.461] iteration 12375: total_loss: 0.380506, loss_sup: 0.062240, loss_mps: 0.103998, loss_cps: 0.214267
[13:12:43.607] iteration 12376: total_loss: 0.363452, loss_sup: 0.085950, loss_mps: 0.098559, loss_cps: 0.178943
[13:12:43.753] iteration 12377: total_loss: 0.325943, loss_sup: 0.100379, loss_mps: 0.077297, loss_cps: 0.148267
[13:12:43.899] iteration 12378: total_loss: 0.471079, loss_sup: 0.055515, loss_mps: 0.128083, loss_cps: 0.287481
[13:12:44.045] iteration 12379: total_loss: 0.475754, loss_sup: 0.136135, loss_mps: 0.113373, loss_cps: 0.226245
[13:12:44.190] iteration 12380: total_loss: 0.238091, loss_sup: 0.024849, loss_mps: 0.075054, loss_cps: 0.138188
[13:12:44.337] iteration 12381: total_loss: 0.325758, loss_sup: 0.042566, loss_mps: 0.093895, loss_cps: 0.189297
[13:12:44.483] iteration 12382: total_loss: 0.200293, loss_sup: 0.012628, loss_mps: 0.067773, loss_cps: 0.119892
[13:12:44.629] iteration 12383: total_loss: 0.518957, loss_sup: 0.066710, loss_mps: 0.134662, loss_cps: 0.317585
[13:12:44.775] iteration 12384: total_loss: 0.193815, loss_sup: 0.040635, loss_mps: 0.055288, loss_cps: 0.097892
[13:12:44.922] iteration 12385: total_loss: 0.287669, loss_sup: 0.027113, loss_mps: 0.085238, loss_cps: 0.175318
[13:12:45.068] iteration 12386: total_loss: 0.268956, loss_sup: 0.050316, loss_mps: 0.075742, loss_cps: 0.142898
[13:12:45.214] iteration 12387: total_loss: 0.386730, loss_sup: 0.082671, loss_mps: 0.103260, loss_cps: 0.200800
[13:12:45.359] iteration 12388: total_loss: 0.238947, loss_sup: 0.035522, loss_mps: 0.070889, loss_cps: 0.132535
[13:12:45.505] iteration 12389: total_loss: 0.264839, loss_sup: 0.081613, loss_mps: 0.069386, loss_cps: 0.113839
[13:12:45.651] iteration 12390: total_loss: 0.488600, loss_sup: 0.142480, loss_mps: 0.113113, loss_cps: 0.233007
[13:12:45.797] iteration 12391: total_loss: 0.444360, loss_sup: 0.085242, loss_mps: 0.114303, loss_cps: 0.244815
[13:12:45.943] iteration 12392: total_loss: 0.286706, loss_sup: 0.036790, loss_mps: 0.085995, loss_cps: 0.163922
[13:12:46.089] iteration 12393: total_loss: 0.334695, loss_sup: 0.053724, loss_mps: 0.093407, loss_cps: 0.187564
[13:12:46.235] iteration 12394: total_loss: 0.311346, loss_sup: 0.006031, loss_mps: 0.098535, loss_cps: 0.206780
[13:12:46.380] iteration 12395: total_loss: 0.399711, loss_sup: 0.152806, loss_mps: 0.082216, loss_cps: 0.164690
[13:12:46.529] iteration 12396: total_loss: 0.184386, loss_sup: 0.018933, loss_mps: 0.058592, loss_cps: 0.106862
[13:12:46.675] iteration 12397: total_loss: 0.276035, loss_sup: 0.067843, loss_mps: 0.074433, loss_cps: 0.133759
[13:12:46.821] iteration 12398: total_loss: 0.318143, loss_sup: 0.090294, loss_mps: 0.080063, loss_cps: 0.147785
[13:12:46.967] iteration 12399: total_loss: 0.245193, loss_sup: 0.039125, loss_mps: 0.075782, loss_cps: 0.130286
[13:12:47.114] iteration 12400: total_loss: 0.468980, loss_sup: 0.239955, loss_mps: 0.082568, loss_cps: 0.146458
[13:12:47.114] Evaluation Started ==>
[13:12:58.432] ==> valid iteration 12400: unet metrics: {'dc': 0.6025069087178188, 'jc': 0.48617289288760784, 'pre': 0.785680562902011, 'hd': 5.482323931403948}, ynet metrics: {'dc': 0.5530906866077678, 'jc': 0.4409497047211234, 'pre': 0.7837221006433389, 'hd': 5.59748393118735}.
[13:12:58.433] Evaluation Finished!⏹️
[13:12:58.582] iteration 12401: total_loss: 0.263539, loss_sup: 0.071206, loss_mps: 0.067357, loss_cps: 0.124977
[13:12:58.729] iteration 12402: total_loss: 0.286122, loss_sup: 0.052846, loss_mps: 0.079749, loss_cps: 0.153527
[13:12:58.876] iteration 12403: total_loss: 0.670373, loss_sup: 0.291648, loss_mps: 0.127064, loss_cps: 0.251661
[13:12:59.022] iteration 12404: total_loss: 0.352890, loss_sup: 0.077951, loss_mps: 0.091541, loss_cps: 0.183398
[13:12:59.179] iteration 12405: total_loss: 0.345559, loss_sup: 0.100466, loss_mps: 0.080450, loss_cps: 0.164643
[13:12:59.325] iteration 12406: total_loss: 0.541351, loss_sup: 0.139071, loss_mps: 0.125737, loss_cps: 0.276544
[13:12:59.470] iteration 12407: total_loss: 0.186022, loss_sup: 0.027008, loss_mps: 0.057844, loss_cps: 0.101171
[13:12:59.616] iteration 12408: total_loss: 0.512243, loss_sup: 0.135502, loss_mps: 0.121743, loss_cps: 0.254998
[13:12:59.761] iteration 12409: total_loss: 0.336483, loss_sup: 0.149127, loss_mps: 0.068260, loss_cps: 0.119096
[13:12:59.907] iteration 12410: total_loss: 0.516423, loss_sup: 0.087652, loss_mps: 0.134995, loss_cps: 0.293776
[13:13:00.056] iteration 12411: total_loss: 0.280922, loss_sup: 0.031671, loss_mps: 0.089755, loss_cps: 0.159496
[13:13:00.202] iteration 12412: total_loss: 0.222995, loss_sup: 0.018900, loss_mps: 0.073436, loss_cps: 0.130658
[13:13:00.348] iteration 12413: total_loss: 0.208484, loss_sup: 0.079437, loss_mps: 0.051131, loss_cps: 0.077916
[13:13:00.494] iteration 12414: total_loss: 0.249398, loss_sup: 0.044972, loss_mps: 0.073155, loss_cps: 0.131271
[13:13:00.642] iteration 12415: total_loss: 0.224148, loss_sup: 0.017744, loss_mps: 0.074989, loss_cps: 0.131416
[13:13:00.789] iteration 12416: total_loss: 0.193242, loss_sup: 0.010332, loss_mps: 0.064934, loss_cps: 0.117975
[13:13:00.935] iteration 12417: total_loss: 0.221793, loss_sup: 0.048491, loss_mps: 0.065132, loss_cps: 0.108170
[13:13:01.081] iteration 12418: total_loss: 0.429543, loss_sup: 0.057823, loss_mps: 0.123190, loss_cps: 0.248530
[13:13:01.230] iteration 12419: total_loss: 0.259914, loss_sup: 0.020469, loss_mps: 0.082397, loss_cps: 0.157048
[13:13:01.377] iteration 12420: total_loss: 0.392189, loss_sup: 0.064789, loss_mps: 0.105295, loss_cps: 0.222105
[13:13:01.522] iteration 12421: total_loss: 0.229820, loss_sup: 0.017886, loss_mps: 0.076496, loss_cps: 0.135438
[13:13:01.668] iteration 12422: total_loss: 0.315061, loss_sup: 0.011802, loss_mps: 0.101691, loss_cps: 0.201568
[13:13:01.814] iteration 12423: total_loss: 0.483053, loss_sup: 0.037602, loss_mps: 0.144221, loss_cps: 0.301230
[13:13:01.960] iteration 12424: total_loss: 0.480384, loss_sup: 0.025541, loss_mps: 0.149691, loss_cps: 0.305152
[13:13:02.107] iteration 12425: total_loss: 0.258045, loss_sup: 0.038244, loss_mps: 0.078356, loss_cps: 0.141446
[13:13:02.253] iteration 12426: total_loss: 0.416453, loss_sup: 0.052154, loss_mps: 0.118198, loss_cps: 0.246101
[13:13:02.398] iteration 12427: total_loss: 0.713694, loss_sup: 0.371721, loss_mps: 0.112596, loss_cps: 0.229376
[13:13:02.544] iteration 12428: total_loss: 0.123193, loss_sup: 0.021639, loss_mps: 0.041736, loss_cps: 0.059818
[13:13:02.690] iteration 12429: total_loss: 0.301779, loss_sup: 0.080384, loss_mps: 0.078547, loss_cps: 0.142849
[13:13:02.835] iteration 12430: total_loss: 0.269812, loss_sup: 0.088720, loss_mps: 0.063194, loss_cps: 0.117898
[13:13:02.982] iteration 12431: total_loss: 0.268653, loss_sup: 0.044437, loss_mps: 0.077391, loss_cps: 0.146826
[13:13:03.128] iteration 12432: total_loss: 0.258783, loss_sup: 0.051973, loss_mps: 0.071616, loss_cps: 0.135194
[13:13:03.274] iteration 12433: total_loss: 0.240314, loss_sup: 0.054957, loss_mps: 0.065406, loss_cps: 0.119950
[13:13:03.420] iteration 12434: total_loss: 0.204196, loss_sup: 0.011746, loss_mps: 0.064902, loss_cps: 0.127549
[13:13:03.566] iteration 12435: total_loss: 0.538023, loss_sup: 0.147274, loss_mps: 0.119739, loss_cps: 0.271010
[13:13:03.713] iteration 12436: total_loss: 0.193493, loss_sup: 0.007880, loss_mps: 0.067957, loss_cps: 0.117656
[13:13:03.859] iteration 12437: total_loss: 0.313803, loss_sup: 0.051804, loss_mps: 0.089831, loss_cps: 0.172168
[13:13:04.005] iteration 12438: total_loss: 0.200951, loss_sup: 0.030257, loss_mps: 0.062476, loss_cps: 0.108219
[13:13:04.152] iteration 12439: total_loss: 0.227531, loss_sup: 0.045844, loss_mps: 0.064227, loss_cps: 0.117460
[13:13:04.299] iteration 12440: total_loss: 0.231872, loss_sup: 0.069723, loss_mps: 0.059550, loss_cps: 0.102599
[13:13:04.444] iteration 12441: total_loss: 0.191871, loss_sup: 0.015133, loss_mps: 0.064483, loss_cps: 0.112255
[13:13:04.590] iteration 12442: total_loss: 0.307611, loss_sup: 0.074819, loss_mps: 0.079189, loss_cps: 0.153603
[13:13:04.736] iteration 12443: total_loss: 0.378067, loss_sup: 0.186572, loss_mps: 0.068497, loss_cps: 0.122998
[13:13:04.883] iteration 12444: total_loss: 0.262245, loss_sup: 0.061477, loss_mps: 0.075171, loss_cps: 0.125598
[13:13:05.029] iteration 12445: total_loss: 0.461780, loss_sup: 0.121538, loss_mps: 0.111603, loss_cps: 0.228639
[13:13:05.174] iteration 12446: total_loss: 0.542220, loss_sup: 0.245673, loss_mps: 0.098820, loss_cps: 0.197727
[13:13:05.320] iteration 12447: total_loss: 0.290709, loss_sup: 0.030650, loss_mps: 0.093147, loss_cps: 0.166912
[13:13:05.467] iteration 12448: total_loss: 0.347750, loss_sup: 0.053611, loss_mps: 0.101175, loss_cps: 0.192964
[13:13:05.612] iteration 12449: total_loss: 0.261923, loss_sup: 0.079533, loss_mps: 0.065450, loss_cps: 0.116941
[13:13:05.760] iteration 12450: total_loss: 0.407383, loss_sup: 0.155055, loss_mps: 0.084558, loss_cps: 0.167771
[13:13:05.907] iteration 12451: total_loss: 0.333558, loss_sup: 0.097656, loss_mps: 0.086085, loss_cps: 0.149816
[13:13:06.053] iteration 12452: total_loss: 0.267188, loss_sup: 0.025280, loss_mps: 0.080285, loss_cps: 0.161622
[13:13:06.199] iteration 12453: total_loss: 0.214530, loss_sup: 0.014388, loss_mps: 0.068679, loss_cps: 0.131463
[13:13:06.345] iteration 12454: total_loss: 0.412854, loss_sup: 0.087474, loss_mps: 0.109704, loss_cps: 0.215676
[13:13:06.491] iteration 12455: total_loss: 0.167757, loss_sup: 0.025920, loss_mps: 0.050873, loss_cps: 0.090964
[13:13:06.636] iteration 12456: total_loss: 0.547464, loss_sup: 0.159356, loss_mps: 0.125386, loss_cps: 0.262722
[13:13:06.782] iteration 12457: total_loss: 0.313821, loss_sup: 0.090721, loss_mps: 0.075428, loss_cps: 0.147672
[13:13:06.927] iteration 12458: total_loss: 0.335734, loss_sup: 0.013326, loss_mps: 0.105724, loss_cps: 0.216684
[13:13:07.075] iteration 12459: total_loss: 0.261687, loss_sup: 0.022988, loss_mps: 0.084174, loss_cps: 0.154525
[13:13:07.221] iteration 12460: total_loss: 0.408999, loss_sup: 0.031691, loss_mps: 0.120747, loss_cps: 0.256560
[13:13:07.366] iteration 12461: total_loss: 0.533452, loss_sup: 0.106796, loss_mps: 0.134931, loss_cps: 0.291726
[13:13:07.513] iteration 12462: total_loss: 0.246083, loss_sup: 0.012625, loss_mps: 0.078303, loss_cps: 0.155155
[13:13:07.658] iteration 12463: total_loss: 0.596310, loss_sup: 0.249114, loss_mps: 0.110939, loss_cps: 0.236257
[13:13:07.804] iteration 12464: total_loss: 0.502154, loss_sup: 0.139752, loss_mps: 0.118145, loss_cps: 0.244257
[13:13:07.950] iteration 12465: total_loss: 0.324771, loss_sup: 0.051177, loss_mps: 0.091342, loss_cps: 0.182252
[13:13:08.095] iteration 12466: total_loss: 0.644271, loss_sup: 0.062529, loss_mps: 0.180541, loss_cps: 0.401201
[13:13:08.241] iteration 12467: total_loss: 0.316870, loss_sup: 0.072822, loss_mps: 0.081858, loss_cps: 0.162190
[13:13:08.388] iteration 12468: total_loss: 0.170777, loss_sup: 0.037003, loss_mps: 0.049398, loss_cps: 0.084375
[13:13:08.533] iteration 12469: total_loss: 0.348383, loss_sup: 0.061516, loss_mps: 0.092634, loss_cps: 0.194233
[13:13:08.681] iteration 12470: total_loss: 0.228124, loss_sup: 0.043690, loss_mps: 0.070265, loss_cps: 0.114169
[13:13:08.827] iteration 12471: total_loss: 0.425817, loss_sup: 0.047425, loss_mps: 0.122663, loss_cps: 0.255729
[13:13:08.973] iteration 12472: total_loss: 0.349981, loss_sup: 0.048840, loss_mps: 0.098944, loss_cps: 0.202198
[13:13:09.118] iteration 12473: total_loss: 0.305865, loss_sup: 0.170565, loss_mps: 0.050726, loss_cps: 0.084574
[13:13:09.265] iteration 12474: total_loss: 0.593768, loss_sup: 0.313764, loss_mps: 0.093511, loss_cps: 0.186493
[13:13:09.412] iteration 12475: total_loss: 0.216562, loss_sup: 0.025058, loss_mps: 0.067441, loss_cps: 0.124063
[13:13:09.558] iteration 12476: total_loss: 0.253734, loss_sup: 0.049099, loss_mps: 0.072407, loss_cps: 0.132227
[13:13:09.704] iteration 12477: total_loss: 0.230238, loss_sup: 0.010813, loss_mps: 0.076303, loss_cps: 0.143122
[13:13:09.851] iteration 12478: total_loss: 0.279122, loss_sup: 0.049953, loss_mps: 0.082884, loss_cps: 0.146285
[13:13:09.996] iteration 12479: total_loss: 0.202164, loss_sup: 0.025841, loss_mps: 0.064234, loss_cps: 0.112090
[13:13:10.142] iteration 12480: total_loss: 0.284278, loss_sup: 0.071994, loss_mps: 0.075811, loss_cps: 0.136473
[13:13:10.288] iteration 12481: total_loss: 0.290523, loss_sup: 0.138528, loss_mps: 0.057838, loss_cps: 0.094157
[13:13:10.434] iteration 12482: total_loss: 0.371649, loss_sup: 0.013737, loss_mps: 0.113810, loss_cps: 0.244102
[13:13:10.580] iteration 12483: total_loss: 0.386513, loss_sup: 0.118776, loss_mps: 0.096461, loss_cps: 0.171276
[13:13:10.726] iteration 12484: total_loss: 0.542401, loss_sup: 0.347892, loss_mps: 0.074835, loss_cps: 0.119674
[13:13:10.873] iteration 12485: total_loss: 0.261210, loss_sup: 0.067084, loss_mps: 0.071837, loss_cps: 0.122289
[13:13:11.018] iteration 12486: total_loss: 0.301524, loss_sup: 0.073604, loss_mps: 0.078442, loss_cps: 0.149478
[13:13:11.164] iteration 12487: total_loss: 0.729155, loss_sup: 0.084723, loss_mps: 0.195808, loss_cps: 0.448623
[13:13:11.311] iteration 12488: total_loss: 0.480896, loss_sup: 0.064313, loss_mps: 0.142454, loss_cps: 0.274129
[13:13:11.457] iteration 12489: total_loss: 0.380156, loss_sup: 0.110921, loss_mps: 0.089134, loss_cps: 0.180100
[13:13:11.603] iteration 12490: total_loss: 0.243443, loss_sup: 0.027265, loss_mps: 0.077315, loss_cps: 0.138862
[13:13:11.749] iteration 12491: total_loss: 0.168735, loss_sup: 0.018785, loss_mps: 0.056543, loss_cps: 0.093407
[13:13:11.895] iteration 12492: total_loss: 0.229045, loss_sup: 0.025039, loss_mps: 0.075245, loss_cps: 0.128762
[13:13:12.041] iteration 12493: total_loss: 0.228538, loss_sup: 0.019408, loss_mps: 0.075755, loss_cps: 0.133375
[13:13:12.188] iteration 12494: total_loss: 0.352680, loss_sup: 0.117093, loss_mps: 0.081710, loss_cps: 0.153877
[13:13:12.334] iteration 12495: total_loss: 0.437479, loss_sup: 0.046499, loss_mps: 0.124731, loss_cps: 0.266249
[13:13:12.481] iteration 12496: total_loss: 0.284519, loss_sup: 0.063769, loss_mps: 0.074755, loss_cps: 0.145995
[13:13:12.627] iteration 12497: total_loss: 0.311850, loss_sup: 0.121445, loss_mps: 0.066848, loss_cps: 0.123557
[13:13:12.773] iteration 12498: total_loss: 0.570417, loss_sup: 0.361644, loss_mps: 0.070861, loss_cps: 0.137912
[13:13:12.919] iteration 12499: total_loss: 0.391375, loss_sup: 0.143085, loss_mps: 0.084154, loss_cps: 0.164136
[13:13:13.066] iteration 12500: total_loss: 0.499125, loss_sup: 0.067435, loss_mps: 0.137267, loss_cps: 0.294423
[13:13:13.066] Evaluation Started ==>
[13:13:24.489] ==> valid iteration 12500: unet metrics: {'dc': 0.6474301123878836, 'jc': 0.5319027732680081, 'pre': 0.7752217767226111, 'hd': 5.481626354672321}, ynet metrics: {'dc': 0.6333853082915041, 'jc': 0.5106185998826042, 'pre': 0.76746394559904, 'hd': 5.800088267818752}.
[13:13:24.492] Evaluation Finished!⏹️
[13:13:24.642] iteration 12501: total_loss: 0.232821, loss_sup: 0.008682, loss_mps: 0.079467, loss_cps: 0.144672
[13:13:24.789] iteration 12502: total_loss: 0.362570, loss_sup: 0.059012, loss_mps: 0.099004, loss_cps: 0.204554
[13:13:24.935] iteration 12503: total_loss: 0.244573, loss_sup: 0.033675, loss_mps: 0.074564, loss_cps: 0.136334
[13:13:25.081] iteration 12504: total_loss: 0.445643, loss_sup: 0.193703, loss_mps: 0.088627, loss_cps: 0.163313
[13:13:25.227] iteration 12505: total_loss: 0.275199, loss_sup: 0.100286, loss_mps: 0.063791, loss_cps: 0.111121
[13:13:25.373] iteration 12506: total_loss: 0.330126, loss_sup: 0.085188, loss_mps: 0.083153, loss_cps: 0.161785
[13:13:25.522] iteration 12507: total_loss: 0.462778, loss_sup: 0.115951, loss_mps: 0.112174, loss_cps: 0.234652
[13:13:25.668] iteration 12508: total_loss: 0.449164, loss_sup: 0.181993, loss_mps: 0.093944, loss_cps: 0.173227
[13:13:25.813] iteration 12509: total_loss: 0.190125, loss_sup: 0.030019, loss_mps: 0.061161, loss_cps: 0.098945
[13:13:25.958] iteration 12510: total_loss: 0.480104, loss_sup: 0.192966, loss_mps: 0.093422, loss_cps: 0.193716
[13:13:26.104] iteration 12511: total_loss: 0.820049, loss_sup: 0.379881, loss_mps: 0.135764, loss_cps: 0.304404
[13:13:26.250] iteration 12512: total_loss: 0.353931, loss_sup: 0.050276, loss_mps: 0.104284, loss_cps: 0.199371
[13:13:26.396] iteration 12513: total_loss: 0.625253, loss_sup: 0.195277, loss_mps: 0.139698, loss_cps: 0.290279
[13:13:26.542] iteration 12514: total_loss: 0.460235, loss_sup: 0.169059, loss_mps: 0.102721, loss_cps: 0.188454
[13:13:26.687] iteration 12515: total_loss: 0.256378, loss_sup: 0.013734, loss_mps: 0.086011, loss_cps: 0.156633
[13:13:26.833] iteration 12516: total_loss: 0.444755, loss_sup: 0.147955, loss_mps: 0.107152, loss_cps: 0.189648
[13:13:26.980] iteration 12517: total_loss: 0.288942, loss_sup: 0.039679, loss_mps: 0.089496, loss_cps: 0.159766
[13:13:27.126] iteration 12518: total_loss: 0.248544, loss_sup: 0.033991, loss_mps: 0.076432, loss_cps: 0.138121
[13:13:27.271] iteration 12519: total_loss: 0.292744, loss_sup: 0.094831, loss_mps: 0.074056, loss_cps: 0.123857
[13:13:27.419] iteration 12520: total_loss: 0.439216, loss_sup: 0.087679, loss_mps: 0.115117, loss_cps: 0.236420
[13:13:27.564] iteration 12521: total_loss: 0.439885, loss_sup: 0.090928, loss_mps: 0.114460, loss_cps: 0.234497
[13:13:27.709] iteration 12522: total_loss: 0.502082, loss_sup: 0.058271, loss_mps: 0.139135, loss_cps: 0.304677
[13:13:27.854] iteration 12523: total_loss: 0.553787, loss_sup: 0.182607, loss_mps: 0.125416, loss_cps: 0.245764
[13:13:28.001] iteration 12524: total_loss: 0.305034, loss_sup: 0.032234, loss_mps: 0.099542, loss_cps: 0.173257
[13:13:28.146] iteration 12525: total_loss: 0.310128, loss_sup: 0.153972, loss_mps: 0.056423, loss_cps: 0.099734
[13:13:28.291] iteration 12526: total_loss: 0.221145, loss_sup: 0.036669, loss_mps: 0.069656, loss_cps: 0.114821
[13:13:28.437] iteration 12527: total_loss: 0.286087, loss_sup: 0.054766, loss_mps: 0.082068, loss_cps: 0.149253
[13:13:28.582] iteration 12528: total_loss: 0.240000, loss_sup: 0.019907, loss_mps: 0.078187, loss_cps: 0.141906
[13:13:28.727] iteration 12529: total_loss: 0.157996, loss_sup: 0.021103, loss_mps: 0.053494, loss_cps: 0.083399
[13:13:28.872] iteration 12530: total_loss: 0.328583, loss_sup: 0.034068, loss_mps: 0.100483, loss_cps: 0.194032
[13:13:29.018] iteration 12531: total_loss: 0.270606, loss_sup: 0.045672, loss_mps: 0.078597, loss_cps: 0.146338
[13:13:29.164] iteration 12532: total_loss: 0.492542, loss_sup: 0.114772, loss_mps: 0.125832, loss_cps: 0.251938
[13:13:29.309] iteration 12533: total_loss: 0.464622, loss_sup: 0.115119, loss_mps: 0.120614, loss_cps: 0.228890
[13:13:29.454] iteration 12534: total_loss: 0.406265, loss_sup: 0.222210, loss_mps: 0.068923, loss_cps: 0.115132
[13:13:29.600] iteration 12535: total_loss: 0.545117, loss_sup: 0.075267, loss_mps: 0.156524, loss_cps: 0.313326
[13:13:29.746] iteration 12536: total_loss: 0.436925, loss_sup: 0.180434, loss_mps: 0.093397, loss_cps: 0.163094
[13:13:29.891] iteration 12537: total_loss: 0.309162, loss_sup: 0.067165, loss_mps: 0.082552, loss_cps: 0.159445
[13:13:30.037] iteration 12538: total_loss: 0.258441, loss_sup: 0.050689, loss_mps: 0.077789, loss_cps: 0.129963
[13:13:30.182] iteration 12539: total_loss: 0.187177, loss_sup: 0.021772, loss_mps: 0.060011, loss_cps: 0.105394
[13:13:30.243] iteration 12540: total_loss: 0.723781, loss_sup: 0.591646, loss_mps: 0.049629, loss_cps: 0.082506
[13:13:31.627] iteration 12541: total_loss: 0.205450, loss_sup: 0.030390, loss_mps: 0.064042, loss_cps: 0.111018
[13:13:31.775] iteration 12542: total_loss: 0.400313, loss_sup: 0.136491, loss_mps: 0.092352, loss_cps: 0.171471
[13:13:31.922] iteration 12543: total_loss: 0.523017, loss_sup: 0.211842, loss_mps: 0.099724, loss_cps: 0.211451
[13:13:32.074] iteration 12544: total_loss: 0.328690, loss_sup: 0.063301, loss_mps: 0.092926, loss_cps: 0.172463
[13:13:32.220] iteration 12545: total_loss: 0.296312, loss_sup: 0.015935, loss_mps: 0.094913, loss_cps: 0.185465
[13:13:32.366] iteration 12546: total_loss: 0.408864, loss_sup: 0.109017, loss_mps: 0.106475, loss_cps: 0.193372
[13:13:32.513] iteration 12547: total_loss: 0.343563, loss_sup: 0.048154, loss_mps: 0.098323, loss_cps: 0.197087
[13:13:32.660] iteration 12548: total_loss: 0.613464, loss_sup: 0.297315, loss_mps: 0.106498, loss_cps: 0.209651
[13:13:32.806] iteration 12549: total_loss: 0.478593, loss_sup: 0.173690, loss_mps: 0.105441, loss_cps: 0.199463
[13:13:32.953] iteration 12550: total_loss: 0.364503, loss_sup: 0.028458, loss_mps: 0.111756, loss_cps: 0.224289
[13:13:33.099] iteration 12551: total_loss: 0.302868, loss_sup: 0.029569, loss_mps: 0.094905, loss_cps: 0.178393
[13:13:33.245] iteration 12552: total_loss: 0.230569, loss_sup: 0.083515, loss_mps: 0.059895, loss_cps: 0.087159
[13:13:33.391] iteration 12553: total_loss: 0.253040, loss_sup: 0.058911, loss_mps: 0.070988, loss_cps: 0.123142
[13:13:33.537] iteration 12554: total_loss: 0.753664, loss_sup: 0.135585, loss_mps: 0.195961, loss_cps: 0.422118
[13:13:33.683] iteration 12555: total_loss: 0.416662, loss_sup: 0.160246, loss_mps: 0.088165, loss_cps: 0.168252
[13:13:33.829] iteration 12556: total_loss: 0.199299, loss_sup: 0.014830, loss_mps: 0.073207, loss_cps: 0.111262
[13:13:33.976] iteration 12557: total_loss: 0.363786, loss_sup: 0.108981, loss_mps: 0.092698, loss_cps: 0.162107
[13:13:34.122] iteration 12558: total_loss: 0.391648, loss_sup: 0.230069, loss_mps: 0.061336, loss_cps: 0.100244
[13:13:34.271] iteration 12559: total_loss: 0.296754, loss_sup: 0.045500, loss_mps: 0.086745, loss_cps: 0.164508
[13:13:34.420] iteration 12560: total_loss: 0.406371, loss_sup: 0.045666, loss_mps: 0.123424, loss_cps: 0.237280
[13:13:34.567] iteration 12561: total_loss: 0.311968, loss_sup: 0.130062, loss_mps: 0.066934, loss_cps: 0.114972
[13:13:34.713] iteration 12562: total_loss: 0.394386, loss_sup: 0.077782, loss_mps: 0.103689, loss_cps: 0.212915
[13:13:34.860] iteration 12563: total_loss: 0.496547, loss_sup: 0.127363, loss_mps: 0.117974, loss_cps: 0.251211
[13:13:35.006] iteration 12564: total_loss: 0.386517, loss_sup: 0.111897, loss_mps: 0.092521, loss_cps: 0.182099
[13:13:35.154] iteration 12565: total_loss: 0.553523, loss_sup: 0.139197, loss_mps: 0.134221, loss_cps: 0.280105
[13:13:35.300] iteration 12566: total_loss: 0.538107, loss_sup: 0.215829, loss_mps: 0.109527, loss_cps: 0.212752
[13:13:35.450] iteration 12567: total_loss: 0.205251, loss_sup: 0.019193, loss_mps: 0.066484, loss_cps: 0.119574
[13:13:35.597] iteration 12568: total_loss: 0.402375, loss_sup: 0.039616, loss_mps: 0.119733, loss_cps: 0.243026
[13:13:35.745] iteration 12569: total_loss: 0.456176, loss_sup: 0.083319, loss_mps: 0.124698, loss_cps: 0.248159
[13:13:35.892] iteration 12570: total_loss: 0.362442, loss_sup: 0.078695, loss_mps: 0.096421, loss_cps: 0.187326
[13:13:36.038] iteration 12571: total_loss: 0.405660, loss_sup: 0.063866, loss_mps: 0.116953, loss_cps: 0.224841
[13:13:36.186] iteration 12572: total_loss: 0.151257, loss_sup: 0.006253, loss_mps: 0.057313, loss_cps: 0.087691
[13:13:36.332] iteration 12573: total_loss: 0.245694, loss_sup: 0.057860, loss_mps: 0.073124, loss_cps: 0.114710
[13:13:36.478] iteration 12574: total_loss: 0.257398, loss_sup: 0.092035, loss_mps: 0.060346, loss_cps: 0.105017
[13:13:36.624] iteration 12575: total_loss: 0.344374, loss_sup: 0.055610, loss_mps: 0.096346, loss_cps: 0.192418
[13:13:36.771] iteration 12576: total_loss: 0.291674, loss_sup: 0.039143, loss_mps: 0.083423, loss_cps: 0.169108
[13:13:36.919] iteration 12577: total_loss: 0.471772, loss_sup: 0.042355, loss_mps: 0.138843, loss_cps: 0.290573
[13:13:37.065] iteration 12578: total_loss: 0.243475, loss_sup: 0.007652, loss_mps: 0.083878, loss_cps: 0.151945
[13:13:37.211] iteration 12579: total_loss: 0.372291, loss_sup: 0.060416, loss_mps: 0.101396, loss_cps: 0.210480
[13:13:37.358] iteration 12580: total_loss: 0.188576, loss_sup: 0.020666, loss_mps: 0.060922, loss_cps: 0.106989
[13:13:37.504] iteration 12581: total_loss: 0.389771, loss_sup: 0.027691, loss_mps: 0.119805, loss_cps: 0.242275
[13:13:37.651] iteration 12582: total_loss: 0.376742, loss_sup: 0.017036, loss_mps: 0.109536, loss_cps: 0.250170
[13:13:37.797] iteration 12583: total_loss: 0.372411, loss_sup: 0.202354, loss_mps: 0.063097, loss_cps: 0.106961
[13:13:37.944] iteration 12584: total_loss: 0.234776, loss_sup: 0.019113, loss_mps: 0.074501, loss_cps: 0.141163
[13:13:38.090] iteration 12585: total_loss: 0.316667, loss_sup: 0.092131, loss_mps: 0.079362, loss_cps: 0.145175
[13:13:38.236] iteration 12586: total_loss: 0.348660, loss_sup: 0.070101, loss_mps: 0.089101, loss_cps: 0.189458
[13:13:38.382] iteration 12587: total_loss: 0.329696, loss_sup: 0.076277, loss_mps: 0.084176, loss_cps: 0.169243
[13:13:38.528] iteration 12588: total_loss: 0.185910, loss_sup: 0.043073, loss_mps: 0.052789, loss_cps: 0.090048
[13:13:38.676] iteration 12589: total_loss: 0.390857, loss_sup: 0.116320, loss_mps: 0.089821, loss_cps: 0.184717
[13:13:38.822] iteration 12590: total_loss: 0.264472, loss_sup: 0.010206, loss_mps: 0.082852, loss_cps: 0.171413
[13:13:38.971] iteration 12591: total_loss: 0.505900, loss_sup: 0.209722, loss_mps: 0.099583, loss_cps: 0.196595
[13:13:39.118] iteration 12592: total_loss: 0.186604, loss_sup: 0.064255, loss_mps: 0.046485, loss_cps: 0.075865
[13:13:39.264] iteration 12593: total_loss: 0.352101, loss_sup: 0.127056, loss_mps: 0.076291, loss_cps: 0.148753
[13:13:39.410] iteration 12594: total_loss: 0.315123, loss_sup: 0.071518, loss_mps: 0.084846, loss_cps: 0.158759
[13:13:39.556] iteration 12595: total_loss: 0.373542, loss_sup: 0.039038, loss_mps: 0.111199, loss_cps: 0.223305
[13:13:39.704] iteration 12596: total_loss: 0.419574, loss_sup: 0.024140, loss_mps: 0.125722, loss_cps: 0.269712
[13:13:39.849] iteration 12597: total_loss: 0.148408, loss_sup: 0.008600, loss_mps: 0.052388, loss_cps: 0.087419
[13:13:39.996] iteration 12598: total_loss: 0.381541, loss_sup: 0.116959, loss_mps: 0.092988, loss_cps: 0.171593
[13:13:40.142] iteration 12599: total_loss: 0.253511, loss_sup: 0.075721, loss_mps: 0.063779, loss_cps: 0.114011
[13:13:40.289] iteration 12600: total_loss: 0.235370, loss_sup: 0.052897, loss_mps: 0.065958, loss_cps: 0.116515
[13:13:40.289] Evaluation Started ==>
[13:13:51.653] ==> valid iteration 12600: unet metrics: {'dc': 0.6740371813473971, 'jc': 0.5539653430528494, 'pre': 0.7411513017643744, 'hd': 5.898261321611861}, ynet metrics: {'dc': 0.5988804019544522, 'jc': 0.4790596838288003, 'pre': 0.7744616274405741, 'hd': 5.84523819980848}.
[13:13:51.713] ==> New best valid dice for unet: 0.674037, at iteration 12600
[13:13:51.716] Evaluation Finished!⏹️
[13:13:51.870] iteration 12601: total_loss: 0.544582, loss_sup: 0.138993, loss_mps: 0.133881, loss_cps: 0.271708
[13:13:52.017] iteration 12602: total_loss: 0.453436, loss_sup: 0.080908, loss_mps: 0.121882, loss_cps: 0.250646
[13:13:52.162] iteration 12603: total_loss: 0.585085, loss_sup: 0.132758, loss_mps: 0.141491, loss_cps: 0.310837
[13:13:52.307] iteration 12604: total_loss: 0.275339, loss_sup: 0.024067, loss_mps: 0.084948, loss_cps: 0.166324
[13:13:52.454] iteration 12605: total_loss: 0.350241, loss_sup: 0.053064, loss_mps: 0.103460, loss_cps: 0.193717
[13:13:52.600] iteration 12606: total_loss: 0.375709, loss_sup: 0.105870, loss_mps: 0.087237, loss_cps: 0.182601
[13:13:52.746] iteration 12607: total_loss: 0.368895, loss_sup: 0.103438, loss_mps: 0.092128, loss_cps: 0.173328
[13:13:52.892] iteration 12608: total_loss: 0.306461, loss_sup: 0.021600, loss_mps: 0.100265, loss_cps: 0.184596
[13:13:53.038] iteration 12609: total_loss: 0.392579, loss_sup: 0.107602, loss_mps: 0.097583, loss_cps: 0.187394
[13:13:53.184] iteration 12610: total_loss: 0.494510, loss_sup: 0.048408, loss_mps: 0.140269, loss_cps: 0.305834
[13:13:53.329] iteration 12611: total_loss: 0.321689, loss_sup: 0.051655, loss_mps: 0.092962, loss_cps: 0.177072
[13:13:53.476] iteration 12612: total_loss: 0.217555, loss_sup: 0.015828, loss_mps: 0.069314, loss_cps: 0.132413
[13:13:53.623] iteration 12613: total_loss: 0.718970, loss_sup: 0.090056, loss_mps: 0.192464, loss_cps: 0.436450
[13:13:53.769] iteration 12614: total_loss: 0.427530, loss_sup: 0.035225, loss_mps: 0.119902, loss_cps: 0.272403
[13:13:53.915] iteration 12615: total_loss: 0.283699, loss_sup: 0.055828, loss_mps: 0.083407, loss_cps: 0.144464
[13:13:54.064] iteration 12616: total_loss: 0.288445, loss_sup: 0.145689, loss_mps: 0.052743, loss_cps: 0.090012
[13:13:54.210] iteration 12617: total_loss: 0.409605, loss_sup: 0.072967, loss_mps: 0.107999, loss_cps: 0.228640
[13:13:54.355] iteration 12618: total_loss: 0.260031, loss_sup: 0.011586, loss_mps: 0.083860, loss_cps: 0.164586
[13:13:54.502] iteration 12619: total_loss: 0.458597, loss_sup: 0.114866, loss_mps: 0.107544, loss_cps: 0.236187
[13:13:54.648] iteration 12620: total_loss: 0.407941, loss_sup: 0.200363, loss_mps: 0.073744, loss_cps: 0.133834
[13:13:54.793] iteration 12621: total_loss: 0.361884, loss_sup: 0.044768, loss_mps: 0.099762, loss_cps: 0.217354
[13:13:54.939] iteration 12622: total_loss: 0.260256, loss_sup: 0.024159, loss_mps: 0.079610, loss_cps: 0.156487
[13:13:55.084] iteration 12623: total_loss: 0.364488, loss_sup: 0.078263, loss_mps: 0.097585, loss_cps: 0.188640
[13:13:55.229] iteration 12624: total_loss: 0.337291, loss_sup: 0.067058, loss_mps: 0.093041, loss_cps: 0.177193
[13:13:55.375] iteration 12625: total_loss: 0.370439, loss_sup: 0.067420, loss_mps: 0.098211, loss_cps: 0.204808
[13:13:55.520] iteration 12626: total_loss: 0.370399, loss_sup: 0.065716, loss_mps: 0.105920, loss_cps: 0.198763
[13:13:55.666] iteration 12627: total_loss: 0.400712, loss_sup: 0.041134, loss_mps: 0.116339, loss_cps: 0.243239
[13:13:55.812] iteration 12628: total_loss: 0.310623, loss_sup: 0.028904, loss_mps: 0.093227, loss_cps: 0.188492
[13:13:55.959] iteration 12629: total_loss: 0.403465, loss_sup: 0.027793, loss_mps: 0.126403, loss_cps: 0.249269
[13:13:56.105] iteration 12630: total_loss: 0.329235, loss_sup: 0.156347, loss_mps: 0.061522, loss_cps: 0.111366
[13:13:56.251] iteration 12631: total_loss: 0.210989, loss_sup: 0.025226, loss_mps: 0.062868, loss_cps: 0.122895
[13:13:56.399] iteration 12632: total_loss: 0.671660, loss_sup: 0.256908, loss_mps: 0.132791, loss_cps: 0.281960
[13:13:56.544] iteration 12633: total_loss: 0.194223, loss_sup: 0.029325, loss_mps: 0.060979, loss_cps: 0.103920
[13:13:56.693] iteration 12634: total_loss: 0.226507, loss_sup: 0.073115, loss_mps: 0.057128, loss_cps: 0.096263
[13:13:56.839] iteration 12635: total_loss: 0.232358, loss_sup: 0.063830, loss_mps: 0.059588, loss_cps: 0.108940
[13:13:56.986] iteration 12636: total_loss: 0.230443, loss_sup: 0.016404, loss_mps: 0.072687, loss_cps: 0.141352
[13:13:57.135] iteration 12637: total_loss: 0.139627, loss_sup: 0.016963, loss_mps: 0.046527, loss_cps: 0.076137
[13:13:57.281] iteration 12638: total_loss: 0.384751, loss_sup: 0.134674, loss_mps: 0.084510, loss_cps: 0.165567
[13:13:57.427] iteration 12639: total_loss: 0.653042, loss_sup: 0.221301, loss_mps: 0.133898, loss_cps: 0.297843
[13:13:57.572] iteration 12640: total_loss: 0.406698, loss_sup: 0.078603, loss_mps: 0.107782, loss_cps: 0.220313
[13:13:57.718] iteration 12641: total_loss: 0.347324, loss_sup: 0.099664, loss_mps: 0.083090, loss_cps: 0.164570
[13:13:57.864] iteration 12642: total_loss: 0.279610, loss_sup: 0.041141, loss_mps: 0.081019, loss_cps: 0.157450
[13:13:58.012] iteration 12643: total_loss: 0.339056, loss_sup: 0.092527, loss_mps: 0.086729, loss_cps: 0.159800
[13:13:58.158] iteration 12644: total_loss: 0.396654, loss_sup: 0.122033, loss_mps: 0.088574, loss_cps: 0.186047
[13:13:58.303] iteration 12645: total_loss: 0.329961, loss_sup: 0.027691, loss_mps: 0.099839, loss_cps: 0.202431
[13:13:58.450] iteration 12646: total_loss: 0.444449, loss_sup: 0.154243, loss_mps: 0.100157, loss_cps: 0.190050
[13:13:58.596] iteration 12647: total_loss: 0.644498, loss_sup: 0.124346, loss_mps: 0.155902, loss_cps: 0.364250
[13:13:58.742] iteration 12648: total_loss: 0.434465, loss_sup: 0.085987, loss_mps: 0.110054, loss_cps: 0.238424
[13:13:58.888] iteration 12649: total_loss: 0.365109, loss_sup: 0.105674, loss_mps: 0.088142, loss_cps: 0.171292
[13:13:59.038] iteration 12650: total_loss: 0.324552, loss_sup: 0.060224, loss_mps: 0.095574, loss_cps: 0.168755
[13:13:59.184] iteration 12651: total_loss: 0.543601, loss_sup: 0.197637, loss_mps: 0.116565, loss_cps: 0.229399
[13:13:59.330] iteration 12652: total_loss: 0.358248, loss_sup: 0.038015, loss_mps: 0.099657, loss_cps: 0.220576
[13:13:59.476] iteration 12653: total_loss: 0.354115, loss_sup: 0.077472, loss_mps: 0.095097, loss_cps: 0.181547
[13:13:59.621] iteration 12654: total_loss: 1.016395, loss_sup: 0.434725, loss_mps: 0.181868, loss_cps: 0.399802
[13:13:59.768] iteration 12655: total_loss: 0.274285, loss_sup: 0.042003, loss_mps: 0.084072, loss_cps: 0.148210
[13:13:59.914] iteration 12656: total_loss: 0.428235, loss_sup: 0.118657, loss_mps: 0.110083, loss_cps: 0.199495
[13:14:00.061] iteration 12657: total_loss: 0.214864, loss_sup: 0.028005, loss_mps: 0.072331, loss_cps: 0.114528
[13:14:00.210] iteration 12658: total_loss: 0.306917, loss_sup: 0.072559, loss_mps: 0.089624, loss_cps: 0.144734
[13:14:00.359] iteration 12659: total_loss: 0.573433, loss_sup: 0.027885, loss_mps: 0.182061, loss_cps: 0.363487
[13:14:00.505] iteration 12660: total_loss: 0.424024, loss_sup: 0.075423, loss_mps: 0.122700, loss_cps: 0.225902
[13:14:00.651] iteration 12661: total_loss: 0.510586, loss_sup: 0.121827, loss_mps: 0.131100, loss_cps: 0.257658
[13:14:00.797] iteration 12662: total_loss: 0.348779, loss_sup: 0.095685, loss_mps: 0.088644, loss_cps: 0.164451
[13:14:00.943] iteration 12663: total_loss: 0.471016, loss_sup: 0.027902, loss_mps: 0.150178, loss_cps: 0.292936
[13:14:01.092] iteration 12664: total_loss: 0.348084, loss_sup: 0.094794, loss_mps: 0.094826, loss_cps: 0.158464
[13:14:01.238] iteration 12665: total_loss: 0.303919, loss_sup: 0.056248, loss_mps: 0.086843, loss_cps: 0.160827
[13:14:01.385] iteration 12666: total_loss: 0.183926, loss_sup: 0.049247, loss_mps: 0.052875, loss_cps: 0.081804
[13:14:01.531] iteration 12667: total_loss: 0.323409, loss_sup: 0.036107, loss_mps: 0.101696, loss_cps: 0.185606
[13:14:01.677] iteration 12668: total_loss: 0.668179, loss_sup: 0.326206, loss_mps: 0.116339, loss_cps: 0.225633
[13:14:01.823] iteration 12669: total_loss: 0.369676, loss_sup: 0.024075, loss_mps: 0.115298, loss_cps: 0.230303
[13:14:01.970] iteration 12670: total_loss: 0.532785, loss_sup: 0.231596, loss_mps: 0.099451, loss_cps: 0.201738
[13:14:02.116] iteration 12671: total_loss: 0.151410, loss_sup: 0.020173, loss_mps: 0.050001, loss_cps: 0.081236
[13:14:02.263] iteration 12672: total_loss: 0.564185, loss_sup: 0.177768, loss_mps: 0.133033, loss_cps: 0.253384
[13:14:02.409] iteration 12673: total_loss: 0.338619, loss_sup: 0.061558, loss_mps: 0.092109, loss_cps: 0.184951
[13:14:02.556] iteration 12674: total_loss: 0.521957, loss_sup: 0.212547, loss_mps: 0.106483, loss_cps: 0.202927
[13:14:02.702] iteration 12675: total_loss: 0.320572, loss_sup: 0.039374, loss_mps: 0.102687, loss_cps: 0.178512
[13:14:02.849] iteration 12676: total_loss: 0.419843, loss_sup: 0.086932, loss_mps: 0.110159, loss_cps: 0.222752
[13:14:02.997] iteration 12677: total_loss: 0.314738, loss_sup: 0.117850, loss_mps: 0.073901, loss_cps: 0.122987
[13:14:03.143] iteration 12678: total_loss: 0.730691, loss_sup: 0.270621, loss_mps: 0.147946, loss_cps: 0.312123
[13:14:03.289] iteration 12679: total_loss: 0.266886, loss_sup: 0.053119, loss_mps: 0.075981, loss_cps: 0.137786
[13:14:03.435] iteration 12680: total_loss: 0.258325, loss_sup: 0.014843, loss_mps: 0.087966, loss_cps: 0.155517
[13:14:03.580] iteration 12681: total_loss: 0.229070, loss_sup: 0.004573, loss_mps: 0.080325, loss_cps: 0.144173
[13:14:03.726] iteration 12682: total_loss: 0.255165, loss_sup: 0.026171, loss_mps: 0.078614, loss_cps: 0.150380
[13:14:03.873] iteration 12683: total_loss: 0.455018, loss_sup: 0.139176, loss_mps: 0.109142, loss_cps: 0.206700
[13:14:04.021] iteration 12684: total_loss: 0.371517, loss_sup: 0.109461, loss_mps: 0.090697, loss_cps: 0.171358
[13:14:04.167] iteration 12685: total_loss: 0.438770, loss_sup: 0.200326, loss_mps: 0.083714, loss_cps: 0.154731
[13:14:04.313] iteration 12686: total_loss: 0.256016, loss_sup: 0.018833, loss_mps: 0.084822, loss_cps: 0.152361
[13:14:04.458] iteration 12687: total_loss: 0.317090, loss_sup: 0.108725, loss_mps: 0.074029, loss_cps: 0.134336
[13:14:04.604] iteration 12688: total_loss: 0.225621, loss_sup: 0.082007, loss_mps: 0.057224, loss_cps: 0.086390
[13:14:04.750] iteration 12689: total_loss: 0.287834, loss_sup: 0.104812, loss_mps: 0.069428, loss_cps: 0.113594
[13:14:04.896] iteration 12690: total_loss: 0.216723, loss_sup: 0.018692, loss_mps: 0.066932, loss_cps: 0.131099
[13:14:05.041] iteration 12691: total_loss: 0.304269, loss_sup: 0.048215, loss_mps: 0.088138, loss_cps: 0.167916
[13:14:05.189] iteration 12692: total_loss: 0.153993, loss_sup: 0.007493, loss_mps: 0.057909, loss_cps: 0.088591
[13:14:05.334] iteration 12693: total_loss: 0.452868, loss_sup: 0.105329, loss_mps: 0.113814, loss_cps: 0.233726
[13:14:05.480] iteration 12694: total_loss: 0.265833, loss_sup: 0.037552, loss_mps: 0.081438, loss_cps: 0.146843
[13:14:05.625] iteration 12695: total_loss: 0.270423, loss_sup: 0.116061, loss_mps: 0.059778, loss_cps: 0.094584
[13:14:05.773] iteration 12696: total_loss: 0.273926, loss_sup: 0.054692, loss_mps: 0.077328, loss_cps: 0.141905
[13:14:05.919] iteration 12697: total_loss: 0.192221, loss_sup: 0.028892, loss_mps: 0.060211, loss_cps: 0.103118
[13:14:06.067] iteration 12698: total_loss: 0.321782, loss_sup: 0.045110, loss_mps: 0.093386, loss_cps: 0.183286
[13:14:06.212] iteration 12699: total_loss: 0.504025, loss_sup: 0.053743, loss_mps: 0.142050, loss_cps: 0.308231
[13:14:06.359] iteration 12700: total_loss: 0.397044, loss_sup: 0.087992, loss_mps: 0.105224, loss_cps: 0.203828
[13:14:06.359] Evaluation Started ==>
[13:14:17.703] ==> valid iteration 12700: unet metrics: {'dc': 0.6668490104953175, 'jc': 0.5464679396120693, 'pre': 0.7783068820016434, 'hd': 5.614024954100303}, ynet metrics: {'dc': 0.6117284548527826, 'jc': 0.49260029405470834, 'pre': 0.7745702724620939, 'hd': 5.703027137661205}.
[13:14:17.705] Evaluation Finished!⏹️
[13:14:17.858] iteration 12701: total_loss: 0.308814, loss_sup: 0.101803, loss_mps: 0.072234, loss_cps: 0.134777
[13:14:18.005] iteration 12702: total_loss: 0.208773, loss_sup: 0.004571, loss_mps: 0.072470, loss_cps: 0.131732
[13:14:18.152] iteration 12703: total_loss: 0.295410, loss_sup: 0.101466, loss_mps: 0.070629, loss_cps: 0.123315
[13:14:18.297] iteration 12704: total_loss: 0.225240, loss_sup: 0.035297, loss_mps: 0.070404, loss_cps: 0.119539
[13:14:18.444] iteration 12705: total_loss: 0.262899, loss_sup: 0.037720, loss_mps: 0.079329, loss_cps: 0.145851
[13:14:18.590] iteration 12706: total_loss: 0.328840, loss_sup: 0.022573, loss_mps: 0.098099, loss_cps: 0.208168
[13:14:18.737] iteration 12707: total_loss: 0.435217, loss_sup: 0.118934, loss_mps: 0.104485, loss_cps: 0.211798
[13:14:18.885] iteration 12708: total_loss: 0.348713, loss_sup: 0.020083, loss_mps: 0.104491, loss_cps: 0.224139
[13:14:19.032] iteration 12709: total_loss: 0.541115, loss_sup: 0.161645, loss_mps: 0.122345, loss_cps: 0.257125
[13:14:19.178] iteration 12710: total_loss: 0.259860, loss_sup: 0.064955, loss_mps: 0.069574, loss_cps: 0.125331
[13:14:19.324] iteration 12711: total_loss: 0.340020, loss_sup: 0.023858, loss_mps: 0.100218, loss_cps: 0.215945
[13:14:19.470] iteration 12712: total_loss: 0.311672, loss_sup: 0.159606, loss_mps: 0.055689, loss_cps: 0.096377
[13:14:19.616] iteration 12713: total_loss: 0.353969, loss_sup: 0.049381, loss_mps: 0.102023, loss_cps: 0.202565
[13:14:19.762] iteration 12714: total_loss: 0.489790, loss_sup: 0.256181, loss_mps: 0.083607, loss_cps: 0.150002
[13:14:19.907] iteration 12715: total_loss: 0.511305, loss_sup: 0.140332, loss_mps: 0.124640, loss_cps: 0.246334
[13:14:20.053] iteration 12716: total_loss: 0.330657, loss_sup: 0.093314, loss_mps: 0.083577, loss_cps: 0.153765
[13:14:20.198] iteration 12717: total_loss: 0.313430, loss_sup: 0.074613, loss_mps: 0.083420, loss_cps: 0.155397
[13:14:20.344] iteration 12718: total_loss: 0.321559, loss_sup: 0.086206, loss_mps: 0.084158, loss_cps: 0.151195
[13:14:20.489] iteration 12719: total_loss: 0.241487, loss_sup: 0.069048, loss_mps: 0.064336, loss_cps: 0.108103
[13:14:20.634] iteration 12720: total_loss: 0.541829, loss_sup: 0.159462, loss_mps: 0.129037, loss_cps: 0.253330
[13:14:20.780] iteration 12721: total_loss: 0.336028, loss_sup: 0.050555, loss_mps: 0.095602, loss_cps: 0.189871
[13:14:20.926] iteration 12722: total_loss: 0.256800, loss_sup: 0.061898, loss_mps: 0.071147, loss_cps: 0.123754
[13:14:21.071] iteration 12723: total_loss: 0.313937, loss_sup: 0.045710, loss_mps: 0.088855, loss_cps: 0.179371
[13:14:21.218] iteration 12724: total_loss: 0.572647, loss_sup: 0.168661, loss_mps: 0.131901, loss_cps: 0.272085
[13:14:21.363] iteration 12725: total_loss: 0.496911, loss_sup: 0.165924, loss_mps: 0.110093, loss_cps: 0.220894
[13:14:21.509] iteration 12726: total_loss: 0.424542, loss_sup: 0.130336, loss_mps: 0.106587, loss_cps: 0.187619
[13:14:21.655] iteration 12727: total_loss: 0.581225, loss_sup: 0.238026, loss_mps: 0.114140, loss_cps: 0.229059
[13:14:21.801] iteration 12728: total_loss: 0.300101, loss_sup: 0.084072, loss_mps: 0.076239, loss_cps: 0.139790
[13:14:21.947] iteration 12729: total_loss: 0.209813, loss_sup: 0.022675, loss_mps: 0.067014, loss_cps: 0.120124
[13:14:22.097] iteration 12730: total_loss: 0.778375, loss_sup: 0.171307, loss_mps: 0.191265, loss_cps: 0.415803
[13:14:22.244] iteration 12731: total_loss: 0.365002, loss_sup: 0.038024, loss_mps: 0.111565, loss_cps: 0.215413
[13:14:22.391] iteration 12732: total_loss: 0.205738, loss_sup: 0.013400, loss_mps: 0.068562, loss_cps: 0.123775
[13:14:22.536] iteration 12733: total_loss: 0.252669, loss_sup: 0.008318, loss_mps: 0.082735, loss_cps: 0.161617
[13:14:22.683] iteration 12734: total_loss: 0.206015, loss_sup: 0.029464, loss_mps: 0.071101, loss_cps: 0.105450
[13:14:22.829] iteration 12735: total_loss: 0.337270, loss_sup: 0.106858, loss_mps: 0.084503, loss_cps: 0.145909
[13:14:22.980] iteration 12736: total_loss: 0.371054, loss_sup: 0.085978, loss_mps: 0.101396, loss_cps: 0.183680
[13:14:23.126] iteration 12737: total_loss: 0.322174, loss_sup: 0.103330, loss_mps: 0.079561, loss_cps: 0.139283
[13:14:23.273] iteration 12738: total_loss: 0.541963, loss_sup: 0.167673, loss_mps: 0.125892, loss_cps: 0.248399
[13:14:23.420] iteration 12739: total_loss: 0.675079, loss_sup: 0.087575, loss_mps: 0.183153, loss_cps: 0.404351
[13:14:23.567] iteration 12740: total_loss: 0.322680, loss_sup: 0.065016, loss_mps: 0.091036, loss_cps: 0.166628
[13:14:23.716] iteration 12741: total_loss: 0.323760, loss_sup: 0.040017, loss_mps: 0.101728, loss_cps: 0.182014
[13:14:23.865] iteration 12742: total_loss: 0.267437, loss_sup: 0.080192, loss_mps: 0.068293, loss_cps: 0.118953
[13:14:24.012] iteration 12743: total_loss: 0.265900, loss_sup: 0.099093, loss_mps: 0.061011, loss_cps: 0.105796
[13:14:24.158] iteration 12744: total_loss: 0.248140, loss_sup: 0.056624, loss_mps: 0.072474, loss_cps: 0.119042
[13:14:24.308] iteration 12745: total_loss: 0.352303, loss_sup: 0.117581, loss_mps: 0.084784, loss_cps: 0.149938
[13:14:24.455] iteration 12746: total_loss: 0.223841, loss_sup: 0.036723, loss_mps: 0.070356, loss_cps: 0.116762
[13:14:24.601] iteration 12747: total_loss: 0.328868, loss_sup: 0.072914, loss_mps: 0.088678, loss_cps: 0.167276
[13:14:24.749] iteration 12748: total_loss: 0.156078, loss_sup: 0.016438, loss_mps: 0.052549, loss_cps: 0.087092
[13:14:24.894] iteration 12749: total_loss: 0.184741, loss_sup: 0.019770, loss_mps: 0.062651, loss_cps: 0.102320
[13:14:25.042] iteration 12750: total_loss: 0.251069, loss_sup: 0.026009, loss_mps: 0.076719, loss_cps: 0.148342
[13:14:25.189] iteration 12751: total_loss: 0.360106, loss_sup: 0.100709, loss_mps: 0.093550, loss_cps: 0.165847
[13:14:25.335] iteration 12752: total_loss: 0.356612, loss_sup: 0.034237, loss_mps: 0.107107, loss_cps: 0.215268
[13:14:25.481] iteration 12753: total_loss: 0.403685, loss_sup: 0.188234, loss_mps: 0.079752, loss_cps: 0.135699
[13:14:25.627] iteration 12754: total_loss: 0.256291, loss_sup: 0.023831, loss_mps: 0.081174, loss_cps: 0.151286
[13:14:25.774] iteration 12755: total_loss: 0.453600, loss_sup: 0.072306, loss_mps: 0.120479, loss_cps: 0.260815
[13:14:25.920] iteration 12756: total_loss: 0.287658, loss_sup: 0.067850, loss_mps: 0.075479, loss_cps: 0.144329
[13:14:26.066] iteration 12757: total_loss: 0.256963, loss_sup: 0.023922, loss_mps: 0.077483, loss_cps: 0.155559
[13:14:26.212] iteration 12758: total_loss: 0.429585, loss_sup: 0.108158, loss_mps: 0.104313, loss_cps: 0.217114
[13:14:26.358] iteration 12759: total_loss: 0.279093, loss_sup: 0.047298, loss_mps: 0.082100, loss_cps: 0.149695
[13:14:26.505] iteration 12760: total_loss: 0.366013, loss_sup: 0.174741, loss_mps: 0.064887, loss_cps: 0.126385
[13:14:26.651] iteration 12761: total_loss: 0.333158, loss_sup: 0.069954, loss_mps: 0.089158, loss_cps: 0.174046
[13:14:26.797] iteration 12762: total_loss: 0.355876, loss_sup: 0.077184, loss_mps: 0.090985, loss_cps: 0.187706
[13:14:26.944] iteration 12763: total_loss: 0.247901, loss_sup: 0.036897, loss_mps: 0.078404, loss_cps: 0.132600
[13:14:27.091] iteration 12764: total_loss: 0.222129, loss_sup: 0.028318, loss_mps: 0.066954, loss_cps: 0.126857
[13:14:27.237] iteration 12765: total_loss: 0.390986, loss_sup: 0.094858, loss_mps: 0.097944, loss_cps: 0.198183
[13:14:27.383] iteration 12766: total_loss: 0.655992, loss_sup: 0.097973, loss_mps: 0.165018, loss_cps: 0.393000
[13:14:27.529] iteration 12767: total_loss: 0.250046, loss_sup: 0.031075, loss_mps: 0.075216, loss_cps: 0.143756
[13:14:27.676] iteration 12768: total_loss: 0.358622, loss_sup: 0.107399, loss_mps: 0.082611, loss_cps: 0.168612
[13:14:27.822] iteration 12769: total_loss: 0.335591, loss_sup: 0.051894, loss_mps: 0.096686, loss_cps: 0.187011
[13:14:27.969] iteration 12770: total_loss: 0.460432, loss_sup: 0.124995, loss_mps: 0.109358, loss_cps: 0.226080
[13:14:28.117] iteration 12771: total_loss: 0.416846, loss_sup: 0.203206, loss_mps: 0.074815, loss_cps: 0.138825
[13:14:28.263] iteration 12772: total_loss: 0.371410, loss_sup: 0.093427, loss_mps: 0.088829, loss_cps: 0.189155
[13:14:28.409] iteration 12773: total_loss: 0.249669, loss_sup: 0.028746, loss_mps: 0.073218, loss_cps: 0.147705
[13:14:28.555] iteration 12774: total_loss: 0.353542, loss_sup: 0.039040, loss_mps: 0.107536, loss_cps: 0.206965
[13:14:28.701] iteration 12775: total_loss: 0.237272, loss_sup: 0.044461, loss_mps: 0.067350, loss_cps: 0.125461
[13:14:28.848] iteration 12776: total_loss: 0.361763, loss_sup: 0.102713, loss_mps: 0.089280, loss_cps: 0.169770
[13:14:28.994] iteration 12777: total_loss: 0.228076, loss_sup: 0.074609, loss_mps: 0.056824, loss_cps: 0.096644
[13:14:29.140] iteration 12778: total_loss: 0.501695, loss_sup: 0.104040, loss_mps: 0.130993, loss_cps: 0.266662
[13:14:29.286] iteration 12779: total_loss: 0.451469, loss_sup: 0.127593, loss_mps: 0.109962, loss_cps: 0.213914
[13:14:29.432] iteration 12780: total_loss: 0.252442, loss_sup: 0.087177, loss_mps: 0.056298, loss_cps: 0.108968
[13:14:29.579] iteration 12781: total_loss: 0.485005, loss_sup: 0.173415, loss_mps: 0.107371, loss_cps: 0.204219
[13:14:29.726] iteration 12782: total_loss: 0.196209, loss_sup: 0.031009, loss_mps: 0.058585, loss_cps: 0.106615
[13:14:29.872] iteration 12783: total_loss: 0.253573, loss_sup: 0.087502, loss_mps: 0.063190, loss_cps: 0.102881
[13:14:30.018] iteration 12784: total_loss: 0.308665, loss_sup: 0.088060, loss_mps: 0.075382, loss_cps: 0.145223
[13:14:30.164] iteration 12785: total_loss: 0.340152, loss_sup: 0.067484, loss_mps: 0.092730, loss_cps: 0.179938
[13:14:30.310] iteration 12786: total_loss: 0.506979, loss_sup: 0.145942, loss_mps: 0.111091, loss_cps: 0.249945
[13:14:30.456] iteration 12787: total_loss: 0.256910, loss_sup: 0.010871, loss_mps: 0.087589, loss_cps: 0.158450
[13:14:30.604] iteration 12788: total_loss: 0.228241, loss_sup: 0.022258, loss_mps: 0.074836, loss_cps: 0.131147
[13:14:30.750] iteration 12789: total_loss: 0.340102, loss_sup: 0.056088, loss_mps: 0.102457, loss_cps: 0.181557
[13:14:30.897] iteration 12790: total_loss: 0.320513, loss_sup: 0.086994, loss_mps: 0.082432, loss_cps: 0.151087
[13:14:31.044] iteration 12791: total_loss: 0.421218, loss_sup: 0.070829, loss_mps: 0.118930, loss_cps: 0.231460
[13:14:31.190] iteration 12792: total_loss: 0.462646, loss_sup: 0.099947, loss_mps: 0.116861, loss_cps: 0.245838
[13:14:31.336] iteration 12793: total_loss: 0.217041, loss_sup: 0.040375, loss_mps: 0.066203, loss_cps: 0.110463
[13:14:31.484] iteration 12794: total_loss: 0.127090, loss_sup: 0.028938, loss_mps: 0.038144, loss_cps: 0.060008
[13:14:31.632] iteration 12795: total_loss: 0.370192, loss_sup: 0.117911, loss_mps: 0.083645, loss_cps: 0.168636
[13:14:31.779] iteration 12796: total_loss: 0.332525, loss_sup: 0.028155, loss_mps: 0.103943, loss_cps: 0.200428
[13:14:31.927] iteration 12797: total_loss: 0.319642, loss_sup: 0.079990, loss_mps: 0.082803, loss_cps: 0.156849
[13:14:32.073] iteration 12798: total_loss: 0.193876, loss_sup: 0.091770, loss_mps: 0.040414, loss_cps: 0.061692
[13:14:32.219] iteration 12799: total_loss: 0.478727, loss_sup: 0.234134, loss_mps: 0.087890, loss_cps: 0.156702
[13:14:32.366] iteration 12800: total_loss: 0.447018, loss_sup: 0.073087, loss_mps: 0.119075, loss_cps: 0.254856
[13:14:32.366] Evaluation Started ==>
[13:14:43.725] ==> valid iteration 12800: unet metrics: {'dc': 0.6087643108645664, 'jc': 0.49402069295378925, 'pre': 0.769684980829275, 'hd': 5.548139872682753}, ynet metrics: {'dc': 0.6128851330151399, 'jc': 0.49439726053646554, 'pre': 0.7539834977777041, 'hd': 5.836078901727453}.
[13:14:43.727] Evaluation Finished!⏹️
[13:14:43.880] iteration 12801: total_loss: 0.416046, loss_sup: 0.171716, loss_mps: 0.085701, loss_cps: 0.158629
[13:14:44.030] iteration 12802: total_loss: 0.350776, loss_sup: 0.016737, loss_mps: 0.109911, loss_cps: 0.224128
[13:14:44.177] iteration 12803: total_loss: 0.233423, loss_sup: 0.041256, loss_mps: 0.067463, loss_cps: 0.124704
[13:14:44.322] iteration 12804: total_loss: 0.259914, loss_sup: 0.044198, loss_mps: 0.074958, loss_cps: 0.140759
[13:14:44.468] iteration 12805: total_loss: 0.390259, loss_sup: 0.003381, loss_mps: 0.124159, loss_cps: 0.262719
[13:14:44.613] iteration 12806: total_loss: 0.398929, loss_sup: 0.068581, loss_mps: 0.108025, loss_cps: 0.222322
[13:14:44.759] iteration 12807: total_loss: 0.532683, loss_sup: 0.055790, loss_mps: 0.145943, loss_cps: 0.330951
[13:14:44.906] iteration 12808: total_loss: 0.302565, loss_sup: 0.095617, loss_mps: 0.073998, loss_cps: 0.132950
[13:14:45.051] iteration 12809: total_loss: 0.472996, loss_sup: 0.111550, loss_mps: 0.117411, loss_cps: 0.244035
[13:14:45.199] iteration 12810: total_loss: 0.191508, loss_sup: 0.020451, loss_mps: 0.059945, loss_cps: 0.111112
[13:14:45.348] iteration 12811: total_loss: 0.330007, loss_sup: 0.128243, loss_mps: 0.066961, loss_cps: 0.134803
[13:14:45.494] iteration 12812: total_loss: 0.441535, loss_sup: 0.119862, loss_mps: 0.110590, loss_cps: 0.211083
[13:14:45.639] iteration 12813: total_loss: 0.437326, loss_sup: 0.104855, loss_mps: 0.108884, loss_cps: 0.223587
[13:14:45.785] iteration 12814: total_loss: 0.586134, loss_sup: 0.091357, loss_mps: 0.152522, loss_cps: 0.342255
[13:14:45.930] iteration 12815: total_loss: 0.330692, loss_sup: 0.106614, loss_mps: 0.075335, loss_cps: 0.148743
[13:14:46.075] iteration 12816: total_loss: 0.321627, loss_sup: 0.134218, loss_mps: 0.069241, loss_cps: 0.118168
[13:14:46.224] iteration 12817: total_loss: 0.255910, loss_sup: 0.025247, loss_mps: 0.083568, loss_cps: 0.147095
[13:14:46.368] iteration 12818: total_loss: 0.386700, loss_sup: 0.119434, loss_mps: 0.092576, loss_cps: 0.174690
[13:14:46.516] iteration 12819: total_loss: 0.474531, loss_sup: 0.045580, loss_mps: 0.134797, loss_cps: 0.294154
[13:14:46.663] iteration 12820: total_loss: 0.428372, loss_sup: 0.048639, loss_mps: 0.124866, loss_cps: 0.254866
[13:14:46.808] iteration 12821: total_loss: 0.246011, loss_sup: 0.050710, loss_mps: 0.075863, loss_cps: 0.119438
[13:14:46.954] iteration 12822: total_loss: 0.348675, loss_sup: 0.056665, loss_mps: 0.098803, loss_cps: 0.193207
[13:14:47.099] iteration 12823: total_loss: 0.584167, loss_sup: 0.166677, loss_mps: 0.131387, loss_cps: 0.286102
[13:14:47.245] iteration 12824: total_loss: 0.367637, loss_sup: 0.129402, loss_mps: 0.083655, loss_cps: 0.154580
[13:14:47.391] iteration 12825: total_loss: 0.915697, loss_sup: 0.320588, loss_mps: 0.192726, loss_cps: 0.402384
[13:14:47.539] iteration 12826: total_loss: 0.253715, loss_sup: 0.040647, loss_mps: 0.074941, loss_cps: 0.138126
[13:14:47.684] iteration 12827: total_loss: 0.328161, loss_sup: 0.069767, loss_mps: 0.088506, loss_cps: 0.169887
[13:14:47.830] iteration 12828: total_loss: 0.513044, loss_sup: 0.287130, loss_mps: 0.081315, loss_cps: 0.144600
[13:14:47.980] iteration 12829: total_loss: 0.232174, loss_sup: 0.012821, loss_mps: 0.078501, loss_cps: 0.140852
[13:14:48.127] iteration 12830: total_loss: 0.467323, loss_sup: 0.040275, loss_mps: 0.141166, loss_cps: 0.285882
[13:14:48.273] iteration 12831: total_loss: 0.292921, loss_sup: 0.013127, loss_mps: 0.094634, loss_cps: 0.185160
[13:14:48.422] iteration 12832: total_loss: 0.407209, loss_sup: 0.197952, loss_mps: 0.077036, loss_cps: 0.132221
[13:14:48.568] iteration 12833: total_loss: 0.193004, loss_sup: 0.014525, loss_mps: 0.064408, loss_cps: 0.114070
[13:14:48.714] iteration 12834: total_loss: 0.273041, loss_sup: 0.021871, loss_mps: 0.089966, loss_cps: 0.161205
[13:14:48.860] iteration 12835: total_loss: 0.373856, loss_sup: 0.094253, loss_mps: 0.097095, loss_cps: 0.182508
[13:14:49.005] iteration 12836: total_loss: 0.205784, loss_sup: 0.040583, loss_mps: 0.060611, loss_cps: 0.104590
[13:14:49.151] iteration 12837: total_loss: 0.203721, loss_sup: 0.049147, loss_mps: 0.058399, loss_cps: 0.096175
[13:14:49.297] iteration 12838: total_loss: 0.509461, loss_sup: 0.082023, loss_mps: 0.140539, loss_cps: 0.286899
[13:14:49.446] iteration 12839: total_loss: 0.287936, loss_sup: 0.056649, loss_mps: 0.080055, loss_cps: 0.151232
[13:14:49.591] iteration 12840: total_loss: 0.238713, loss_sup: 0.025799, loss_mps: 0.076523, loss_cps: 0.136390
[13:14:49.737] iteration 12841: total_loss: 0.815390, loss_sup: 0.341093, loss_mps: 0.149513, loss_cps: 0.324783
[13:14:49.887] iteration 12842: total_loss: 0.317216, loss_sup: 0.077895, loss_mps: 0.084371, loss_cps: 0.154950
[13:14:50.032] iteration 12843: total_loss: 0.442141, loss_sup: 0.131035, loss_mps: 0.111269, loss_cps: 0.199837
[13:14:50.178] iteration 12844: total_loss: 0.466442, loss_sup: 0.060815, loss_mps: 0.133543, loss_cps: 0.272084
[13:14:50.324] iteration 12845: total_loss: 0.481741, loss_sup: 0.201871, loss_mps: 0.094279, loss_cps: 0.185591
[13:14:50.469] iteration 12846: total_loss: 0.285045, loss_sup: 0.034521, loss_mps: 0.088806, loss_cps: 0.161718
[13:14:50.615] iteration 12847: total_loss: 0.271379, loss_sup: 0.005116, loss_mps: 0.093182, loss_cps: 0.173080
[13:14:50.760] iteration 12848: total_loss: 0.321693, loss_sup: 0.121956, loss_mps: 0.070119, loss_cps: 0.129617
[13:14:50.906] iteration 12849: total_loss: 0.369252, loss_sup: 0.145137, loss_mps: 0.079973, loss_cps: 0.144143
[13:14:51.051] iteration 12850: total_loss: 0.307106, loss_sup: 0.047527, loss_mps: 0.089256, loss_cps: 0.170323
[13:14:51.197] iteration 12851: total_loss: 0.601222, loss_sup: 0.121170, loss_mps: 0.156966, loss_cps: 0.323086
[13:14:51.343] iteration 12852: total_loss: 0.351020, loss_sup: 0.196929, loss_mps: 0.056388, loss_cps: 0.097704
[13:14:51.488] iteration 12853: total_loss: 0.373733, loss_sup: 0.064895, loss_mps: 0.108899, loss_cps: 0.199940
[13:14:51.634] iteration 12854: total_loss: 0.526971, loss_sup: 0.250294, loss_mps: 0.099579, loss_cps: 0.177098
[13:14:51.779] iteration 12855: total_loss: 0.544148, loss_sup: 0.094914, loss_mps: 0.142915, loss_cps: 0.306319
[13:14:51.926] iteration 12856: total_loss: 0.417913, loss_sup: 0.061482, loss_mps: 0.124522, loss_cps: 0.231908
[13:14:52.074] iteration 12857: total_loss: 0.412852, loss_sup: 0.070053, loss_mps: 0.112628, loss_cps: 0.230171
[13:14:52.220] iteration 12858: total_loss: 0.454199, loss_sup: 0.140184, loss_mps: 0.106084, loss_cps: 0.207930
[13:14:52.366] iteration 12859: total_loss: 0.442011, loss_sup: 0.043176, loss_mps: 0.127926, loss_cps: 0.270909
[13:14:52.511] iteration 12860: total_loss: 0.301121, loss_sup: 0.075237, loss_mps: 0.079502, loss_cps: 0.146382
[13:14:52.657] iteration 12861: total_loss: 0.565091, loss_sup: 0.041615, loss_mps: 0.160014, loss_cps: 0.363462
[13:14:52.802] iteration 12862: total_loss: 0.507580, loss_sup: 0.123839, loss_mps: 0.124875, loss_cps: 0.258867
[13:14:52.948] iteration 12863: total_loss: 0.368600, loss_sup: 0.104589, loss_mps: 0.089982, loss_cps: 0.174029
[13:14:53.094] iteration 12864: total_loss: 0.458760, loss_sup: 0.246840, loss_mps: 0.076214, loss_cps: 0.135706
[13:14:53.240] iteration 12865: total_loss: 0.328658, loss_sup: 0.044440, loss_mps: 0.096016, loss_cps: 0.188202
[13:14:53.385] iteration 12866: total_loss: 0.367050, loss_sup: 0.071108, loss_mps: 0.100157, loss_cps: 0.195785
[13:14:53.531] iteration 12867: total_loss: 0.625197, loss_sup: 0.104268, loss_mps: 0.160664, loss_cps: 0.360265
[13:14:53.677] iteration 12868: total_loss: 0.583436, loss_sup: 0.131774, loss_mps: 0.145691, loss_cps: 0.305972
[13:14:53.823] iteration 12869: total_loss: 0.545707, loss_sup: 0.149154, loss_mps: 0.128473, loss_cps: 0.268081
[13:14:53.968] iteration 12870: total_loss: 0.616756, loss_sup: 0.191408, loss_mps: 0.139145, loss_cps: 0.286202
[13:14:54.115] iteration 12871: total_loss: 0.576841, loss_sup: 0.212390, loss_mps: 0.124422, loss_cps: 0.240029
[13:14:54.260] iteration 12872: total_loss: 0.190103, loss_sup: 0.035290, loss_mps: 0.057772, loss_cps: 0.097040
[13:14:54.406] iteration 12873: total_loss: 0.315312, loss_sup: 0.038969, loss_mps: 0.091493, loss_cps: 0.184850
[13:14:54.552] iteration 12874: total_loss: 0.292326, loss_sup: 0.019519, loss_mps: 0.096437, loss_cps: 0.176369
[13:14:54.698] iteration 12875: total_loss: 0.405826, loss_sup: 0.076949, loss_mps: 0.104868, loss_cps: 0.224009
[13:14:54.847] iteration 12876: total_loss: 0.437886, loss_sup: 0.153255, loss_mps: 0.091313, loss_cps: 0.193319
[13:14:54.993] iteration 12877: total_loss: 0.409684, loss_sup: 0.131603, loss_mps: 0.101630, loss_cps: 0.176451
[13:14:55.140] iteration 12878: total_loss: 0.329696, loss_sup: 0.088832, loss_mps: 0.084083, loss_cps: 0.156781
[13:14:55.286] iteration 12879: total_loss: 0.458461, loss_sup: 0.157509, loss_mps: 0.102968, loss_cps: 0.197985
[13:14:55.435] iteration 12880: total_loss: 0.387893, loss_sup: 0.065287, loss_mps: 0.107611, loss_cps: 0.214995
[13:14:55.584] iteration 12881: total_loss: 0.279619, loss_sup: 0.052922, loss_mps: 0.084288, loss_cps: 0.142409
[13:14:55.730] iteration 12882: total_loss: 0.432619, loss_sup: 0.042047, loss_mps: 0.132991, loss_cps: 0.257580
[13:14:55.876] iteration 12883: total_loss: 0.327597, loss_sup: 0.049979, loss_mps: 0.098317, loss_cps: 0.179301
[13:14:56.021] iteration 12884: total_loss: 0.260196, loss_sup: 0.027489, loss_mps: 0.079932, loss_cps: 0.152775
[13:14:56.169] iteration 12885: total_loss: 0.201798, loss_sup: 0.013790, loss_mps: 0.073540, loss_cps: 0.114469
[13:14:56.316] iteration 12886: total_loss: 0.306807, loss_sup: 0.077645, loss_mps: 0.080307, loss_cps: 0.148855
[13:14:56.461] iteration 12887: total_loss: 0.263816, loss_sup: 0.025456, loss_mps: 0.085544, loss_cps: 0.152815
[13:14:56.607] iteration 12888: total_loss: 0.230770, loss_sup: 0.012831, loss_mps: 0.080916, loss_cps: 0.137023
[13:14:56.755] iteration 12889: total_loss: 0.396792, loss_sup: 0.032590, loss_mps: 0.125276, loss_cps: 0.238927
[13:14:56.902] iteration 12890: total_loss: 0.313467, loss_sup: 0.087474, loss_mps: 0.083412, loss_cps: 0.142581
[13:14:57.048] iteration 12891: total_loss: 0.251982, loss_sup: 0.036826, loss_mps: 0.077694, loss_cps: 0.137462
[13:14:57.194] iteration 12892: total_loss: 0.217411, loss_sup: 0.005238, loss_mps: 0.078703, loss_cps: 0.133471
[13:14:57.340] iteration 12893: total_loss: 0.270234, loss_sup: 0.041349, loss_mps: 0.080654, loss_cps: 0.148231
[13:14:57.486] iteration 12894: total_loss: 0.274158, loss_sup: 0.108423, loss_mps: 0.059260, loss_cps: 0.106475
[13:14:57.633] iteration 12895: total_loss: 0.261650, loss_sup: 0.050487, loss_mps: 0.074390, loss_cps: 0.136773
[13:14:57.779] iteration 12896: total_loss: 0.294156, loss_sup: 0.068780, loss_mps: 0.078392, loss_cps: 0.146984
[13:14:57.925] iteration 12897: total_loss: 0.524634, loss_sup: 0.278304, loss_mps: 0.084705, loss_cps: 0.161624
[13:14:58.073] iteration 12898: total_loss: 0.260956, loss_sup: 0.035332, loss_mps: 0.081951, loss_cps: 0.143673
[13:14:58.219] iteration 12899: total_loss: 0.265176, loss_sup: 0.073763, loss_mps: 0.066970, loss_cps: 0.124443
[13:14:58.366] iteration 12900: total_loss: 0.613233, loss_sup: 0.185816, loss_mps: 0.132350, loss_cps: 0.295067
[13:14:58.367] Evaluation Started ==>
[13:15:09.706] ==> valid iteration 12900: unet metrics: {'dc': 0.6823513714257766, 'jc': 0.565875491258458, 'pre': 0.7605403067409957, 'hd': 5.640266131551039}, ynet metrics: {'dc': 0.6138707591567253, 'jc': 0.49536594981042936, 'pre': 0.7762289067257745, 'hd': 5.735667391869419}.
[13:15:09.765] ==> New best valid dice for unet: 0.682351, at iteration 12900
[13:15:09.767] Evaluation Finished!⏹️
[13:15:09.920] iteration 12901: total_loss: 0.423309, loss_sup: 0.070806, loss_mps: 0.111119, loss_cps: 0.241384
[13:15:10.070] iteration 12902: total_loss: 0.393548, loss_sup: 0.083242, loss_mps: 0.104357, loss_cps: 0.205949
[13:15:10.216] iteration 12903: total_loss: 0.586061, loss_sup: 0.124233, loss_mps: 0.146235, loss_cps: 0.315593
[13:15:10.360] iteration 12904: total_loss: 0.504045, loss_sup: 0.263278, loss_mps: 0.091475, loss_cps: 0.149291
[13:15:10.506] iteration 12905: total_loss: 0.461036, loss_sup: 0.085502, loss_mps: 0.123715, loss_cps: 0.251818
[13:15:10.652] iteration 12906: total_loss: 0.336934, loss_sup: 0.144444, loss_mps: 0.068704, loss_cps: 0.123786
[13:15:10.799] iteration 12907: total_loss: 0.345234, loss_sup: 0.082076, loss_mps: 0.091620, loss_cps: 0.171539
[13:15:10.945] iteration 12908: total_loss: 0.274755, loss_sup: 0.033887, loss_mps: 0.084327, loss_cps: 0.156541
[13:15:11.093] iteration 12909: total_loss: 0.465486, loss_sup: 0.090438, loss_mps: 0.126317, loss_cps: 0.248732
[13:15:11.238] iteration 12910: total_loss: 0.537597, loss_sup: 0.121853, loss_mps: 0.130340, loss_cps: 0.285405
[13:15:11.384] iteration 12911: total_loss: 0.533977, loss_sup: 0.022247, loss_mps: 0.155347, loss_cps: 0.356382
[13:15:11.530] iteration 12912: total_loss: 0.319949, loss_sup: 0.050390, loss_mps: 0.095760, loss_cps: 0.173800
[13:15:11.675] iteration 12913: total_loss: 0.288007, loss_sup: 0.039418, loss_mps: 0.084864, loss_cps: 0.163725
[13:15:11.821] iteration 12914: total_loss: 0.447201, loss_sup: 0.199340, loss_mps: 0.087112, loss_cps: 0.160749
[13:15:11.972] iteration 12915: total_loss: 0.395140, loss_sup: 0.013802, loss_mps: 0.122317, loss_cps: 0.259022
[13:15:12.117] iteration 12916: total_loss: 0.231426, loss_sup: 0.045816, loss_mps: 0.067337, loss_cps: 0.118273
[13:15:12.264] iteration 12917: total_loss: 0.264774, loss_sup: 0.030215, loss_mps: 0.081039, loss_cps: 0.153519
[13:15:12.410] iteration 12918: total_loss: 0.450949, loss_sup: 0.236790, loss_mps: 0.073617, loss_cps: 0.140542
[13:15:12.556] iteration 12919: total_loss: 0.727923, loss_sup: 0.127479, loss_mps: 0.182605, loss_cps: 0.417839
[13:15:12.702] iteration 12920: total_loss: 0.354092, loss_sup: 0.058725, loss_mps: 0.091939, loss_cps: 0.203428
[13:15:12.848] iteration 12921: total_loss: 0.557169, loss_sup: 0.245065, loss_mps: 0.104320, loss_cps: 0.207784
[13:15:12.993] iteration 12922: total_loss: 0.430808, loss_sup: 0.071009, loss_mps: 0.111293, loss_cps: 0.248506
[13:15:13.142] iteration 12923: total_loss: 0.396143, loss_sup: 0.048273, loss_mps: 0.114414, loss_cps: 0.233456
[13:15:13.287] iteration 12924: total_loss: 0.336126, loss_sup: 0.104562, loss_mps: 0.079060, loss_cps: 0.152504
[13:15:13.435] iteration 12925: total_loss: 0.398180, loss_sup: 0.092480, loss_mps: 0.101898, loss_cps: 0.203802
[13:15:13.581] iteration 12926: total_loss: 0.397002, loss_sup: 0.039242, loss_mps: 0.111401, loss_cps: 0.246359
[13:15:13.727] iteration 12927: total_loss: 0.257304, loss_sup: 0.081610, loss_mps: 0.065938, loss_cps: 0.109756
[13:15:13.872] iteration 12928: total_loss: 0.364328, loss_sup: 0.015999, loss_mps: 0.114234, loss_cps: 0.234095
[13:15:14.019] iteration 12929: total_loss: 0.298512, loss_sup: 0.121233, loss_mps: 0.067132, loss_cps: 0.110146
[13:15:14.166] iteration 12930: total_loss: 0.463269, loss_sup: 0.034336, loss_mps: 0.139592, loss_cps: 0.289341
[13:15:14.311] iteration 12931: total_loss: 0.489861, loss_sup: 0.161033, loss_mps: 0.106847, loss_cps: 0.221981
[13:15:14.459] iteration 12932: total_loss: 0.317869, loss_sup: 0.034028, loss_mps: 0.098360, loss_cps: 0.185482
[13:15:14.605] iteration 12933: total_loss: 0.307537, loss_sup: 0.063571, loss_mps: 0.087255, loss_cps: 0.156711
[13:15:14.750] iteration 12934: total_loss: 0.366774, loss_sup: 0.079170, loss_mps: 0.099831, loss_cps: 0.187773
[13:15:14.896] iteration 12935: total_loss: 0.231331, loss_sup: 0.059744, loss_mps: 0.061404, loss_cps: 0.110182
[13:15:15.042] iteration 12936: total_loss: 0.334789, loss_sup: 0.038179, loss_mps: 0.099351, loss_cps: 0.197260
[13:15:15.187] iteration 12937: total_loss: 0.746040, loss_sup: 0.048384, loss_mps: 0.210125, loss_cps: 0.487530
[13:15:15.334] iteration 12938: total_loss: 0.657845, loss_sup: 0.228924, loss_mps: 0.147967, loss_cps: 0.280954
[13:15:15.483] iteration 12939: total_loss: 0.547065, loss_sup: 0.084275, loss_mps: 0.159044, loss_cps: 0.303747
[13:15:15.630] iteration 12940: total_loss: 0.304975, loss_sup: 0.082056, loss_mps: 0.079058, loss_cps: 0.143862
[13:15:15.775] iteration 12941: total_loss: 0.440115, loss_sup: 0.061114, loss_mps: 0.127198, loss_cps: 0.251803
[13:15:15.921] iteration 12942: total_loss: 0.400614, loss_sup: 0.060279, loss_mps: 0.111827, loss_cps: 0.228508
[13:15:16.068] iteration 12943: total_loss: 0.224165, loss_sup: 0.038515, loss_mps: 0.065984, loss_cps: 0.119665
[13:15:16.214] iteration 12944: total_loss: 0.496171, loss_sup: 0.108529, loss_mps: 0.128619, loss_cps: 0.259023
[13:15:16.359] iteration 12945: total_loss: 0.202094, loss_sup: 0.018915, loss_mps: 0.063776, loss_cps: 0.119403
[13:15:16.505] iteration 12946: total_loss: 0.247851, loss_sup: 0.049523, loss_mps: 0.068546, loss_cps: 0.129781
[13:15:16.651] iteration 12947: total_loss: 0.296842, loss_sup: 0.081556, loss_mps: 0.076388, loss_cps: 0.138897
[13:15:16.797] iteration 12948: total_loss: 0.345199, loss_sup: 0.070414, loss_mps: 0.093555, loss_cps: 0.181231
[13:15:16.942] iteration 12949: total_loss: 0.430301, loss_sup: 0.062514, loss_mps: 0.118064, loss_cps: 0.249724
[13:15:17.088] iteration 12950: total_loss: 0.422747, loss_sup: 0.082822, loss_mps: 0.112021, loss_cps: 0.227903
[13:15:17.234] iteration 12951: total_loss: 0.282919, loss_sup: 0.019993, loss_mps: 0.091519, loss_cps: 0.171407
[13:15:17.379] iteration 12952: total_loss: 0.308273, loss_sup: 0.059287, loss_mps: 0.089288, loss_cps: 0.159698
[13:15:17.525] iteration 12953: total_loss: 0.558523, loss_sup: 0.219744, loss_mps: 0.117659, loss_cps: 0.221120
[13:15:17.671] iteration 12954: total_loss: 0.366967, loss_sup: 0.110530, loss_mps: 0.086575, loss_cps: 0.169862
[13:15:17.817] iteration 12955: total_loss: 0.598821, loss_sup: 0.225101, loss_mps: 0.121232, loss_cps: 0.252487
[13:15:17.963] iteration 12956: total_loss: 0.611829, loss_sup: 0.115542, loss_mps: 0.158025, loss_cps: 0.338262
[13:15:18.108] iteration 12957: total_loss: 0.329444, loss_sup: 0.117913, loss_mps: 0.072776, loss_cps: 0.138755
[13:15:18.170] iteration 12958: total_loss: 0.199978, loss_sup: 0.008174, loss_mps: 0.070191, loss_cps: 0.121613
[13:15:19.375] iteration 12959: total_loss: 0.473566, loss_sup: 0.194506, loss_mps: 0.093012, loss_cps: 0.186048
[13:15:19.525] iteration 12960: total_loss: 0.377542, loss_sup: 0.067451, loss_mps: 0.105200, loss_cps: 0.204891
[13:15:19.674] iteration 12961: total_loss: 0.248411, loss_sup: 0.030487, loss_mps: 0.075454, loss_cps: 0.142470
[13:15:19.821] iteration 12962: total_loss: 0.442261, loss_sup: 0.086859, loss_mps: 0.118569, loss_cps: 0.236832
[13:15:19.968] iteration 12963: total_loss: 0.634200, loss_sup: 0.126428, loss_mps: 0.159374, loss_cps: 0.348398
[13:15:20.115] iteration 12964: total_loss: 0.341749, loss_sup: 0.035060, loss_mps: 0.103257, loss_cps: 0.203432
[13:15:20.261] iteration 12965: total_loss: 0.312337, loss_sup: 0.043529, loss_mps: 0.093538, loss_cps: 0.175270
[13:15:20.413] iteration 12966: total_loss: 0.382772, loss_sup: 0.012160, loss_mps: 0.118304, loss_cps: 0.252308
[13:15:20.560] iteration 12967: total_loss: 0.407228, loss_sup: 0.022529, loss_mps: 0.128356, loss_cps: 0.256343
[13:15:20.708] iteration 12968: total_loss: 0.440918, loss_sup: 0.166053, loss_mps: 0.095249, loss_cps: 0.179617
[13:15:20.855] iteration 12969: total_loss: 0.269671, loss_sup: 0.029182, loss_mps: 0.087606, loss_cps: 0.152883
[13:15:21.001] iteration 12970: total_loss: 0.289100, loss_sup: 0.082992, loss_mps: 0.074677, loss_cps: 0.131432
[13:15:21.148] iteration 12971: total_loss: 0.346646, loss_sup: 0.076341, loss_mps: 0.090706, loss_cps: 0.179598
[13:15:21.293] iteration 12972: total_loss: 0.415022, loss_sup: 0.103823, loss_mps: 0.107832, loss_cps: 0.203367
[13:15:21.439] iteration 12973: total_loss: 0.504338, loss_sup: 0.126330, loss_mps: 0.124308, loss_cps: 0.253700
[13:15:21.585] iteration 12974: total_loss: 0.363593, loss_sup: 0.057831, loss_mps: 0.105145, loss_cps: 0.200617
[13:15:21.734] iteration 12975: total_loss: 0.241204, loss_sup: 0.039569, loss_mps: 0.073295, loss_cps: 0.128340
[13:15:21.880] iteration 12976: total_loss: 0.300904, loss_sup: 0.098903, loss_mps: 0.074560, loss_cps: 0.127441
[13:15:22.030] iteration 12977: total_loss: 0.361996, loss_sup: 0.064526, loss_mps: 0.100460, loss_cps: 0.197010
[13:15:22.176] iteration 12978: total_loss: 0.340643, loss_sup: 0.056053, loss_mps: 0.093099, loss_cps: 0.191491
[13:15:22.323] iteration 12979: total_loss: 0.357833, loss_sup: 0.152586, loss_mps: 0.074306, loss_cps: 0.130941
[13:15:22.469] iteration 12980: total_loss: 0.292875, loss_sup: 0.052303, loss_mps: 0.086180, loss_cps: 0.154392
[13:15:22.615] iteration 12981: total_loss: 0.360856, loss_sup: 0.065826, loss_mps: 0.100835, loss_cps: 0.194194
[13:15:22.766] iteration 12982: total_loss: 0.359156, loss_sup: 0.041070, loss_mps: 0.104855, loss_cps: 0.213231
[13:15:22.912] iteration 12983: total_loss: 0.244500, loss_sup: 0.066295, loss_mps: 0.066857, loss_cps: 0.111348
[13:15:23.058] iteration 12984: total_loss: 0.486915, loss_sup: 0.159223, loss_mps: 0.110222, loss_cps: 0.217469
[13:15:23.204] iteration 12985: total_loss: 0.357031, loss_sup: 0.126766, loss_mps: 0.082028, loss_cps: 0.148237
[13:15:23.350] iteration 12986: total_loss: 0.269555, loss_sup: 0.023583, loss_mps: 0.085482, loss_cps: 0.160489
[13:15:23.495] iteration 12987: total_loss: 0.360524, loss_sup: 0.014882, loss_mps: 0.112334, loss_cps: 0.233308
[13:15:23.642] iteration 12988: total_loss: 0.295846, loss_sup: 0.051585, loss_mps: 0.087517, loss_cps: 0.156744
[13:15:23.788] iteration 12989: total_loss: 0.152975, loss_sup: 0.005721, loss_mps: 0.052758, loss_cps: 0.094496
[13:15:23.935] iteration 12990: total_loss: 0.421559, loss_sup: 0.048301, loss_mps: 0.126980, loss_cps: 0.246277
[13:15:24.081] iteration 12991: total_loss: 0.226777, loss_sup: 0.033036, loss_mps: 0.069455, loss_cps: 0.124285
[13:15:24.227] iteration 12992: total_loss: 0.543954, loss_sup: 0.021323, loss_mps: 0.164239, loss_cps: 0.358391
[13:15:24.373] iteration 12993: total_loss: 0.435297, loss_sup: 0.063382, loss_mps: 0.125290, loss_cps: 0.246625
[13:15:24.518] iteration 12994: total_loss: 0.350851, loss_sup: 0.034053, loss_mps: 0.107545, loss_cps: 0.209253
[13:15:24.665] iteration 12995: total_loss: 0.261312, loss_sup: 0.055438, loss_mps: 0.074020, loss_cps: 0.131854
[13:15:24.814] iteration 12996: total_loss: 0.478157, loss_sup: 0.132240, loss_mps: 0.110840, loss_cps: 0.235077
[13:15:24.962] iteration 12997: total_loss: 0.361521, loss_sup: 0.120724, loss_mps: 0.084481, loss_cps: 0.156317
[13:15:25.108] iteration 12998: total_loss: 0.412344, loss_sup: 0.103502, loss_mps: 0.096594, loss_cps: 0.212248
[13:15:25.255] iteration 12999: total_loss: 0.448974, loss_sup: 0.091646, loss_mps: 0.111943, loss_cps: 0.245385
[13:15:25.401] iteration 13000: total_loss: 0.394373, loss_sup: 0.062307, loss_mps: 0.109240, loss_cps: 0.222825
[13:15:25.401] Evaluation Started ==>
[13:15:36.707] ==> valid iteration 13000: unet metrics: {'dc': 0.6647452812826651, 'jc': 0.54597093141766, 'pre': 0.7556316341094603, 'hd': 5.631030535429894}, ynet metrics: {'dc': 0.6247266118204303, 'jc': 0.5093104954606132, 'pre': 0.769137930909563, 'hd': 5.639975997207624}.
[13:15:36.709] Evaluation Finished!⏹️
[13:15:36.861] iteration 13001: total_loss: 0.333490, loss_sup: 0.017880, loss_mps: 0.102016, loss_cps: 0.213593
[13:15:37.010] iteration 13002: total_loss: 0.248068, loss_sup: 0.019626, loss_mps: 0.074107, loss_cps: 0.154335
[13:15:37.155] iteration 13003: total_loss: 0.350694, loss_sup: 0.121321, loss_mps: 0.075705, loss_cps: 0.153668
[13:15:37.301] iteration 13004: total_loss: 0.356691, loss_sup: 0.073899, loss_mps: 0.088846, loss_cps: 0.193946
[13:15:37.447] iteration 13005: total_loss: 0.704797, loss_sup: 0.082300, loss_mps: 0.184003, loss_cps: 0.438494
[13:15:37.592] iteration 13006: total_loss: 0.166879, loss_sup: 0.014501, loss_mps: 0.057855, loss_cps: 0.094523
[13:15:37.739] iteration 13007: total_loss: 0.276878, loss_sup: 0.058334, loss_mps: 0.079545, loss_cps: 0.139000
[13:15:37.885] iteration 13008: total_loss: 0.254689, loss_sup: 0.045929, loss_mps: 0.076165, loss_cps: 0.132595
[13:15:38.033] iteration 13009: total_loss: 0.381498, loss_sup: 0.072751, loss_mps: 0.102298, loss_cps: 0.206449
[13:15:38.181] iteration 13010: total_loss: 0.404658, loss_sup: 0.102230, loss_mps: 0.100263, loss_cps: 0.202166
[13:15:38.326] iteration 13011: total_loss: 0.262581, loss_sup: 0.080971, loss_mps: 0.066113, loss_cps: 0.115498
[13:15:38.473] iteration 13012: total_loss: 0.203446, loss_sup: 0.038549, loss_mps: 0.060696, loss_cps: 0.104200
[13:15:38.618] iteration 13013: total_loss: 0.273610, loss_sup: 0.010583, loss_mps: 0.091175, loss_cps: 0.171852
[13:15:38.764] iteration 13014: total_loss: 0.331115, loss_sup: 0.064973, loss_mps: 0.089160, loss_cps: 0.176983
[13:15:38.910] iteration 13015: total_loss: 0.318548, loss_sup: 0.031879, loss_mps: 0.093567, loss_cps: 0.193103
[13:15:39.059] iteration 13016: total_loss: 0.556150, loss_sup: 0.045857, loss_mps: 0.154732, loss_cps: 0.355561
[13:15:39.206] iteration 13017: total_loss: 0.238447, loss_sup: 0.050245, loss_mps: 0.069893, loss_cps: 0.118310
[13:15:39.352] iteration 13018: total_loss: 0.256950, loss_sup: 0.105928, loss_mps: 0.057287, loss_cps: 0.093735
[13:15:39.498] iteration 13019: total_loss: 0.230386, loss_sup: 0.089833, loss_mps: 0.050184, loss_cps: 0.090369
[13:15:39.644] iteration 13020: total_loss: 0.294023, loss_sup: 0.020974, loss_mps: 0.094254, loss_cps: 0.178795
[13:15:39.791] iteration 13021: total_loss: 0.434052, loss_sup: 0.146962, loss_mps: 0.098519, loss_cps: 0.188571
[13:15:39.938] iteration 13022: total_loss: 0.401908, loss_sup: 0.093652, loss_mps: 0.107462, loss_cps: 0.200794
[13:15:40.083] iteration 13023: total_loss: 0.599647, loss_sup: 0.340971, loss_mps: 0.086854, loss_cps: 0.171822
[13:15:40.229] iteration 13024: total_loss: 0.360465, loss_sup: 0.082216, loss_mps: 0.095378, loss_cps: 0.182871
[13:15:40.376] iteration 13025: total_loss: 0.272412, loss_sup: 0.038551, loss_mps: 0.076943, loss_cps: 0.156918
[13:15:40.525] iteration 13026: total_loss: 0.426522, loss_sup: 0.087946, loss_mps: 0.115167, loss_cps: 0.223409
[13:15:40.670] iteration 13027: total_loss: 0.337147, loss_sup: 0.068549, loss_mps: 0.090298, loss_cps: 0.178300
[13:15:40.816] iteration 13028: total_loss: 0.615553, loss_sup: 0.338614, loss_mps: 0.094092, loss_cps: 0.182848
[13:15:40.962] iteration 13029: total_loss: 0.317853, loss_sup: 0.035508, loss_mps: 0.096566, loss_cps: 0.185780
[13:15:41.109] iteration 13030: total_loss: 0.414491, loss_sup: 0.074235, loss_mps: 0.111036, loss_cps: 0.229220
[13:15:41.255] iteration 13031: total_loss: 0.270006, loss_sup: 0.025585, loss_mps: 0.086421, loss_cps: 0.158000
[13:15:41.401] iteration 13032: total_loss: 0.516275, loss_sup: 0.206889, loss_mps: 0.104797, loss_cps: 0.204589
[13:15:41.546] iteration 13033: total_loss: 0.311533, loss_sup: 0.052080, loss_mps: 0.086015, loss_cps: 0.173438
[13:15:41.694] iteration 13034: total_loss: 0.288863, loss_sup: 0.046274, loss_mps: 0.087550, loss_cps: 0.155039
[13:15:41.840] iteration 13035: total_loss: 0.658305, loss_sup: 0.118160, loss_mps: 0.172962, loss_cps: 0.367183
[13:15:41.986] iteration 13036: total_loss: 0.516765, loss_sup: 0.200273, loss_mps: 0.105073, loss_cps: 0.211420
[13:15:42.131] iteration 13037: total_loss: 0.579802, loss_sup: 0.187653, loss_mps: 0.124971, loss_cps: 0.267178
[13:15:42.279] iteration 13038: total_loss: 0.683208, loss_sup: 0.220956, loss_mps: 0.148818, loss_cps: 0.313434
[13:15:42.424] iteration 13039: total_loss: 0.190240, loss_sup: 0.037515, loss_mps: 0.061796, loss_cps: 0.090929
[13:15:42.569] iteration 13040: total_loss: 0.329928, loss_sup: 0.051500, loss_mps: 0.094549, loss_cps: 0.183878
[13:15:42.715] iteration 13041: total_loss: 0.294809, loss_sup: 0.091908, loss_mps: 0.074909, loss_cps: 0.127992
[13:15:42.862] iteration 13042: total_loss: 0.542675, loss_sup: 0.122707, loss_mps: 0.135453, loss_cps: 0.284515
[13:15:43.009] iteration 13043: total_loss: 0.419929, loss_sup: 0.126014, loss_mps: 0.102848, loss_cps: 0.191066
[13:15:43.155] iteration 13044: total_loss: 0.252008, loss_sup: 0.027856, loss_mps: 0.079276, loss_cps: 0.144876
[13:15:43.301] iteration 13045: total_loss: 0.400688, loss_sup: 0.106016, loss_mps: 0.103195, loss_cps: 0.191477
[13:15:43.454] iteration 13046: total_loss: 0.320523, loss_sup: 0.051813, loss_mps: 0.091685, loss_cps: 0.177026
[13:15:43.600] iteration 13047: total_loss: 0.398544, loss_sup: 0.080577, loss_mps: 0.106709, loss_cps: 0.211259
[13:15:43.747] iteration 13048: total_loss: 0.411992, loss_sup: 0.090291, loss_mps: 0.112773, loss_cps: 0.208927
[13:15:43.894] iteration 13049: total_loss: 0.398604, loss_sup: 0.022067, loss_mps: 0.120695, loss_cps: 0.255842
[13:15:44.040] iteration 13050: total_loss: 0.371280, loss_sup: 0.068623, loss_mps: 0.100267, loss_cps: 0.202390
[13:15:44.185] iteration 13051: total_loss: 0.445175, loss_sup: 0.119227, loss_mps: 0.112254, loss_cps: 0.213694
[13:15:44.330] iteration 13052: total_loss: 0.543142, loss_sup: 0.284287, loss_mps: 0.092163, loss_cps: 0.166693
[13:15:44.477] iteration 13053: total_loss: 0.222352, loss_sup: 0.033652, loss_mps: 0.070129, loss_cps: 0.118571
[13:15:44.622] iteration 13054: total_loss: 0.590528, loss_sup: 0.197961, loss_mps: 0.131170, loss_cps: 0.261397
[13:15:44.768] iteration 13055: total_loss: 0.422954, loss_sup: 0.065827, loss_mps: 0.114498, loss_cps: 0.242629
[13:15:44.913] iteration 13056: total_loss: 0.265555, loss_sup: 0.028017, loss_mps: 0.084074, loss_cps: 0.153464
[13:15:45.059] iteration 13057: total_loss: 0.339796, loss_sup: 0.040528, loss_mps: 0.105384, loss_cps: 0.193884
[13:15:45.204] iteration 13058: total_loss: 0.319160, loss_sup: 0.057455, loss_mps: 0.093431, loss_cps: 0.168274
[13:15:45.350] iteration 13059: total_loss: 0.203693, loss_sup: 0.038570, loss_mps: 0.060767, loss_cps: 0.104357
[13:15:45.495] iteration 13060: total_loss: 0.240522, loss_sup: 0.037476, loss_mps: 0.073067, loss_cps: 0.129979
[13:15:45.645] iteration 13061: total_loss: 0.283401, loss_sup: 0.067466, loss_mps: 0.079588, loss_cps: 0.136347
[13:15:45.790] iteration 13062: total_loss: 0.226568, loss_sup: 0.019715, loss_mps: 0.071579, loss_cps: 0.135274
[13:15:45.936] iteration 13063: total_loss: 0.364231, loss_sup: 0.118453, loss_mps: 0.084508, loss_cps: 0.161270
[13:15:46.083] iteration 13064: total_loss: 0.383056, loss_sup: 0.125525, loss_mps: 0.086182, loss_cps: 0.171348
[13:15:46.230] iteration 13065: total_loss: 0.384122, loss_sup: 0.088467, loss_mps: 0.101102, loss_cps: 0.194554
[13:15:46.375] iteration 13066: total_loss: 0.429137, loss_sup: 0.199836, loss_mps: 0.078254, loss_cps: 0.151046
[13:15:46.521] iteration 13067: total_loss: 0.304229, loss_sup: 0.064791, loss_mps: 0.085087, loss_cps: 0.154351
[13:15:46.667] iteration 13068: total_loss: 0.465224, loss_sup: 0.216070, loss_mps: 0.085007, loss_cps: 0.164147
[13:15:46.814] iteration 13069: total_loss: 0.381458, loss_sup: 0.039709, loss_mps: 0.114022, loss_cps: 0.227727
[13:15:46.960] iteration 13070: total_loss: 0.198097, loss_sup: 0.027526, loss_mps: 0.060229, loss_cps: 0.110341
[13:15:47.105] iteration 13071: total_loss: 0.347486, loss_sup: 0.034465, loss_mps: 0.098683, loss_cps: 0.214338
[13:15:47.251] iteration 13072: total_loss: 0.331272, loss_sup: 0.033510, loss_mps: 0.099252, loss_cps: 0.198510
[13:15:47.398] iteration 13073: total_loss: 0.318786, loss_sup: 0.107737, loss_mps: 0.075079, loss_cps: 0.135970
[13:15:47.543] iteration 13074: total_loss: 0.410587, loss_sup: 0.070660, loss_mps: 0.111771, loss_cps: 0.228156
[13:15:47.689] iteration 13075: total_loss: 0.267060, loss_sup: 0.086892, loss_mps: 0.070208, loss_cps: 0.109960
[13:15:47.837] iteration 13076: total_loss: 0.346599, loss_sup: 0.023309, loss_mps: 0.107066, loss_cps: 0.216225
[13:15:47.982] iteration 13077: total_loss: 0.592665, loss_sup: 0.098498, loss_mps: 0.158447, loss_cps: 0.335720
[13:15:48.128] iteration 13078: total_loss: 0.343746, loss_sup: 0.067177, loss_mps: 0.088465, loss_cps: 0.188104
[13:15:48.275] iteration 13079: total_loss: 0.319992, loss_sup: 0.017376, loss_mps: 0.097541, loss_cps: 0.205075
[13:15:48.421] iteration 13080: total_loss: 0.573623, loss_sup: 0.127481, loss_mps: 0.145102, loss_cps: 0.301040
[13:15:48.566] iteration 13081: total_loss: 0.186813, loss_sup: 0.010151, loss_mps: 0.066147, loss_cps: 0.110514
[13:15:48.712] iteration 13082: total_loss: 0.338894, loss_sup: 0.066600, loss_mps: 0.092977, loss_cps: 0.179317
[13:15:48.859] iteration 13083: total_loss: 0.593048, loss_sup: 0.167197, loss_mps: 0.134549, loss_cps: 0.291302
[13:15:49.005] iteration 13084: total_loss: 0.421834, loss_sup: 0.159480, loss_mps: 0.089188, loss_cps: 0.173166
[13:15:49.150] iteration 13085: total_loss: 0.329917, loss_sup: 0.040376, loss_mps: 0.099372, loss_cps: 0.190169
[13:15:49.296] iteration 13086: total_loss: 0.287601, loss_sup: 0.044172, loss_mps: 0.085408, loss_cps: 0.158021
[13:15:49.443] iteration 13087: total_loss: 0.292517, loss_sup: 0.055310, loss_mps: 0.091025, loss_cps: 0.146182
[13:15:49.592] iteration 13088: total_loss: 0.260081, loss_sup: 0.099839, loss_mps: 0.058526, loss_cps: 0.101716
[13:15:49.739] iteration 13089: total_loss: 0.313592, loss_sup: 0.129007, loss_mps: 0.067856, loss_cps: 0.116728
[13:15:49.886] iteration 13090: total_loss: 0.172023, loss_sup: 0.019831, loss_mps: 0.056316, loss_cps: 0.095876
[13:15:50.032] iteration 13091: total_loss: 0.330895, loss_sup: 0.079326, loss_mps: 0.088311, loss_cps: 0.163259
[13:15:50.178] iteration 13092: total_loss: 0.687905, loss_sup: 0.374991, loss_mps: 0.101434, loss_cps: 0.211481
[13:15:50.324] iteration 13093: total_loss: 0.278182, loss_sup: 0.065077, loss_mps: 0.075545, loss_cps: 0.137560
[13:15:50.471] iteration 13094: total_loss: 0.235309, loss_sup: 0.018682, loss_mps: 0.070522, loss_cps: 0.146105
[13:15:50.617] iteration 13095: total_loss: 0.253541, loss_sup: 0.043618, loss_mps: 0.074242, loss_cps: 0.135681
[13:15:50.766] iteration 13096: total_loss: 0.284552, loss_sup: 0.029567, loss_mps: 0.089752, loss_cps: 0.165232
[13:15:50.913] iteration 13097: total_loss: 0.336448, loss_sup: 0.080187, loss_mps: 0.088432, loss_cps: 0.167829
[13:15:51.060] iteration 13098: total_loss: 0.588224, loss_sup: 0.043758, loss_mps: 0.171510, loss_cps: 0.372956
[13:15:51.207] iteration 13099: total_loss: 0.287677, loss_sup: 0.069614, loss_mps: 0.077694, loss_cps: 0.140369
[13:15:51.354] iteration 13100: total_loss: 0.286428, loss_sup: 0.021976, loss_mps: 0.093152, loss_cps: 0.171299
[13:15:51.354] Evaluation Started ==>
[13:16:02.727] ==> valid iteration 13100: unet metrics: {'dc': 0.6423894204462278, 'jc': 0.5233453918496228, 'pre': 0.7468838806045348, 'hd': 5.784533834127362}, ynet metrics: {'dc': 0.6067524244336632, 'jc': 0.4869264184901182, 'pre': 0.7888677327311269, 'hd': 5.661191628066535}.
[13:16:02.730] Evaluation Finished!⏹️
[13:16:02.883] iteration 13101: total_loss: 0.198592, loss_sup: 0.025466, loss_mps: 0.064964, loss_cps: 0.108161
[13:16:03.034] iteration 13102: total_loss: 0.275024, loss_sup: 0.029451, loss_mps: 0.084384, loss_cps: 0.161189
[13:16:03.183] iteration 13103: total_loss: 0.598484, loss_sup: 0.183031, loss_mps: 0.131234, loss_cps: 0.284219
[13:16:03.328] iteration 13104: total_loss: 0.362022, loss_sup: 0.056989, loss_mps: 0.103161, loss_cps: 0.201872
[13:16:03.475] iteration 13105: total_loss: 0.548756, loss_sup: 0.059411, loss_mps: 0.159801, loss_cps: 0.329544
[13:16:03.622] iteration 13106: total_loss: 0.241820, loss_sup: 0.007377, loss_mps: 0.079583, loss_cps: 0.154860
[13:16:03.771] iteration 13107: total_loss: 0.307858, loss_sup: 0.021215, loss_mps: 0.102214, loss_cps: 0.184429
[13:16:03.917] iteration 13108: total_loss: 0.208325, loss_sup: 0.011719, loss_mps: 0.071224, loss_cps: 0.125382
[13:16:04.063] iteration 13109: total_loss: 0.465947, loss_sup: 0.134331, loss_mps: 0.113492, loss_cps: 0.218124
[13:16:04.208] iteration 13110: total_loss: 0.392388, loss_sup: 0.102202, loss_mps: 0.099322, loss_cps: 0.190864
[13:16:04.354] iteration 13111: total_loss: 0.500374, loss_sup: 0.205160, loss_mps: 0.099902, loss_cps: 0.195312
[13:16:04.502] iteration 13112: total_loss: 0.415588, loss_sup: 0.115731, loss_mps: 0.103308, loss_cps: 0.196549
[13:16:04.648] iteration 13113: total_loss: 0.227338, loss_sup: 0.030659, loss_mps: 0.071112, loss_cps: 0.125567
[13:16:04.794] iteration 13114: total_loss: 0.374164, loss_sup: 0.080855, loss_mps: 0.101902, loss_cps: 0.191407
[13:16:04.939] iteration 13115: total_loss: 0.359686, loss_sup: 0.123039, loss_mps: 0.082269, loss_cps: 0.154378
[13:16:05.088] iteration 13116: total_loss: 0.360281, loss_sup: 0.043391, loss_mps: 0.102387, loss_cps: 0.214504
[13:16:05.236] iteration 13117: total_loss: 0.519393, loss_sup: 0.173281, loss_mps: 0.115623, loss_cps: 0.230490
[13:16:05.382] iteration 13118: total_loss: 0.515589, loss_sup: 0.208479, loss_mps: 0.106677, loss_cps: 0.200432
[13:16:05.528] iteration 13119: total_loss: 0.360315, loss_sup: 0.068809, loss_mps: 0.096827, loss_cps: 0.194679
[13:16:05.674] iteration 13120: total_loss: 0.276776, loss_sup: 0.018970, loss_mps: 0.092686, loss_cps: 0.165120
[13:16:05.823] iteration 13121: total_loss: 0.368681, loss_sup: 0.083288, loss_mps: 0.099905, loss_cps: 0.185489
[13:16:05.969] iteration 13122: total_loss: 0.356286, loss_sup: 0.015626, loss_mps: 0.111369, loss_cps: 0.229291
[13:16:06.115] iteration 13123: total_loss: 0.225042, loss_sup: 0.019605, loss_mps: 0.070151, loss_cps: 0.135286
[13:16:06.260] iteration 13124: total_loss: 0.190014, loss_sup: 0.009981, loss_mps: 0.067298, loss_cps: 0.112734
[13:16:06.410] iteration 13125: total_loss: 0.308755, loss_sup: 0.151988, loss_mps: 0.057810, loss_cps: 0.098957
[13:16:06.557] iteration 13126: total_loss: 0.200171, loss_sup: 0.045377, loss_mps: 0.054959, loss_cps: 0.099834
[13:16:06.704] iteration 13127: total_loss: 0.321694, loss_sup: 0.126570, loss_mps: 0.070335, loss_cps: 0.124789
[13:16:06.850] iteration 13128: total_loss: 0.311161, loss_sup: 0.137964, loss_mps: 0.065184, loss_cps: 0.108013
[13:16:06.996] iteration 13129: total_loss: 0.185314, loss_sup: 0.027676, loss_mps: 0.057850, loss_cps: 0.099787
[13:16:07.142] iteration 13130: total_loss: 0.526582, loss_sup: 0.016707, loss_mps: 0.153268, loss_cps: 0.356607
[13:16:07.288] iteration 13131: total_loss: 0.151118, loss_sup: 0.010359, loss_mps: 0.051862, loss_cps: 0.088897
[13:16:07.433] iteration 13132: total_loss: 0.349903, loss_sup: 0.052176, loss_mps: 0.096518, loss_cps: 0.201210
[13:16:07.580] iteration 13133: total_loss: 0.206143, loss_sup: 0.017327, loss_mps: 0.069641, loss_cps: 0.119175
[13:16:07.725] iteration 13134: total_loss: 0.343037, loss_sup: 0.075367, loss_mps: 0.088936, loss_cps: 0.178734
[13:16:07.871] iteration 13135: total_loss: 0.199492, loss_sup: 0.020043, loss_mps: 0.065880, loss_cps: 0.113569
[13:16:08.017] iteration 13136: total_loss: 0.473332, loss_sup: 0.148091, loss_mps: 0.109653, loss_cps: 0.215588
[13:16:08.162] iteration 13137: total_loss: 0.430050, loss_sup: 0.087721, loss_mps: 0.111548, loss_cps: 0.230781
[13:16:08.308] iteration 13138: total_loss: 0.445899, loss_sup: 0.117943, loss_mps: 0.107341, loss_cps: 0.220616
[13:16:08.454] iteration 13139: total_loss: 0.258055, loss_sup: 0.020223, loss_mps: 0.083924, loss_cps: 0.153907
[13:16:08.600] iteration 13140: total_loss: 0.258719, loss_sup: 0.005308, loss_mps: 0.086865, loss_cps: 0.166546
[13:16:08.746] iteration 13141: total_loss: 0.299197, loss_sup: 0.097447, loss_mps: 0.069692, loss_cps: 0.132058
[13:16:08.892] iteration 13142: total_loss: 0.258290, loss_sup: 0.014993, loss_mps: 0.087911, loss_cps: 0.155386
[13:16:09.038] iteration 13143: total_loss: 0.212825, loss_sup: 0.052898, loss_mps: 0.059282, loss_cps: 0.100645
[13:16:09.184] iteration 13144: total_loss: 0.254071, loss_sup: 0.054103, loss_mps: 0.071176, loss_cps: 0.128792
[13:16:09.330] iteration 13145: total_loss: 0.346690, loss_sup: 0.026077, loss_mps: 0.107662, loss_cps: 0.212951
[13:16:09.476] iteration 13146: total_loss: 0.360194, loss_sup: 0.023041, loss_mps: 0.110064, loss_cps: 0.227089
[13:16:09.622] iteration 13147: total_loss: 0.312094, loss_sup: 0.016127, loss_mps: 0.102986, loss_cps: 0.192982
[13:16:09.768] iteration 13148: total_loss: 0.530840, loss_sup: 0.042084, loss_mps: 0.155433, loss_cps: 0.333323
[13:16:09.914] iteration 13149: total_loss: 0.537436, loss_sup: 0.157841, loss_mps: 0.121449, loss_cps: 0.258146
[13:16:10.061] iteration 13150: total_loss: 0.331363, loss_sup: 0.045508, loss_mps: 0.094371, loss_cps: 0.191484
[13:16:10.207] iteration 13151: total_loss: 0.448421, loss_sup: 0.091198, loss_mps: 0.118857, loss_cps: 0.238365
[13:16:10.353] iteration 13152: total_loss: 0.512066, loss_sup: 0.172808, loss_mps: 0.108277, loss_cps: 0.230981
[13:16:10.499] iteration 13153: total_loss: 0.216236, loss_sup: 0.063346, loss_mps: 0.055251, loss_cps: 0.097639
[13:16:10.644] iteration 13154: total_loss: 0.349329, loss_sup: 0.058086, loss_mps: 0.092112, loss_cps: 0.199130
[13:16:10.790] iteration 13155: total_loss: 0.422624, loss_sup: 0.047823, loss_mps: 0.119691, loss_cps: 0.255110
[13:16:10.935] iteration 13156: total_loss: 0.333888, loss_sup: 0.034630, loss_mps: 0.098220, loss_cps: 0.201038
[13:16:11.081] iteration 13157: total_loss: 0.368386, loss_sup: 0.040558, loss_mps: 0.107984, loss_cps: 0.219844
[13:16:11.227] iteration 13158: total_loss: 0.284173, loss_sup: 0.085235, loss_mps: 0.067839, loss_cps: 0.131099
[13:16:11.374] iteration 13159: total_loss: 0.286756, loss_sup: 0.033395, loss_mps: 0.083934, loss_cps: 0.169427
[13:16:11.519] iteration 13160: total_loss: 0.279454, loss_sup: 0.028192, loss_mps: 0.083180, loss_cps: 0.168083
[13:16:11.665] iteration 13161: total_loss: 0.199422, loss_sup: 0.037438, loss_mps: 0.059992, loss_cps: 0.101993
[13:16:11.811] iteration 13162: total_loss: 0.254007, loss_sup: 0.022469, loss_mps: 0.084311, loss_cps: 0.147227
[13:16:11.957] iteration 13163: total_loss: 0.602154, loss_sup: 0.183267, loss_mps: 0.134289, loss_cps: 0.284599
[13:16:12.103] iteration 13164: total_loss: 0.736172, loss_sup: 0.280878, loss_mps: 0.156059, loss_cps: 0.299235
[13:16:12.249] iteration 13165: total_loss: 0.335805, loss_sup: 0.017969, loss_mps: 0.100139, loss_cps: 0.217697
[13:16:12.394] iteration 13166: total_loss: 0.375572, loss_sup: 0.154801, loss_mps: 0.073107, loss_cps: 0.147664
[13:16:12.540] iteration 13167: total_loss: 0.452745, loss_sup: 0.148290, loss_mps: 0.101669, loss_cps: 0.202786
[13:16:12.686] iteration 13168: total_loss: 0.481936, loss_sup: 0.144201, loss_mps: 0.105372, loss_cps: 0.232364
[13:16:12.831] iteration 13169: total_loss: 0.292977, loss_sup: 0.073172, loss_mps: 0.077133, loss_cps: 0.142672
[13:16:12.977] iteration 13170: total_loss: 0.434362, loss_sup: 0.065399, loss_mps: 0.117620, loss_cps: 0.251342
[13:16:13.123] iteration 13171: total_loss: 0.305829, loss_sup: 0.028262, loss_mps: 0.092560, loss_cps: 0.185007
[13:16:13.268] iteration 13172: total_loss: 0.350456, loss_sup: 0.079849, loss_mps: 0.095111, loss_cps: 0.175497
[13:16:13.414] iteration 13173: total_loss: 0.298438, loss_sup: 0.008023, loss_mps: 0.103944, loss_cps: 0.186471
[13:16:13.560] iteration 13174: total_loss: 0.474286, loss_sup: 0.129380, loss_mps: 0.110233, loss_cps: 0.234673
[13:16:13.705] iteration 13175: total_loss: 0.266673, loss_sup: 0.052132, loss_mps: 0.075429, loss_cps: 0.139111
[13:16:13.851] iteration 13176: total_loss: 0.287624, loss_sup: 0.038410, loss_mps: 0.092102, loss_cps: 0.157112
[13:16:13.997] iteration 13177: total_loss: 0.291355, loss_sup: 0.051430, loss_mps: 0.089314, loss_cps: 0.150610
[13:16:14.142] iteration 13178: total_loss: 0.435000, loss_sup: 0.151681, loss_mps: 0.093963, loss_cps: 0.189356
[13:16:14.288] iteration 13179: total_loss: 0.346079, loss_sup: 0.076364, loss_mps: 0.088557, loss_cps: 0.181159
[13:16:14.434] iteration 13180: total_loss: 0.413995, loss_sup: 0.176206, loss_mps: 0.082009, loss_cps: 0.155781
[13:16:14.580] iteration 13181: total_loss: 0.387873, loss_sup: 0.209464, loss_mps: 0.064801, loss_cps: 0.113607
[13:16:14.725] iteration 13182: total_loss: 0.257478, loss_sup: 0.068895, loss_mps: 0.072642, loss_cps: 0.115941
[13:16:14.871] iteration 13183: total_loss: 0.178980, loss_sup: 0.029841, loss_mps: 0.058185, loss_cps: 0.090954
[13:16:15.017] iteration 13184: total_loss: 0.337296, loss_sup: 0.018506, loss_mps: 0.107843, loss_cps: 0.210947
[13:16:15.163] iteration 13185: total_loss: 0.352029, loss_sup: 0.030882, loss_mps: 0.107525, loss_cps: 0.213622
[13:16:15.309] iteration 13186: total_loss: 0.234549, loss_sup: 0.019784, loss_mps: 0.074918, loss_cps: 0.139847
[13:16:15.455] iteration 13187: total_loss: 0.414230, loss_sup: 0.152291, loss_mps: 0.090089, loss_cps: 0.171850
[13:16:15.600] iteration 13188: total_loss: 0.244387, loss_sup: 0.038063, loss_mps: 0.070203, loss_cps: 0.136120
[13:16:15.747] iteration 13189: total_loss: 0.218973, loss_sup: 0.066273, loss_mps: 0.059078, loss_cps: 0.093623
[13:16:15.893] iteration 13190: total_loss: 0.413205, loss_sup: 0.142762, loss_mps: 0.093896, loss_cps: 0.176547
[13:16:16.039] iteration 13191: total_loss: 0.458817, loss_sup: 0.099201, loss_mps: 0.115470, loss_cps: 0.244146
[13:16:16.185] iteration 13192: total_loss: 0.410714, loss_sup: 0.124358, loss_mps: 0.095838, loss_cps: 0.190518
[13:16:16.331] iteration 13193: total_loss: 0.348927, loss_sup: 0.158299, loss_mps: 0.068969, loss_cps: 0.121659
[13:16:16.478] iteration 13194: total_loss: 0.269604, loss_sup: 0.030510, loss_mps: 0.079663, loss_cps: 0.159431
[13:16:16.626] iteration 13195: total_loss: 0.211256, loss_sup: 0.050114, loss_mps: 0.058980, loss_cps: 0.102163
[13:16:16.772] iteration 13196: total_loss: 0.595956, loss_sup: 0.245906, loss_mps: 0.112437, loss_cps: 0.237613
[13:16:16.919] iteration 13197: total_loss: 0.343841, loss_sup: 0.116482, loss_mps: 0.082455, loss_cps: 0.144905
[13:16:17.065] iteration 13198: total_loss: 0.437471, loss_sup: 0.129529, loss_mps: 0.107994, loss_cps: 0.199947
[13:16:17.211] iteration 13199: total_loss: 0.202890, loss_sup: 0.022702, loss_mps: 0.064629, loss_cps: 0.115559
[13:16:17.357] iteration 13200: total_loss: 0.231294, loss_sup: 0.029861, loss_mps: 0.069891, loss_cps: 0.131541
[13:16:17.357] Evaluation Started ==>
[13:16:28.717] ==> valid iteration 13200: unet metrics: {'dc': 0.6551048128278502, 'jc': 0.5329814825644635, 'pre': 0.748397279425379, 'hd': 5.933540512264103}, ynet metrics: {'dc': 0.6018914660381705, 'jc': 0.48470030105355194, 'pre': 0.7961441148792955, 'hd': 5.53498865427081}.
[13:16:28.718] Evaluation Finished!⏹️
[13:16:28.870] iteration 13201: total_loss: 0.589599, loss_sup: 0.329346, loss_mps: 0.091169, loss_cps: 0.169084
[13:16:29.021] iteration 13202: total_loss: 0.333273, loss_sup: 0.054614, loss_mps: 0.094246, loss_cps: 0.184413
[13:16:29.167] iteration 13203: total_loss: 0.312452, loss_sup: 0.090218, loss_mps: 0.079721, loss_cps: 0.142513
[13:16:29.312] iteration 13204: total_loss: 0.434419, loss_sup: 0.064253, loss_mps: 0.120646, loss_cps: 0.249521
[13:16:29.458] iteration 13205: total_loss: 0.327141, loss_sup: 0.066454, loss_mps: 0.090537, loss_cps: 0.170149
[13:16:29.603] iteration 13206: total_loss: 0.532224, loss_sup: 0.025202, loss_mps: 0.163154, loss_cps: 0.343869
[13:16:29.751] iteration 13207: total_loss: 0.232004, loss_sup: 0.018060, loss_mps: 0.078805, loss_cps: 0.135139
[13:16:29.899] iteration 13208: total_loss: 0.751055, loss_sup: 0.301907, loss_mps: 0.141656, loss_cps: 0.307491
[13:16:30.046] iteration 13209: total_loss: 0.514935, loss_sup: 0.076411, loss_mps: 0.133728, loss_cps: 0.304796
[13:16:30.192] iteration 13210: total_loss: 0.497630, loss_sup: 0.091007, loss_mps: 0.133739, loss_cps: 0.272883
[13:16:30.337] iteration 13211: total_loss: 0.377547, loss_sup: 0.051024, loss_mps: 0.107874, loss_cps: 0.218649
[13:16:30.483] iteration 13212: total_loss: 0.493034, loss_sup: 0.079343, loss_mps: 0.139165, loss_cps: 0.274526
[13:16:30.629] iteration 13213: total_loss: 0.337667, loss_sup: 0.088886, loss_mps: 0.088513, loss_cps: 0.160268
[13:16:30.774] iteration 13214: total_loss: 0.341873, loss_sup: 0.020897, loss_mps: 0.109044, loss_cps: 0.211932
[13:16:30.919] iteration 13215: total_loss: 0.247026, loss_sup: 0.027212, loss_mps: 0.077716, loss_cps: 0.142099
[13:16:31.065] iteration 13216: total_loss: 0.391380, loss_sup: 0.044821, loss_mps: 0.115467, loss_cps: 0.231092
[13:16:31.211] iteration 13217: total_loss: 0.583830, loss_sup: 0.063943, loss_mps: 0.158867, loss_cps: 0.361020
[13:16:31.359] iteration 13218: total_loss: 0.315788, loss_sup: 0.094282, loss_mps: 0.077289, loss_cps: 0.144217
[13:16:31.504] iteration 13219: total_loss: 0.422028, loss_sup: 0.193718, loss_mps: 0.081019, loss_cps: 0.147292
[13:16:31.650] iteration 13220: total_loss: 0.398171, loss_sup: 0.110627, loss_mps: 0.097674, loss_cps: 0.189869
[13:16:31.795] iteration 13221: total_loss: 0.268243, loss_sup: 0.040134, loss_mps: 0.081781, loss_cps: 0.146329
[13:16:31.940] iteration 13222: total_loss: 0.762939, loss_sup: 0.432206, loss_mps: 0.107240, loss_cps: 0.223493
[13:16:32.086] iteration 13223: total_loss: 0.309372, loss_sup: 0.040919, loss_mps: 0.096412, loss_cps: 0.172042
[13:16:32.231] iteration 13224: total_loss: 0.363808, loss_sup: 0.038029, loss_mps: 0.111541, loss_cps: 0.214238
[13:16:32.377] iteration 13225: total_loss: 0.449226, loss_sup: 0.073307, loss_mps: 0.122108, loss_cps: 0.253811
[13:16:32.523] iteration 13226: total_loss: 0.311505, loss_sup: 0.039615, loss_mps: 0.096581, loss_cps: 0.175309
[13:16:32.668] iteration 13227: total_loss: 0.315692, loss_sup: 0.026032, loss_mps: 0.097694, loss_cps: 0.191966
[13:16:32.816] iteration 13228: total_loss: 0.383762, loss_sup: 0.031767, loss_mps: 0.119888, loss_cps: 0.232107
[13:16:32.961] iteration 13229: total_loss: 0.375271, loss_sup: 0.049139, loss_mps: 0.104840, loss_cps: 0.221291
[13:16:33.107] iteration 13230: total_loss: 0.847935, loss_sup: 0.506736, loss_mps: 0.115569, loss_cps: 0.225630
[13:16:33.254] iteration 13231: total_loss: 0.439423, loss_sup: 0.065033, loss_mps: 0.128649, loss_cps: 0.245741
[13:16:33.403] iteration 13232: total_loss: 0.365806, loss_sup: 0.053056, loss_mps: 0.106270, loss_cps: 0.206479
[13:16:33.550] iteration 13233: total_loss: 0.380607, loss_sup: 0.166765, loss_mps: 0.080627, loss_cps: 0.133215
[13:16:33.696] iteration 13234: total_loss: 0.221548, loss_sup: 0.019464, loss_mps: 0.076201, loss_cps: 0.125883
[13:16:33.842] iteration 13235: total_loss: 0.190344, loss_sup: 0.057613, loss_mps: 0.052131, loss_cps: 0.080600
[13:16:33.989] iteration 13236: total_loss: 0.218259, loss_sup: 0.019152, loss_mps: 0.072257, loss_cps: 0.126850
[13:16:34.135] iteration 13237: total_loss: 0.901694, loss_sup: 0.365185, loss_mps: 0.168391, loss_cps: 0.368118
[13:16:34.281] iteration 13238: total_loss: 0.406830, loss_sup: 0.096097, loss_mps: 0.107320, loss_cps: 0.203413
[13:16:34.427] iteration 13239: total_loss: 0.374768, loss_sup: 0.090943, loss_mps: 0.100903, loss_cps: 0.182923
[13:16:34.573] iteration 13240: total_loss: 0.473538, loss_sup: 0.160871, loss_mps: 0.105372, loss_cps: 0.207295
[13:16:34.720] iteration 13241: total_loss: 0.412515, loss_sup: 0.152689, loss_mps: 0.093267, loss_cps: 0.166559
[13:16:34.866] iteration 13242: total_loss: 0.551502, loss_sup: 0.116471, loss_mps: 0.148102, loss_cps: 0.286929
[13:16:35.012] iteration 13243: total_loss: 0.289207, loss_sup: 0.061577, loss_mps: 0.078561, loss_cps: 0.149069
[13:16:35.159] iteration 13244: total_loss: 0.511474, loss_sup: 0.102318, loss_mps: 0.136978, loss_cps: 0.272178
[13:16:35.305] iteration 13245: total_loss: 0.196983, loss_sup: 0.030926, loss_mps: 0.062427, loss_cps: 0.103630
[13:16:35.450] iteration 13246: total_loss: 0.219556, loss_sup: 0.016249, loss_mps: 0.073842, loss_cps: 0.129465
[13:16:35.596] iteration 13247: total_loss: 0.194152, loss_sup: 0.014169, loss_mps: 0.065146, loss_cps: 0.114837
[13:16:35.744] iteration 13248: total_loss: 0.224248, loss_sup: 0.051023, loss_mps: 0.065200, loss_cps: 0.108025
[13:16:35.890] iteration 13249: total_loss: 0.320246, loss_sup: 0.074162, loss_mps: 0.089267, loss_cps: 0.156817
[13:16:36.036] iteration 13250: total_loss: 0.347813, loss_sup: 0.074503, loss_mps: 0.100190, loss_cps: 0.173120
[13:16:36.182] iteration 13251: total_loss: 0.560227, loss_sup: 0.182885, loss_mps: 0.125000, loss_cps: 0.252342
[13:16:36.328] iteration 13252: total_loss: 0.309810, loss_sup: 0.093258, loss_mps: 0.081163, loss_cps: 0.135389
[13:16:36.476] iteration 13253: total_loss: 0.417095, loss_sup: 0.138326, loss_mps: 0.096109, loss_cps: 0.182659
[13:16:36.621] iteration 13254: total_loss: 0.585133, loss_sup: 0.162065, loss_mps: 0.138841, loss_cps: 0.284226
[13:16:36.767] iteration 13255: total_loss: 0.347322, loss_sup: 0.117473, loss_mps: 0.086310, loss_cps: 0.143539
[13:16:36.913] iteration 13256: total_loss: 0.264445, loss_sup: 0.032843, loss_mps: 0.082986, loss_cps: 0.148617
[13:16:37.059] iteration 13257: total_loss: 0.491912, loss_sup: 0.277582, loss_mps: 0.079092, loss_cps: 0.135238
[13:16:37.206] iteration 13258: total_loss: 0.359763, loss_sup: 0.090333, loss_mps: 0.092669, loss_cps: 0.176761
[13:16:37.354] iteration 13259: total_loss: 0.368918, loss_sup: 0.049469, loss_mps: 0.110286, loss_cps: 0.209164
[13:16:37.504] iteration 13260: total_loss: 0.239536, loss_sup: 0.012442, loss_mps: 0.085856, loss_cps: 0.141238
[13:16:37.650] iteration 13261: total_loss: 0.415158, loss_sup: 0.029535, loss_mps: 0.133698, loss_cps: 0.251925
[13:16:37.796] iteration 13262: total_loss: 0.254142, loss_sup: 0.081040, loss_mps: 0.064240, loss_cps: 0.108862
[13:16:37.942] iteration 13263: total_loss: 0.454313, loss_sup: 0.024843, loss_mps: 0.138129, loss_cps: 0.291341
[13:16:38.091] iteration 13264: total_loss: 0.264718, loss_sup: 0.055779, loss_mps: 0.076028, loss_cps: 0.132911
[13:16:38.237] iteration 13265: total_loss: 0.346252, loss_sup: 0.070665, loss_mps: 0.092438, loss_cps: 0.183149
[13:16:38.384] iteration 13266: total_loss: 0.519188, loss_sup: 0.095906, loss_mps: 0.138786, loss_cps: 0.284496
[13:16:38.531] iteration 13267: total_loss: 0.594769, loss_sup: 0.080088, loss_mps: 0.160608, loss_cps: 0.354072
[13:16:38.676] iteration 13268: total_loss: 0.194597, loss_sup: 0.022211, loss_mps: 0.063523, loss_cps: 0.108863
[13:16:38.822] iteration 13269: total_loss: 0.368530, loss_sup: 0.131271, loss_mps: 0.087525, loss_cps: 0.149733
[13:16:38.968] iteration 13270: total_loss: 0.403248, loss_sup: 0.029860, loss_mps: 0.125451, loss_cps: 0.247937
[13:16:39.114] iteration 13271: total_loss: 0.225562, loss_sup: 0.073887, loss_mps: 0.056064, loss_cps: 0.095611
[13:16:39.260] iteration 13272: total_loss: 0.253960, loss_sup: 0.038427, loss_mps: 0.082220, loss_cps: 0.133314
[13:16:39.406] iteration 13273: total_loss: 0.299925, loss_sup: 0.011355, loss_mps: 0.097429, loss_cps: 0.191141
[13:16:39.552] iteration 13274: total_loss: 0.285180, loss_sup: 0.010790, loss_mps: 0.093701, loss_cps: 0.180689
[13:16:39.698] iteration 13275: total_loss: 0.190009, loss_sup: 0.023326, loss_mps: 0.064265, loss_cps: 0.102419
[13:16:39.844] iteration 13276: total_loss: 0.334049, loss_sup: 0.078608, loss_mps: 0.089063, loss_cps: 0.166378
[13:16:39.989] iteration 13277: total_loss: 0.270366, loss_sup: 0.021793, loss_mps: 0.085538, loss_cps: 0.163035
[13:16:40.136] iteration 13278: total_loss: 0.411060, loss_sup: 0.199497, loss_mps: 0.074820, loss_cps: 0.136744
[13:16:40.282] iteration 13279: total_loss: 0.233722, loss_sup: 0.046025, loss_mps: 0.064466, loss_cps: 0.123231
[13:16:40.428] iteration 13280: total_loss: 0.328805, loss_sup: 0.056575, loss_mps: 0.093172, loss_cps: 0.179058
[13:16:40.574] iteration 13281: total_loss: 0.414161, loss_sup: 0.111515, loss_mps: 0.100974, loss_cps: 0.201672
[13:16:40.720] iteration 13282: total_loss: 0.310786, loss_sup: 0.060267, loss_mps: 0.086899, loss_cps: 0.163619
[13:16:40.866] iteration 13283: total_loss: 0.209267, loss_sup: 0.012470, loss_mps: 0.071032, loss_cps: 0.125764
[13:16:41.012] iteration 13284: total_loss: 0.370313, loss_sup: 0.104686, loss_mps: 0.089473, loss_cps: 0.176155
[13:16:41.159] iteration 13285: total_loss: 0.251733, loss_sup: 0.020917, loss_mps: 0.081517, loss_cps: 0.149299
[13:16:41.306] iteration 13286: total_loss: 0.224222, loss_sup: 0.006943, loss_mps: 0.078344, loss_cps: 0.138935
[13:16:41.452] iteration 13287: total_loss: 0.589949, loss_sup: 0.236644, loss_mps: 0.115147, loss_cps: 0.238157
[13:16:41.600] iteration 13288: total_loss: 0.407130, loss_sup: 0.223940, loss_mps: 0.066542, loss_cps: 0.116648
[13:16:41.746] iteration 13289: total_loss: 0.445145, loss_sup: 0.240524, loss_mps: 0.073003, loss_cps: 0.131619
[13:16:41.892] iteration 13290: total_loss: 0.458719, loss_sup: 0.132282, loss_mps: 0.108141, loss_cps: 0.218295
[13:16:42.038] iteration 13291: total_loss: 0.193863, loss_sup: 0.015066, loss_mps: 0.064256, loss_cps: 0.114541
[13:16:42.185] iteration 13292: total_loss: 0.221940, loss_sup: 0.034994, loss_mps: 0.064032, loss_cps: 0.122913
[13:16:42.331] iteration 13293: total_loss: 0.336050, loss_sup: 0.019237, loss_mps: 0.106041, loss_cps: 0.210771
[13:16:42.478] iteration 13294: total_loss: 0.641710, loss_sup: 0.299488, loss_mps: 0.121629, loss_cps: 0.220593
[13:16:42.624] iteration 13295: total_loss: 0.358231, loss_sup: 0.040730, loss_mps: 0.106704, loss_cps: 0.210797
[13:16:42.770] iteration 13296: total_loss: 0.274407, loss_sup: 0.120758, loss_mps: 0.058421, loss_cps: 0.095228
[13:16:42.916] iteration 13297: total_loss: 0.246123, loss_sup: 0.031855, loss_mps: 0.074518, loss_cps: 0.139750
[13:16:43.064] iteration 13298: total_loss: 0.319079, loss_sup: 0.049183, loss_mps: 0.088977, loss_cps: 0.180918
[13:16:43.210] iteration 13299: total_loss: 0.305438, loss_sup: 0.038641, loss_mps: 0.094353, loss_cps: 0.172444
[13:16:43.357] iteration 13300: total_loss: 0.263045, loss_sup: 0.081019, loss_mps: 0.069921, loss_cps: 0.112106
[13:16:43.357] Evaluation Started ==>
[13:16:54.782] ==> valid iteration 13300: unet metrics: {'dc': 0.634028395144183, 'jc': 0.5187220013072635, 'pre': 0.7721504752666004, 'hd': 5.5390573824084015}, ynet metrics: {'dc': 0.6121645292957649, 'jc': 0.4937336385959931, 'pre': 0.7396599375755589, 'hd': 5.8006143294453265}.
[13:16:54.784] Evaluation Finished!⏹️
[13:16:54.938] iteration 13301: total_loss: 0.335487, loss_sup: 0.052637, loss_mps: 0.097936, loss_cps: 0.184914
[13:16:55.086] iteration 13302: total_loss: 0.443203, loss_sup: 0.138026, loss_mps: 0.102902, loss_cps: 0.202275
[13:16:55.232] iteration 13303: total_loss: 0.224556, loss_sup: 0.020118, loss_mps: 0.075332, loss_cps: 0.129106
[13:16:55.378] iteration 13304: total_loss: 0.538886, loss_sup: 0.176449, loss_mps: 0.128586, loss_cps: 0.233852
[13:16:55.525] iteration 13305: total_loss: 0.300300, loss_sup: 0.026010, loss_mps: 0.096621, loss_cps: 0.177669
[13:16:55.671] iteration 13306: total_loss: 0.528991, loss_sup: 0.064769, loss_mps: 0.146163, loss_cps: 0.318059
[13:16:55.817] iteration 13307: total_loss: 0.325216, loss_sup: 0.062076, loss_mps: 0.091852, loss_cps: 0.171287
[13:16:55.963] iteration 13308: total_loss: 0.215297, loss_sup: 0.012579, loss_mps: 0.072838, loss_cps: 0.129880
[13:16:56.111] iteration 13309: total_loss: 0.306760, loss_sup: 0.015754, loss_mps: 0.093200, loss_cps: 0.197806
[13:16:56.257] iteration 13310: total_loss: 0.481453, loss_sup: 0.132037, loss_mps: 0.116305, loss_cps: 0.233110
[13:16:56.404] iteration 13311: total_loss: 0.487131, loss_sup: 0.141907, loss_mps: 0.125234, loss_cps: 0.219990
[13:16:56.550] iteration 13312: total_loss: 0.368327, loss_sup: 0.082908, loss_mps: 0.092252, loss_cps: 0.193167
[13:16:56.697] iteration 13313: total_loss: 0.238187, loss_sup: 0.066039, loss_mps: 0.062669, loss_cps: 0.109479
[13:16:56.844] iteration 13314: total_loss: 0.458267, loss_sup: 0.096478, loss_mps: 0.122300, loss_cps: 0.239489
[13:16:56.991] iteration 13315: total_loss: 0.475287, loss_sup: 0.145808, loss_mps: 0.115697, loss_cps: 0.213782
[13:16:57.138] iteration 13316: total_loss: 0.176124, loss_sup: 0.014967, loss_mps: 0.058649, loss_cps: 0.102508
[13:16:57.285] iteration 13317: total_loss: 0.603493, loss_sup: 0.084688, loss_mps: 0.161226, loss_cps: 0.357579
[13:16:57.432] iteration 13318: total_loss: 0.515079, loss_sup: 0.064247, loss_mps: 0.144425, loss_cps: 0.306408
[13:16:57.578] iteration 13319: total_loss: 0.836233, loss_sup: 0.550539, loss_mps: 0.100708, loss_cps: 0.184985
[13:16:57.728] iteration 13320: total_loss: 0.367627, loss_sup: 0.014041, loss_mps: 0.110405, loss_cps: 0.243181
[13:16:57.877] iteration 13321: total_loss: 0.281433, loss_sup: 0.044426, loss_mps: 0.082199, loss_cps: 0.154808
[13:16:58.023] iteration 13322: total_loss: 0.481055, loss_sup: 0.069209, loss_mps: 0.131592, loss_cps: 0.280254
[13:16:58.170] iteration 13323: total_loss: 0.540372, loss_sup: 0.078842, loss_mps: 0.149028, loss_cps: 0.312502
[13:16:58.316] iteration 13324: total_loss: 0.415051, loss_sup: 0.057834, loss_mps: 0.118264, loss_cps: 0.238952
[13:16:58.462] iteration 13325: total_loss: 0.399991, loss_sup: 0.054207, loss_mps: 0.113611, loss_cps: 0.232173
[13:16:58.607] iteration 13326: total_loss: 0.393173, loss_sup: 0.065141, loss_mps: 0.107368, loss_cps: 0.220664
[13:16:58.753] iteration 13327: total_loss: 0.319545, loss_sup: 0.057981, loss_mps: 0.093608, loss_cps: 0.167956
[13:16:58.899] iteration 13328: total_loss: 0.504296, loss_sup: 0.201402, loss_mps: 0.106648, loss_cps: 0.196247
[13:16:59.045] iteration 13329: total_loss: 0.687888, loss_sup: 0.173780, loss_mps: 0.157901, loss_cps: 0.356207
[13:16:59.192] iteration 13330: total_loss: 0.254214, loss_sup: 0.052929, loss_mps: 0.075621, loss_cps: 0.125663
[13:16:59.338] iteration 13331: total_loss: 0.561914, loss_sup: 0.050632, loss_mps: 0.162700, loss_cps: 0.348583
[13:16:59.489] iteration 13332: total_loss: 0.218617, loss_sup: 0.042377, loss_mps: 0.065794, loss_cps: 0.110446
[13:16:59.635] iteration 13333: total_loss: 0.432072, loss_sup: 0.063269, loss_mps: 0.118462, loss_cps: 0.250341
[13:16:59.780] iteration 13334: total_loss: 0.598348, loss_sup: 0.082453, loss_mps: 0.170030, loss_cps: 0.345865
[13:16:59.926] iteration 13335: total_loss: 0.378403, loss_sup: 0.067195, loss_mps: 0.110640, loss_cps: 0.200567
[13:17:00.072] iteration 13336: total_loss: 0.385447, loss_sup: 0.093694, loss_mps: 0.106756, loss_cps: 0.184998
[13:17:00.218] iteration 13337: total_loss: 0.235025, loss_sup: 0.039891, loss_mps: 0.074762, loss_cps: 0.120373
[13:17:00.364] iteration 13338: total_loss: 0.520381, loss_sup: 0.010578, loss_mps: 0.164686, loss_cps: 0.345118
[13:17:00.510] iteration 13339: total_loss: 0.244688, loss_sup: 0.021843, loss_mps: 0.076759, loss_cps: 0.146086
[13:17:00.657] iteration 13340: total_loss: 0.504493, loss_sup: 0.109914, loss_mps: 0.129639, loss_cps: 0.264940
[13:17:00.803] iteration 13341: total_loss: 0.391332, loss_sup: 0.040259, loss_mps: 0.120754, loss_cps: 0.230319
[13:17:00.950] iteration 13342: total_loss: 0.377351, loss_sup: 0.052866, loss_mps: 0.105992, loss_cps: 0.218493
[13:17:01.096] iteration 13343: total_loss: 0.417561, loss_sup: 0.032770, loss_mps: 0.127673, loss_cps: 0.257117
[13:17:01.242] iteration 13344: total_loss: 0.410086, loss_sup: 0.103044, loss_mps: 0.107021, loss_cps: 0.200021
[13:17:01.388] iteration 13345: total_loss: 0.459484, loss_sup: 0.196726, loss_mps: 0.092175, loss_cps: 0.170583
[13:17:01.533] iteration 13346: total_loss: 0.216805, loss_sup: 0.045468, loss_mps: 0.066557, loss_cps: 0.104780
[13:17:01.679] iteration 13347: total_loss: 0.279415, loss_sup: 0.037853, loss_mps: 0.087068, loss_cps: 0.154494
[13:17:01.829] iteration 13348: total_loss: 0.277749, loss_sup: 0.024187, loss_mps: 0.087630, loss_cps: 0.165932
[13:17:01.975] iteration 13349: total_loss: 0.308230, loss_sup: 0.095822, loss_mps: 0.074445, loss_cps: 0.137962
[13:17:02.121] iteration 13350: total_loss: 0.315199, loss_sup: 0.058167, loss_mps: 0.090597, loss_cps: 0.166435
[13:17:02.268] iteration 13351: total_loss: 0.404539, loss_sup: 0.118569, loss_mps: 0.099001, loss_cps: 0.186969
[13:17:02.413] iteration 13352: total_loss: 0.357030, loss_sup: 0.045633, loss_mps: 0.107328, loss_cps: 0.204068
[13:17:02.559] iteration 13353: total_loss: 0.337742, loss_sup: 0.086146, loss_mps: 0.086158, loss_cps: 0.165438
[13:17:02.706] iteration 13354: total_loss: 0.350474, loss_sup: 0.068818, loss_mps: 0.094834, loss_cps: 0.186822
[13:17:02.854] iteration 13355: total_loss: 0.326064, loss_sup: 0.072295, loss_mps: 0.087244, loss_cps: 0.166524
[13:17:03.001] iteration 13356: total_loss: 0.489514, loss_sup: 0.104961, loss_mps: 0.120655, loss_cps: 0.263898
[13:17:03.147] iteration 13357: total_loss: 0.449195, loss_sup: 0.097878, loss_mps: 0.111316, loss_cps: 0.240000
[13:17:03.293] iteration 13358: total_loss: 0.318163, loss_sup: 0.035193, loss_mps: 0.096421, loss_cps: 0.186549
[13:17:03.439] iteration 13359: total_loss: 0.319466, loss_sup: 0.085679, loss_mps: 0.084758, loss_cps: 0.149030
[13:17:03.585] iteration 13360: total_loss: 0.277297, loss_sup: 0.009360, loss_mps: 0.090343, loss_cps: 0.177595
[13:17:03.731] iteration 13361: total_loss: 1.009695, loss_sup: 0.162090, loss_mps: 0.251844, loss_cps: 0.595760
[13:17:03.877] iteration 13362: total_loss: 0.384107, loss_sup: 0.167243, loss_mps: 0.081120, loss_cps: 0.135744
[13:17:04.023] iteration 13363: total_loss: 0.236533, loss_sup: 0.058450, loss_mps: 0.068824, loss_cps: 0.109258
[13:17:04.170] iteration 13364: total_loss: 0.262642, loss_sup: 0.049906, loss_mps: 0.077992, loss_cps: 0.134745
[13:17:04.316] iteration 13365: total_loss: 0.360655, loss_sup: 0.085770, loss_mps: 0.096106, loss_cps: 0.178780
[13:17:04.462] iteration 13366: total_loss: 0.287134, loss_sup: 0.093789, loss_mps: 0.071655, loss_cps: 0.121690
[13:17:04.607] iteration 13367: total_loss: 0.328324, loss_sup: 0.064442, loss_mps: 0.091176, loss_cps: 0.172706
[13:17:04.754] iteration 13368: total_loss: 0.527274, loss_sup: 0.185458, loss_mps: 0.110738, loss_cps: 0.231079
[13:17:04.899] iteration 13369: total_loss: 0.183311, loss_sup: 0.004136, loss_mps: 0.064127, loss_cps: 0.115048
[13:17:05.045] iteration 13370: total_loss: 0.221473, loss_sup: 0.015024, loss_mps: 0.076865, loss_cps: 0.129584
[13:17:05.194] iteration 13371: total_loss: 0.376372, loss_sup: 0.085116, loss_mps: 0.103317, loss_cps: 0.187939
[13:17:05.339] iteration 13372: total_loss: 0.214002, loss_sup: 0.048225, loss_mps: 0.063993, loss_cps: 0.101784
[13:17:05.486] iteration 13373: total_loss: 0.220823, loss_sup: 0.047552, loss_mps: 0.062099, loss_cps: 0.111172
[13:17:05.632] iteration 13374: total_loss: 0.247622, loss_sup: 0.025460, loss_mps: 0.082488, loss_cps: 0.139674
[13:17:05.778] iteration 13375: total_loss: 0.472449, loss_sup: 0.125599, loss_mps: 0.117653, loss_cps: 0.229197
[13:17:05.838] iteration 13376: total_loss: 0.238664, loss_sup: 0.045051, loss_mps: 0.076593, loss_cps: 0.117019
[13:17:07.147] iteration 13377: total_loss: 0.423402, loss_sup: 0.027454, loss_mps: 0.132584, loss_cps: 0.263364
[13:17:07.296] iteration 13378: total_loss: 0.299498, loss_sup: 0.081736, loss_mps: 0.074103, loss_cps: 0.143659
[13:17:07.443] iteration 13379: total_loss: 0.400953, loss_sup: 0.030066, loss_mps: 0.123629, loss_cps: 0.247258
[13:17:07.590] iteration 13380: total_loss: 0.322789, loss_sup: 0.097209, loss_mps: 0.082633, loss_cps: 0.142947
[13:17:07.736] iteration 13381: total_loss: 0.370659, loss_sup: 0.064449, loss_mps: 0.100823, loss_cps: 0.205387
[13:17:07.883] iteration 13382: total_loss: 0.314201, loss_sup: 0.031596, loss_mps: 0.099386, loss_cps: 0.183219
[13:17:08.029] iteration 13383: total_loss: 0.203337, loss_sup: 0.042127, loss_mps: 0.059721, loss_cps: 0.101489
[13:17:08.182] iteration 13384: total_loss: 0.343016, loss_sup: 0.034402, loss_mps: 0.103082, loss_cps: 0.205531
[13:17:08.328] iteration 13385: total_loss: 0.468236, loss_sup: 0.132100, loss_mps: 0.111510, loss_cps: 0.224626
[13:17:08.475] iteration 13386: total_loss: 0.208097, loss_sup: 0.022907, loss_mps: 0.070793, loss_cps: 0.114398
[13:17:08.621] iteration 13387: total_loss: 0.228181, loss_sup: 0.033669, loss_mps: 0.068313, loss_cps: 0.126199
[13:17:08.769] iteration 13388: total_loss: 0.517383, loss_sup: 0.079912, loss_mps: 0.129412, loss_cps: 0.308059
[13:17:08.915] iteration 13389: total_loss: 0.346505, loss_sup: 0.016765, loss_mps: 0.107211, loss_cps: 0.222529
[13:17:09.061] iteration 13390: total_loss: 0.322297, loss_sup: 0.047113, loss_mps: 0.090972, loss_cps: 0.184212
[13:17:09.207] iteration 13391: total_loss: 0.378257, loss_sup: 0.020899, loss_mps: 0.121305, loss_cps: 0.236053
[13:17:09.356] iteration 13392: total_loss: 0.206054, loss_sup: 0.020977, loss_mps: 0.065649, loss_cps: 0.119428
[13:17:09.502] iteration 13393: total_loss: 0.276145, loss_sup: 0.018140, loss_mps: 0.090733, loss_cps: 0.167271
[13:17:09.648] iteration 13394: total_loss: 0.361659, loss_sup: 0.053206, loss_mps: 0.104992, loss_cps: 0.203461
[13:17:09.795] iteration 13395: total_loss: 0.419322, loss_sup: 0.091697, loss_mps: 0.104918, loss_cps: 0.222706
[13:17:09.942] iteration 13396: total_loss: 0.558143, loss_sup: 0.106391, loss_mps: 0.137238, loss_cps: 0.314513
[13:17:10.089] iteration 13397: total_loss: 0.294826, loss_sup: 0.025182, loss_mps: 0.092490, loss_cps: 0.177155
[13:17:10.235] iteration 13398: total_loss: 0.352335, loss_sup: 0.083712, loss_mps: 0.094384, loss_cps: 0.174240
[13:17:10.382] iteration 13399: total_loss: 0.315279, loss_sup: 0.064046, loss_mps: 0.091742, loss_cps: 0.159490
[13:17:10.528] iteration 13400: total_loss: 0.661927, loss_sup: 0.044845, loss_mps: 0.197429, loss_cps: 0.419653
[13:17:10.528] Evaluation Started ==>
[13:17:21.853] ==> valid iteration 13400: unet metrics: {'dc': 0.6497220334479616, 'jc': 0.5347675706290099, 'pre': 0.7525496897969399, 'hd': 5.6122357351512795}, ynet metrics: {'dc': 0.5810673456315352, 'jc': 0.46665250931909613, 'pre': 0.7821374076167655, 'hd': 5.622909637131633}.
[13:17:21.855] Evaluation Finished!⏹️
[13:17:22.008] iteration 13401: total_loss: 0.203868, loss_sup: 0.007077, loss_mps: 0.066734, loss_cps: 0.130057
[13:17:22.158] iteration 13402: total_loss: 0.311356, loss_sup: 0.046257, loss_mps: 0.089125, loss_cps: 0.175974
[13:17:22.304] iteration 13403: total_loss: 0.282580, loss_sup: 0.047146, loss_mps: 0.077548, loss_cps: 0.157887
[13:17:22.449] iteration 13404: total_loss: 0.247287, loss_sup: 0.063641, loss_mps: 0.062814, loss_cps: 0.120832
[13:17:22.597] iteration 13405: total_loss: 0.433565, loss_sup: 0.216789, loss_mps: 0.079270, loss_cps: 0.137506
[13:17:22.744] iteration 13406: total_loss: 0.518393, loss_sup: 0.303365, loss_mps: 0.073900, loss_cps: 0.141127
[13:17:22.890] iteration 13407: total_loss: 0.554814, loss_sup: 0.114772, loss_mps: 0.140932, loss_cps: 0.299110
[13:17:23.036] iteration 13408: total_loss: 0.295433, loss_sup: 0.035305, loss_mps: 0.088153, loss_cps: 0.171975
[13:17:23.181] iteration 13409: total_loss: 0.310651, loss_sup: 0.089369, loss_mps: 0.076431, loss_cps: 0.144852
[13:17:23.327] iteration 13410: total_loss: 0.358607, loss_sup: 0.019294, loss_mps: 0.107769, loss_cps: 0.231544
[13:17:23.474] iteration 13411: total_loss: 0.309089, loss_sup: 0.034087, loss_mps: 0.088725, loss_cps: 0.186278
[13:17:23.621] iteration 13412: total_loss: 0.419960, loss_sup: 0.170040, loss_mps: 0.085947, loss_cps: 0.163973
[13:17:23.771] iteration 13413: total_loss: 0.420827, loss_sup: 0.079540, loss_mps: 0.114489, loss_cps: 0.226798
[13:17:23.921] iteration 13414: total_loss: 0.378598, loss_sup: 0.128660, loss_mps: 0.084511, loss_cps: 0.165427
[13:17:24.067] iteration 13415: total_loss: 0.605050, loss_sup: 0.109752, loss_mps: 0.153078, loss_cps: 0.342220
[13:17:24.214] iteration 13416: total_loss: 0.370329, loss_sup: 0.121503, loss_mps: 0.088425, loss_cps: 0.160402
[13:17:24.364] iteration 13417: total_loss: 0.844649, loss_sup: 0.283234, loss_mps: 0.174378, loss_cps: 0.387037
[13:17:24.509] iteration 13418: total_loss: 0.375165, loss_sup: 0.091392, loss_mps: 0.093406, loss_cps: 0.190367
[13:17:24.656] iteration 13419: total_loss: 0.290097, loss_sup: 0.026431, loss_mps: 0.092310, loss_cps: 0.171356
[13:17:24.804] iteration 13420: total_loss: 0.157367, loss_sup: 0.003445, loss_mps: 0.058499, loss_cps: 0.095424
[13:17:24.950] iteration 13421: total_loss: 0.408002, loss_sup: 0.004406, loss_mps: 0.126028, loss_cps: 0.277568
[13:17:25.096] iteration 13422: total_loss: 0.375301, loss_sup: 0.133105, loss_mps: 0.082882, loss_cps: 0.159315
[13:17:25.242] iteration 13423: total_loss: 0.313382, loss_sup: 0.020800, loss_mps: 0.099781, loss_cps: 0.192801
[13:17:25.387] iteration 13424: total_loss: 0.464131, loss_sup: 0.107097, loss_mps: 0.118203, loss_cps: 0.238830
[13:17:25.535] iteration 13425: total_loss: 0.220198, loss_sup: 0.036373, loss_mps: 0.070770, loss_cps: 0.113055
[13:17:25.682] iteration 13426: total_loss: 0.612816, loss_sup: 0.145177, loss_mps: 0.148188, loss_cps: 0.319451
[13:17:25.827] iteration 13427: total_loss: 1.157471, loss_sup: 0.304863, loss_mps: 0.267399, loss_cps: 0.585209
[13:17:25.973] iteration 13428: total_loss: 0.324208, loss_sup: 0.040513, loss_mps: 0.096059, loss_cps: 0.187637
[13:17:26.120] iteration 13429: total_loss: 0.347607, loss_sup: 0.116583, loss_mps: 0.078115, loss_cps: 0.152909
[13:17:26.266] iteration 13430: total_loss: 0.240988, loss_sup: 0.066807, loss_mps: 0.067333, loss_cps: 0.106849
[13:17:26.412] iteration 13431: total_loss: 0.461515, loss_sup: 0.093307, loss_mps: 0.125444, loss_cps: 0.242765
[13:17:26.558] iteration 13432: total_loss: 0.426220, loss_sup: 0.030281, loss_mps: 0.126945, loss_cps: 0.268994
[13:17:26.705] iteration 13433: total_loss: 0.322022, loss_sup: 0.092813, loss_mps: 0.080349, loss_cps: 0.148860
[13:17:26.856] iteration 13434: total_loss: 0.598991, loss_sup: 0.073390, loss_mps: 0.172792, loss_cps: 0.352809
[13:17:27.009] iteration 13435: total_loss: 0.431877, loss_sup: 0.084765, loss_mps: 0.116199, loss_cps: 0.230913
[13:17:27.155] iteration 13436: total_loss: 0.583761, loss_sup: 0.281168, loss_mps: 0.103010, loss_cps: 0.199583
[13:17:27.302] iteration 13437: total_loss: 0.507552, loss_sup: 0.121050, loss_mps: 0.128633, loss_cps: 0.257869
[13:17:27.447] iteration 13438: total_loss: 0.548806, loss_sup: 0.167857, loss_mps: 0.125751, loss_cps: 0.255198
[13:17:27.593] iteration 13439: total_loss: 0.334724, loss_sup: 0.075899, loss_mps: 0.089403, loss_cps: 0.169422
[13:17:27.739] iteration 13440: total_loss: 0.451192, loss_sup: 0.087820, loss_mps: 0.122603, loss_cps: 0.240769
[13:17:27.885] iteration 13441: total_loss: 0.356255, loss_sup: 0.033303, loss_mps: 0.106520, loss_cps: 0.216432
[13:17:28.034] iteration 13442: total_loss: 0.415473, loss_sup: 0.019876, loss_mps: 0.128890, loss_cps: 0.266708
[13:17:28.180] iteration 13443: total_loss: 0.459901, loss_sup: 0.055895, loss_mps: 0.133926, loss_cps: 0.270080
[13:17:28.326] iteration 13444: total_loss: 0.389040, loss_sup: 0.083225, loss_mps: 0.106080, loss_cps: 0.199736
[13:17:28.472] iteration 13445: total_loss: 0.369810, loss_sup: 0.084753, loss_mps: 0.097629, loss_cps: 0.187428
[13:17:28.618] iteration 13446: total_loss: 0.395825, loss_sup: 0.151583, loss_mps: 0.084018, loss_cps: 0.160225
[13:17:28.764] iteration 13447: total_loss: 0.527211, loss_sup: 0.133514, loss_mps: 0.131860, loss_cps: 0.261837
[13:17:28.910] iteration 13448: total_loss: 0.414498, loss_sup: 0.141218, loss_mps: 0.087674, loss_cps: 0.185606
[13:17:29.056] iteration 13449: total_loss: 0.243846, loss_sup: 0.027867, loss_mps: 0.075125, loss_cps: 0.140854
[13:17:29.202] iteration 13450: total_loss: 0.239394, loss_sup: 0.053975, loss_mps: 0.072158, loss_cps: 0.113261
[13:17:29.348] iteration 13451: total_loss: 0.357111, loss_sup: 0.109044, loss_mps: 0.084911, loss_cps: 0.163156
[13:17:29.494] iteration 13452: total_loss: 0.225715, loss_sup: 0.056464, loss_mps: 0.061865, loss_cps: 0.107385
[13:17:29.641] iteration 13453: total_loss: 0.659205, loss_sup: 0.224413, loss_mps: 0.138431, loss_cps: 0.296362
[13:17:29.788] iteration 13454: total_loss: 0.463223, loss_sup: 0.114298, loss_mps: 0.123500, loss_cps: 0.225425
[13:17:29.935] iteration 13455: total_loss: 0.349717, loss_sup: 0.087879, loss_mps: 0.093644, loss_cps: 0.168193
[13:17:30.081] iteration 13456: total_loss: 0.280737, loss_sup: 0.063170, loss_mps: 0.076826, loss_cps: 0.140741
[13:17:30.227] iteration 13457: total_loss: 0.388082, loss_sup: 0.132263, loss_mps: 0.094767, loss_cps: 0.161052
[13:17:30.373] iteration 13458: total_loss: 0.461518, loss_sup: 0.054415, loss_mps: 0.124931, loss_cps: 0.282172
[13:17:30.519] iteration 13459: total_loss: 0.156118, loss_sup: 0.018781, loss_mps: 0.052547, loss_cps: 0.084789
[13:17:30.665] iteration 13460: total_loss: 0.335434, loss_sup: 0.024358, loss_mps: 0.104624, loss_cps: 0.206452
[13:17:30.813] iteration 13461: total_loss: 0.206870, loss_sup: 0.031855, loss_mps: 0.063918, loss_cps: 0.111097
[13:17:30.960] iteration 13462: total_loss: 0.352698, loss_sup: 0.026701, loss_mps: 0.107630, loss_cps: 0.218367
[13:17:31.106] iteration 13463: total_loss: 0.356414, loss_sup: 0.047973, loss_mps: 0.108772, loss_cps: 0.199670
[13:17:31.255] iteration 13464: total_loss: 0.358081, loss_sup: 0.069272, loss_mps: 0.094934, loss_cps: 0.193875
[13:17:31.401] iteration 13465: total_loss: 0.331507, loss_sup: 0.100486, loss_mps: 0.084699, loss_cps: 0.146322
[13:17:31.552] iteration 13466: total_loss: 0.361863, loss_sup: 0.156719, loss_mps: 0.071445, loss_cps: 0.133699
[13:17:31.699] iteration 13467: total_loss: 0.258772, loss_sup: 0.024273, loss_mps: 0.082804, loss_cps: 0.151696
[13:17:31.850] iteration 13468: total_loss: 0.410980, loss_sup: 0.097350, loss_mps: 0.105087, loss_cps: 0.208542
[13:17:31.996] iteration 13469: total_loss: 0.232908, loss_sup: 0.046518, loss_mps: 0.065729, loss_cps: 0.120661
[13:17:32.142] iteration 13470: total_loss: 0.379579, loss_sup: 0.095969, loss_mps: 0.099255, loss_cps: 0.184355
[13:17:32.288] iteration 13471: total_loss: 0.227336, loss_sup: 0.049578, loss_mps: 0.066770, loss_cps: 0.110987
[13:17:32.435] iteration 13472: total_loss: 0.291717, loss_sup: 0.029262, loss_mps: 0.094366, loss_cps: 0.168089
[13:17:32.580] iteration 13473: total_loss: 0.372501, loss_sup: 0.024851, loss_mps: 0.113525, loss_cps: 0.234126
[13:17:32.726] iteration 13474: total_loss: 0.545809, loss_sup: 0.233963, loss_mps: 0.103367, loss_cps: 0.208480
[13:17:32.872] iteration 13475: total_loss: 0.419692, loss_sup: 0.090421, loss_mps: 0.115229, loss_cps: 0.214042
[13:17:33.019] iteration 13476: total_loss: 0.250053, loss_sup: 0.027239, loss_mps: 0.077062, loss_cps: 0.145752
[13:17:33.167] iteration 13477: total_loss: 0.454364, loss_sup: 0.099050, loss_mps: 0.112196, loss_cps: 0.243118
[13:17:33.315] iteration 13478: total_loss: 0.265126, loss_sup: 0.029179, loss_mps: 0.083039, loss_cps: 0.152908
[13:17:33.461] iteration 13479: total_loss: 0.483564, loss_sup: 0.116915, loss_mps: 0.122054, loss_cps: 0.244594
[13:17:33.607] iteration 13480: total_loss: 0.234093, loss_sup: 0.077679, loss_mps: 0.060502, loss_cps: 0.095912
[13:17:33.753] iteration 13481: total_loss: 0.525075, loss_sup: 0.225757, loss_mps: 0.101597, loss_cps: 0.197720
[13:17:33.899] iteration 13482: total_loss: 0.495763, loss_sup: 0.114120, loss_mps: 0.122894, loss_cps: 0.258748
[13:17:34.046] iteration 13483: total_loss: 0.207859, loss_sup: 0.056620, loss_mps: 0.057401, loss_cps: 0.093838
[13:17:34.191] iteration 13484: total_loss: 0.245738, loss_sup: 0.036622, loss_mps: 0.075783, loss_cps: 0.133334
[13:17:34.337] iteration 13485: total_loss: 0.263850, loss_sup: 0.074018, loss_mps: 0.069644, loss_cps: 0.120188
[13:17:34.483] iteration 13486: total_loss: 0.268391, loss_sup: 0.022723, loss_mps: 0.086850, loss_cps: 0.158819
[13:17:34.630] iteration 13487: total_loss: 0.177755, loss_sup: 0.005289, loss_mps: 0.062987, loss_cps: 0.109479
[13:17:34.775] iteration 13488: total_loss: 0.332467, loss_sup: 0.027150, loss_mps: 0.095418, loss_cps: 0.209899
[13:17:34.922] iteration 13489: total_loss: 0.365215, loss_sup: 0.033184, loss_mps: 0.108818, loss_cps: 0.223214
[13:17:35.068] iteration 13490: total_loss: 0.463617, loss_sup: 0.099695, loss_mps: 0.118431, loss_cps: 0.245491
[13:17:35.214] iteration 13491: total_loss: 0.341236, loss_sup: 0.066239, loss_mps: 0.099049, loss_cps: 0.175948
[13:17:35.360] iteration 13492: total_loss: 0.280261, loss_sup: 0.052217, loss_mps: 0.082788, loss_cps: 0.145255
[13:17:35.507] iteration 13493: total_loss: 0.244852, loss_sup: 0.034707, loss_mps: 0.072411, loss_cps: 0.137735
[13:17:35.653] iteration 13494: total_loss: 0.298453, loss_sup: 0.104430, loss_mps: 0.072373, loss_cps: 0.121650
[13:17:35.799] iteration 13495: total_loss: 0.252049, loss_sup: 0.008824, loss_mps: 0.085678, loss_cps: 0.157546
[13:17:35.946] iteration 13496: total_loss: 0.563930, loss_sup: 0.086127, loss_mps: 0.145837, loss_cps: 0.331965
[13:17:36.092] iteration 13497: total_loss: 0.537136, loss_sup: 0.024132, loss_mps: 0.155134, loss_cps: 0.357870
[13:17:36.239] iteration 13498: total_loss: 0.478920, loss_sup: 0.093924, loss_mps: 0.125429, loss_cps: 0.259567
[13:17:36.386] iteration 13499: total_loss: 0.436637, loss_sup: 0.122321, loss_mps: 0.103527, loss_cps: 0.210790
[13:17:36.531] iteration 13500: total_loss: 0.375888, loss_sup: 0.137053, loss_mps: 0.085336, loss_cps: 0.153499
[13:17:36.532] Evaluation Started ==>
[13:17:47.816] ==> valid iteration 13500: unet metrics: {'dc': 0.6150724003500923, 'jc': 0.49964553047588905, 'pre': 0.752133770716881, 'hd': 5.588943901891915}, ynet metrics: {'dc': 0.608165249874317, 'jc': 0.49002670128268305, 'pre': 0.7670074093549015, 'hd': 5.734745236821632}.
[13:17:47.818] Evaluation Finished!⏹️
[13:17:47.969] iteration 13501: total_loss: 0.472224, loss_sup: 0.159365, loss_mps: 0.100051, loss_cps: 0.212808
[13:17:48.117] iteration 13502: total_loss: 0.481763, loss_sup: 0.079691, loss_mps: 0.131431, loss_cps: 0.270641
[13:17:48.263] iteration 13503: total_loss: 0.274900, loss_sup: 0.080918, loss_mps: 0.069901, loss_cps: 0.124082
[13:17:48.409] iteration 13504: total_loss: 0.410260, loss_sup: 0.234019, loss_mps: 0.064601, loss_cps: 0.111640
[13:17:48.555] iteration 13505: total_loss: 0.279556, loss_sup: 0.043126, loss_mps: 0.079547, loss_cps: 0.156883
[13:17:48.700] iteration 13506: total_loss: 0.586327, loss_sup: 0.173576, loss_mps: 0.131909, loss_cps: 0.280842
[13:17:48.846] iteration 13507: total_loss: 0.373408, loss_sup: 0.021740, loss_mps: 0.111274, loss_cps: 0.240394
[13:17:48.991] iteration 13508: total_loss: 0.513024, loss_sup: 0.133798, loss_mps: 0.127913, loss_cps: 0.251313
[13:17:49.138] iteration 13509: total_loss: 0.346328, loss_sup: 0.078089, loss_mps: 0.091044, loss_cps: 0.177194
[13:17:49.286] iteration 13510: total_loss: 0.913201, loss_sup: 0.388891, loss_mps: 0.163848, loss_cps: 0.360462
[13:17:49.432] iteration 13511: total_loss: 0.268086, loss_sup: 0.061720, loss_mps: 0.079039, loss_cps: 0.127327
[13:17:49.579] iteration 13512: total_loss: 0.138981, loss_sup: 0.013570, loss_mps: 0.050989, loss_cps: 0.074422
[13:17:49.725] iteration 13513: total_loss: 0.348543, loss_sup: 0.027504, loss_mps: 0.109641, loss_cps: 0.211398
[13:17:49.870] iteration 13514: total_loss: 0.451006, loss_sup: 0.024599, loss_mps: 0.139429, loss_cps: 0.286977
[13:17:50.017] iteration 13515: total_loss: 0.194540, loss_sup: 0.012900, loss_mps: 0.067563, loss_cps: 0.114078
[13:17:50.163] iteration 13516: total_loss: 0.646808, loss_sup: 0.137361, loss_mps: 0.169143, loss_cps: 0.340304
[13:17:50.310] iteration 13517: total_loss: 0.408122, loss_sup: 0.037371, loss_mps: 0.122749, loss_cps: 0.248002
[13:17:50.456] iteration 13518: total_loss: 0.271323, loss_sup: 0.036041, loss_mps: 0.080916, loss_cps: 0.154366
[13:17:50.602] iteration 13519: total_loss: 0.303670, loss_sup: 0.071801, loss_mps: 0.081129, loss_cps: 0.150740
[13:17:50.747] iteration 13520: total_loss: 0.398186, loss_sup: 0.029477, loss_mps: 0.125278, loss_cps: 0.243432
[13:17:50.893] iteration 13521: total_loss: 0.464335, loss_sup: 0.051182, loss_mps: 0.135520, loss_cps: 0.277633
[13:17:51.045] iteration 13522: total_loss: 0.480451, loss_sup: 0.084841, loss_mps: 0.133165, loss_cps: 0.262445
[13:17:51.192] iteration 13523: total_loss: 0.300756, loss_sup: 0.018026, loss_mps: 0.094075, loss_cps: 0.188655
[13:17:51.339] iteration 13524: total_loss: 0.301658, loss_sup: 0.007588, loss_mps: 0.102668, loss_cps: 0.191402
[13:17:51.485] iteration 13525: total_loss: 0.552093, loss_sup: 0.110971, loss_mps: 0.147209, loss_cps: 0.293913
[13:17:51.632] iteration 13526: total_loss: 0.264429, loss_sup: 0.109025, loss_mps: 0.059343, loss_cps: 0.096061
[13:17:51.778] iteration 13527: total_loss: 0.376636, loss_sup: 0.129050, loss_mps: 0.088097, loss_cps: 0.159490
[13:17:51.924] iteration 13528: total_loss: 0.285655, loss_sup: 0.024568, loss_mps: 0.091666, loss_cps: 0.169421
[13:17:52.072] iteration 13529: total_loss: 0.416833, loss_sup: 0.056487, loss_mps: 0.120367, loss_cps: 0.239979
[13:17:52.218] iteration 13530: total_loss: 0.265670, loss_sup: 0.031261, loss_mps: 0.078702, loss_cps: 0.155707
[13:17:52.366] iteration 13531: total_loss: 0.588826, loss_sup: 0.150373, loss_mps: 0.143041, loss_cps: 0.295413
[13:17:52.512] iteration 13532: total_loss: 0.274642, loss_sup: 0.005135, loss_mps: 0.093255, loss_cps: 0.176252
[13:17:52.659] iteration 13533: total_loss: 0.361806, loss_sup: 0.100961, loss_mps: 0.095827, loss_cps: 0.165018
[13:17:52.805] iteration 13534: total_loss: 0.183584, loss_sup: 0.024425, loss_mps: 0.056252, loss_cps: 0.102907
[13:17:52.953] iteration 13535: total_loss: 0.221954, loss_sup: 0.019989, loss_mps: 0.074117, loss_cps: 0.127848
[13:17:53.099] iteration 13536: total_loss: 0.361243, loss_sup: 0.024971, loss_mps: 0.108289, loss_cps: 0.227983
[13:17:53.245] iteration 13537: total_loss: 0.432294, loss_sup: 0.095687, loss_mps: 0.114245, loss_cps: 0.222362
[13:17:53.391] iteration 13538: total_loss: 0.499651, loss_sup: 0.095676, loss_mps: 0.128953, loss_cps: 0.275021
[13:17:53.537] iteration 13539: total_loss: 0.238613, loss_sup: 0.013347, loss_mps: 0.078216, loss_cps: 0.147050
[13:17:53.683] iteration 13540: total_loss: 0.370861, loss_sup: 0.083306, loss_mps: 0.102230, loss_cps: 0.185325
[13:17:53.829] iteration 13541: total_loss: 0.476930, loss_sup: 0.050067, loss_mps: 0.135158, loss_cps: 0.291706
[13:17:53.974] iteration 13542: total_loss: 0.177332, loss_sup: 0.035993, loss_mps: 0.053744, loss_cps: 0.087595
[13:17:54.120] iteration 13543: total_loss: 0.255111, loss_sup: 0.040721, loss_mps: 0.076175, loss_cps: 0.138215
[13:17:54.266] iteration 13544: total_loss: 0.385871, loss_sup: 0.075669, loss_mps: 0.104185, loss_cps: 0.206017
[13:17:54.414] iteration 13545: total_loss: 0.439951, loss_sup: 0.169451, loss_mps: 0.091393, loss_cps: 0.179107
[13:17:54.559] iteration 13546: total_loss: 0.254928, loss_sup: 0.040684, loss_mps: 0.075646, loss_cps: 0.138598
[13:17:54.705] iteration 13547: total_loss: 0.548397, loss_sup: 0.265565, loss_mps: 0.098251, loss_cps: 0.184580
[13:17:54.851] iteration 13548: total_loss: 0.351007, loss_sup: 0.075959, loss_mps: 0.087688, loss_cps: 0.187361
[13:17:54.996] iteration 13549: total_loss: 0.172881, loss_sup: 0.020686, loss_mps: 0.057102, loss_cps: 0.095093
[13:17:55.142] iteration 13550: total_loss: 0.305163, loss_sup: 0.027113, loss_mps: 0.093543, loss_cps: 0.184507
[13:17:55.289] iteration 13551: total_loss: 0.362869, loss_sup: 0.025436, loss_mps: 0.116396, loss_cps: 0.221037
[13:17:55.435] iteration 13552: total_loss: 0.293950, loss_sup: 0.053129, loss_mps: 0.088673, loss_cps: 0.152149
[13:17:55.580] iteration 13553: total_loss: 0.387237, loss_sup: 0.068538, loss_mps: 0.105632, loss_cps: 0.213067
[13:17:55.726] iteration 13554: total_loss: 0.419715, loss_sup: 0.052626, loss_mps: 0.119545, loss_cps: 0.247544
[13:17:55.873] iteration 13555: total_loss: 0.632146, loss_sup: 0.024345, loss_mps: 0.189446, loss_cps: 0.418355
[13:17:56.019] iteration 13556: total_loss: 0.187506, loss_sup: 0.008887, loss_mps: 0.065320, loss_cps: 0.113299
[13:17:56.165] iteration 13557: total_loss: 0.553700, loss_sup: 0.067394, loss_mps: 0.159155, loss_cps: 0.327151
[13:17:56.311] iteration 13558: total_loss: 0.252655, loss_sup: 0.007414, loss_mps: 0.084168, loss_cps: 0.161073
[13:17:56.461] iteration 13559: total_loss: 0.232093, loss_sup: 0.056416, loss_mps: 0.065704, loss_cps: 0.109973
[13:17:56.606] iteration 13560: total_loss: 0.331626, loss_sup: 0.056365, loss_mps: 0.098777, loss_cps: 0.176484
[13:17:56.754] iteration 13561: total_loss: 0.257048, loss_sup: 0.059157, loss_mps: 0.073790, loss_cps: 0.124101
[13:17:56.900] iteration 13562: total_loss: 0.331419, loss_sup: 0.034299, loss_mps: 0.100772, loss_cps: 0.196348
[13:17:57.048] iteration 13563: total_loss: 0.730287, loss_sup: 0.269104, loss_mps: 0.145732, loss_cps: 0.315450
[13:17:57.193] iteration 13564: total_loss: 0.429100, loss_sup: 0.050936, loss_mps: 0.126317, loss_cps: 0.251848
[13:17:57.339] iteration 13565: total_loss: 0.397989, loss_sup: 0.029424, loss_mps: 0.123409, loss_cps: 0.245156
[13:17:57.485] iteration 13566: total_loss: 0.480454, loss_sup: 0.060382, loss_mps: 0.137669, loss_cps: 0.282403
[13:17:57.638] iteration 13567: total_loss: 0.556075, loss_sup: 0.058475, loss_mps: 0.159934, loss_cps: 0.337666
[13:17:57.787] iteration 13568: total_loss: 0.385673, loss_sup: 0.087692, loss_mps: 0.104037, loss_cps: 0.193944
[13:17:57.933] iteration 13569: total_loss: 0.455527, loss_sup: 0.136987, loss_mps: 0.106339, loss_cps: 0.212201
[13:17:58.079] iteration 13570: total_loss: 0.256956, loss_sup: 0.045697, loss_mps: 0.076561, loss_cps: 0.134699
[13:17:58.226] iteration 13571: total_loss: 0.393574, loss_sup: 0.109330, loss_mps: 0.095262, loss_cps: 0.188982
[13:17:58.371] iteration 13572: total_loss: 0.365586, loss_sup: 0.112310, loss_mps: 0.087007, loss_cps: 0.166270
[13:17:58.517] iteration 13573: total_loss: 0.347274, loss_sup: 0.065528, loss_mps: 0.092354, loss_cps: 0.189392
[13:17:58.665] iteration 13574: total_loss: 0.429972, loss_sup: 0.194682, loss_mps: 0.082321, loss_cps: 0.152968
[13:17:58.810] iteration 13575: total_loss: 0.420418, loss_sup: 0.063395, loss_mps: 0.109919, loss_cps: 0.247104
[13:17:58.956] iteration 13576: total_loss: 0.236999, loss_sup: 0.011501, loss_mps: 0.078350, loss_cps: 0.147147
[13:17:59.102] iteration 13577: total_loss: 0.299393, loss_sup: 0.051467, loss_mps: 0.086034, loss_cps: 0.161892
[13:17:59.248] iteration 13578: total_loss: 0.384162, loss_sup: 0.095649, loss_mps: 0.096848, loss_cps: 0.191665
[13:17:59.394] iteration 13579: total_loss: 0.236298, loss_sup: 0.020135, loss_mps: 0.077254, loss_cps: 0.138910
[13:17:59.539] iteration 13580: total_loss: 0.597260, loss_sup: 0.236828, loss_mps: 0.121294, loss_cps: 0.239139
[13:17:59.685] iteration 13581: total_loss: 0.420820, loss_sup: 0.118598, loss_mps: 0.100655, loss_cps: 0.201568
[13:17:59.831] iteration 13582: total_loss: 0.304603, loss_sup: 0.009461, loss_mps: 0.098533, loss_cps: 0.196610
[13:17:59.977] iteration 13583: total_loss: 0.486674, loss_sup: 0.134260, loss_mps: 0.115598, loss_cps: 0.236817
[13:18:00.123] iteration 13584: total_loss: 0.245115, loss_sup: 0.017456, loss_mps: 0.078081, loss_cps: 0.149578
[13:18:00.269] iteration 13585: total_loss: 0.489382, loss_sup: 0.060169, loss_mps: 0.137603, loss_cps: 0.291610
[13:18:00.415] iteration 13586: total_loss: 0.437951, loss_sup: 0.078585, loss_mps: 0.117546, loss_cps: 0.241820
[13:18:00.561] iteration 13587: total_loss: 0.246811, loss_sup: 0.017781, loss_mps: 0.083248, loss_cps: 0.145782
[13:18:00.707] iteration 13588: total_loss: 0.258865, loss_sup: 0.067901, loss_mps: 0.072457, loss_cps: 0.118507
[13:18:00.853] iteration 13589: total_loss: 0.403213, loss_sup: 0.082840, loss_mps: 0.109628, loss_cps: 0.210745
[13:18:00.999] iteration 13590: total_loss: 0.818628, loss_sup: 0.520505, loss_mps: 0.105114, loss_cps: 0.193009
[13:18:01.145] iteration 13591: total_loss: 0.534553, loss_sup: 0.213684, loss_mps: 0.110769, loss_cps: 0.210100
[13:18:01.291] iteration 13592: total_loss: 0.504955, loss_sup: 0.148272, loss_mps: 0.115386, loss_cps: 0.241297
[13:18:01.439] iteration 13593: total_loss: 0.347807, loss_sup: 0.046240, loss_mps: 0.105466, loss_cps: 0.196101
[13:18:01.585] iteration 13594: total_loss: 0.453484, loss_sup: 0.035409, loss_mps: 0.140550, loss_cps: 0.277524
[13:18:01.731] iteration 13595: total_loss: 0.397444, loss_sup: 0.019193, loss_mps: 0.127214, loss_cps: 0.251037
[13:18:01.877] iteration 13596: total_loss: 0.833648, loss_sup: 0.155292, loss_mps: 0.214403, loss_cps: 0.463953
[13:18:02.024] iteration 13597: total_loss: 0.324421, loss_sup: 0.045495, loss_mps: 0.104217, loss_cps: 0.174710
[13:18:02.170] iteration 13598: total_loss: 0.366518, loss_sup: 0.057572, loss_mps: 0.109318, loss_cps: 0.199629
[13:18:02.318] iteration 13599: total_loss: 0.298428, loss_sup: 0.030306, loss_mps: 0.090002, loss_cps: 0.178121
[13:18:02.464] iteration 13600: total_loss: 0.639576, loss_sup: 0.079212, loss_mps: 0.170971, loss_cps: 0.389393
[13:18:02.464] Evaluation Started ==>
[13:18:13.759] ==> valid iteration 13600: unet metrics: {'dc': 0.641220881678831, 'jc': 0.5170208590979114, 'pre': 0.721444484541602, 'hd': 6.04962454909884}, ynet metrics: {'dc': 0.5824193517079014, 'jc': 0.47157658405244784, 'pre': 0.74220473986537, 'hd': 5.658262306377592}.
[13:18:13.762] Evaluation Finished!⏹️
[13:18:13.913] iteration 13601: total_loss: 0.238324, loss_sup: 0.031257, loss_mps: 0.075151, loss_cps: 0.131917
[13:18:14.060] iteration 13602: total_loss: 0.342994, loss_sup: 0.058362, loss_mps: 0.101774, loss_cps: 0.182857
[13:18:14.206] iteration 13603: total_loss: 0.511224, loss_sup: 0.059352, loss_mps: 0.157613, loss_cps: 0.294259
[13:18:14.352] iteration 13604: total_loss: 0.377659, loss_sup: 0.041060, loss_mps: 0.115907, loss_cps: 0.220691
[13:18:14.499] iteration 13605: total_loss: 0.387986, loss_sup: 0.038270, loss_mps: 0.115858, loss_cps: 0.233859
[13:18:14.645] iteration 13606: total_loss: 0.263166, loss_sup: 0.043205, loss_mps: 0.076367, loss_cps: 0.143594
[13:18:14.790] iteration 13607: total_loss: 0.333409, loss_sup: 0.090892, loss_mps: 0.084584, loss_cps: 0.157933
[13:18:14.936] iteration 13608: total_loss: 0.681531, loss_sup: 0.087488, loss_mps: 0.188583, loss_cps: 0.405460
[13:18:15.082] iteration 13609: total_loss: 0.687830, loss_sup: 0.178459, loss_mps: 0.164909, loss_cps: 0.344463
[13:18:15.228] iteration 13610: total_loss: 0.300825, loss_sup: 0.044992, loss_mps: 0.090745, loss_cps: 0.165088
[13:18:15.374] iteration 13611: total_loss: 0.427598, loss_sup: 0.079092, loss_mps: 0.118237, loss_cps: 0.230270
[13:18:15.519] iteration 13612: total_loss: 1.066196, loss_sup: 0.303410, loss_mps: 0.237766, loss_cps: 0.525019
[13:18:15.665] iteration 13613: total_loss: 0.923887, loss_sup: 0.481363, loss_mps: 0.138752, loss_cps: 0.303772
[13:18:15.811] iteration 13614: total_loss: 0.334746, loss_sup: 0.019841, loss_mps: 0.101321, loss_cps: 0.213583
[13:18:15.959] iteration 13615: total_loss: 0.373201, loss_sup: 0.131674, loss_mps: 0.085994, loss_cps: 0.155532
[13:18:16.105] iteration 13616: total_loss: 0.675005, loss_sup: 0.091731, loss_mps: 0.176243, loss_cps: 0.407031
[13:18:16.251] iteration 13617: total_loss: 0.642462, loss_sup: 0.263205, loss_mps: 0.121050, loss_cps: 0.258207
[13:18:16.397] iteration 13618: total_loss: 0.297889, loss_sup: 0.006612, loss_mps: 0.098453, loss_cps: 0.192824
[13:18:16.542] iteration 13619: total_loss: 0.391136, loss_sup: 0.127585, loss_mps: 0.096565, loss_cps: 0.166985
[13:18:16.688] iteration 13620: total_loss: 0.394819, loss_sup: 0.090282, loss_mps: 0.109653, loss_cps: 0.194884
[13:18:16.833] iteration 13621: total_loss: 0.450396, loss_sup: 0.268360, loss_mps: 0.068690, loss_cps: 0.113346
[13:18:16.980] iteration 13622: total_loss: 0.358485, loss_sup: 0.103085, loss_mps: 0.086855, loss_cps: 0.168544
[13:18:17.126] iteration 13623: total_loss: 0.553831, loss_sup: 0.201897, loss_mps: 0.123669, loss_cps: 0.228265
[13:18:17.273] iteration 13624: total_loss: 0.648359, loss_sup: 0.175251, loss_mps: 0.153774, loss_cps: 0.319335
[13:18:17.419] iteration 13625: total_loss: 0.296988, loss_sup: 0.025330, loss_mps: 0.091883, loss_cps: 0.179776
[13:18:17.565] iteration 13626: total_loss: 0.314330, loss_sup: 0.042258, loss_mps: 0.097841, loss_cps: 0.174231
[13:18:17.711] iteration 13627: total_loss: 0.391860, loss_sup: 0.139054, loss_mps: 0.090030, loss_cps: 0.162775
[13:18:17.856] iteration 13628: total_loss: 0.427384, loss_sup: 0.027643, loss_mps: 0.131669, loss_cps: 0.268072
[13:18:18.003] iteration 13629: total_loss: 0.229661, loss_sup: 0.012265, loss_mps: 0.080319, loss_cps: 0.137076
[13:18:18.150] iteration 13630: total_loss: 0.389628, loss_sup: 0.120936, loss_mps: 0.098297, loss_cps: 0.170396
[13:18:18.298] iteration 13631: total_loss: 0.349864, loss_sup: 0.135939, loss_mps: 0.083783, loss_cps: 0.130141
[13:18:18.444] iteration 13632: total_loss: 0.470201, loss_sup: 0.058223, loss_mps: 0.138476, loss_cps: 0.273502
[13:18:18.590] iteration 13633: total_loss: 0.256454, loss_sup: 0.013583, loss_mps: 0.090258, loss_cps: 0.152612
[13:18:18.736] iteration 13634: total_loss: 0.640187, loss_sup: 0.114171, loss_mps: 0.171998, loss_cps: 0.354017
[13:18:18.881] iteration 13635: total_loss: 0.680892, loss_sup: 0.190107, loss_mps: 0.165594, loss_cps: 0.325191
[13:18:19.027] iteration 13636: total_loss: 0.247985, loss_sup: 0.042645, loss_mps: 0.077802, loss_cps: 0.127538
[13:18:19.173] iteration 13637: total_loss: 0.262871, loss_sup: 0.045297, loss_mps: 0.083908, loss_cps: 0.133666
[13:18:19.318] iteration 13638: total_loss: 0.249870, loss_sup: 0.061197, loss_mps: 0.069721, loss_cps: 0.118953
[13:18:19.468] iteration 13639: total_loss: 0.405650, loss_sup: 0.122524, loss_mps: 0.098696, loss_cps: 0.184431
[13:18:19.614] iteration 13640: total_loss: 0.419812, loss_sup: 0.004998, loss_mps: 0.142385, loss_cps: 0.272429
[13:18:19.760] iteration 13641: total_loss: 0.375537, loss_sup: 0.042857, loss_mps: 0.116781, loss_cps: 0.215898
[13:18:19.906] iteration 13642: total_loss: 0.412047, loss_sup: 0.161176, loss_mps: 0.092109, loss_cps: 0.158762
[13:18:20.053] iteration 13643: total_loss: 0.257302, loss_sup: 0.032425, loss_mps: 0.080418, loss_cps: 0.144460
[13:18:20.200] iteration 13644: total_loss: 0.320480, loss_sup: 0.059654, loss_mps: 0.088073, loss_cps: 0.172753
[13:18:20.347] iteration 13645: total_loss: 0.393002, loss_sup: 0.092229, loss_mps: 0.096305, loss_cps: 0.204469
[13:18:20.493] iteration 13646: total_loss: 0.410764, loss_sup: 0.062008, loss_mps: 0.117199, loss_cps: 0.231557
[13:18:20.639] iteration 13647: total_loss: 0.261131, loss_sup: 0.051095, loss_mps: 0.074075, loss_cps: 0.135961
[13:18:20.785] iteration 13648: total_loss: 0.497441, loss_sup: 0.149215, loss_mps: 0.119439, loss_cps: 0.228788
[13:18:20.931] iteration 13649: total_loss: 0.331595, loss_sup: 0.057529, loss_mps: 0.092808, loss_cps: 0.181257
[13:18:21.079] iteration 13650: total_loss: 0.328418, loss_sup: 0.027918, loss_mps: 0.100917, loss_cps: 0.199583
[13:18:21.225] iteration 13651: total_loss: 0.515360, loss_sup: 0.132915, loss_mps: 0.123469, loss_cps: 0.258976
[13:18:21.372] iteration 13652: total_loss: 0.260678, loss_sup: 0.073073, loss_mps: 0.067984, loss_cps: 0.119622
[13:18:21.519] iteration 13653: total_loss: 0.524647, loss_sup: 0.098896, loss_mps: 0.135001, loss_cps: 0.290750
[13:18:21.665] iteration 13654: total_loss: 0.512459, loss_sup: 0.039990, loss_mps: 0.146409, loss_cps: 0.326061
[13:18:21.811] iteration 13655: total_loss: 0.357924, loss_sup: 0.020265, loss_mps: 0.104526, loss_cps: 0.233133
[13:18:21.957] iteration 13656: total_loss: 0.626169, loss_sup: 0.073154, loss_mps: 0.161456, loss_cps: 0.391558
[13:18:22.103] iteration 13657: total_loss: 0.229088, loss_sup: 0.042606, loss_mps: 0.068469, loss_cps: 0.118013
[13:18:22.250] iteration 13658: total_loss: 0.541333, loss_sup: 0.136901, loss_mps: 0.120606, loss_cps: 0.283826
[13:18:22.395] iteration 13659: total_loss: 0.285437, loss_sup: 0.036037, loss_mps: 0.083488, loss_cps: 0.165912
[13:18:22.541] iteration 13660: total_loss: 0.942663, loss_sup: 0.328002, loss_mps: 0.185247, loss_cps: 0.429414
[13:18:22.690] iteration 13661: total_loss: 0.356630, loss_sup: 0.017335, loss_mps: 0.113020, loss_cps: 0.226274
[13:18:22.840] iteration 13662: total_loss: 0.321281, loss_sup: 0.041801, loss_mps: 0.090391, loss_cps: 0.189089
[13:18:22.986] iteration 13663: total_loss: 0.462213, loss_sup: 0.121336, loss_mps: 0.102573, loss_cps: 0.238304
[13:18:23.132] iteration 13664: total_loss: 0.639543, loss_sup: 0.170271, loss_mps: 0.158987, loss_cps: 0.310285
[13:18:23.278] iteration 13665: total_loss: 0.338539, loss_sup: 0.054731, loss_mps: 0.101399, loss_cps: 0.182409
[13:18:23.426] iteration 13666: total_loss: 0.372362, loss_sup: 0.104179, loss_mps: 0.089530, loss_cps: 0.178653
[13:18:23.572] iteration 13667: total_loss: 0.446859, loss_sup: 0.087378, loss_mps: 0.114975, loss_cps: 0.244506
[13:18:23.718] iteration 13668: total_loss: 0.344560, loss_sup: 0.079633, loss_mps: 0.091508, loss_cps: 0.173419
[13:18:23.865] iteration 13669: total_loss: 0.390313, loss_sup: 0.138151, loss_mps: 0.088596, loss_cps: 0.163565
[13:18:24.010] iteration 13670: total_loss: 0.566656, loss_sup: 0.215906, loss_mps: 0.116083, loss_cps: 0.234667
[13:18:24.157] iteration 13671: total_loss: 0.777153, loss_sup: 0.255685, loss_mps: 0.167948, loss_cps: 0.353520
[13:18:24.303] iteration 13672: total_loss: 0.367790, loss_sup: 0.032057, loss_mps: 0.113459, loss_cps: 0.222274
[13:18:24.449] iteration 13673: total_loss: 0.335467, loss_sup: 0.051991, loss_mps: 0.093964, loss_cps: 0.189513
[13:18:24.595] iteration 13674: total_loss: 0.541110, loss_sup: 0.074791, loss_mps: 0.150601, loss_cps: 0.315718
[13:18:24.741] iteration 13675: total_loss: 0.206311, loss_sup: 0.006498, loss_mps: 0.071432, loss_cps: 0.128382
[13:18:24.887] iteration 13676: total_loss: 0.413410, loss_sup: 0.076787, loss_mps: 0.107491, loss_cps: 0.229133
[13:18:25.033] iteration 13677: total_loss: 0.704386, loss_sup: 0.207473, loss_mps: 0.166476, loss_cps: 0.330437
[13:18:25.179] iteration 13678: total_loss: 0.809158, loss_sup: 0.193120, loss_mps: 0.192398, loss_cps: 0.423640
[13:18:25.325] iteration 13679: total_loss: 0.530016, loss_sup: 0.037213, loss_mps: 0.160570, loss_cps: 0.332233
[13:18:25.471] iteration 13680: total_loss: 0.420115, loss_sup: 0.047125, loss_mps: 0.124736, loss_cps: 0.248253
[13:18:25.619] iteration 13681: total_loss: 0.200992, loss_sup: 0.026558, loss_mps: 0.068052, loss_cps: 0.106381
[13:18:25.766] iteration 13682: total_loss: 0.377231, loss_sup: 0.204386, loss_mps: 0.067427, loss_cps: 0.105418
[13:18:25.912] iteration 13683: total_loss: 0.361515, loss_sup: 0.165865, loss_mps: 0.077249, loss_cps: 0.118401
[13:18:26.057] iteration 13684: total_loss: 0.420890, loss_sup: 0.088488, loss_mps: 0.117466, loss_cps: 0.214936
[13:18:26.205] iteration 13685: total_loss: 0.260613, loss_sup: 0.028398, loss_mps: 0.086120, loss_cps: 0.146094
[13:18:26.351] iteration 13686: total_loss: 0.476108, loss_sup: 0.119816, loss_mps: 0.132534, loss_cps: 0.223758
[13:18:26.497] iteration 13687: total_loss: 0.563628, loss_sup: 0.233024, loss_mps: 0.117034, loss_cps: 0.213570
[13:18:26.643] iteration 13688: total_loss: 0.319162, loss_sup: 0.008698, loss_mps: 0.110934, loss_cps: 0.199531
[13:18:26.793] iteration 13689: total_loss: 0.279090, loss_sup: 0.037513, loss_mps: 0.087098, loss_cps: 0.154479
[13:18:26.938] iteration 13690: total_loss: 0.233866, loss_sup: 0.046935, loss_mps: 0.066769, loss_cps: 0.120163
[13:18:27.085] iteration 13691: total_loss: 0.568010, loss_sup: 0.130035, loss_mps: 0.155215, loss_cps: 0.282760
[13:18:27.231] iteration 13692: total_loss: 0.190022, loss_sup: 0.016290, loss_mps: 0.067131, loss_cps: 0.106600
[13:18:27.377] iteration 13693: total_loss: 0.431086, loss_sup: 0.198104, loss_mps: 0.084217, loss_cps: 0.148765
[13:18:27.524] iteration 13694: total_loss: 0.465913, loss_sup: 0.078110, loss_mps: 0.124052, loss_cps: 0.263750
[13:18:27.671] iteration 13695: total_loss: 0.151366, loss_sup: 0.035301, loss_mps: 0.046821, loss_cps: 0.069244
[13:18:27.817] iteration 13696: total_loss: 0.472129, loss_sup: 0.128505, loss_mps: 0.124232, loss_cps: 0.219393
[13:18:27.967] iteration 13697: total_loss: 0.266637, loss_sup: 0.048319, loss_mps: 0.078493, loss_cps: 0.139825
[13:18:28.114] iteration 13698: total_loss: 0.575284, loss_sup: 0.094755, loss_mps: 0.157493, loss_cps: 0.323037
[13:18:28.261] iteration 13699: total_loss: 0.469681, loss_sup: 0.089929, loss_mps: 0.126748, loss_cps: 0.253004
[13:18:28.408] iteration 13700: total_loss: 0.216797, loss_sup: 0.006024, loss_mps: 0.077995, loss_cps: 0.132778
[13:18:28.408] Evaluation Started ==>
[13:18:39.735] ==> valid iteration 13700: unet metrics: {'dc': 0.6384871953783653, 'jc': 0.5167047955970967, 'pre': 0.7850737783993681, 'hd': 5.738724177391181}, ynet metrics: {'dc': 0.599282289933536, 'jc': 0.483327772988203, 'pre': 0.7634394129706317, 'hd': 5.642293065525864}.
[13:18:39.738] Evaluation Finished!⏹️
[13:18:39.890] iteration 13701: total_loss: 0.230006, loss_sup: 0.048742, loss_mps: 0.067417, loss_cps: 0.113847
[13:18:40.038] iteration 13702: total_loss: 0.302916, loss_sup: 0.020539, loss_mps: 0.101476, loss_cps: 0.180901
[13:18:40.186] iteration 13703: total_loss: 0.362482, loss_sup: 0.058528, loss_mps: 0.107790, loss_cps: 0.196165
[13:18:40.331] iteration 13704: total_loss: 0.339426, loss_sup: 0.033793, loss_mps: 0.100024, loss_cps: 0.205608
[13:18:40.478] iteration 13705: total_loss: 0.702274, loss_sup: 0.365451, loss_mps: 0.113313, loss_cps: 0.223510
[13:18:40.624] iteration 13706: total_loss: 0.275571, loss_sup: 0.043107, loss_mps: 0.082016, loss_cps: 0.150447
[13:18:40.769] iteration 13707: total_loss: 0.248996, loss_sup: 0.076491, loss_mps: 0.066329, loss_cps: 0.106176
[13:18:40.915] iteration 13708: total_loss: 0.261675, loss_sup: 0.038633, loss_mps: 0.074344, loss_cps: 0.148697
[13:18:41.061] iteration 13709: total_loss: 0.236624, loss_sup: 0.004411, loss_mps: 0.082835, loss_cps: 0.149378
[13:18:41.207] iteration 13710: total_loss: 0.383479, loss_sup: 0.154452, loss_mps: 0.080966, loss_cps: 0.148060
[13:18:41.357] iteration 13711: total_loss: 0.292508, loss_sup: 0.026514, loss_mps: 0.090141, loss_cps: 0.175853
[13:18:41.505] iteration 13712: total_loss: 0.407211, loss_sup: 0.095169, loss_mps: 0.110801, loss_cps: 0.201240
[13:18:41.651] iteration 13713: total_loss: 0.264356, loss_sup: 0.060715, loss_mps: 0.070141, loss_cps: 0.133500
[13:18:41.797] iteration 13714: total_loss: 0.510111, loss_sup: 0.166727, loss_mps: 0.118942, loss_cps: 0.224442
[13:18:41.943] iteration 13715: total_loss: 0.347898, loss_sup: 0.066347, loss_mps: 0.099707, loss_cps: 0.181845
[13:18:42.091] iteration 13716: total_loss: 0.347344, loss_sup: 0.072936, loss_mps: 0.095387, loss_cps: 0.179021
[13:18:42.237] iteration 13717: total_loss: 0.292373, loss_sup: 0.083141, loss_mps: 0.077007, loss_cps: 0.132225
[13:18:42.383] iteration 13718: total_loss: 0.315471, loss_sup: 0.071642, loss_mps: 0.087882, loss_cps: 0.155947
[13:18:42.530] iteration 13719: total_loss: 0.321346, loss_sup: 0.099720, loss_mps: 0.078064, loss_cps: 0.143562
[13:18:42.675] iteration 13720: total_loss: 0.350101, loss_sup: 0.062321, loss_mps: 0.096351, loss_cps: 0.191429
[13:18:42.821] iteration 13721: total_loss: 0.340409, loss_sup: 0.032697, loss_mps: 0.101832, loss_cps: 0.205880
[13:18:42.967] iteration 13722: total_loss: 0.384660, loss_sup: 0.091035, loss_mps: 0.104484, loss_cps: 0.189141
[13:18:43.112] iteration 13723: total_loss: 0.340430, loss_sup: 0.048485, loss_mps: 0.093867, loss_cps: 0.198078
[13:18:43.258] iteration 13724: total_loss: 0.364644, loss_sup: 0.142602, loss_mps: 0.081911, loss_cps: 0.140131
[13:18:43.403] iteration 13725: total_loss: 0.267496, loss_sup: 0.024809, loss_mps: 0.084266, loss_cps: 0.158421
[13:18:43.549] iteration 13726: total_loss: 0.433306, loss_sup: 0.103436, loss_mps: 0.112941, loss_cps: 0.216928
[13:18:43.694] iteration 13727: total_loss: 0.233224, loss_sup: 0.044722, loss_mps: 0.072066, loss_cps: 0.116436
[13:18:43.840] iteration 13728: total_loss: 0.359057, loss_sup: 0.093377, loss_mps: 0.090650, loss_cps: 0.175029
[13:18:43.986] iteration 13729: total_loss: 0.543676, loss_sup: 0.075090, loss_mps: 0.150144, loss_cps: 0.318443
[13:18:44.132] iteration 13730: total_loss: 0.311865, loss_sup: 0.048736, loss_mps: 0.092616, loss_cps: 0.170513
[13:18:44.277] iteration 13731: total_loss: 0.203811, loss_sup: 0.012736, loss_mps: 0.066597, loss_cps: 0.124479
[13:18:44.425] iteration 13732: total_loss: 0.337718, loss_sup: 0.015052, loss_mps: 0.107813, loss_cps: 0.214853
[13:18:44.571] iteration 13733: total_loss: 0.153590, loss_sup: 0.010674, loss_mps: 0.053619, loss_cps: 0.089297
[13:18:44.717] iteration 13734: total_loss: 0.503585, loss_sup: 0.142022, loss_mps: 0.118179, loss_cps: 0.243383
[13:18:44.863] iteration 13735: total_loss: 0.354692, loss_sup: 0.034011, loss_mps: 0.110735, loss_cps: 0.209945
[13:18:45.009] iteration 13736: total_loss: 0.373892, loss_sup: 0.098622, loss_mps: 0.090489, loss_cps: 0.184781
[13:18:45.155] iteration 13737: total_loss: 0.275382, loss_sup: 0.056227, loss_mps: 0.075655, loss_cps: 0.143500
[13:18:45.302] iteration 13738: total_loss: 0.288646, loss_sup: 0.122415, loss_mps: 0.059984, loss_cps: 0.106247
[13:18:45.448] iteration 13739: total_loss: 0.252738, loss_sup: 0.007112, loss_mps: 0.080927, loss_cps: 0.164699
[13:18:45.593] iteration 13740: total_loss: 0.307969, loss_sup: 0.070844, loss_mps: 0.078957, loss_cps: 0.158167
[13:18:45.739] iteration 13741: total_loss: 0.263606, loss_sup: 0.028214, loss_mps: 0.079526, loss_cps: 0.155867
[13:18:45.885] iteration 13742: total_loss: 0.187915, loss_sup: 0.036899, loss_mps: 0.053706, loss_cps: 0.097310
[13:18:46.031] iteration 13743: total_loss: 0.172820, loss_sup: 0.026050, loss_mps: 0.053593, loss_cps: 0.093177
[13:18:46.177] iteration 13744: total_loss: 0.334708, loss_sup: 0.044479, loss_mps: 0.095963, loss_cps: 0.194266
[13:18:46.324] iteration 13745: total_loss: 0.193070, loss_sup: 0.011890, loss_mps: 0.062766, loss_cps: 0.118414
[13:18:46.470] iteration 13746: total_loss: 0.622708, loss_sup: 0.099437, loss_mps: 0.171800, loss_cps: 0.351471
[13:18:46.615] iteration 13747: total_loss: 0.222242, loss_sup: 0.012571, loss_mps: 0.078479, loss_cps: 0.131191
[13:18:46.761] iteration 13748: total_loss: 0.217048, loss_sup: 0.038761, loss_mps: 0.061855, loss_cps: 0.116432
[13:18:46.906] iteration 13749: total_loss: 0.215727, loss_sup: 0.026384, loss_mps: 0.069236, loss_cps: 0.120107
[13:18:47.053] iteration 13750: total_loss: 0.330606, loss_sup: 0.034258, loss_mps: 0.097084, loss_cps: 0.199264
[13:18:47.199] iteration 13751: total_loss: 0.418049, loss_sup: 0.013467, loss_mps: 0.130634, loss_cps: 0.273949
[13:18:47.345] iteration 13752: total_loss: 0.300997, loss_sup: 0.006150, loss_mps: 0.100666, loss_cps: 0.194181
[13:18:47.493] iteration 13753: total_loss: 0.385051, loss_sup: 0.037956, loss_mps: 0.110610, loss_cps: 0.236485
[13:18:47.642] iteration 13754: total_loss: 0.202244, loss_sup: 0.009195, loss_mps: 0.071163, loss_cps: 0.121885
[13:18:47.788] iteration 13755: total_loss: 0.279519, loss_sup: 0.088186, loss_mps: 0.064798, loss_cps: 0.126535
[13:18:47.934] iteration 13756: total_loss: 0.338446, loss_sup: 0.159089, loss_mps: 0.063368, loss_cps: 0.115989
[13:18:48.079] iteration 13757: total_loss: 0.132366, loss_sup: 0.012765, loss_mps: 0.043634, loss_cps: 0.075967
[13:18:48.226] iteration 13758: total_loss: 0.312440, loss_sup: 0.052212, loss_mps: 0.084750, loss_cps: 0.175479
[13:18:48.371] iteration 13759: total_loss: 0.283341, loss_sup: 0.109833, loss_mps: 0.061944, loss_cps: 0.111564
[13:18:48.517] iteration 13760: total_loss: 0.433727, loss_sup: 0.115574, loss_mps: 0.107644, loss_cps: 0.210509
[13:18:48.662] iteration 13761: total_loss: 0.280267, loss_sup: 0.015240, loss_mps: 0.086178, loss_cps: 0.178850
[13:18:48.809] iteration 13762: total_loss: 0.348008, loss_sup: 0.109322, loss_mps: 0.077404, loss_cps: 0.161282
[13:18:48.954] iteration 13763: total_loss: 0.335562, loss_sup: 0.139854, loss_mps: 0.070724, loss_cps: 0.124983
[13:18:49.099] iteration 13764: total_loss: 0.384001, loss_sup: 0.097391, loss_mps: 0.098390, loss_cps: 0.188219
[13:18:49.245] iteration 13765: total_loss: 0.322592, loss_sup: 0.018822, loss_mps: 0.101416, loss_cps: 0.202354
[13:18:49.390] iteration 13766: total_loss: 0.262498, loss_sup: 0.030982, loss_mps: 0.079468, loss_cps: 0.152048
[13:18:49.538] iteration 13767: total_loss: 0.542286, loss_sup: 0.213162, loss_mps: 0.113583, loss_cps: 0.215541
[13:18:49.684] iteration 13768: total_loss: 0.436169, loss_sup: 0.188886, loss_mps: 0.084328, loss_cps: 0.162954
[13:18:49.831] iteration 13769: total_loss: 0.258472, loss_sup: 0.033823, loss_mps: 0.075976, loss_cps: 0.148673
[13:18:49.978] iteration 13770: total_loss: 0.307622, loss_sup: 0.041114, loss_mps: 0.087145, loss_cps: 0.179364
[13:18:50.124] iteration 13771: total_loss: 0.293090, loss_sup: 0.118843, loss_mps: 0.061304, loss_cps: 0.112943
[13:18:50.271] iteration 13772: total_loss: 0.324249, loss_sup: 0.025656, loss_mps: 0.097651, loss_cps: 0.200942
[13:18:50.417] iteration 13773: total_loss: 0.474073, loss_sup: 0.170045, loss_mps: 0.107972, loss_cps: 0.196057
[13:18:50.566] iteration 13774: total_loss: 0.272226, loss_sup: 0.053749, loss_mps: 0.076534, loss_cps: 0.141943
[13:18:50.712] iteration 13775: total_loss: 0.322377, loss_sup: 0.059936, loss_mps: 0.090320, loss_cps: 0.172121
[13:18:50.859] iteration 13776: total_loss: 0.249955, loss_sup: 0.037447, loss_mps: 0.074494, loss_cps: 0.138015
[13:18:51.005] iteration 13777: total_loss: 0.293718, loss_sup: 0.011139, loss_mps: 0.099775, loss_cps: 0.182805
[13:18:51.152] iteration 13778: total_loss: 0.467363, loss_sup: 0.103536, loss_mps: 0.121055, loss_cps: 0.242772
[13:18:51.299] iteration 13779: total_loss: 0.288963, loss_sup: 0.002612, loss_mps: 0.096185, loss_cps: 0.190166
[13:18:51.445] iteration 13780: total_loss: 0.257277, loss_sup: 0.029095, loss_mps: 0.079220, loss_cps: 0.148963
[13:18:51.592] iteration 13781: total_loss: 0.477970, loss_sup: 0.184712, loss_mps: 0.101647, loss_cps: 0.191610
[13:18:51.742] iteration 13782: total_loss: 0.432076, loss_sup: 0.058654, loss_mps: 0.119653, loss_cps: 0.253770
[13:18:51.888] iteration 13783: total_loss: 0.693882, loss_sup: 0.211575, loss_mps: 0.155298, loss_cps: 0.327009
[13:18:52.034] iteration 13784: total_loss: 0.295027, loss_sup: 0.019335, loss_mps: 0.096885, loss_cps: 0.178806
[13:18:52.181] iteration 13785: total_loss: 0.244798, loss_sup: 0.074539, loss_mps: 0.065114, loss_cps: 0.105145
[13:18:52.327] iteration 13786: total_loss: 0.226151, loss_sup: 0.030104, loss_mps: 0.070730, loss_cps: 0.125317
[13:18:52.473] iteration 13787: total_loss: 0.404184, loss_sup: 0.097415, loss_mps: 0.100064, loss_cps: 0.206705
[13:18:52.619] iteration 13788: total_loss: 0.396705, loss_sup: 0.215402, loss_mps: 0.066801, loss_cps: 0.114503
[13:18:52.764] iteration 13789: total_loss: 0.320341, loss_sup: 0.013826, loss_mps: 0.100865, loss_cps: 0.205650
[13:18:52.910] iteration 13790: total_loss: 0.303191, loss_sup: 0.101857, loss_mps: 0.070323, loss_cps: 0.131011
[13:18:53.056] iteration 13791: total_loss: 0.516819, loss_sup: 0.182371, loss_mps: 0.109185, loss_cps: 0.225263
[13:18:53.203] iteration 13792: total_loss: 0.487475, loss_sup: 0.097573, loss_mps: 0.122402, loss_cps: 0.267500
[13:18:53.349] iteration 13793: total_loss: 0.587388, loss_sup: 0.265101, loss_mps: 0.105019, loss_cps: 0.217267
[13:18:53.410] iteration 13794: total_loss: 0.151111, loss_sup: 0.003371, loss_mps: 0.053615, loss_cps: 0.094124
[13:18:54.619] iteration 13795: total_loss: 0.264292, loss_sup: 0.059442, loss_mps: 0.072785, loss_cps: 0.132065
[13:18:54.767] iteration 13796: total_loss: 0.423174, loss_sup: 0.099312, loss_mps: 0.100117, loss_cps: 0.223744
[13:18:54.914] iteration 13797: total_loss: 0.324208, loss_sup: 0.064438, loss_mps: 0.094085, loss_cps: 0.165685
[13:18:55.063] iteration 13798: total_loss: 0.295575, loss_sup: 0.039500, loss_mps: 0.091145, loss_cps: 0.164930
[13:18:55.210] iteration 13799: total_loss: 0.875175, loss_sup: 0.102826, loss_mps: 0.233576, loss_cps: 0.538773
[13:18:55.356] iteration 13800: total_loss: 0.428761, loss_sup: 0.021112, loss_mps: 0.132007, loss_cps: 0.275642
[13:18:55.356] Evaluation Started ==>
[13:19:06.755] ==> valid iteration 13800: unet metrics: {'dc': 0.6338975064194151, 'jc': 0.5205951853230034, 'pre': 0.7107452283574427, 'hd': 5.839057771793256}, ynet metrics: {'dc': 0.6054599446943757, 'jc': 0.4870693920377825, 'pre': 0.7642774987508885, 'hd': 5.831094500792805}.
[13:19:06.757] Evaluation Finished!⏹️
[13:19:06.907] iteration 13801: total_loss: 0.341376, loss_sup: 0.131149, loss_mps: 0.076454, loss_cps: 0.133773
[13:19:07.055] iteration 13802: total_loss: 0.317227, loss_sup: 0.027485, loss_mps: 0.099458, loss_cps: 0.190284
[13:19:07.203] iteration 13803: total_loss: 0.339754, loss_sup: 0.023537, loss_mps: 0.104302, loss_cps: 0.211915
[13:19:07.348] iteration 13804: total_loss: 0.517768, loss_sup: 0.064055, loss_mps: 0.141186, loss_cps: 0.312527
[13:19:07.496] iteration 13805: total_loss: 0.799170, loss_sup: 0.370982, loss_mps: 0.140574, loss_cps: 0.287615
[13:19:07.642] iteration 13806: total_loss: 0.377643, loss_sup: 0.051377, loss_mps: 0.112803, loss_cps: 0.213462
[13:19:07.787] iteration 13807: total_loss: 0.345925, loss_sup: 0.049287, loss_mps: 0.103806, loss_cps: 0.192832
[13:19:07.933] iteration 13808: total_loss: 0.986611, loss_sup: 0.067509, loss_mps: 0.276389, loss_cps: 0.642713
[13:19:08.078] iteration 13809: total_loss: 0.548565, loss_sup: 0.234134, loss_mps: 0.104806, loss_cps: 0.209624
[13:19:08.223] iteration 13810: total_loss: 0.398277, loss_sup: 0.085849, loss_mps: 0.109318, loss_cps: 0.203110
[13:19:08.369] iteration 13811: total_loss: 0.263334, loss_sup: 0.076139, loss_mps: 0.066863, loss_cps: 0.120331
[13:19:08.515] iteration 13812: total_loss: 0.605362, loss_sup: 0.070927, loss_mps: 0.169105, loss_cps: 0.365330
[13:19:08.661] iteration 13813: total_loss: 0.350837, loss_sup: 0.027833, loss_mps: 0.108494, loss_cps: 0.214511
[13:19:08.807] iteration 13814: total_loss: 0.358728, loss_sup: 0.118928, loss_mps: 0.084136, loss_cps: 0.155664
[13:19:08.954] iteration 13815: total_loss: 0.457025, loss_sup: 0.140604, loss_mps: 0.106486, loss_cps: 0.209934
[13:19:09.099] iteration 13816: total_loss: 0.387677, loss_sup: 0.078155, loss_mps: 0.102341, loss_cps: 0.207180
[13:19:09.244] iteration 13817: total_loss: 0.322241, loss_sup: 0.033539, loss_mps: 0.097118, loss_cps: 0.191584
[13:19:09.390] iteration 13818: total_loss: 0.340128, loss_sup: 0.063400, loss_mps: 0.100265, loss_cps: 0.176464
[13:19:09.535] iteration 13819: total_loss: 0.375983, loss_sup: 0.057645, loss_mps: 0.109439, loss_cps: 0.208899
[13:19:09.681] iteration 13820: total_loss: 0.229672, loss_sup: 0.006310, loss_mps: 0.079514, loss_cps: 0.143847
[13:19:09.826] iteration 13821: total_loss: 0.511833, loss_sup: 0.141087, loss_mps: 0.118185, loss_cps: 0.252561
[13:19:09.972] iteration 13822: total_loss: 0.303088, loss_sup: 0.056080, loss_mps: 0.088715, loss_cps: 0.158293
[13:19:10.118] iteration 13823: total_loss: 0.371667, loss_sup: 0.082031, loss_mps: 0.098914, loss_cps: 0.190722
[13:19:10.263] iteration 13824: total_loss: 0.279038, loss_sup: 0.041829, loss_mps: 0.085262, loss_cps: 0.151947
[13:19:10.409] iteration 13825: total_loss: 0.395610, loss_sup: 0.012793, loss_mps: 0.125866, loss_cps: 0.256950
[13:19:10.554] iteration 13826: total_loss: 0.456793, loss_sup: 0.059522, loss_mps: 0.131252, loss_cps: 0.266019
[13:19:10.701] iteration 13827: total_loss: 0.497580, loss_sup: 0.066199, loss_mps: 0.142186, loss_cps: 0.289196
[13:19:10.847] iteration 13828: total_loss: 0.481955, loss_sup: 0.068746, loss_mps: 0.129469, loss_cps: 0.283740
[13:19:10.993] iteration 13829: total_loss: 0.516730, loss_sup: 0.156940, loss_mps: 0.125044, loss_cps: 0.234746
[13:19:11.139] iteration 13830: total_loss: 0.563491, loss_sup: 0.082152, loss_mps: 0.154030, loss_cps: 0.327309
[13:19:11.285] iteration 13831: total_loss: 0.228222, loss_sup: 0.008700, loss_mps: 0.079893, loss_cps: 0.139629
[13:19:11.432] iteration 13832: total_loss: 0.276849, loss_sup: 0.068843, loss_mps: 0.074414, loss_cps: 0.133592
[13:19:11.579] iteration 13833: total_loss: 0.593723, loss_sup: 0.130286, loss_mps: 0.145473, loss_cps: 0.317965
[13:19:11.724] iteration 13834: total_loss: 0.539708, loss_sup: 0.027735, loss_mps: 0.163221, loss_cps: 0.348752
[13:19:11.870] iteration 13835: total_loss: 0.478047, loss_sup: 0.151637, loss_mps: 0.114872, loss_cps: 0.211539
[13:19:12.017] iteration 13836: total_loss: 0.339208, loss_sup: 0.026169, loss_mps: 0.108085, loss_cps: 0.204955
[13:19:12.163] iteration 13837: total_loss: 0.563546, loss_sup: 0.110848, loss_mps: 0.155379, loss_cps: 0.297319
[13:19:12.309] iteration 13838: total_loss: 0.343328, loss_sup: 0.022099, loss_mps: 0.114965, loss_cps: 0.206265
[13:19:12.454] iteration 13839: total_loss: 0.260948, loss_sup: 0.057242, loss_mps: 0.072769, loss_cps: 0.130937
[13:19:12.600] iteration 13840: total_loss: 0.363438, loss_sup: 0.026549, loss_mps: 0.111023, loss_cps: 0.225866
[13:19:12.747] iteration 13841: total_loss: 0.317418, loss_sup: 0.030737, loss_mps: 0.095849, loss_cps: 0.190832
[13:19:12.893] iteration 13842: total_loss: 0.327716, loss_sup: 0.075186, loss_mps: 0.093819, loss_cps: 0.158711
[13:19:13.039] iteration 13843: total_loss: 0.442097, loss_sup: 0.041456, loss_mps: 0.132677, loss_cps: 0.267964
[13:19:13.184] iteration 13844: total_loss: 0.285185, loss_sup: 0.039654, loss_mps: 0.086905, loss_cps: 0.158626
[13:19:13.330] iteration 13845: total_loss: 0.289381, loss_sup: 0.031310, loss_mps: 0.088253, loss_cps: 0.169818
[13:19:13.476] iteration 13846: total_loss: 0.346211, loss_sup: 0.098512, loss_mps: 0.085492, loss_cps: 0.162206
[13:19:13.622] iteration 13847: total_loss: 0.500125, loss_sup: 0.188901, loss_mps: 0.110684, loss_cps: 0.200540
[13:19:13.767] iteration 13848: total_loss: 0.356197, loss_sup: 0.044999, loss_mps: 0.106018, loss_cps: 0.205179
[13:19:13.913] iteration 13849: total_loss: 0.667377, loss_sup: 0.182813, loss_mps: 0.158783, loss_cps: 0.325781
[13:19:14.060] iteration 13850: total_loss: 0.284886, loss_sup: 0.019154, loss_mps: 0.091929, loss_cps: 0.173803
[13:19:14.206] iteration 13851: total_loss: 0.397366, loss_sup: 0.118254, loss_mps: 0.094622, loss_cps: 0.184490
[13:19:14.353] iteration 13852: total_loss: 0.175133, loss_sup: 0.027865, loss_mps: 0.057986, loss_cps: 0.089282
[13:19:14.499] iteration 13853: total_loss: 0.724214, loss_sup: 0.152763, loss_mps: 0.180372, loss_cps: 0.391079
[13:19:14.645] iteration 13854: total_loss: 0.156491, loss_sup: 0.014048, loss_mps: 0.053400, loss_cps: 0.089044
[13:19:14.791] iteration 13855: total_loss: 0.357824, loss_sup: 0.076977, loss_mps: 0.096583, loss_cps: 0.184264
[13:19:14.938] iteration 13856: total_loss: 0.438411, loss_sup: 0.183315, loss_mps: 0.090295, loss_cps: 0.164801
[13:19:15.083] iteration 13857: total_loss: 0.479631, loss_sup: 0.014560, loss_mps: 0.141287, loss_cps: 0.323784
[13:19:15.230] iteration 13858: total_loss: 0.742063, loss_sup: 0.371202, loss_mps: 0.126367, loss_cps: 0.244493
[13:19:15.376] iteration 13859: total_loss: 0.543949, loss_sup: 0.108486, loss_mps: 0.144121, loss_cps: 0.291342
[13:19:15.523] iteration 13860: total_loss: 0.275721, loss_sup: 0.036130, loss_mps: 0.087899, loss_cps: 0.151692
[13:19:15.669] iteration 13861: total_loss: 0.434534, loss_sup: 0.154200, loss_mps: 0.104050, loss_cps: 0.176284
[13:19:15.817] iteration 13862: total_loss: 0.695085, loss_sup: 0.115595, loss_mps: 0.185549, loss_cps: 0.393941
[13:19:15.962] iteration 13863: total_loss: 0.460431, loss_sup: 0.061389, loss_mps: 0.130812, loss_cps: 0.268230
[13:19:16.109] iteration 13864: total_loss: 0.255206, loss_sup: 0.047799, loss_mps: 0.076188, loss_cps: 0.131218
[13:19:16.255] iteration 13865: total_loss: 0.323951, loss_sup: 0.047534, loss_mps: 0.097528, loss_cps: 0.178889
[13:19:16.404] iteration 13866: total_loss: 0.289793, loss_sup: 0.028437, loss_mps: 0.089474, loss_cps: 0.171882
[13:19:16.550] iteration 13867: total_loss: 0.214744, loss_sup: 0.009807, loss_mps: 0.073193, loss_cps: 0.131744
[13:19:16.696] iteration 13868: total_loss: 0.358848, loss_sup: 0.022318, loss_mps: 0.115472, loss_cps: 0.221058
[13:19:16.846] iteration 13869: total_loss: 0.535393, loss_sup: 0.252982, loss_mps: 0.091349, loss_cps: 0.191061
[13:19:16.991] iteration 13870: total_loss: 0.338088, loss_sup: 0.046057, loss_mps: 0.097796, loss_cps: 0.194235
[13:19:17.137] iteration 13871: total_loss: 0.338543, loss_sup: 0.063053, loss_mps: 0.091581, loss_cps: 0.183909
[13:19:17.283] iteration 13872: total_loss: 0.471300, loss_sup: 0.075377, loss_mps: 0.122847, loss_cps: 0.273076
[13:19:17.432] iteration 13873: total_loss: 0.373216, loss_sup: 0.153566, loss_mps: 0.077920, loss_cps: 0.141730
[13:19:17.580] iteration 13874: total_loss: 0.343830, loss_sup: 0.023612, loss_mps: 0.108443, loss_cps: 0.211776
[13:19:17.726] iteration 13875: total_loss: 0.562733, loss_sup: 0.084454, loss_mps: 0.151592, loss_cps: 0.326687
[13:19:17.872] iteration 13876: total_loss: 0.473432, loss_sup: 0.130886, loss_mps: 0.116014, loss_cps: 0.226532
[13:19:18.018] iteration 13877: total_loss: 0.428404, loss_sup: 0.080640, loss_mps: 0.114666, loss_cps: 0.233097
[13:19:18.164] iteration 13878: total_loss: 0.593029, loss_sup: 0.169506, loss_mps: 0.137579, loss_cps: 0.285945
[13:19:18.309] iteration 13879: total_loss: 0.382242, loss_sup: 0.037286, loss_mps: 0.115283, loss_cps: 0.229673
[13:19:18.454] iteration 13880: total_loss: 0.316419, loss_sup: 0.019763, loss_mps: 0.100994, loss_cps: 0.195663
[13:19:18.600] iteration 13881: total_loss: 0.870569, loss_sup: 0.296375, loss_mps: 0.181441, loss_cps: 0.392753
[13:19:18.745] iteration 13882: total_loss: 0.828107, loss_sup: 0.061474, loss_mps: 0.236706, loss_cps: 0.529927
[13:19:18.891] iteration 13883: total_loss: 0.634280, loss_sup: 0.118298, loss_mps: 0.167614, loss_cps: 0.348367
[13:19:19.036] iteration 13884: total_loss: 0.670301, loss_sup: 0.299472, loss_mps: 0.133396, loss_cps: 0.237434
[13:19:19.182] iteration 13885: total_loss: 0.759881, loss_sup: 0.325308, loss_mps: 0.152270, loss_cps: 0.282304
[13:19:19.328] iteration 13886: total_loss: 0.481685, loss_sup: 0.087869, loss_mps: 0.131619, loss_cps: 0.262198
[13:19:19.474] iteration 13887: total_loss: 0.403244, loss_sup: 0.050473, loss_mps: 0.117285, loss_cps: 0.235486
[13:19:19.621] iteration 13888: total_loss: 0.409762, loss_sup: 0.137678, loss_mps: 0.098422, loss_cps: 0.173663
[13:19:19.766] iteration 13889: total_loss: 0.482298, loss_sup: 0.050235, loss_mps: 0.136718, loss_cps: 0.295345
[13:19:19.912] iteration 13890: total_loss: 0.357249, loss_sup: 0.076359, loss_mps: 0.098318, loss_cps: 0.182571
[13:19:20.058] iteration 13891: total_loss: 0.638446, loss_sup: 0.161624, loss_mps: 0.157645, loss_cps: 0.319177
[13:19:20.203] iteration 13892: total_loss: 0.501754, loss_sup: 0.210011, loss_mps: 0.109452, loss_cps: 0.182291
[13:19:20.349] iteration 13893: total_loss: 0.258569, loss_sup: 0.038713, loss_mps: 0.081785, loss_cps: 0.138071
[13:19:20.495] iteration 13894: total_loss: 0.381217, loss_sup: 0.112031, loss_mps: 0.106939, loss_cps: 0.162248
[13:19:20.641] iteration 13895: total_loss: 0.400373, loss_sup: 0.090299, loss_mps: 0.118124, loss_cps: 0.191951
[13:19:20.790] iteration 13896: total_loss: 0.837224, loss_sup: 0.087094, loss_mps: 0.245300, loss_cps: 0.504829
[13:19:20.936] iteration 13897: total_loss: 0.492186, loss_sup: 0.053553, loss_mps: 0.156143, loss_cps: 0.282490
[13:19:21.082] iteration 13898: total_loss: 0.381615, loss_sup: 0.059793, loss_mps: 0.113452, loss_cps: 0.208370
[13:19:21.228] iteration 13899: total_loss: 0.514089, loss_sup: 0.256220, loss_mps: 0.091652, loss_cps: 0.166216
[13:19:21.376] iteration 13900: total_loss: 0.391415, loss_sup: 0.081784, loss_mps: 0.113436, loss_cps: 0.196196
[13:19:21.376] Evaluation Started ==>
[13:19:32.719] ==> valid iteration 13900: unet metrics: {'dc': 0.6527830431072537, 'jc': 0.5368511075928545, 'pre': 0.7444208899490669, 'hd': 5.7628430405045385}, ynet metrics: {'dc': 0.6106788696829341, 'jc': 0.49322687032523466, 'pre': 0.7602625300165801, 'hd': 5.707333226541654}.
[13:19:32.720] Evaluation Finished!⏹️
[13:19:32.873] iteration 13901: total_loss: 0.198651, loss_sup: 0.015681, loss_mps: 0.072007, loss_cps: 0.110963
[13:19:33.020] iteration 13902: total_loss: 0.415581, loss_sup: 0.028407, loss_mps: 0.123509, loss_cps: 0.263665
[13:19:33.166] iteration 13903: total_loss: 0.326073, loss_sup: 0.053230, loss_mps: 0.097098, loss_cps: 0.175745
[13:19:33.311] iteration 13904: total_loss: 0.347646, loss_sup: 0.015668, loss_mps: 0.118970, loss_cps: 0.213007
[13:19:33.456] iteration 13905: total_loss: 0.318104, loss_sup: 0.067853, loss_mps: 0.092004, loss_cps: 0.158248
[13:19:33.601] iteration 13906: total_loss: 0.372675, loss_sup: 0.046805, loss_mps: 0.117138, loss_cps: 0.208732
[13:19:33.747] iteration 13907: total_loss: 0.450744, loss_sup: 0.128435, loss_mps: 0.109018, loss_cps: 0.213292
[13:19:33.893] iteration 13908: total_loss: 0.384592, loss_sup: 0.110134, loss_mps: 0.098494, loss_cps: 0.175964
[13:19:34.038] iteration 13909: total_loss: 0.312345, loss_sup: 0.015243, loss_mps: 0.105507, loss_cps: 0.191595
[13:19:34.183] iteration 13910: total_loss: 0.294118, loss_sup: 0.042184, loss_mps: 0.089257, loss_cps: 0.162677
[13:19:34.328] iteration 13911: total_loss: 0.198893, loss_sup: 0.014554, loss_mps: 0.069400, loss_cps: 0.114939
[13:19:34.473] iteration 13912: total_loss: 0.407199, loss_sup: 0.113127, loss_mps: 0.107120, loss_cps: 0.186952
[13:19:34.619] iteration 13913: total_loss: 0.404221, loss_sup: 0.054559, loss_mps: 0.121823, loss_cps: 0.227840
[13:19:34.763] iteration 13914: total_loss: 0.298266, loss_sup: 0.103078, loss_mps: 0.071877, loss_cps: 0.123311
[13:19:34.909] iteration 13915: total_loss: 0.328803, loss_sup: 0.041956, loss_mps: 0.100920, loss_cps: 0.185927
[13:19:35.054] iteration 13916: total_loss: 0.290824, loss_sup: 0.045958, loss_mps: 0.090686, loss_cps: 0.154180
[13:19:35.200] iteration 13917: total_loss: 0.363841, loss_sup: 0.042048, loss_mps: 0.110982, loss_cps: 0.210811
[13:19:35.345] iteration 13918: total_loss: 0.442206, loss_sup: 0.086320, loss_mps: 0.119942, loss_cps: 0.235944
[13:19:35.491] iteration 13919: total_loss: 0.334914, loss_sup: 0.101896, loss_mps: 0.088005, loss_cps: 0.145012
[13:19:35.637] iteration 13920: total_loss: 0.433975, loss_sup: 0.094928, loss_mps: 0.114933, loss_cps: 0.224113
[13:19:35.782] iteration 13921: total_loss: 0.293404, loss_sup: 0.055307, loss_mps: 0.082468, loss_cps: 0.155629
[13:19:35.928] iteration 13922: total_loss: 0.424165, loss_sup: 0.174576, loss_mps: 0.089716, loss_cps: 0.159874
[13:19:36.073] iteration 13923: total_loss: 0.545680, loss_sup: 0.248039, loss_mps: 0.099578, loss_cps: 0.198063
[13:19:36.219] iteration 13924: total_loss: 0.294575, loss_sup: 0.081952, loss_mps: 0.074738, loss_cps: 0.137886
[13:19:36.364] iteration 13925: total_loss: 0.312745, loss_sup: 0.071004, loss_mps: 0.082335, loss_cps: 0.159405
[13:19:36.510] iteration 13926: total_loss: 0.453882, loss_sup: 0.050895, loss_mps: 0.124931, loss_cps: 0.278056
[13:19:36.655] iteration 13927: total_loss: 0.348411, loss_sup: 0.010612, loss_mps: 0.112801, loss_cps: 0.224998
[13:19:36.801] iteration 13928: total_loss: 0.506342, loss_sup: 0.089393, loss_mps: 0.130826, loss_cps: 0.286123
[13:19:36.946] iteration 13929: total_loss: 0.358021, loss_sup: 0.014076, loss_mps: 0.115395, loss_cps: 0.228550
[13:19:37.092] iteration 13930: total_loss: 0.437074, loss_sup: 0.179115, loss_mps: 0.088968, loss_cps: 0.168990
[13:19:37.238] iteration 13931: total_loss: 0.576778, loss_sup: 0.274511, loss_mps: 0.107106, loss_cps: 0.195161
[13:19:37.384] iteration 13932: total_loss: 0.457169, loss_sup: 0.096696, loss_mps: 0.123875, loss_cps: 0.236598
[13:19:37.529] iteration 13933: total_loss: 0.373989, loss_sup: 0.066192, loss_mps: 0.100858, loss_cps: 0.206938
[13:19:37.675] iteration 13934: total_loss: 0.488040, loss_sup: 0.050204, loss_mps: 0.141211, loss_cps: 0.296625
[13:19:37.821] iteration 13935: total_loss: 0.382307, loss_sup: 0.096719, loss_mps: 0.097086, loss_cps: 0.188501
[13:19:37.966] iteration 13936: total_loss: 0.481876, loss_sup: 0.172964, loss_mps: 0.105633, loss_cps: 0.203279
[13:19:38.111] iteration 13937: total_loss: 0.607858, loss_sup: 0.096262, loss_mps: 0.157473, loss_cps: 0.354124
[13:19:38.257] iteration 13938: total_loss: 0.469063, loss_sup: 0.064911, loss_mps: 0.128681, loss_cps: 0.275470
[13:19:38.402] iteration 13939: total_loss: 0.174631, loss_sup: 0.015825, loss_mps: 0.062018, loss_cps: 0.096788
[13:19:38.548] iteration 13940: total_loss: 0.229478, loss_sup: 0.091579, loss_mps: 0.050874, loss_cps: 0.087025
[13:19:38.696] iteration 13941: total_loss: 0.583956, loss_sup: 0.098340, loss_mps: 0.150159, loss_cps: 0.335457
[13:19:38.842] iteration 13942: total_loss: 0.483554, loss_sup: 0.050377, loss_mps: 0.140056, loss_cps: 0.293122
[13:19:38.987] iteration 13943: total_loss: 0.189202, loss_sup: 0.045767, loss_mps: 0.057672, loss_cps: 0.085763
[13:19:39.132] iteration 13944: total_loss: 0.201179, loss_sup: 0.011405, loss_mps: 0.067899, loss_cps: 0.121875
[13:19:39.278] iteration 13945: total_loss: 0.262805, loss_sup: 0.016939, loss_mps: 0.087626, loss_cps: 0.158240
[13:19:39.424] iteration 13946: total_loss: 0.547192, loss_sup: 0.122899, loss_mps: 0.135296, loss_cps: 0.288996
[13:19:39.569] iteration 13947: total_loss: 0.880266, loss_sup: 0.144653, loss_mps: 0.229150, loss_cps: 0.506463
[13:19:39.714] iteration 13948: total_loss: 0.278271, loss_sup: 0.053770, loss_mps: 0.076926, loss_cps: 0.147575
[13:19:39.860] iteration 13949: total_loss: 0.398149, loss_sup: 0.094700, loss_mps: 0.103048, loss_cps: 0.200401
[13:19:40.006] iteration 13950: total_loss: 0.435272, loss_sup: 0.203061, loss_mps: 0.081082, loss_cps: 0.151129
[13:19:40.154] iteration 13951: total_loss: 0.322170, loss_sup: 0.080843, loss_mps: 0.085970, loss_cps: 0.155357
[13:19:40.300] iteration 13952: total_loss: 0.466009, loss_sup: 0.237901, loss_mps: 0.083272, loss_cps: 0.144835
[13:19:40.445] iteration 13953: total_loss: 0.327303, loss_sup: 0.051765, loss_mps: 0.099199, loss_cps: 0.176339
[13:19:40.591] iteration 13954: total_loss: 0.435977, loss_sup: 0.052585, loss_mps: 0.128850, loss_cps: 0.254542
[13:19:40.736] iteration 13955: total_loss: 0.352845, loss_sup: 0.039559, loss_mps: 0.110226, loss_cps: 0.203060
[13:19:40.882] iteration 13956: total_loss: 0.391422, loss_sup: 0.038467, loss_mps: 0.121379, loss_cps: 0.231577
[13:19:41.027] iteration 13957: total_loss: 0.632267, loss_sup: 0.041256, loss_mps: 0.180715, loss_cps: 0.410296
[13:19:41.173] iteration 13958: total_loss: 0.566807, loss_sup: 0.238207, loss_mps: 0.111792, loss_cps: 0.216808
[13:19:41.319] iteration 13959: total_loss: 0.324775, loss_sup: 0.024211, loss_mps: 0.104950, loss_cps: 0.195614
[13:19:41.464] iteration 13960: total_loss: 0.575653, loss_sup: 0.036818, loss_mps: 0.173983, loss_cps: 0.364852
[13:19:41.610] iteration 13961: total_loss: 0.440343, loss_sup: 0.059238, loss_mps: 0.132275, loss_cps: 0.248830
[13:19:41.755] iteration 13962: total_loss: 0.472278, loss_sup: 0.119757, loss_mps: 0.118796, loss_cps: 0.233724
[13:19:41.901] iteration 13963: total_loss: 0.875547, loss_sup: 0.487191, loss_mps: 0.132024, loss_cps: 0.256331
[13:19:42.046] iteration 13964: total_loss: 0.426736, loss_sup: 0.024641, loss_mps: 0.137362, loss_cps: 0.264732
[13:19:42.194] iteration 13965: total_loss: 0.551479, loss_sup: 0.191232, loss_mps: 0.129591, loss_cps: 0.230656
[13:19:42.341] iteration 13966: total_loss: 0.510634, loss_sup: 0.019514, loss_mps: 0.155377, loss_cps: 0.335744
[13:19:42.487] iteration 13967: total_loss: 0.330640, loss_sup: 0.084068, loss_mps: 0.088549, loss_cps: 0.158022
[13:19:42.632] iteration 13968: total_loss: 0.239542, loss_sup: 0.052254, loss_mps: 0.068924, loss_cps: 0.118364
[13:19:42.778] iteration 13969: total_loss: 0.349350, loss_sup: 0.024262, loss_mps: 0.109413, loss_cps: 0.215674
[13:19:42.928] iteration 13970: total_loss: 0.567437, loss_sup: 0.129252, loss_mps: 0.145377, loss_cps: 0.292807
[13:19:43.073] iteration 13971: total_loss: 0.423764, loss_sup: 0.055290, loss_mps: 0.122037, loss_cps: 0.246438
[13:19:43.219] iteration 13972: total_loss: 0.679783, loss_sup: 0.101306, loss_mps: 0.184328, loss_cps: 0.394149
[13:19:43.364] iteration 13973: total_loss: 0.282764, loss_sup: 0.034086, loss_mps: 0.087557, loss_cps: 0.161121
[13:19:43.511] iteration 13974: total_loss: 0.488645, loss_sup: 0.192043, loss_mps: 0.102294, loss_cps: 0.194308
[13:19:43.657] iteration 13975: total_loss: 0.534618, loss_sup: 0.300772, loss_mps: 0.088341, loss_cps: 0.145506
[13:19:43.803] iteration 13976: total_loss: 0.313286, loss_sup: 0.056732, loss_mps: 0.092476, loss_cps: 0.164078
[13:19:43.948] iteration 13977: total_loss: 0.254704, loss_sup: 0.051894, loss_mps: 0.075058, loss_cps: 0.127752
[13:19:44.093] iteration 13978: total_loss: 0.590021, loss_sup: 0.157931, loss_mps: 0.145830, loss_cps: 0.286260
[13:19:44.239] iteration 13979: total_loss: 0.306472, loss_sup: 0.062247, loss_mps: 0.088326, loss_cps: 0.155899
[13:19:44.385] iteration 13980: total_loss: 0.332390, loss_sup: 0.084176, loss_mps: 0.087276, loss_cps: 0.160938
[13:19:44.532] iteration 13981: total_loss: 0.224209, loss_sup: 0.044576, loss_mps: 0.069993, loss_cps: 0.109640
[13:19:44.679] iteration 13982: total_loss: 0.380924, loss_sup: 0.050559, loss_mps: 0.110831, loss_cps: 0.219534
[13:19:44.827] iteration 13983: total_loss: 0.437713, loss_sup: 0.060945, loss_mps: 0.121771, loss_cps: 0.254998
[13:19:44.973] iteration 13984: total_loss: 0.375552, loss_sup: 0.117520, loss_mps: 0.088241, loss_cps: 0.169791
[13:19:45.120] iteration 13985: total_loss: 0.297562, loss_sup: 0.051609, loss_mps: 0.089745, loss_cps: 0.156209
[13:19:45.266] iteration 13986: total_loss: 0.299356, loss_sup: 0.042984, loss_mps: 0.093659, loss_cps: 0.162713
[13:19:45.411] iteration 13987: total_loss: 0.604278, loss_sup: 0.167004, loss_mps: 0.148223, loss_cps: 0.289052
[13:19:45.557] iteration 13988: total_loss: 0.457985, loss_sup: 0.064703, loss_mps: 0.133581, loss_cps: 0.259701
[13:19:45.703] iteration 13989: total_loss: 0.334560, loss_sup: 0.074086, loss_mps: 0.092851, loss_cps: 0.167623
[13:19:45.849] iteration 13990: total_loss: 0.293032, loss_sup: 0.106800, loss_mps: 0.071596, loss_cps: 0.114637
[13:19:45.996] iteration 13991: total_loss: 0.304739, loss_sup: 0.168999, loss_mps: 0.051509, loss_cps: 0.084231
[13:19:46.142] iteration 13992: total_loss: 0.684345, loss_sup: 0.262254, loss_mps: 0.137651, loss_cps: 0.284440
[13:19:46.287] iteration 13993: total_loss: 0.420616, loss_sup: 0.206348, loss_mps: 0.077740, loss_cps: 0.136528
[13:19:46.433] iteration 13994: total_loss: 0.208481, loss_sup: 0.071733, loss_mps: 0.051751, loss_cps: 0.084997
[13:19:46.581] iteration 13995: total_loss: 0.294443, loss_sup: 0.041367, loss_mps: 0.092063, loss_cps: 0.161012
[13:19:46.728] iteration 13996: total_loss: 0.346718, loss_sup: 0.085153, loss_mps: 0.088984, loss_cps: 0.172581
[13:19:46.876] iteration 13997: total_loss: 0.499091, loss_sup: 0.083831, loss_mps: 0.139415, loss_cps: 0.275844
[13:19:47.022] iteration 13998: total_loss: 0.333628, loss_sup: 0.069472, loss_mps: 0.092223, loss_cps: 0.171934
[13:19:47.169] iteration 13999: total_loss: 0.325035, loss_sup: 0.037622, loss_mps: 0.099370, loss_cps: 0.188043
[13:19:47.316] iteration 14000: total_loss: 0.446996, loss_sup: 0.129131, loss_mps: 0.114154, loss_cps: 0.203711
[13:19:47.316] Evaluation Started ==>
[13:19:58.666] ==> valid iteration 14000: unet metrics: {'dc': 0.642383080662949, 'jc': 0.5215419090238655, 'pre': 0.7628570555998909, 'hd': 5.698565381705256}, ynet metrics: {'dc': 0.5946639042964141, 'jc': 0.4797342554343828, 'pre': 0.780155560832162, 'hd': 5.632500312025717}.
[13:19:58.668] Evaluation Finished!⏹️
[13:19:58.818] iteration 14001: total_loss: 0.275895, loss_sup: 0.063302, loss_mps: 0.080935, loss_cps: 0.131658
[13:19:58.966] iteration 14002: total_loss: 0.310188, loss_sup: 0.056548, loss_mps: 0.095907, loss_cps: 0.157733
[13:19:59.112] iteration 14003: total_loss: 0.349597, loss_sup: 0.032222, loss_mps: 0.112563, loss_cps: 0.204811
[13:19:59.259] iteration 14004: total_loss: 0.382365, loss_sup: 0.057233, loss_mps: 0.116046, loss_cps: 0.209086
[13:19:59.406] iteration 14005: total_loss: 0.489509, loss_sup: 0.041397, loss_mps: 0.140900, loss_cps: 0.307213
[13:19:59.551] iteration 14006: total_loss: 0.345442, loss_sup: 0.066443, loss_mps: 0.098297, loss_cps: 0.180703
[13:19:59.697] iteration 14007: total_loss: 0.402113, loss_sup: 0.038729, loss_mps: 0.124558, loss_cps: 0.238825
[13:19:59.842] iteration 14008: total_loss: 0.218805, loss_sup: 0.016381, loss_mps: 0.072769, loss_cps: 0.129655
[13:19:59.988] iteration 14009: total_loss: 0.375561, loss_sup: 0.046689, loss_mps: 0.110236, loss_cps: 0.218636
[13:20:00.138] iteration 14010: total_loss: 0.481655, loss_sup: 0.022716, loss_mps: 0.151233, loss_cps: 0.307706
[13:20:00.284] iteration 14011: total_loss: 0.327239, loss_sup: 0.030684, loss_mps: 0.103500, loss_cps: 0.193054
[13:20:00.432] iteration 14012: total_loss: 0.512910, loss_sup: 0.126540, loss_mps: 0.122974, loss_cps: 0.263395
[13:20:00.578] iteration 14013: total_loss: 0.332790, loss_sup: 0.041255, loss_mps: 0.098919, loss_cps: 0.192615
[13:20:00.724] iteration 14014: total_loss: 0.242940, loss_sup: 0.025862, loss_mps: 0.079926, loss_cps: 0.137152
[13:20:00.872] iteration 14015: total_loss: 0.343063, loss_sup: 0.005005, loss_mps: 0.108599, loss_cps: 0.229459
[13:20:01.020] iteration 14016: total_loss: 0.535668, loss_sup: 0.121948, loss_mps: 0.134202, loss_cps: 0.279518
[13:20:01.171] iteration 14017: total_loss: 0.321184, loss_sup: 0.068012, loss_mps: 0.084010, loss_cps: 0.169162
[13:20:01.316] iteration 14018: total_loss: 0.197134, loss_sup: 0.008281, loss_mps: 0.065077, loss_cps: 0.123776
[13:20:01.464] iteration 14019: total_loss: 0.587574, loss_sup: 0.231731, loss_mps: 0.120151, loss_cps: 0.235692
[13:20:01.612] iteration 14020: total_loss: 0.395740, loss_sup: 0.112733, loss_mps: 0.091539, loss_cps: 0.191469
[13:20:01.758] iteration 14021: total_loss: 0.343950, loss_sup: 0.070848, loss_mps: 0.089167, loss_cps: 0.183936
[13:20:01.905] iteration 14022: total_loss: 0.360899, loss_sup: 0.043193, loss_mps: 0.105526, loss_cps: 0.212180
[13:20:02.051] iteration 14023: total_loss: 0.387774, loss_sup: 0.057048, loss_mps: 0.114135, loss_cps: 0.216592
[13:20:02.197] iteration 14024: total_loss: 0.433639, loss_sup: 0.016387, loss_mps: 0.139753, loss_cps: 0.277499
[13:20:02.342] iteration 14025: total_loss: 0.324341, loss_sup: 0.062208, loss_mps: 0.087051, loss_cps: 0.175082
[13:20:02.490] iteration 14026: total_loss: 0.329447, loss_sup: 0.041589, loss_mps: 0.094727, loss_cps: 0.193131
[13:20:02.636] iteration 14027: total_loss: 0.598892, loss_sup: 0.209298, loss_mps: 0.132051, loss_cps: 0.257542
[13:20:02.782] iteration 14028: total_loss: 0.387876, loss_sup: 0.039290, loss_mps: 0.113581, loss_cps: 0.235005
[13:20:02.927] iteration 14029: total_loss: 0.626103, loss_sup: 0.054228, loss_mps: 0.175045, loss_cps: 0.396830
[13:20:03.074] iteration 14030: total_loss: 0.285530, loss_sup: 0.043096, loss_mps: 0.079437, loss_cps: 0.162997
[13:20:03.220] iteration 14031: total_loss: 0.448125, loss_sup: 0.101938, loss_mps: 0.115075, loss_cps: 0.231111
[13:20:03.365] iteration 14032: total_loss: 0.461084, loss_sup: 0.042082, loss_mps: 0.127355, loss_cps: 0.291647
[13:20:03.511] iteration 14033: total_loss: 0.335218, loss_sup: 0.071643, loss_mps: 0.088169, loss_cps: 0.175405
[13:20:03.660] iteration 14034: total_loss: 0.341389, loss_sup: 0.032429, loss_mps: 0.102071, loss_cps: 0.206888
[13:20:03.807] iteration 14035: total_loss: 0.434820, loss_sup: 0.151006, loss_mps: 0.092009, loss_cps: 0.191804
[13:20:03.952] iteration 14036: total_loss: 0.549429, loss_sup: 0.196948, loss_mps: 0.114026, loss_cps: 0.238454
[13:20:04.098] iteration 14037: total_loss: 0.251207, loss_sup: 0.048988, loss_mps: 0.075229, loss_cps: 0.126990
[13:20:04.244] iteration 14038: total_loss: 0.370770, loss_sup: 0.034390, loss_mps: 0.111239, loss_cps: 0.225141
[13:20:04.390] iteration 14039: total_loss: 0.612152, loss_sup: 0.270312, loss_mps: 0.114704, loss_cps: 0.227136
[13:20:04.536] iteration 14040: total_loss: 0.472217, loss_sup: 0.078759, loss_mps: 0.127339, loss_cps: 0.266119
[13:20:04.681] iteration 14041: total_loss: 0.675993, loss_sup: 0.373797, loss_mps: 0.106025, loss_cps: 0.196171
[13:20:04.827] iteration 14042: total_loss: 0.279568, loss_sup: 0.025543, loss_mps: 0.083511, loss_cps: 0.170514
[13:20:04.972] iteration 14043: total_loss: 0.380349, loss_sup: 0.018298, loss_mps: 0.118546, loss_cps: 0.243505
[13:20:05.118] iteration 14044: total_loss: 0.472808, loss_sup: 0.048540, loss_mps: 0.141846, loss_cps: 0.282422
[13:20:05.263] iteration 14045: total_loss: 0.415655, loss_sup: 0.107878, loss_mps: 0.110403, loss_cps: 0.197374
[13:20:05.410] iteration 14046: total_loss: 0.495545, loss_sup: 0.116019, loss_mps: 0.118578, loss_cps: 0.260948
[13:20:05.558] iteration 14047: total_loss: 0.341428, loss_sup: 0.087015, loss_mps: 0.087006, loss_cps: 0.167407
[13:20:05.706] iteration 14048: total_loss: 0.653937, loss_sup: 0.204699, loss_mps: 0.142707, loss_cps: 0.306532
[13:20:05.852] iteration 14049: total_loss: 0.306488, loss_sup: 0.067604, loss_mps: 0.090537, loss_cps: 0.148347
[13:20:05.998] iteration 14050: total_loss: 0.384059, loss_sup: 0.052378, loss_mps: 0.119589, loss_cps: 0.212093
[13:20:06.145] iteration 14051: total_loss: 0.383195, loss_sup: 0.021045, loss_mps: 0.123107, loss_cps: 0.239044
[13:20:06.291] iteration 14052: total_loss: 0.498730, loss_sup: 0.083202, loss_mps: 0.141120, loss_cps: 0.274407
[13:20:06.438] iteration 14053: total_loss: 0.779612, loss_sup: 0.206639, loss_mps: 0.186597, loss_cps: 0.386376
[13:20:06.584] iteration 14054: total_loss: 0.247604, loss_sup: 0.028320, loss_mps: 0.078768, loss_cps: 0.140516
[13:20:06.732] iteration 14055: total_loss: 0.319607, loss_sup: 0.019918, loss_mps: 0.102104, loss_cps: 0.197586
[13:20:06.877] iteration 14056: total_loss: 0.365765, loss_sup: 0.028659, loss_mps: 0.113716, loss_cps: 0.223390
[13:20:07.023] iteration 14057: total_loss: 0.361107, loss_sup: 0.039966, loss_mps: 0.113518, loss_cps: 0.207623
[13:20:07.170] iteration 14058: total_loss: 0.279166, loss_sup: 0.019079, loss_mps: 0.090123, loss_cps: 0.169964
[13:20:07.321] iteration 14059: total_loss: 0.426337, loss_sup: 0.124229, loss_mps: 0.106561, loss_cps: 0.195547
[13:20:07.467] iteration 14060: total_loss: 0.346648, loss_sup: 0.106639, loss_mps: 0.091434, loss_cps: 0.148576
[13:20:07.617] iteration 14061: total_loss: 0.491536, loss_sup: 0.043954, loss_mps: 0.151677, loss_cps: 0.295905
[13:20:07.763] iteration 14062: total_loss: 0.779835, loss_sup: 0.389849, loss_mps: 0.131672, loss_cps: 0.258314
[13:20:07.908] iteration 14063: total_loss: 0.424438, loss_sup: 0.044181, loss_mps: 0.130627, loss_cps: 0.249631
[13:20:08.054] iteration 14064: total_loss: 0.733963, loss_sup: 0.106465, loss_mps: 0.199833, loss_cps: 0.427665
[13:20:08.199] iteration 14065: total_loss: 0.434948, loss_sup: 0.087172, loss_mps: 0.120067, loss_cps: 0.227709
[13:20:08.345] iteration 14066: total_loss: 0.473781, loss_sup: 0.146015, loss_mps: 0.114305, loss_cps: 0.213461
[13:20:08.491] iteration 14067: total_loss: 0.638813, loss_sup: 0.189805, loss_mps: 0.137857, loss_cps: 0.311151
[13:20:08.637] iteration 14068: total_loss: 0.421729, loss_sup: 0.083202, loss_mps: 0.120976, loss_cps: 0.217550
[13:20:08.782] iteration 14069: total_loss: 0.530790, loss_sup: 0.087712, loss_mps: 0.141862, loss_cps: 0.301216
[13:20:08.928] iteration 14070: total_loss: 0.357639, loss_sup: 0.078142, loss_mps: 0.096707, loss_cps: 0.182791
[13:20:09.074] iteration 14071: total_loss: 0.588485, loss_sup: 0.194021, loss_mps: 0.131981, loss_cps: 0.262484
[13:20:09.220] iteration 14072: total_loss: 0.278962, loss_sup: 0.037509, loss_mps: 0.087361, loss_cps: 0.154092
[13:20:09.365] iteration 14073: total_loss: 0.350138, loss_sup: 0.021725, loss_mps: 0.108731, loss_cps: 0.219682
[13:20:09.511] iteration 14074: total_loss: 0.648619, loss_sup: 0.198009, loss_mps: 0.145248, loss_cps: 0.305362
[13:20:09.657] iteration 14075: total_loss: 0.258096, loss_sup: 0.060744, loss_mps: 0.072948, loss_cps: 0.124403
[13:20:09.803] iteration 14076: total_loss: 0.230944, loss_sup: 0.053620, loss_mps: 0.067308, loss_cps: 0.110017
[13:20:09.949] iteration 14077: total_loss: 0.415435, loss_sup: 0.130074, loss_mps: 0.102340, loss_cps: 0.183022
[13:20:10.095] iteration 14078: total_loss: 0.239318, loss_sup: 0.026537, loss_mps: 0.072047, loss_cps: 0.140734
[13:20:10.240] iteration 14079: total_loss: 0.528937, loss_sup: 0.130033, loss_mps: 0.128388, loss_cps: 0.270516
[13:20:10.386] iteration 14080: total_loss: 0.629453, loss_sup: 0.100714, loss_mps: 0.167268, loss_cps: 0.361470
[13:20:10.532] iteration 14081: total_loss: 0.283961, loss_sup: 0.018722, loss_mps: 0.092319, loss_cps: 0.172920
[13:20:10.678] iteration 14082: total_loss: 1.467725, loss_sup: 0.635355, loss_mps: 0.257191, loss_cps: 0.575179
[13:20:10.825] iteration 14083: total_loss: 0.826055, loss_sup: 0.142435, loss_mps: 0.216133, loss_cps: 0.467487
[13:20:10.971] iteration 14084: total_loss: 0.383317, loss_sup: 0.136815, loss_mps: 0.090279, loss_cps: 0.156222
[13:20:11.119] iteration 14085: total_loss: 0.442818, loss_sup: 0.105855, loss_mps: 0.114532, loss_cps: 0.222432
[13:20:11.265] iteration 14086: total_loss: 0.458578, loss_sup: 0.198054, loss_mps: 0.094244, loss_cps: 0.166281
[13:20:11.411] iteration 14087: total_loss: 0.346253, loss_sup: 0.074542, loss_mps: 0.096019, loss_cps: 0.175692
[13:20:11.561] iteration 14088: total_loss: 0.373237, loss_sup: 0.095124, loss_mps: 0.095382, loss_cps: 0.182730
[13:20:11.708] iteration 14089: total_loss: 0.466075, loss_sup: 0.109609, loss_mps: 0.121422, loss_cps: 0.235043
[13:20:11.854] iteration 14090: total_loss: 0.571494, loss_sup: 0.057081, loss_mps: 0.165327, loss_cps: 0.349086
[13:20:12.001] iteration 14091: total_loss: 0.463231, loss_sup: 0.130920, loss_mps: 0.112075, loss_cps: 0.220235
[13:20:12.147] iteration 14092: total_loss: 1.195102, loss_sup: 0.118772, loss_mps: 0.330003, loss_cps: 0.746327
[13:20:12.293] iteration 14093: total_loss: 0.423125, loss_sup: 0.024064, loss_mps: 0.132123, loss_cps: 0.266938
[13:20:12.439] iteration 14094: total_loss: 0.330060, loss_sup: 0.014771, loss_mps: 0.110462, loss_cps: 0.204827
[13:20:12.586] iteration 14095: total_loss: 0.833590, loss_sup: 0.257955, loss_mps: 0.192076, loss_cps: 0.383558
[13:20:12.733] iteration 14096: total_loss: 0.390103, loss_sup: 0.103958, loss_mps: 0.099892, loss_cps: 0.186253
[13:20:12.881] iteration 14097: total_loss: 0.351694, loss_sup: 0.059803, loss_mps: 0.103033, loss_cps: 0.188858
[13:20:13.028] iteration 14098: total_loss: 0.488983, loss_sup: 0.083229, loss_mps: 0.134583, loss_cps: 0.271171
[13:20:13.174] iteration 14099: total_loss: 0.384219, loss_sup: 0.056793, loss_mps: 0.111965, loss_cps: 0.215460
[13:20:13.320] iteration 14100: total_loss: 0.253640, loss_sup: 0.033186, loss_mps: 0.078272, loss_cps: 0.142181
[13:20:13.320] Evaluation Started ==>
[13:20:24.629] ==> valid iteration 14100: unet metrics: {'dc': 0.6083246431685502, 'jc': 0.49465335813400796, 'pre': 0.7300129034057107, 'hd': 5.704882447157962}, ynet metrics: {'dc': 0.5753734386912439, 'jc': 0.4620352881726306, 'pre': 0.7548910573909581, 'hd': 5.656264457216912}.
[13:20:24.631] Evaluation Finished!⏹️
[13:20:24.784] iteration 14101: total_loss: 0.360093, loss_sup: 0.035751, loss_mps: 0.109914, loss_cps: 0.214428
[13:20:24.934] iteration 14102: total_loss: 0.207978, loss_sup: 0.026707, loss_mps: 0.069763, loss_cps: 0.111508
[13:20:25.079] iteration 14103: total_loss: 0.376134, loss_sup: 0.075174, loss_mps: 0.109553, loss_cps: 0.191407
[13:20:25.224] iteration 14104: total_loss: 0.237379, loss_sup: 0.013910, loss_mps: 0.081342, loss_cps: 0.142126
[13:20:25.370] iteration 14105: total_loss: 0.595341, loss_sup: 0.247124, loss_mps: 0.122820, loss_cps: 0.225397
[13:20:25.516] iteration 14106: total_loss: 0.449996, loss_sup: 0.209064, loss_mps: 0.090280, loss_cps: 0.150653
[13:20:25.661] iteration 14107: total_loss: 0.317552, loss_sup: 0.032172, loss_mps: 0.098974, loss_cps: 0.186406
[13:20:25.807] iteration 14108: total_loss: 0.406178, loss_sup: 0.096550, loss_mps: 0.103476, loss_cps: 0.206153
[13:20:25.952] iteration 14109: total_loss: 0.335928, loss_sup: 0.048272, loss_mps: 0.102053, loss_cps: 0.185603
[13:20:26.097] iteration 14110: total_loss: 0.437516, loss_sup: 0.103564, loss_mps: 0.113447, loss_cps: 0.220504
[13:20:26.242] iteration 14111: total_loss: 0.369538, loss_sup: 0.092555, loss_mps: 0.093705, loss_cps: 0.183278
[13:20:26.388] iteration 14112: total_loss: 0.692729, loss_sup: 0.217082, loss_mps: 0.161645, loss_cps: 0.314002
[13:20:26.533] iteration 14113: total_loss: 0.516052, loss_sup: 0.101968, loss_mps: 0.136022, loss_cps: 0.278062
[13:20:26.679] iteration 14114: total_loss: 0.696139, loss_sup: 0.118377, loss_mps: 0.179558, loss_cps: 0.398204
[13:20:26.825] iteration 14115: total_loss: 0.359983, loss_sup: 0.041588, loss_mps: 0.107310, loss_cps: 0.211085
[13:20:26.970] iteration 14116: total_loss: 0.603887, loss_sup: 0.159541, loss_mps: 0.146967, loss_cps: 0.297379
[13:20:27.116] iteration 14117: total_loss: 0.381895, loss_sup: 0.133951, loss_mps: 0.084308, loss_cps: 0.163637
[13:20:27.262] iteration 14118: total_loss: 0.235175, loss_sup: 0.027371, loss_mps: 0.075969, loss_cps: 0.131836
[13:20:27.408] iteration 14119: total_loss: 0.451596, loss_sup: 0.104584, loss_mps: 0.117418, loss_cps: 0.229595
[13:20:27.553] iteration 14120: total_loss: 0.487519, loss_sup: 0.079800, loss_mps: 0.140602, loss_cps: 0.267117
[13:20:27.699] iteration 14121: total_loss: 0.301593, loss_sup: 0.053980, loss_mps: 0.085305, loss_cps: 0.162308
[13:20:27.845] iteration 14122: total_loss: 0.341461, loss_sup: 0.059553, loss_mps: 0.094376, loss_cps: 0.187533
[13:20:27.997] iteration 14123: total_loss: 0.626081, loss_sup: 0.072252, loss_mps: 0.175786, loss_cps: 0.378042
[13:20:28.143] iteration 14124: total_loss: 0.259720, loss_sup: 0.066631, loss_mps: 0.070967, loss_cps: 0.122122
[13:20:28.290] iteration 14125: total_loss: 0.285808, loss_sup: 0.049583, loss_mps: 0.087657, loss_cps: 0.148568
[13:20:28.435] iteration 14126: total_loss: 0.645908, loss_sup: 0.135657, loss_mps: 0.161338, loss_cps: 0.348914
[13:20:28.581] iteration 14127: total_loss: 0.750150, loss_sup: 0.339308, loss_mps: 0.138536, loss_cps: 0.272306
[13:20:28.727] iteration 14128: total_loss: 0.625893, loss_sup: 0.045259, loss_mps: 0.182399, loss_cps: 0.398235
[13:20:28.873] iteration 14129: total_loss: 0.353048, loss_sup: 0.007701, loss_mps: 0.113118, loss_cps: 0.232230
[13:20:29.018] iteration 14130: total_loss: 0.330510, loss_sup: 0.041936, loss_mps: 0.096858, loss_cps: 0.191716
[13:20:29.164] iteration 14131: total_loss: 0.578893, loss_sup: 0.151905, loss_mps: 0.143831, loss_cps: 0.283157
[13:20:29.310] iteration 14132: total_loss: 0.354494, loss_sup: 0.121651, loss_mps: 0.083056, loss_cps: 0.149787
[13:20:29.457] iteration 14133: total_loss: 0.390452, loss_sup: 0.076813, loss_mps: 0.112400, loss_cps: 0.201239
[13:20:29.603] iteration 14134: total_loss: 0.370880, loss_sup: 0.072020, loss_mps: 0.108134, loss_cps: 0.190726
[13:20:29.748] iteration 14135: total_loss: 0.255732, loss_sup: 0.044256, loss_mps: 0.075925, loss_cps: 0.135550
[13:20:29.894] iteration 14136: total_loss: 0.445221, loss_sup: 0.112241, loss_mps: 0.123478, loss_cps: 0.209502
[13:20:30.039] iteration 14137: total_loss: 0.305801, loss_sup: 0.088581, loss_mps: 0.080199, loss_cps: 0.137021
[13:20:30.185] iteration 14138: total_loss: 0.616851, loss_sup: 0.127211, loss_mps: 0.156208, loss_cps: 0.333432
[13:20:30.331] iteration 14139: total_loss: 0.219351, loss_sup: 0.026708, loss_mps: 0.072697, loss_cps: 0.119946
[13:20:30.477] iteration 14140: total_loss: 0.434412, loss_sup: 0.057352, loss_mps: 0.123195, loss_cps: 0.253865
[13:20:30.625] iteration 14141: total_loss: 0.791257, loss_sup: 0.111235, loss_mps: 0.215091, loss_cps: 0.464931
[13:20:30.770] iteration 14142: total_loss: 0.282240, loss_sup: 0.044695, loss_mps: 0.087004, loss_cps: 0.150541
[13:20:30.917] iteration 14143: total_loss: 0.234572, loss_sup: 0.023894, loss_mps: 0.074722, loss_cps: 0.135957
[13:20:31.065] iteration 14144: total_loss: 0.249813, loss_sup: 0.036819, loss_mps: 0.079075, loss_cps: 0.133919
[13:20:31.210] iteration 14145: total_loss: 0.175348, loss_sup: 0.025207, loss_mps: 0.057366, loss_cps: 0.092775
[13:20:31.356] iteration 14146: total_loss: 0.302984, loss_sup: 0.022617, loss_mps: 0.096490, loss_cps: 0.183876
[13:20:31.504] iteration 14147: total_loss: 0.524296, loss_sup: 0.098795, loss_mps: 0.142675, loss_cps: 0.282827
[13:20:31.650] iteration 14148: total_loss: 0.370831, loss_sup: 0.076913, loss_mps: 0.103736, loss_cps: 0.190182
[13:20:31.796] iteration 14149: total_loss: 0.260060, loss_sup: 0.051197, loss_mps: 0.075727, loss_cps: 0.133137
[13:20:31.942] iteration 14150: total_loss: 0.320731, loss_sup: 0.042949, loss_mps: 0.098491, loss_cps: 0.179291
[13:20:32.090] iteration 14151: total_loss: 0.298912, loss_sup: 0.014031, loss_mps: 0.096148, loss_cps: 0.188733
[13:20:32.237] iteration 14152: total_loss: 0.188204, loss_sup: 0.013037, loss_mps: 0.065024, loss_cps: 0.110144
[13:20:32.383] iteration 14153: total_loss: 0.493035, loss_sup: 0.076979, loss_mps: 0.132658, loss_cps: 0.283398
[13:20:32.530] iteration 14154: total_loss: 0.306409, loss_sup: 0.163348, loss_mps: 0.055932, loss_cps: 0.087130
[13:20:32.675] iteration 14155: total_loss: 0.301097, loss_sup: 0.098265, loss_mps: 0.071791, loss_cps: 0.131040
[13:20:32.821] iteration 14156: total_loss: 0.552114, loss_sup: 0.092493, loss_mps: 0.148986, loss_cps: 0.310635
[13:20:32.967] iteration 14157: total_loss: 0.468394, loss_sup: 0.196675, loss_mps: 0.094330, loss_cps: 0.177389
[13:20:33.113] iteration 14158: total_loss: 0.224742, loss_sup: 0.038431, loss_mps: 0.068201, loss_cps: 0.118110
[13:20:33.260] iteration 14159: total_loss: 0.494774, loss_sup: 0.190173, loss_mps: 0.102596, loss_cps: 0.202006
[13:20:33.407] iteration 14160: total_loss: 0.360730, loss_sup: 0.043077, loss_mps: 0.103786, loss_cps: 0.213867
[13:20:33.554] iteration 14161: total_loss: 0.462965, loss_sup: 0.081848, loss_mps: 0.121725, loss_cps: 0.259393
[13:20:33.701] iteration 14162: total_loss: 0.345357, loss_sup: 0.102133, loss_mps: 0.084880, loss_cps: 0.158344
[13:20:33.847] iteration 14163: total_loss: 0.215662, loss_sup: 0.062569, loss_mps: 0.058446, loss_cps: 0.094647
[13:20:33.992] iteration 14164: total_loss: 0.294218, loss_sup: 0.061701, loss_mps: 0.085213, loss_cps: 0.147304
[13:20:34.138] iteration 14165: total_loss: 0.645246, loss_sup: 0.071884, loss_mps: 0.171731, loss_cps: 0.401631
[13:20:34.283] iteration 14166: total_loss: 0.299154, loss_sup: 0.111772, loss_mps: 0.070292, loss_cps: 0.117090
[13:20:34.430] iteration 14167: total_loss: 0.632850, loss_sup: 0.159503, loss_mps: 0.161237, loss_cps: 0.312110
[13:20:34.576] iteration 14168: total_loss: 0.340467, loss_sup: 0.065342, loss_mps: 0.095433, loss_cps: 0.179693
[13:20:34.723] iteration 14169: total_loss: 0.713422, loss_sup: 0.108794, loss_mps: 0.189102, loss_cps: 0.415526
[13:20:34.872] iteration 14170: total_loss: 0.461859, loss_sup: 0.065305, loss_mps: 0.133898, loss_cps: 0.262655
[13:20:35.019] iteration 14171: total_loss: 0.338153, loss_sup: 0.175833, loss_mps: 0.061906, loss_cps: 0.100414
[13:20:35.165] iteration 14172: total_loss: 0.600358, loss_sup: 0.054110, loss_mps: 0.174018, loss_cps: 0.372230
[13:20:35.311] iteration 14173: total_loss: 0.426995, loss_sup: 0.120745, loss_mps: 0.100586, loss_cps: 0.205665
[13:20:35.458] iteration 14174: total_loss: 0.269102, loss_sup: 0.016666, loss_mps: 0.086126, loss_cps: 0.166310
[13:20:35.604] iteration 14175: total_loss: 0.392546, loss_sup: 0.061386, loss_mps: 0.106908, loss_cps: 0.224252
[13:20:35.750] iteration 14176: total_loss: 0.368534, loss_sup: 0.050711, loss_mps: 0.107268, loss_cps: 0.210555
[13:20:35.896] iteration 14177: total_loss: 0.263894, loss_sup: 0.015444, loss_mps: 0.087363, loss_cps: 0.161087
[13:20:36.042] iteration 14178: total_loss: 0.394626, loss_sup: 0.117587, loss_mps: 0.094297, loss_cps: 0.182742
[13:20:36.188] iteration 14179: total_loss: 0.401524, loss_sup: 0.049123, loss_mps: 0.115256, loss_cps: 0.237145
[13:20:36.333] iteration 14180: total_loss: 0.589473, loss_sup: 0.203053, loss_mps: 0.127297, loss_cps: 0.259122
[13:20:36.481] iteration 14181: total_loss: 0.292826, loss_sup: 0.053116, loss_mps: 0.083819, loss_cps: 0.155891
[13:20:36.627] iteration 14182: total_loss: 0.234771, loss_sup: 0.017245, loss_mps: 0.073139, loss_cps: 0.144386
[13:20:36.774] iteration 14183: total_loss: 0.474504, loss_sup: 0.047458, loss_mps: 0.143728, loss_cps: 0.283317
[13:20:36.922] iteration 14184: total_loss: 0.646899, loss_sup: 0.169701, loss_mps: 0.151877, loss_cps: 0.325322
[13:20:37.070] iteration 14185: total_loss: 0.356254, loss_sup: 0.034841, loss_mps: 0.110545, loss_cps: 0.210868
[13:20:37.218] iteration 14186: total_loss: 0.180282, loss_sup: 0.007350, loss_mps: 0.066200, loss_cps: 0.106732
[13:20:37.367] iteration 14187: total_loss: 0.555670, loss_sup: 0.252841, loss_mps: 0.105850, loss_cps: 0.196979
[13:20:37.514] iteration 14188: total_loss: 0.388805, loss_sup: 0.040892, loss_mps: 0.123095, loss_cps: 0.224818
[13:20:37.662] iteration 14189: total_loss: 0.253324, loss_sup: 0.040730, loss_mps: 0.077003, loss_cps: 0.135591
[13:20:37.812] iteration 14190: total_loss: 0.314189, loss_sup: 0.041300, loss_mps: 0.095483, loss_cps: 0.177406
[13:20:37.958] iteration 14191: total_loss: 0.324519, loss_sup: 0.027414, loss_mps: 0.106612, loss_cps: 0.190493
[13:20:38.104] iteration 14192: total_loss: 0.384134, loss_sup: 0.082578, loss_mps: 0.100409, loss_cps: 0.201146
[13:20:38.250] iteration 14193: total_loss: 0.714123, loss_sup: 0.143763, loss_mps: 0.181239, loss_cps: 0.389121
[13:20:38.396] iteration 14194: total_loss: 0.509586, loss_sup: 0.188898, loss_mps: 0.112737, loss_cps: 0.207950
[13:20:38.542] iteration 14195: total_loss: 0.450003, loss_sup: 0.072000, loss_mps: 0.129181, loss_cps: 0.248822
[13:20:38.688] iteration 14196: total_loss: 0.347225, loss_sup: 0.094400, loss_mps: 0.091001, loss_cps: 0.161824
[13:20:38.837] iteration 14197: total_loss: 0.331200, loss_sup: 0.048759, loss_mps: 0.104958, loss_cps: 0.177482
[13:20:38.983] iteration 14198: total_loss: 0.341544, loss_sup: 0.084188, loss_mps: 0.094173, loss_cps: 0.163183
[13:20:39.129] iteration 14199: total_loss: 0.442283, loss_sup: 0.017130, loss_mps: 0.129527, loss_cps: 0.295625
[13:20:39.274] iteration 14200: total_loss: 0.412123, loss_sup: 0.036127, loss_mps: 0.127505, loss_cps: 0.248491
[13:20:39.274] Evaluation Started ==>
[13:20:50.635] ==> valid iteration 14200: unet metrics: {'dc': 0.6440932157977846, 'jc': 0.5292515853535259, 'pre': 0.7486349195381075, 'hd': 5.589607334760117}, ynet metrics: {'dc': 0.5547870965890931, 'jc': 0.4430689098467386, 'pre': 0.7921029807424075, 'hd': 5.687195101060833}.
[13:20:50.636] Evaluation Finished!⏹️
[13:20:50.788] iteration 14201: total_loss: 0.316408, loss_sup: 0.022749, loss_mps: 0.101143, loss_cps: 0.192516
[13:20:50.938] iteration 14202: total_loss: 0.333229, loss_sup: 0.025423, loss_mps: 0.109819, loss_cps: 0.197988
[13:20:51.083] iteration 14203: total_loss: 0.314414, loss_sup: 0.107809, loss_mps: 0.074994, loss_cps: 0.131611
[13:20:51.228] iteration 14204: total_loss: 0.521085, loss_sup: 0.092215, loss_mps: 0.137509, loss_cps: 0.291362
[13:20:51.373] iteration 14205: total_loss: 0.209957, loss_sup: 0.019175, loss_mps: 0.074134, loss_cps: 0.116649
[13:20:51.518] iteration 14206: total_loss: 0.276748, loss_sup: 0.062678, loss_mps: 0.078099, loss_cps: 0.135971
[13:20:51.663] iteration 14207: total_loss: 0.379783, loss_sup: 0.045067, loss_mps: 0.119572, loss_cps: 0.215144
[13:20:51.808] iteration 14208: total_loss: 0.373122, loss_sup: 0.026343, loss_mps: 0.114212, loss_cps: 0.232567
[13:20:51.953] iteration 14209: total_loss: 0.366151, loss_sup: 0.104837, loss_mps: 0.092365, loss_cps: 0.168949
[13:20:52.098] iteration 14210: total_loss: 0.483825, loss_sup: 0.058846, loss_mps: 0.138530, loss_cps: 0.286449
[13:20:52.244] iteration 14211: total_loss: 0.240695, loss_sup: 0.052936, loss_mps: 0.073267, loss_cps: 0.114492
[13:20:52.307] iteration 14212: total_loss: 0.696606, loss_sup: 0.184700, loss_mps: 0.169552, loss_cps: 0.342355
[13:20:53.508] iteration 14213: total_loss: 0.508225, loss_sup: 0.168087, loss_mps: 0.114366, loss_cps: 0.225772
[13:20:53.657] iteration 14214: total_loss: 0.562616, loss_sup: 0.159322, loss_mps: 0.132065, loss_cps: 0.271228
[13:20:53.803] iteration 14215: total_loss: 0.685627, loss_sup: 0.247114, loss_mps: 0.145480, loss_cps: 0.293034
[13:20:53.949] iteration 14216: total_loss: 0.663803, loss_sup: 0.111579, loss_mps: 0.167808, loss_cps: 0.384416
[13:20:54.094] iteration 14217: total_loss: 0.464337, loss_sup: 0.152193, loss_mps: 0.101498, loss_cps: 0.210646
[13:20:54.241] iteration 14218: total_loss: 0.410847, loss_sup: 0.030196, loss_mps: 0.129110, loss_cps: 0.251541
[13:20:54.387] iteration 14219: total_loss: 0.852904, loss_sup: 0.253265, loss_mps: 0.187031, loss_cps: 0.412609
[13:20:54.533] iteration 14220: total_loss: 0.352820, loss_sup: 0.149005, loss_mps: 0.074429, loss_cps: 0.129387
[13:20:54.680] iteration 14221: total_loss: 0.633333, loss_sup: 0.107017, loss_mps: 0.166443, loss_cps: 0.359873
[13:20:54.826] iteration 14222: total_loss: 0.410792, loss_sup: 0.076079, loss_mps: 0.111803, loss_cps: 0.222910
[13:20:54.971] iteration 14223: total_loss: 0.422225, loss_sup: 0.040190, loss_mps: 0.124624, loss_cps: 0.257411
[13:20:55.117] iteration 14224: total_loss: 0.771789, loss_sup: 0.154607, loss_mps: 0.196077, loss_cps: 0.421105
[13:20:55.262] iteration 14225: total_loss: 0.235592, loss_sup: 0.048176, loss_mps: 0.066697, loss_cps: 0.120719
[13:20:55.408] iteration 14226: total_loss: 0.473678, loss_sup: 0.063535, loss_mps: 0.135156, loss_cps: 0.274987
[13:20:55.554] iteration 14227: total_loss: 0.280194, loss_sup: 0.039963, loss_mps: 0.089653, loss_cps: 0.150578
[13:20:55.700] iteration 14228: total_loss: 0.339486, loss_sup: 0.029349, loss_mps: 0.096969, loss_cps: 0.213168
[13:20:55.845] iteration 14229: total_loss: 0.279499, loss_sup: 0.070293, loss_mps: 0.073351, loss_cps: 0.135856
[13:20:55.990] iteration 14230: total_loss: 0.373857, loss_sup: 0.126007, loss_mps: 0.088511, loss_cps: 0.159338
[13:20:56.136] iteration 14231: total_loss: 1.092159, loss_sup: 0.549575, loss_mps: 0.165283, loss_cps: 0.377301
[13:20:56.282] iteration 14232: total_loss: 0.351428, loss_sup: 0.087117, loss_mps: 0.094696, loss_cps: 0.169615
[13:20:56.430] iteration 14233: total_loss: 0.504373, loss_sup: 0.039954, loss_mps: 0.160076, loss_cps: 0.304344
[13:20:56.577] iteration 14234: total_loss: 0.367921, loss_sup: 0.091577, loss_mps: 0.102593, loss_cps: 0.173752
[13:20:56.723] iteration 14235: total_loss: 0.490160, loss_sup: 0.092437, loss_mps: 0.134250, loss_cps: 0.263472
[13:20:56.870] iteration 14236: total_loss: 0.746253, loss_sup: 0.173638, loss_mps: 0.183264, loss_cps: 0.389352
[13:20:57.020] iteration 14237: total_loss: 0.467756, loss_sup: 0.027650, loss_mps: 0.151706, loss_cps: 0.288400
[13:20:57.166] iteration 14238: total_loss: 0.408307, loss_sup: 0.060442, loss_mps: 0.124899, loss_cps: 0.222965
[13:20:57.311] iteration 14239: total_loss: 0.447317, loss_sup: 0.022962, loss_mps: 0.142679, loss_cps: 0.281676
[13:20:57.457] iteration 14240: total_loss: 0.264889, loss_sup: 0.080004, loss_mps: 0.075341, loss_cps: 0.109544
[13:20:57.603] iteration 14241: total_loss: 0.327876, loss_sup: 0.064537, loss_mps: 0.094648, loss_cps: 0.168692
[13:20:57.752] iteration 14242: total_loss: 0.262830, loss_sup: 0.014915, loss_mps: 0.090216, loss_cps: 0.157699
[13:20:57.898] iteration 14243: total_loss: 0.386038, loss_sup: 0.023169, loss_mps: 0.125199, loss_cps: 0.237670
[13:20:58.046] iteration 14244: total_loss: 0.347948, loss_sup: 0.013365, loss_mps: 0.113965, loss_cps: 0.220618
[13:20:58.192] iteration 14245: total_loss: 0.515059, loss_sup: 0.146991, loss_mps: 0.125764, loss_cps: 0.242303
[13:20:58.337] iteration 14246: total_loss: 0.242220, loss_sup: 0.036595, loss_mps: 0.079143, loss_cps: 0.126482
[13:20:58.483] iteration 14247: total_loss: 0.404688, loss_sup: 0.075767, loss_mps: 0.112651, loss_cps: 0.216270
[13:20:58.630] iteration 14248: total_loss: 0.384858, loss_sup: 0.100561, loss_mps: 0.105264, loss_cps: 0.179033
[13:20:58.776] iteration 14249: total_loss: 0.609459, loss_sup: 0.115645, loss_mps: 0.160406, loss_cps: 0.333408
[13:20:58.922] iteration 14250: total_loss: 0.797808, loss_sup: 0.271189, loss_mps: 0.169926, loss_cps: 0.356694
[13:20:59.070] iteration 14251: total_loss: 0.307685, loss_sup: 0.032404, loss_mps: 0.092996, loss_cps: 0.182285
[13:20:59.216] iteration 14252: total_loss: 0.368420, loss_sup: 0.043291, loss_mps: 0.117516, loss_cps: 0.207613
[13:20:59.362] iteration 14253: total_loss: 0.402250, loss_sup: 0.033878, loss_mps: 0.121886, loss_cps: 0.246486
[13:20:59.508] iteration 14254: total_loss: 0.378860, loss_sup: 0.070242, loss_mps: 0.099721, loss_cps: 0.208897
[13:20:59.654] iteration 14255: total_loss: 0.292005, loss_sup: 0.084311, loss_mps: 0.084966, loss_cps: 0.122728
[13:20:59.801] iteration 14256: total_loss: 0.306444, loss_sup: 0.037440, loss_mps: 0.092917, loss_cps: 0.176086
[13:20:59.946] iteration 14257: total_loss: 0.514934, loss_sup: 0.197141, loss_mps: 0.109055, loss_cps: 0.208738
[13:21:00.092] iteration 14258: total_loss: 0.526806, loss_sup: 0.216932, loss_mps: 0.113451, loss_cps: 0.196424
[13:21:00.237] iteration 14259: total_loss: 0.480084, loss_sup: 0.064651, loss_mps: 0.135780, loss_cps: 0.279653
[13:21:00.383] iteration 14260: total_loss: 0.492480, loss_sup: 0.153913, loss_mps: 0.114844, loss_cps: 0.223724
[13:21:00.529] iteration 14261: total_loss: 0.351329, loss_sup: 0.041365, loss_mps: 0.114245, loss_cps: 0.195719
[13:21:00.675] iteration 14262: total_loss: 0.210047, loss_sup: 0.024456, loss_mps: 0.070407, loss_cps: 0.115184
[13:21:00.821] iteration 14263: total_loss: 0.372984, loss_sup: 0.048977, loss_mps: 0.107021, loss_cps: 0.216986
[13:21:00.966] iteration 14264: total_loss: 0.376665, loss_sup: 0.149378, loss_mps: 0.084695, loss_cps: 0.142592
[13:21:01.112] iteration 14265: total_loss: 0.404438, loss_sup: 0.003214, loss_mps: 0.131447, loss_cps: 0.269777
[13:21:01.258] iteration 14266: total_loss: 0.295564, loss_sup: 0.044118, loss_mps: 0.090657, loss_cps: 0.160789
[13:21:01.404] iteration 14267: total_loss: 0.453544, loss_sup: 0.157411, loss_mps: 0.110627, loss_cps: 0.185506
[13:21:01.550] iteration 14268: total_loss: 0.233484, loss_sup: 0.033435, loss_mps: 0.074619, loss_cps: 0.125431
[13:21:01.696] iteration 14269: total_loss: 0.355414, loss_sup: 0.097351, loss_mps: 0.094511, loss_cps: 0.163552
[13:21:01.841] iteration 14270: total_loss: 0.363577, loss_sup: 0.114607, loss_mps: 0.087517, loss_cps: 0.161454
[13:21:01.988] iteration 14271: total_loss: 0.375180, loss_sup: 0.086767, loss_mps: 0.098211, loss_cps: 0.190202
[13:21:02.134] iteration 14272: total_loss: 0.413492, loss_sup: 0.071162, loss_mps: 0.117950, loss_cps: 0.224380
[13:21:02.280] iteration 14273: total_loss: 0.563449, loss_sup: 0.062721, loss_mps: 0.161448, loss_cps: 0.339280
[13:21:02.428] iteration 14274: total_loss: 0.400867, loss_sup: 0.021343, loss_mps: 0.121048, loss_cps: 0.258476
[13:21:02.574] iteration 14275: total_loss: 0.363682, loss_sup: 0.026903, loss_mps: 0.113999, loss_cps: 0.222780
[13:21:02.721] iteration 14276: total_loss: 0.219001, loss_sup: 0.041999, loss_mps: 0.069099, loss_cps: 0.107904
[13:21:02.867] iteration 14277: total_loss: 0.246502, loss_sup: 0.012108, loss_mps: 0.085211, loss_cps: 0.149183
[13:21:03.014] iteration 14278: total_loss: 0.289599, loss_sup: 0.088205, loss_mps: 0.070334, loss_cps: 0.131060
[13:21:03.160] iteration 14279: total_loss: 0.203100, loss_sup: 0.010332, loss_mps: 0.077878, loss_cps: 0.114890
[13:21:03.306] iteration 14280: total_loss: 0.293629, loss_sup: 0.038598, loss_mps: 0.092035, loss_cps: 0.162997
[13:21:03.452] iteration 14281: total_loss: 0.403830, loss_sup: 0.043158, loss_mps: 0.117969, loss_cps: 0.242703
[13:21:03.599] iteration 14282: total_loss: 0.315322, loss_sup: 0.040117, loss_mps: 0.093804, loss_cps: 0.181401
[13:21:03.746] iteration 14283: total_loss: 0.398774, loss_sup: 0.051513, loss_mps: 0.111204, loss_cps: 0.236057
[13:21:03.893] iteration 14284: total_loss: 0.234876, loss_sup: 0.050575, loss_mps: 0.063429, loss_cps: 0.120872
[13:21:04.041] iteration 14285: total_loss: 0.253929, loss_sup: 0.016447, loss_mps: 0.083211, loss_cps: 0.154272
[13:21:04.191] iteration 14286: total_loss: 0.391269, loss_sup: 0.032436, loss_mps: 0.116424, loss_cps: 0.242409
[13:21:04.337] iteration 14287: total_loss: 0.834636, loss_sup: 0.077298, loss_mps: 0.234352, loss_cps: 0.522987
[13:21:04.483] iteration 14288: total_loss: 0.667370, loss_sup: 0.167351, loss_mps: 0.165543, loss_cps: 0.334476
[13:21:04.630] iteration 14289: total_loss: 0.309643, loss_sup: 0.054867, loss_mps: 0.086062, loss_cps: 0.168714
[13:21:04.777] iteration 14290: total_loss: 0.352755, loss_sup: 0.026369, loss_mps: 0.108195, loss_cps: 0.218191
[13:21:04.925] iteration 14291: total_loss: 0.455788, loss_sup: 0.164395, loss_mps: 0.101804, loss_cps: 0.189588
[13:21:05.071] iteration 14292: total_loss: 0.291402, loss_sup: 0.051462, loss_mps: 0.089447, loss_cps: 0.150493
[13:21:05.218] iteration 14293: total_loss: 0.293124, loss_sup: 0.086692, loss_mps: 0.073058, loss_cps: 0.133375
[13:21:05.364] iteration 14294: total_loss: 0.323185, loss_sup: 0.039149, loss_mps: 0.097677, loss_cps: 0.186360
[13:21:05.511] iteration 14295: total_loss: 0.206522, loss_sup: 0.029174, loss_mps: 0.061777, loss_cps: 0.115571
[13:21:05.657] iteration 14296: total_loss: 0.410564, loss_sup: 0.099277, loss_mps: 0.102822, loss_cps: 0.208465
[13:21:05.805] iteration 14297: total_loss: 0.369359, loss_sup: 0.021660, loss_mps: 0.112455, loss_cps: 0.235245
[13:21:05.954] iteration 14298: total_loss: 0.750135, loss_sup: 0.130533, loss_mps: 0.188246, loss_cps: 0.431356
[13:21:06.102] iteration 14299: total_loss: 0.299567, loss_sup: 0.069874, loss_mps: 0.079829, loss_cps: 0.149864
[13:21:06.248] iteration 14300: total_loss: 0.415005, loss_sup: 0.055114, loss_mps: 0.116212, loss_cps: 0.243679
[13:21:06.248] Evaluation Started ==>
[13:21:17.645] ==> valid iteration 14300: unet metrics: {'dc': 0.672364022583429, 'jc': 0.551376057042959, 'pre': 0.7772436141611951, 'hd': 5.59671783327882}, ynet metrics: {'dc': 0.6147080894391517, 'jc': 0.49540316196375445, 'pre': 0.7892316499958657, 'hd': 5.610926875979836}.
[13:21:17.647] Evaluation Finished!⏹️
[13:21:17.799] iteration 14301: total_loss: 0.226829, loss_sup: 0.021159, loss_mps: 0.074736, loss_cps: 0.130934
[13:21:17.949] iteration 14302: total_loss: 0.331221, loss_sup: 0.052280, loss_mps: 0.093995, loss_cps: 0.184946
[13:21:18.094] iteration 14303: total_loss: 0.602515, loss_sup: 0.141386, loss_mps: 0.153310, loss_cps: 0.307820
[13:21:18.240] iteration 14304: total_loss: 0.358756, loss_sup: 0.091319, loss_mps: 0.093180, loss_cps: 0.174258
[13:21:18.386] iteration 14305: total_loss: 0.397059, loss_sup: 0.101151, loss_mps: 0.103724, loss_cps: 0.192183
[13:21:18.531] iteration 14306: total_loss: 0.315839, loss_sup: 0.053815, loss_mps: 0.092635, loss_cps: 0.169389
[13:21:18.676] iteration 14307: total_loss: 0.208633, loss_sup: 0.058339, loss_mps: 0.055877, loss_cps: 0.094416
[13:21:18.822] iteration 14308: total_loss: 0.366122, loss_sup: 0.021518, loss_mps: 0.109973, loss_cps: 0.234631
[13:21:18.967] iteration 14309: total_loss: 0.160053, loss_sup: 0.015352, loss_mps: 0.053580, loss_cps: 0.091121
[13:21:19.113] iteration 14310: total_loss: 0.267927, loss_sup: 0.057337, loss_mps: 0.072131, loss_cps: 0.138460
[13:21:19.261] iteration 14311: total_loss: 0.489016, loss_sup: 0.030609, loss_mps: 0.139972, loss_cps: 0.318435
[13:21:19.407] iteration 14312: total_loss: 0.417923, loss_sup: 0.044587, loss_mps: 0.120803, loss_cps: 0.252532
[13:21:19.556] iteration 14313: total_loss: 0.545652, loss_sup: 0.147456, loss_mps: 0.126604, loss_cps: 0.271592
[13:21:19.701] iteration 14314: total_loss: 0.460835, loss_sup: 0.130329, loss_mps: 0.109425, loss_cps: 0.221081
[13:21:19.847] iteration 14315: total_loss: 0.515527, loss_sup: 0.082957, loss_mps: 0.140811, loss_cps: 0.291758
[13:21:19.992] iteration 14316: total_loss: 0.412079, loss_sup: 0.034479, loss_mps: 0.123175, loss_cps: 0.254424
[13:21:20.137] iteration 14317: total_loss: 0.413086, loss_sup: 0.068206, loss_mps: 0.114100, loss_cps: 0.230780
[13:21:20.282] iteration 14318: total_loss: 0.436145, loss_sup: 0.069538, loss_mps: 0.120296, loss_cps: 0.246312
[13:21:20.427] iteration 14319: total_loss: 0.553743, loss_sup: 0.142688, loss_mps: 0.131293, loss_cps: 0.279762
[13:21:20.573] iteration 14320: total_loss: 0.375780, loss_sup: 0.156364, loss_mps: 0.077204, loss_cps: 0.142212
[13:21:20.720] iteration 14321: total_loss: 0.422841, loss_sup: 0.068175, loss_mps: 0.116825, loss_cps: 0.237841
[13:21:20.865] iteration 14322: total_loss: 0.347337, loss_sup: 0.061101, loss_mps: 0.099609, loss_cps: 0.186627
[13:21:21.011] iteration 14323: total_loss: 0.538182, loss_sup: 0.227034, loss_mps: 0.112000, loss_cps: 0.199148
[13:21:21.155] iteration 14324: total_loss: 0.490209, loss_sup: 0.054719, loss_mps: 0.134197, loss_cps: 0.301293
[13:21:21.300] iteration 14325: total_loss: 0.361282, loss_sup: 0.037676, loss_mps: 0.111226, loss_cps: 0.212380
[13:21:21.448] iteration 14326: total_loss: 0.450145, loss_sup: 0.019856, loss_mps: 0.139866, loss_cps: 0.290424
[13:21:21.595] iteration 14327: total_loss: 0.559920, loss_sup: 0.118231, loss_mps: 0.150129, loss_cps: 0.291560
[13:21:21.740] iteration 14328: total_loss: 0.582936, loss_sup: 0.143236, loss_mps: 0.146640, loss_cps: 0.293060
[13:21:21.886] iteration 14329: total_loss: 0.469784, loss_sup: 0.307596, loss_mps: 0.061087, loss_cps: 0.101101
[13:21:22.033] iteration 14330: total_loss: 0.286230, loss_sup: 0.057374, loss_mps: 0.081222, loss_cps: 0.147634
[13:21:22.180] iteration 14331: total_loss: 0.376449, loss_sup: 0.041107, loss_mps: 0.122550, loss_cps: 0.212792
[13:21:22.328] iteration 14332: total_loss: 0.473742, loss_sup: 0.240619, loss_mps: 0.082282, loss_cps: 0.150841
[13:21:22.474] iteration 14333: total_loss: 0.276347, loss_sup: 0.087408, loss_mps: 0.071092, loss_cps: 0.117847
[13:21:22.620] iteration 14334: total_loss: 0.527956, loss_sup: 0.078492, loss_mps: 0.146188, loss_cps: 0.303276
[13:21:22.765] iteration 14335: total_loss: 0.717851, loss_sup: 0.078802, loss_mps: 0.193696, loss_cps: 0.445353
[13:21:22.911] iteration 14336: total_loss: 0.496854, loss_sup: 0.053803, loss_mps: 0.144870, loss_cps: 0.298182
[13:21:23.057] iteration 14337: total_loss: 0.599632, loss_sup: 0.077308, loss_mps: 0.170664, loss_cps: 0.351660
[13:21:23.202] iteration 14338: total_loss: 0.596982, loss_sup: 0.197938, loss_mps: 0.139497, loss_cps: 0.259548
[13:21:23.349] iteration 14339: total_loss: 0.365676, loss_sup: 0.040130, loss_mps: 0.109800, loss_cps: 0.215746
[13:21:23.495] iteration 14340: total_loss: 0.432158, loss_sup: 0.028152, loss_mps: 0.131260, loss_cps: 0.272746
[13:21:23.641] iteration 14341: total_loss: 0.525105, loss_sup: 0.067756, loss_mps: 0.150722, loss_cps: 0.306627
[13:21:23.787] iteration 14342: total_loss: 0.422287, loss_sup: 0.109030, loss_mps: 0.103953, loss_cps: 0.209304
[13:21:23.933] iteration 14343: total_loss: 0.418558, loss_sup: 0.178811, loss_mps: 0.085963, loss_cps: 0.153783
[13:21:24.078] iteration 14344: total_loss: 0.354016, loss_sup: 0.037444, loss_mps: 0.106878, loss_cps: 0.209694
[13:21:24.224] iteration 14345: total_loss: 0.395888, loss_sup: 0.100685, loss_mps: 0.101802, loss_cps: 0.193400
[13:21:24.370] iteration 14346: total_loss: 0.415266, loss_sup: 0.047181, loss_mps: 0.122369, loss_cps: 0.245717
[13:21:24.519] iteration 14347: total_loss: 0.479929, loss_sup: 0.100932, loss_mps: 0.127507, loss_cps: 0.251490
[13:21:24.664] iteration 14348: total_loss: 0.456743, loss_sup: 0.073681, loss_mps: 0.127437, loss_cps: 0.255625
[13:21:24.810] iteration 14349: total_loss: 0.562469, loss_sup: 0.082916, loss_mps: 0.149985, loss_cps: 0.329569
[13:21:24.956] iteration 14350: total_loss: 0.338828, loss_sup: 0.026055, loss_mps: 0.111474, loss_cps: 0.201298
[13:21:25.102] iteration 14351: total_loss: 0.389474, loss_sup: 0.079968, loss_mps: 0.109872, loss_cps: 0.199635
[13:21:25.248] iteration 14352: total_loss: 0.406549, loss_sup: 0.151170, loss_mps: 0.094618, loss_cps: 0.160762
[13:21:25.394] iteration 14353: total_loss: 0.471585, loss_sup: 0.117868, loss_mps: 0.121422, loss_cps: 0.232295
[13:21:25.541] iteration 14354: total_loss: 0.268470, loss_sup: 0.024337, loss_mps: 0.086208, loss_cps: 0.157924
[13:21:25.687] iteration 14355: total_loss: 0.526298, loss_sup: 0.173112, loss_mps: 0.117953, loss_cps: 0.235233
[13:21:25.833] iteration 14356: total_loss: 0.394515, loss_sup: 0.047330, loss_mps: 0.118977, loss_cps: 0.228207
[13:21:25.979] iteration 14357: total_loss: 0.324413, loss_sup: 0.053758, loss_mps: 0.096106, loss_cps: 0.174549
[13:21:26.128] iteration 14358: total_loss: 0.668542, loss_sup: 0.156717, loss_mps: 0.160878, loss_cps: 0.350947
[13:21:26.273] iteration 14359: total_loss: 0.399285, loss_sup: 0.059477, loss_mps: 0.114321, loss_cps: 0.225487
[13:21:26.420] iteration 14360: total_loss: 0.734894, loss_sup: 0.092536, loss_mps: 0.205242, loss_cps: 0.437116
[13:21:26.566] iteration 14361: total_loss: 0.440660, loss_sup: 0.136111, loss_mps: 0.103177, loss_cps: 0.201371
[13:21:26.712] iteration 14362: total_loss: 0.277503, loss_sup: 0.017392, loss_mps: 0.093920, loss_cps: 0.166191
[13:21:26.858] iteration 14363: total_loss: 0.487770, loss_sup: 0.060962, loss_mps: 0.139899, loss_cps: 0.286909
[13:21:27.004] iteration 14364: total_loss: 0.372314, loss_sup: 0.021163, loss_mps: 0.118050, loss_cps: 0.233101
[13:21:27.149] iteration 14365: total_loss: 0.400586, loss_sup: 0.076424, loss_mps: 0.105213, loss_cps: 0.218949
[13:21:27.297] iteration 14366: total_loss: 0.561433, loss_sup: 0.116176, loss_mps: 0.142752, loss_cps: 0.302506
[13:21:27.443] iteration 14367: total_loss: 0.693326, loss_sup: 0.130827, loss_mps: 0.170652, loss_cps: 0.391847
[13:21:27.591] iteration 14368: total_loss: 0.366861, loss_sup: 0.068986, loss_mps: 0.106054, loss_cps: 0.191821
[13:21:27.736] iteration 14369: total_loss: 0.367320, loss_sup: 0.170667, loss_mps: 0.073994, loss_cps: 0.122659
[13:21:27.884] iteration 14370: total_loss: 0.578192, loss_sup: 0.080017, loss_mps: 0.161328, loss_cps: 0.336848
[13:21:28.031] iteration 14371: total_loss: 0.212148, loss_sup: 0.038311, loss_mps: 0.066662, loss_cps: 0.107175
[13:21:28.176] iteration 14372: total_loss: 0.497970, loss_sup: 0.155570, loss_mps: 0.114932, loss_cps: 0.227468
[13:21:28.324] iteration 14373: total_loss: 0.457954, loss_sup: 0.105273, loss_mps: 0.120723, loss_cps: 0.231959
[13:21:28.471] iteration 14374: total_loss: 0.397947, loss_sup: 0.083096, loss_mps: 0.109630, loss_cps: 0.205220
[13:21:28.617] iteration 14375: total_loss: 0.583486, loss_sup: 0.076845, loss_mps: 0.162926, loss_cps: 0.343715
[13:21:28.762] iteration 14376: total_loss: 0.499745, loss_sup: 0.064334, loss_mps: 0.143866, loss_cps: 0.291545
[13:21:28.908] iteration 14377: total_loss: 0.220593, loss_sup: 0.048761, loss_mps: 0.067092, loss_cps: 0.104740
[13:21:29.056] iteration 14378: total_loss: 0.250268, loss_sup: 0.032476, loss_mps: 0.081860, loss_cps: 0.135933
[13:21:29.202] iteration 14379: total_loss: 0.325276, loss_sup: 0.058501, loss_mps: 0.092813, loss_cps: 0.173962
[13:21:29.349] iteration 14380: total_loss: 0.276479, loss_sup: 0.030731, loss_mps: 0.089846, loss_cps: 0.155901
[13:21:29.495] iteration 14381: total_loss: 0.287106, loss_sup: 0.035634, loss_mps: 0.090111, loss_cps: 0.161361
[13:21:29.644] iteration 14382: total_loss: 0.369180, loss_sup: 0.043611, loss_mps: 0.113356, loss_cps: 0.212214
[13:21:29.790] iteration 14383: total_loss: 0.276864, loss_sup: 0.035608, loss_mps: 0.087762, loss_cps: 0.153493
[13:21:29.935] iteration 14384: total_loss: 0.554481, loss_sup: 0.135112, loss_mps: 0.140619, loss_cps: 0.278750
[13:21:30.081] iteration 14385: total_loss: 0.486313, loss_sup: 0.129000, loss_mps: 0.119364, loss_cps: 0.237948
[13:21:30.232] iteration 14386: total_loss: 0.427873, loss_sup: 0.088521, loss_mps: 0.118182, loss_cps: 0.221170
[13:21:30.381] iteration 14387: total_loss: 0.336099, loss_sup: 0.076165, loss_mps: 0.092609, loss_cps: 0.167325
[13:21:30.528] iteration 14388: total_loss: 0.260572, loss_sup: 0.053010, loss_mps: 0.076008, loss_cps: 0.131554
[13:21:30.674] iteration 14389: total_loss: 0.253070, loss_sup: 0.009933, loss_mps: 0.090911, loss_cps: 0.152225
[13:21:30.820] iteration 14390: total_loss: 0.429621, loss_sup: 0.093294, loss_mps: 0.114771, loss_cps: 0.221557
[13:21:30.967] iteration 14391: total_loss: 0.334628, loss_sup: 0.086755, loss_mps: 0.091064, loss_cps: 0.156809
[13:21:31.116] iteration 14392: total_loss: 0.284544, loss_sup: 0.048975, loss_mps: 0.087180, loss_cps: 0.148389
[13:21:31.262] iteration 14393: total_loss: 0.572651, loss_sup: 0.029106, loss_mps: 0.171428, loss_cps: 0.372117
[13:21:31.408] iteration 14394: total_loss: 0.333999, loss_sup: 0.037767, loss_mps: 0.101390, loss_cps: 0.194842
[13:21:31.555] iteration 14395: total_loss: 0.487692, loss_sup: 0.055463, loss_mps: 0.137411, loss_cps: 0.294818
[13:21:31.701] iteration 14396: total_loss: 0.314105, loss_sup: 0.019117, loss_mps: 0.099504, loss_cps: 0.195484
[13:21:31.847] iteration 14397: total_loss: 0.303475, loss_sup: 0.075733, loss_mps: 0.075927, loss_cps: 0.151815
[13:21:31.993] iteration 14398: total_loss: 0.636905, loss_sup: 0.061620, loss_mps: 0.183408, loss_cps: 0.391877
[13:21:32.142] iteration 14399: total_loss: 0.428454, loss_sup: 0.077165, loss_mps: 0.119361, loss_cps: 0.231928
[13:21:32.291] iteration 14400: total_loss: 0.298005, loss_sup: 0.060744, loss_mps: 0.085996, loss_cps: 0.151266
[13:21:32.291] Evaluation Started ==>
[13:21:43.647] ==> valid iteration 14400: unet metrics: {'dc': 0.6473161500103298, 'jc': 0.524158181447906, 'pre': 0.7543866803732862, 'hd': 5.800240636853183}, ynet metrics: {'dc': 0.5886329256402335, 'jc': 0.47558051527433404, 'pre': 0.7720027894118652, 'hd': 5.742264457780424}.
[13:21:43.649] Evaluation Finished!⏹️
[13:21:43.802] iteration 14401: total_loss: 0.325343, loss_sup: 0.029424, loss_mps: 0.097874, loss_cps: 0.198045
[13:21:43.951] iteration 14402: total_loss: 0.521152, loss_sup: 0.074311, loss_mps: 0.138269, loss_cps: 0.308572
[13:21:44.097] iteration 14403: total_loss: 0.311759, loss_sup: 0.096730, loss_mps: 0.080146, loss_cps: 0.134884
[13:21:44.242] iteration 14404: total_loss: 0.446744, loss_sup: 0.091705, loss_mps: 0.118990, loss_cps: 0.236049
[13:21:44.387] iteration 14405: total_loss: 0.191566, loss_sup: 0.028221, loss_mps: 0.063602, loss_cps: 0.099743
[13:21:44.532] iteration 14406: total_loss: 0.377706, loss_sup: 0.058898, loss_mps: 0.105902, loss_cps: 0.212907
[13:21:44.678] iteration 14407: total_loss: 0.340130, loss_sup: 0.027207, loss_mps: 0.101950, loss_cps: 0.210974
[13:21:44.823] iteration 14408: total_loss: 0.688935, loss_sup: 0.293666, loss_mps: 0.130038, loss_cps: 0.265232
[13:21:44.970] iteration 14409: total_loss: 0.517654, loss_sup: 0.162790, loss_mps: 0.119772, loss_cps: 0.235092
[13:21:45.115] iteration 14410: total_loss: 0.535159, loss_sup: 0.052396, loss_mps: 0.157650, loss_cps: 0.325114
[13:21:45.260] iteration 14411: total_loss: 0.379966, loss_sup: 0.086998, loss_mps: 0.098735, loss_cps: 0.194234
[13:21:45.407] iteration 14412: total_loss: 0.546047, loss_sup: 0.126144, loss_mps: 0.139258, loss_cps: 0.280645
[13:21:45.552] iteration 14413: total_loss: 0.425816, loss_sup: 0.072932, loss_mps: 0.120618, loss_cps: 0.232266
[13:21:45.698] iteration 14414: total_loss: 0.483940, loss_sup: 0.091799, loss_mps: 0.133732, loss_cps: 0.258410
[13:21:45.843] iteration 14415: total_loss: 0.230026, loss_sup: 0.030502, loss_mps: 0.075761, loss_cps: 0.123763
[13:21:45.989] iteration 14416: total_loss: 0.420466, loss_sup: 0.032936, loss_mps: 0.121066, loss_cps: 0.266464
[13:21:46.135] iteration 14417: total_loss: 0.450193, loss_sup: 0.027473, loss_mps: 0.134878, loss_cps: 0.287843
[13:21:46.280] iteration 14418: total_loss: 0.435055, loss_sup: 0.035534, loss_mps: 0.118081, loss_cps: 0.281439
[13:21:46.427] iteration 14419: total_loss: 0.390506, loss_sup: 0.017932, loss_mps: 0.118466, loss_cps: 0.254108
[13:21:46.573] iteration 14420: total_loss: 0.277202, loss_sup: 0.066173, loss_mps: 0.073942, loss_cps: 0.137086
[13:21:46.719] iteration 14421: total_loss: 0.829589, loss_sup: 0.361254, loss_mps: 0.147296, loss_cps: 0.321039
[13:21:46.865] iteration 14422: total_loss: 0.513247, loss_sup: 0.077833, loss_mps: 0.136049, loss_cps: 0.299365
[13:21:47.013] iteration 14423: total_loss: 0.351677, loss_sup: 0.066707, loss_mps: 0.098256, loss_cps: 0.186713
[13:21:47.160] iteration 14424: total_loss: 0.646120, loss_sup: 0.038502, loss_mps: 0.184735, loss_cps: 0.422883
[13:21:47.306] iteration 14425: total_loss: 0.348288, loss_sup: 0.038180, loss_mps: 0.106285, loss_cps: 0.203824
[13:21:47.452] iteration 14426: total_loss: 0.564424, loss_sup: 0.192097, loss_mps: 0.124865, loss_cps: 0.247462
[13:21:47.598] iteration 14427: total_loss: 0.424308, loss_sup: 0.144275, loss_mps: 0.099771, loss_cps: 0.180262
[13:21:47.744] iteration 14428: total_loss: 0.389815, loss_sup: 0.059054, loss_mps: 0.113231, loss_cps: 0.217530
[13:21:47.890] iteration 14429: total_loss: 0.183963, loss_sup: 0.007874, loss_mps: 0.067504, loss_cps: 0.108586
[13:21:48.036] iteration 14430: total_loss: 0.409397, loss_sup: 0.137412, loss_mps: 0.100362, loss_cps: 0.171624
[13:21:48.182] iteration 14431: total_loss: 0.649130, loss_sup: 0.126697, loss_mps: 0.165741, loss_cps: 0.356692
[13:21:48.327] iteration 14432: total_loss: 0.515845, loss_sup: 0.062705, loss_mps: 0.154546, loss_cps: 0.298594
[13:21:48.473] iteration 14433: total_loss: 0.369475, loss_sup: 0.061686, loss_mps: 0.107650, loss_cps: 0.200139
[13:21:48.618] iteration 14434: total_loss: 0.676352, loss_sup: 0.080888, loss_mps: 0.189335, loss_cps: 0.406129
[13:21:48.764] iteration 14435: total_loss: 0.890815, loss_sup: 0.158322, loss_mps: 0.227578, loss_cps: 0.504915
[13:21:48.909] iteration 14436: total_loss: 0.371570, loss_sup: 0.042079, loss_mps: 0.116768, loss_cps: 0.212723
[13:21:49.055] iteration 14437: total_loss: 0.553160, loss_sup: 0.100712, loss_mps: 0.152792, loss_cps: 0.299656
[13:21:49.202] iteration 14438: total_loss: 0.261410, loss_sup: 0.074295, loss_mps: 0.071797, loss_cps: 0.115318
[13:21:49.350] iteration 14439: total_loss: 0.355661, loss_sup: 0.080748, loss_mps: 0.097641, loss_cps: 0.177272
[13:21:49.496] iteration 14440: total_loss: 0.423111, loss_sup: 0.029739, loss_mps: 0.131853, loss_cps: 0.261518
[13:21:49.642] iteration 14441: total_loss: 0.247603, loss_sup: 0.013964, loss_mps: 0.083688, loss_cps: 0.149952
[13:21:49.788] iteration 14442: total_loss: 0.341817, loss_sup: 0.013966, loss_mps: 0.108537, loss_cps: 0.219314
[13:21:49.933] iteration 14443: total_loss: 0.385286, loss_sup: 0.023329, loss_mps: 0.116005, loss_cps: 0.245952
[13:21:50.079] iteration 14444: total_loss: 0.465568, loss_sup: 0.097400, loss_mps: 0.122890, loss_cps: 0.245277
[13:21:50.226] iteration 14445: total_loss: 0.305315, loss_sup: 0.073660, loss_mps: 0.087480, loss_cps: 0.144175
[13:21:50.376] iteration 14446: total_loss: 0.452972, loss_sup: 0.033428, loss_mps: 0.139501, loss_cps: 0.280043
[13:21:50.522] iteration 14447: total_loss: 0.616553, loss_sup: 0.091993, loss_mps: 0.162940, loss_cps: 0.361620
[13:21:50.668] iteration 14448: total_loss: 0.525105, loss_sup: 0.073379, loss_mps: 0.147963, loss_cps: 0.303763
[13:21:50.813] iteration 14449: total_loss: 0.500471, loss_sup: 0.193925, loss_mps: 0.101370, loss_cps: 0.205176
[13:21:50.959] iteration 14450: total_loss: 0.417447, loss_sup: 0.059773, loss_mps: 0.121721, loss_cps: 0.235953
[13:21:51.105] iteration 14451: total_loss: 0.234884, loss_sup: 0.039995, loss_mps: 0.076332, loss_cps: 0.118557
[13:21:51.251] iteration 14452: total_loss: 0.478565, loss_sup: 0.097793, loss_mps: 0.129152, loss_cps: 0.251619
[13:21:51.398] iteration 14453: total_loss: 0.425879, loss_sup: 0.090816, loss_mps: 0.113491, loss_cps: 0.221572
[13:21:51.546] iteration 14454: total_loss: 0.420208, loss_sup: 0.085437, loss_mps: 0.117767, loss_cps: 0.217005
[13:21:51.692] iteration 14455: total_loss: 0.375980, loss_sup: 0.006104, loss_mps: 0.117919, loss_cps: 0.251957
[13:21:51.838] iteration 14456: total_loss: 0.521184, loss_sup: 0.163850, loss_mps: 0.114536, loss_cps: 0.242799
[13:21:51.985] iteration 14457: total_loss: 0.568426, loss_sup: 0.063022, loss_mps: 0.163355, loss_cps: 0.342050
[13:21:52.130] iteration 14458: total_loss: 0.869135, loss_sup: 0.455037, loss_mps: 0.143169, loss_cps: 0.270929
[13:21:52.277] iteration 14459: total_loss: 0.524451, loss_sup: 0.096936, loss_mps: 0.137826, loss_cps: 0.289690
[13:21:52.423] iteration 14460: total_loss: 0.342133, loss_sup: 0.099974, loss_mps: 0.091665, loss_cps: 0.150494
[13:21:52.569] iteration 14461: total_loss: 0.538794, loss_sup: 0.075959, loss_mps: 0.149966, loss_cps: 0.312868
[13:21:52.715] iteration 14462: total_loss: 0.321395, loss_sup: 0.102925, loss_mps: 0.081086, loss_cps: 0.137385
[13:21:52.863] iteration 14463: total_loss: 0.504475, loss_sup: 0.060583, loss_mps: 0.148297, loss_cps: 0.295596
[13:21:53.009] iteration 14464: total_loss: 0.619378, loss_sup: 0.367234, loss_mps: 0.087711, loss_cps: 0.164433
[13:21:53.154] iteration 14465: total_loss: 0.374549, loss_sup: 0.025550, loss_mps: 0.118504, loss_cps: 0.230495
[13:21:53.300] iteration 14466: total_loss: 0.775742, loss_sup: 0.303907, loss_mps: 0.157631, loss_cps: 0.314205
[13:21:53.446] iteration 14467: total_loss: 0.168745, loss_sup: 0.015231, loss_mps: 0.059580, loss_cps: 0.093934
[13:21:53.592] iteration 14468: total_loss: 0.626369, loss_sup: 0.091416, loss_mps: 0.176670, loss_cps: 0.358283
[13:21:53.738] iteration 14469: total_loss: 0.508199, loss_sup: 0.081158, loss_mps: 0.142850, loss_cps: 0.284192
[13:21:53.884] iteration 14470: total_loss: 0.288554, loss_sup: 0.056317, loss_mps: 0.087386, loss_cps: 0.144851
[13:21:54.031] iteration 14471: total_loss: 0.413096, loss_sup: 0.036922, loss_mps: 0.136173, loss_cps: 0.240001
[13:21:54.176] iteration 14472: total_loss: 0.215986, loss_sup: 0.008945, loss_mps: 0.079132, loss_cps: 0.127910
[13:21:54.322] iteration 14473: total_loss: 0.623906, loss_sup: 0.249607, loss_mps: 0.127140, loss_cps: 0.247159
[13:21:54.470] iteration 14474: total_loss: 0.318307, loss_sup: 0.013275, loss_mps: 0.105576, loss_cps: 0.199457
[13:21:54.616] iteration 14475: total_loss: 0.536999, loss_sup: 0.171757, loss_mps: 0.119992, loss_cps: 0.245250
[13:21:54.762] iteration 14476: total_loss: 0.651524, loss_sup: 0.178708, loss_mps: 0.158476, loss_cps: 0.314339
[13:21:54.908] iteration 14477: total_loss: 0.538152, loss_sup: 0.070853, loss_mps: 0.151038, loss_cps: 0.316262
[13:21:55.054] iteration 14478: total_loss: 0.566192, loss_sup: 0.182305, loss_mps: 0.126353, loss_cps: 0.257533
[13:21:55.204] iteration 14479: total_loss: 0.382985, loss_sup: 0.108211, loss_mps: 0.096035, loss_cps: 0.178738
[13:21:55.351] iteration 14480: total_loss: 0.379674, loss_sup: 0.102240, loss_mps: 0.098342, loss_cps: 0.179092
[13:21:55.497] iteration 14481: total_loss: 0.462011, loss_sup: 0.117345, loss_mps: 0.116742, loss_cps: 0.227924
[13:21:55.642] iteration 14482: total_loss: 0.398418, loss_sup: 0.051755, loss_mps: 0.122733, loss_cps: 0.223930
[13:21:55.789] iteration 14483: total_loss: 0.366773, loss_sup: 0.036290, loss_mps: 0.113758, loss_cps: 0.216724
[13:21:55.935] iteration 14484: total_loss: 0.489925, loss_sup: 0.140788, loss_mps: 0.120265, loss_cps: 0.228872
[13:21:56.086] iteration 14485: total_loss: 0.277610, loss_sup: 0.045205, loss_mps: 0.082391, loss_cps: 0.150014
[13:21:56.238] iteration 14486: total_loss: 0.598626, loss_sup: 0.079483, loss_mps: 0.164195, loss_cps: 0.354949
[13:21:56.387] iteration 14487: total_loss: 0.404037, loss_sup: 0.139100, loss_mps: 0.090472, loss_cps: 0.174464
[13:21:56.534] iteration 14488: total_loss: 0.202628, loss_sup: 0.023432, loss_mps: 0.070336, loss_cps: 0.108859
[13:21:56.680] iteration 14489: total_loss: 0.329458, loss_sup: 0.067708, loss_mps: 0.094589, loss_cps: 0.167160
[13:21:56.825] iteration 14490: total_loss: 0.414111, loss_sup: 0.121418, loss_mps: 0.105165, loss_cps: 0.187528
[13:21:56.971] iteration 14491: total_loss: 0.514446, loss_sup: 0.148618, loss_mps: 0.122935, loss_cps: 0.242893
[13:21:57.117] iteration 14492: total_loss: 0.422726, loss_sup: 0.027619, loss_mps: 0.130809, loss_cps: 0.264298
[13:21:57.265] iteration 14493: total_loss: 0.461169, loss_sup: 0.154664, loss_mps: 0.105162, loss_cps: 0.201343
[13:21:57.417] iteration 14494: total_loss: 0.619733, loss_sup: 0.165411, loss_mps: 0.143876, loss_cps: 0.310445
[13:21:57.563] iteration 14495: total_loss: 0.507767, loss_sup: 0.107029, loss_mps: 0.136555, loss_cps: 0.264183
[13:21:57.709] iteration 14496: total_loss: 0.446182, loss_sup: 0.055355, loss_mps: 0.132612, loss_cps: 0.258214
[13:21:57.856] iteration 14497: total_loss: 0.343024, loss_sup: 0.040138, loss_mps: 0.109560, loss_cps: 0.193326
[13:21:58.003] iteration 14498: total_loss: 0.261031, loss_sup: 0.008243, loss_mps: 0.093876, loss_cps: 0.158913
[13:21:58.150] iteration 14499: total_loss: 0.225262, loss_sup: 0.006641, loss_mps: 0.084258, loss_cps: 0.134363
[13:21:58.298] iteration 14500: total_loss: 0.237426, loss_sup: 0.010918, loss_mps: 0.085276, loss_cps: 0.141232
[13:21:58.298] Evaluation Started ==>
[13:22:09.772] ==> valid iteration 14500: unet metrics: {'dc': 0.6361351110697232, 'jc': 0.5200611897647169, 'pre': 0.7749813175626071, 'hd': 5.513251545392346}, ynet metrics: {'dc': 0.5987084947232899, 'jc': 0.48473105822429663, 'pre': 0.7779694446415735, 'hd': 5.631929612576789}.
[13:22:09.773] Evaluation Finished!⏹️
[13:22:09.924] iteration 14501: total_loss: 0.268286, loss_sup: 0.034419, loss_mps: 0.087336, loss_cps: 0.146530
[13:22:10.072] iteration 14502: total_loss: 0.377161, loss_sup: 0.062943, loss_mps: 0.108527, loss_cps: 0.205691
[13:22:10.220] iteration 14503: total_loss: 0.318897, loss_sup: 0.012593, loss_mps: 0.104797, loss_cps: 0.201506
[13:22:10.366] iteration 14504: total_loss: 0.421946, loss_sup: 0.027307, loss_mps: 0.132354, loss_cps: 0.262285
[13:22:10.512] iteration 14505: total_loss: 0.379929, loss_sup: 0.035987, loss_mps: 0.121248, loss_cps: 0.222694
[13:22:10.658] iteration 14506: total_loss: 0.453367, loss_sup: 0.173176, loss_mps: 0.094352, loss_cps: 0.185839
[13:22:10.803] iteration 14507: total_loss: 0.465477, loss_sup: 0.038802, loss_mps: 0.146460, loss_cps: 0.280215
[13:22:10.949] iteration 14508: total_loss: 0.186385, loss_sup: 0.013572, loss_mps: 0.065784, loss_cps: 0.107028
[13:22:11.094] iteration 14509: total_loss: 0.230605, loss_sup: 0.072860, loss_mps: 0.058396, loss_cps: 0.099349
[13:22:11.239] iteration 14510: total_loss: 0.180623, loss_sup: 0.025991, loss_mps: 0.057322, loss_cps: 0.097310
[13:22:11.385] iteration 14511: total_loss: 0.266646, loss_sup: 0.024319, loss_mps: 0.088618, loss_cps: 0.153709
[13:22:11.531] iteration 14512: total_loss: 0.468797, loss_sup: 0.184584, loss_mps: 0.098810, loss_cps: 0.185403
[13:22:11.678] iteration 14513: total_loss: 0.408767, loss_sup: 0.084272, loss_mps: 0.113584, loss_cps: 0.210912
[13:22:11.823] iteration 14514: total_loss: 0.232886, loss_sup: 0.064420, loss_mps: 0.063532, loss_cps: 0.104934
[13:22:11.970] iteration 14515: total_loss: 0.345436, loss_sup: 0.108662, loss_mps: 0.085614, loss_cps: 0.151160
[13:22:12.117] iteration 14516: total_loss: 0.327014, loss_sup: 0.071619, loss_mps: 0.095078, loss_cps: 0.160317
[13:22:12.263] iteration 14517: total_loss: 0.335241, loss_sup: 0.046272, loss_mps: 0.099524, loss_cps: 0.189444
[13:22:12.411] iteration 14518: total_loss: 0.331979, loss_sup: 0.100475, loss_mps: 0.085297, loss_cps: 0.146207
[13:22:12.557] iteration 14519: total_loss: 0.304076, loss_sup: 0.101095, loss_mps: 0.072516, loss_cps: 0.130466
[13:22:12.702] iteration 14520: total_loss: 0.748113, loss_sup: 0.441637, loss_mps: 0.105800, loss_cps: 0.200677
[13:22:12.848] iteration 14521: total_loss: 0.323704, loss_sup: 0.074039, loss_mps: 0.083678, loss_cps: 0.165987
[13:22:12.993] iteration 14522: total_loss: 0.567062, loss_sup: 0.162695, loss_mps: 0.136648, loss_cps: 0.267720
[13:22:13.140] iteration 14523: total_loss: 0.544719, loss_sup: 0.191741, loss_mps: 0.123573, loss_cps: 0.229406
[13:22:13.285] iteration 14524: total_loss: 0.345305, loss_sup: 0.039898, loss_mps: 0.096176, loss_cps: 0.209231
[13:22:13.430] iteration 14525: total_loss: 0.766621, loss_sup: 0.097454, loss_mps: 0.207620, loss_cps: 0.461548
[13:22:13.576] iteration 14526: total_loss: 0.322855, loss_sup: 0.024644, loss_mps: 0.100061, loss_cps: 0.198150
[13:22:13.723] iteration 14527: total_loss: 0.497991, loss_sup: 0.119685, loss_mps: 0.121433, loss_cps: 0.256872
[13:22:13.869] iteration 14528: total_loss: 0.337657, loss_sup: 0.024500, loss_mps: 0.108304, loss_cps: 0.204852
[13:22:14.015] iteration 14529: total_loss: 0.455612, loss_sup: 0.084285, loss_mps: 0.121823, loss_cps: 0.249505
[13:22:14.161] iteration 14530: total_loss: 0.345518, loss_sup: 0.042360, loss_mps: 0.100311, loss_cps: 0.202847
[13:22:14.306] iteration 14531: total_loss: 0.286558, loss_sup: 0.027525, loss_mps: 0.095012, loss_cps: 0.164022
[13:22:14.455] iteration 14532: total_loss: 0.577591, loss_sup: 0.062764, loss_mps: 0.160465, loss_cps: 0.354362
[13:22:14.604] iteration 14533: total_loss: 0.423359, loss_sup: 0.045944, loss_mps: 0.126990, loss_cps: 0.250426
[13:22:14.750] iteration 14534: total_loss: 0.549053, loss_sup: 0.158020, loss_mps: 0.129639, loss_cps: 0.261395
[13:22:14.900] iteration 14535: total_loss: 0.303731, loss_sup: 0.018387, loss_mps: 0.097870, loss_cps: 0.187474
[13:22:15.046] iteration 14536: total_loss: 0.312571, loss_sup: 0.062821, loss_mps: 0.090484, loss_cps: 0.159266
[13:22:15.192] iteration 14537: total_loss: 0.467296, loss_sup: 0.123115, loss_mps: 0.116283, loss_cps: 0.227897
[13:22:15.338] iteration 14538: total_loss: 0.306319, loss_sup: 0.029948, loss_mps: 0.092531, loss_cps: 0.183840
[13:22:15.484] iteration 14539: total_loss: 0.450061, loss_sup: 0.066841, loss_mps: 0.136952, loss_cps: 0.246268
[13:22:15.630] iteration 14540: total_loss: 0.428598, loss_sup: 0.053462, loss_mps: 0.119744, loss_cps: 0.255393
[13:22:15.776] iteration 14541: total_loss: 0.306676, loss_sup: 0.044055, loss_mps: 0.097371, loss_cps: 0.165250
[13:22:15.922] iteration 14542: total_loss: 0.611334, loss_sup: 0.073621, loss_mps: 0.172924, loss_cps: 0.364788
[13:22:16.067] iteration 14543: total_loss: 0.440892, loss_sup: 0.077344, loss_mps: 0.120018, loss_cps: 0.243530
[13:22:16.214] iteration 14544: total_loss: 0.211903, loss_sup: 0.024031, loss_mps: 0.067782, loss_cps: 0.120089
[13:22:16.359] iteration 14545: total_loss: 0.223651, loss_sup: 0.049329, loss_mps: 0.065836, loss_cps: 0.108486
[13:22:16.505] iteration 14546: total_loss: 0.272209, loss_sup: 0.087678, loss_mps: 0.069346, loss_cps: 0.115186
[13:22:16.650] iteration 14547: total_loss: 0.386680, loss_sup: 0.081999, loss_mps: 0.098022, loss_cps: 0.206659
[13:22:16.796] iteration 14548: total_loss: 0.786169, loss_sup: 0.156752, loss_mps: 0.195194, loss_cps: 0.434224
[13:22:16.942] iteration 14549: total_loss: 0.538892, loss_sup: 0.135537, loss_mps: 0.137368, loss_cps: 0.265987
[13:22:17.088] iteration 14550: total_loss: 0.526874, loss_sup: 0.186176, loss_mps: 0.118073, loss_cps: 0.222625
[13:22:17.234] iteration 14551: total_loss: 0.263325, loss_sup: 0.064367, loss_mps: 0.072951, loss_cps: 0.126007
[13:22:17.380] iteration 14552: total_loss: 0.388447, loss_sup: 0.160627, loss_mps: 0.080106, loss_cps: 0.147714
[13:22:17.527] iteration 14553: total_loss: 0.407640, loss_sup: 0.133854, loss_mps: 0.094410, loss_cps: 0.179376
[13:22:17.674] iteration 14554: total_loss: 0.383582, loss_sup: 0.042663, loss_mps: 0.116314, loss_cps: 0.224604
[13:22:17.819] iteration 14555: total_loss: 0.410097, loss_sup: 0.054601, loss_mps: 0.121346, loss_cps: 0.234150
[13:22:17.966] iteration 14556: total_loss: 0.386836, loss_sup: 0.078029, loss_mps: 0.115426, loss_cps: 0.193381
[13:22:18.112] iteration 14557: total_loss: 0.539784, loss_sup: 0.133174, loss_mps: 0.141320, loss_cps: 0.265291
[13:22:18.258] iteration 14558: total_loss: 0.297023, loss_sup: 0.068473, loss_mps: 0.086406, loss_cps: 0.142145
[13:22:18.403] iteration 14559: total_loss: 0.383080, loss_sup: 0.071054, loss_mps: 0.104277, loss_cps: 0.207748
[13:22:18.551] iteration 14560: total_loss: 0.317277, loss_sup: 0.017188, loss_mps: 0.099237, loss_cps: 0.200852
[13:22:18.697] iteration 14561: total_loss: 0.511121, loss_sup: 0.082094, loss_mps: 0.141009, loss_cps: 0.288018
[13:22:18.843] iteration 14562: total_loss: 0.296935, loss_sup: 0.006698, loss_mps: 0.104984, loss_cps: 0.185254
[13:22:18.989] iteration 14563: total_loss: 0.651082, loss_sup: 0.164291, loss_mps: 0.158855, loss_cps: 0.327936
[13:22:19.136] iteration 14564: total_loss: 0.510210, loss_sup: 0.110532, loss_mps: 0.129918, loss_cps: 0.269760
[13:22:19.282] iteration 14565: total_loss: 0.452997, loss_sup: 0.110273, loss_mps: 0.111535, loss_cps: 0.231189
[13:22:19.428] iteration 14566: total_loss: 0.349836, loss_sup: 0.075018, loss_mps: 0.095400, loss_cps: 0.179419
[13:22:19.574] iteration 14567: total_loss: 0.268599, loss_sup: 0.040822, loss_mps: 0.080652, loss_cps: 0.147125
[13:22:19.720] iteration 14568: total_loss: 0.323963, loss_sup: 0.067899, loss_mps: 0.088163, loss_cps: 0.167900
[13:22:19.866] iteration 14569: total_loss: 0.555583, loss_sup: 0.100650, loss_mps: 0.146628, loss_cps: 0.308306
[13:22:20.013] iteration 14570: total_loss: 0.244462, loss_sup: 0.060834, loss_mps: 0.066547, loss_cps: 0.117082
[13:22:20.161] iteration 14571: total_loss: 0.280397, loss_sup: 0.044024, loss_mps: 0.084122, loss_cps: 0.152250
[13:22:20.306] iteration 14572: total_loss: 0.703870, loss_sup: 0.121645, loss_mps: 0.177775, loss_cps: 0.404450
[13:22:20.452] iteration 14573: total_loss: 0.444864, loss_sup: 0.085567, loss_mps: 0.122215, loss_cps: 0.237082
[13:22:20.598] iteration 14574: total_loss: 0.288363, loss_sup: 0.034643, loss_mps: 0.092882, loss_cps: 0.160838
[13:22:20.744] iteration 14575: total_loss: 0.295799, loss_sup: 0.011558, loss_mps: 0.099536, loss_cps: 0.184704
[13:22:20.889] iteration 14576: total_loss: 0.373917, loss_sup: 0.037084, loss_mps: 0.114571, loss_cps: 0.222261
[13:22:21.035] iteration 14577: total_loss: 0.364602, loss_sup: 0.025476, loss_mps: 0.113219, loss_cps: 0.225908
[13:22:21.181] iteration 14578: total_loss: 0.528423, loss_sup: 0.215758, loss_mps: 0.109534, loss_cps: 0.203131
[13:22:21.327] iteration 14579: total_loss: 0.693967, loss_sup: 0.177934, loss_mps: 0.159047, loss_cps: 0.356987
[13:22:21.473] iteration 14580: total_loss: 0.312300, loss_sup: 0.031683, loss_mps: 0.098498, loss_cps: 0.182119
[13:22:21.619] iteration 14581: total_loss: 0.403027, loss_sup: 0.046062, loss_mps: 0.111057, loss_cps: 0.245908
[13:22:21.765] iteration 14582: total_loss: 0.593437, loss_sup: 0.158632, loss_mps: 0.139828, loss_cps: 0.294977
[13:22:21.911] iteration 14583: total_loss: 0.543799, loss_sup: 0.138519, loss_mps: 0.140693, loss_cps: 0.264587
[13:22:22.056] iteration 14584: total_loss: 0.422324, loss_sup: 0.111251, loss_mps: 0.107945, loss_cps: 0.203129
[13:22:22.202] iteration 14585: total_loss: 0.232774, loss_sup: 0.014378, loss_mps: 0.076511, loss_cps: 0.141884
[13:22:22.348] iteration 14586: total_loss: 0.410092, loss_sup: 0.200723, loss_mps: 0.084456, loss_cps: 0.124913
[13:22:22.494] iteration 14587: total_loss: 0.740660, loss_sup: 0.316308, loss_mps: 0.140092, loss_cps: 0.284260
[13:22:22.640] iteration 14588: total_loss: 0.460072, loss_sup: 0.097304, loss_mps: 0.125281, loss_cps: 0.237488
[13:22:22.788] iteration 14589: total_loss: 0.386124, loss_sup: 0.072580, loss_mps: 0.114440, loss_cps: 0.199104
[13:22:22.935] iteration 14590: total_loss: 0.358223, loss_sup: 0.039374, loss_mps: 0.108472, loss_cps: 0.210378
[13:22:23.080] iteration 14591: total_loss: 0.394792, loss_sup: 0.009850, loss_mps: 0.122089, loss_cps: 0.262852
[13:22:23.226] iteration 14592: total_loss: 0.484978, loss_sup: 0.087216, loss_mps: 0.128825, loss_cps: 0.268937
[13:22:23.376] iteration 14593: total_loss: 0.338693, loss_sup: 0.028314, loss_mps: 0.101816, loss_cps: 0.208563
[13:22:23.524] iteration 14594: total_loss: 0.321759, loss_sup: 0.040099, loss_mps: 0.101797, loss_cps: 0.179863
[13:22:23.671] iteration 14595: total_loss: 0.462341, loss_sup: 0.057688, loss_mps: 0.135663, loss_cps: 0.268989
[13:22:23.820] iteration 14596: total_loss: 0.508152, loss_sup: 0.079872, loss_mps: 0.139379, loss_cps: 0.288901
[13:22:23.968] iteration 14597: total_loss: 0.324577, loss_sup: 0.026827, loss_mps: 0.102988, loss_cps: 0.194761
[13:22:24.114] iteration 14598: total_loss: 0.752492, loss_sup: 0.171648, loss_mps: 0.186896, loss_cps: 0.393948
[13:22:24.260] iteration 14599: total_loss: 0.539565, loss_sup: 0.150102, loss_mps: 0.126899, loss_cps: 0.262564
[13:22:24.406] iteration 14600: total_loss: 0.307643, loss_sup: 0.064489, loss_mps: 0.090456, loss_cps: 0.152697
[13:22:24.406] Evaluation Started ==>
[13:22:35.769] ==> valid iteration 14600: unet metrics: {'dc': 0.6428339097153242, 'jc': 0.5279702039406123, 'pre': 0.7792945719412833, 'hd': 5.360439331484299}, ynet metrics: {'dc': 0.5686136230342574, 'jc': 0.4535070763859919, 'pre': 0.7856963321546346, 'hd': 5.560450546873001}.
[13:22:35.771] Evaluation Finished!⏹️
[13:22:35.924] iteration 14601: total_loss: 0.462785, loss_sup: 0.064542, loss_mps: 0.136429, loss_cps: 0.261815
[13:22:36.075] iteration 14602: total_loss: 0.567158, loss_sup: 0.105248, loss_mps: 0.146853, loss_cps: 0.315057
[13:22:36.221] iteration 14603: total_loss: 0.574332, loss_sup: 0.273791, loss_mps: 0.104514, loss_cps: 0.196028
[13:22:36.367] iteration 14604: total_loss: 0.724517, loss_sup: 0.228998, loss_mps: 0.159750, loss_cps: 0.335769
[13:22:36.513] iteration 14605: total_loss: 0.561960, loss_sup: 0.179169, loss_mps: 0.134552, loss_cps: 0.248239
[13:22:36.658] iteration 14606: total_loss: 0.371671, loss_sup: 0.054409, loss_mps: 0.110190, loss_cps: 0.207072
[13:22:36.805] iteration 14607: total_loss: 0.381527, loss_sup: 0.160058, loss_mps: 0.083430, loss_cps: 0.138039
[13:22:36.953] iteration 14608: total_loss: 0.728527, loss_sup: 0.136715, loss_mps: 0.186928, loss_cps: 0.404884
[13:22:37.099] iteration 14609: total_loss: 0.307824, loss_sup: 0.030967, loss_mps: 0.093240, loss_cps: 0.183616
[13:22:37.245] iteration 14610: total_loss: 0.518658, loss_sup: 0.049372, loss_mps: 0.152797, loss_cps: 0.316489
[13:22:37.391] iteration 14611: total_loss: 0.299790, loss_sup: 0.034363, loss_mps: 0.091191, loss_cps: 0.174235
[13:22:37.540] iteration 14612: total_loss: 0.577831, loss_sup: 0.051468, loss_mps: 0.175583, loss_cps: 0.350779
[13:22:37.686] iteration 14613: total_loss: 0.209768, loss_sup: 0.013815, loss_mps: 0.073045, loss_cps: 0.122908
[13:22:37.832] iteration 14614: total_loss: 0.406269, loss_sup: 0.076720, loss_mps: 0.111318, loss_cps: 0.218232
[13:22:37.976] iteration 14615: total_loss: 0.360570, loss_sup: 0.108704, loss_mps: 0.090448, loss_cps: 0.161418
[13:22:38.124] iteration 14616: total_loss: 0.369956, loss_sup: 0.038650, loss_mps: 0.108758, loss_cps: 0.222547
[13:22:38.270] iteration 14617: total_loss: 0.792218, loss_sup: 0.075774, loss_mps: 0.214473, loss_cps: 0.501971
[13:22:38.415] iteration 14618: total_loss: 0.503444, loss_sup: 0.108731, loss_mps: 0.132235, loss_cps: 0.262479
[13:22:38.560] iteration 14619: total_loss: 0.454894, loss_sup: 0.024565, loss_mps: 0.145690, loss_cps: 0.284639
[13:22:38.705] iteration 14620: total_loss: 0.402038, loss_sup: 0.086479, loss_mps: 0.112381, loss_cps: 0.203178
[13:22:38.851] iteration 14621: total_loss: 0.703131, loss_sup: 0.247125, loss_mps: 0.156012, loss_cps: 0.299994
[13:22:38.995] iteration 14622: total_loss: 0.603808, loss_sup: 0.088585, loss_mps: 0.167498, loss_cps: 0.347725
[13:22:39.140] iteration 14623: total_loss: 0.507136, loss_sup: 0.013687, loss_mps: 0.163858, loss_cps: 0.329591
[13:22:39.286] iteration 14624: total_loss: 0.414685, loss_sup: 0.157388, loss_mps: 0.095984, loss_cps: 0.161313
[13:22:39.431] iteration 14625: total_loss: 0.294928, loss_sup: 0.023788, loss_mps: 0.099048, loss_cps: 0.172091
[13:22:39.576] iteration 14626: total_loss: 0.391887, loss_sup: 0.051207, loss_mps: 0.112243, loss_cps: 0.228437
[13:22:39.721] iteration 14627: total_loss: 0.298813, loss_sup: 0.103815, loss_mps: 0.072848, loss_cps: 0.122150
[13:22:39.866] iteration 14628: total_loss: 0.423991, loss_sup: 0.097544, loss_mps: 0.114591, loss_cps: 0.211856
[13:22:40.011] iteration 14629: total_loss: 0.537639, loss_sup: 0.116723, loss_mps: 0.137713, loss_cps: 0.283203
[13:22:40.075] iteration 14630: total_loss: 0.969576, loss_sup: 0.493358, loss_mps: 0.165482, loss_cps: 0.310736
[13:22:41.290] iteration 14631: total_loss: 0.532991, loss_sup: 0.149568, loss_mps: 0.130613, loss_cps: 0.252810
[13:22:41.438] iteration 14632: total_loss: 0.486762, loss_sup: 0.100917, loss_mps: 0.130413, loss_cps: 0.255432
[13:22:41.584] iteration 14633: total_loss: 0.322922, loss_sup: 0.029567, loss_mps: 0.098363, loss_cps: 0.194993
[13:22:41.730] iteration 14634: total_loss: 0.416081, loss_sup: 0.069130, loss_mps: 0.118456, loss_cps: 0.228496
[13:22:41.876] iteration 14635: total_loss: 0.412477, loss_sup: 0.066624, loss_mps: 0.116488, loss_cps: 0.229365
[13:22:42.022] iteration 14636: total_loss: 0.457162, loss_sup: 0.099152, loss_mps: 0.117629, loss_cps: 0.240382
[13:22:42.168] iteration 14637: total_loss: 0.349236, loss_sup: 0.050329, loss_mps: 0.102718, loss_cps: 0.196189
[13:22:42.315] iteration 14638: total_loss: 0.272576, loss_sup: 0.039645, loss_mps: 0.082971, loss_cps: 0.149960
[13:22:42.460] iteration 14639: total_loss: 0.401630, loss_sup: 0.084693, loss_mps: 0.103166, loss_cps: 0.213770
[13:22:42.606] iteration 14640: total_loss: 0.388374, loss_sup: 0.088523, loss_mps: 0.102879, loss_cps: 0.196971
[13:22:42.751] iteration 14641: total_loss: 0.317317, loss_sup: 0.138972, loss_mps: 0.068851, loss_cps: 0.109493
[13:22:42.902] iteration 14642: total_loss: 0.575835, loss_sup: 0.021011, loss_mps: 0.172882, loss_cps: 0.381942
[13:22:43.047] iteration 14643: total_loss: 0.597839, loss_sup: 0.170269, loss_mps: 0.144201, loss_cps: 0.283369
[13:22:43.193] iteration 14644: total_loss: 0.457748, loss_sup: 0.061845, loss_mps: 0.128843, loss_cps: 0.267059
[13:22:43.338] iteration 14645: total_loss: 0.513818, loss_sup: 0.081362, loss_mps: 0.143495, loss_cps: 0.288961
[13:22:43.483] iteration 14646: total_loss: 0.725499, loss_sup: 0.167311, loss_mps: 0.177102, loss_cps: 0.381086
[13:22:43.629] iteration 14647: total_loss: 0.324042, loss_sup: 0.057305, loss_mps: 0.094826, loss_cps: 0.171911
[13:22:43.774] iteration 14648: total_loss: 0.311796, loss_sup: 0.028740, loss_mps: 0.101016, loss_cps: 0.182039
[13:22:43.920] iteration 14649: total_loss: 0.401473, loss_sup: 0.021373, loss_mps: 0.132946, loss_cps: 0.247153
[13:22:44.066] iteration 14650: total_loss: 0.641990, loss_sup: 0.057091, loss_mps: 0.185816, loss_cps: 0.399083
[13:22:44.211] iteration 14651: total_loss: 0.350854, loss_sup: 0.063054, loss_mps: 0.104082, loss_cps: 0.183719
[13:22:44.356] iteration 14652: total_loss: 0.251276, loss_sup: 0.015043, loss_mps: 0.080509, loss_cps: 0.155724
[13:22:44.502] iteration 14653: total_loss: 0.438480, loss_sup: 0.060404, loss_mps: 0.120713, loss_cps: 0.257362
[13:22:44.647] iteration 14654: total_loss: 0.486677, loss_sup: 0.064734, loss_mps: 0.138563, loss_cps: 0.283380
[13:22:44.793] iteration 14655: total_loss: 0.369695, loss_sup: 0.043323, loss_mps: 0.102759, loss_cps: 0.223612
[13:22:44.938] iteration 14656: total_loss: 0.361538, loss_sup: 0.079959, loss_mps: 0.101886, loss_cps: 0.179693
[13:22:45.084] iteration 14657: total_loss: 0.463935, loss_sup: 0.090016, loss_mps: 0.129881, loss_cps: 0.244038
[13:22:45.229] iteration 14658: total_loss: 0.252243, loss_sup: 0.032238, loss_mps: 0.080762, loss_cps: 0.139242
[13:22:45.376] iteration 14659: total_loss: 0.819269, loss_sup: 0.445725, loss_mps: 0.132446, loss_cps: 0.241097
[13:22:45.521] iteration 14660: total_loss: 0.408335, loss_sup: 0.017204, loss_mps: 0.129535, loss_cps: 0.261595
[13:22:45.667] iteration 14661: total_loss: 0.323858, loss_sup: 0.088678, loss_mps: 0.085514, loss_cps: 0.149666
[13:22:45.812] iteration 14662: total_loss: 0.499184, loss_sup: 0.075251, loss_mps: 0.142255, loss_cps: 0.281678
[13:22:45.958] iteration 14663: total_loss: 0.381536, loss_sup: 0.062208, loss_mps: 0.107704, loss_cps: 0.211624
[13:22:46.103] iteration 14664: total_loss: 0.269885, loss_sup: 0.009074, loss_mps: 0.091381, loss_cps: 0.169430
[13:22:46.249] iteration 14665: total_loss: 0.377824, loss_sup: 0.046310, loss_mps: 0.110923, loss_cps: 0.220591
[13:22:46.395] iteration 14666: total_loss: 0.687671, loss_sup: 0.222544, loss_mps: 0.152993, loss_cps: 0.312134
[13:22:46.541] iteration 14667: total_loss: 0.512046, loss_sup: 0.084693, loss_mps: 0.141002, loss_cps: 0.286351
[13:22:46.686] iteration 14668: total_loss: 0.375308, loss_sup: 0.040348, loss_mps: 0.119699, loss_cps: 0.215260
[13:22:46.831] iteration 14669: total_loss: 0.305768, loss_sup: 0.018765, loss_mps: 0.098945, loss_cps: 0.188058
[13:22:46.977] iteration 14670: total_loss: 0.576569, loss_sup: 0.037947, loss_mps: 0.165472, loss_cps: 0.373149
[13:22:47.122] iteration 14671: total_loss: 0.398736, loss_sup: 0.117598, loss_mps: 0.099411, loss_cps: 0.181727
[13:22:47.268] iteration 14672: total_loss: 0.316345, loss_sup: 0.024160, loss_mps: 0.100942, loss_cps: 0.191243
[13:22:47.413] iteration 14673: total_loss: 0.351196, loss_sup: 0.078210, loss_mps: 0.096382, loss_cps: 0.176604
[13:22:47.560] iteration 14674: total_loss: 0.207745, loss_sup: 0.028502, loss_mps: 0.068776, loss_cps: 0.110467
[13:22:47.705] iteration 14675: total_loss: 0.455448, loss_sup: 0.080691, loss_mps: 0.127542, loss_cps: 0.247214
[13:22:47.850] iteration 14676: total_loss: 0.390203, loss_sup: 0.134529, loss_mps: 0.090510, loss_cps: 0.165165
[13:22:47.996] iteration 14677: total_loss: 0.324038, loss_sup: 0.026204, loss_mps: 0.101836, loss_cps: 0.195997
[13:22:48.142] iteration 14678: total_loss: 0.290708, loss_sup: 0.011036, loss_mps: 0.095629, loss_cps: 0.184042
[13:22:48.287] iteration 14679: total_loss: 0.346288, loss_sup: 0.006627, loss_mps: 0.112794, loss_cps: 0.226867
[13:22:48.433] iteration 14680: total_loss: 0.413774, loss_sup: 0.045984, loss_mps: 0.128547, loss_cps: 0.239244
[13:22:48.579] iteration 14681: total_loss: 0.454061, loss_sup: 0.051050, loss_mps: 0.129864, loss_cps: 0.273147
[13:22:48.725] iteration 14682: total_loss: 0.720287, loss_sup: 0.083127, loss_mps: 0.189111, loss_cps: 0.448049
[13:22:48.870] iteration 14683: total_loss: 0.487588, loss_sup: 0.202670, loss_mps: 0.101306, loss_cps: 0.183613
[13:22:49.015] iteration 14684: total_loss: 0.234491, loss_sup: 0.006758, loss_mps: 0.085808, loss_cps: 0.141924
[13:22:49.160] iteration 14685: total_loss: 0.338041, loss_sup: 0.074354, loss_mps: 0.094246, loss_cps: 0.169442
[13:22:49.306] iteration 14686: total_loss: 0.439108, loss_sup: 0.066144, loss_mps: 0.120506, loss_cps: 0.252457
[13:22:49.451] iteration 14687: total_loss: 0.597059, loss_sup: 0.049030, loss_mps: 0.167628, loss_cps: 0.380401
[13:22:49.598] iteration 14688: total_loss: 0.181530, loss_sup: 0.003096, loss_mps: 0.067855, loss_cps: 0.110579
[13:22:49.744] iteration 14689: total_loss: 0.310903, loss_sup: 0.094051, loss_mps: 0.075697, loss_cps: 0.141156
[13:22:49.889] iteration 14690: total_loss: 0.679189, loss_sup: 0.246602, loss_mps: 0.139477, loss_cps: 0.293110
[13:22:50.034] iteration 14691: total_loss: 0.329314, loss_sup: 0.009950, loss_mps: 0.110576, loss_cps: 0.208788
[13:22:50.179] iteration 14692: total_loss: 0.393842, loss_sup: 0.042590, loss_mps: 0.123792, loss_cps: 0.227460
[13:22:50.325] iteration 14693: total_loss: 0.358715, loss_sup: 0.090408, loss_mps: 0.097659, loss_cps: 0.170648
[13:22:50.471] iteration 14694: total_loss: 0.741533, loss_sup: 0.221147, loss_mps: 0.168196, loss_cps: 0.352189
[13:22:50.617] iteration 14695: total_loss: 0.341079, loss_sup: 0.023526, loss_mps: 0.109152, loss_cps: 0.208401
[13:22:50.763] iteration 14696: total_loss: 0.439964, loss_sup: 0.112783, loss_mps: 0.116163, loss_cps: 0.211018
[13:22:50.909] iteration 14697: total_loss: 0.602420, loss_sup: 0.052823, loss_mps: 0.172777, loss_cps: 0.376819
[13:22:51.056] iteration 14698: total_loss: 0.682741, loss_sup: 0.122373, loss_mps: 0.179412, loss_cps: 0.380956
[13:22:51.201] iteration 14699: total_loss: 0.480247, loss_sup: 0.066811, loss_mps: 0.132550, loss_cps: 0.280886
[13:22:51.347] iteration 14700: total_loss: 0.469474, loss_sup: 0.088967, loss_mps: 0.123921, loss_cps: 0.256586
[13:22:51.347] Evaluation Started ==>
[13:23:02.688] ==> valid iteration 14700: unet metrics: {'dc': 0.6167383927556543, 'jc': 0.49671102015501656, 'pre': 0.779231432183652, 'hd': 5.703327893466917}, ynet metrics: {'dc': 0.5980260715185169, 'jc': 0.4793181381515218, 'pre': 0.7664077705278872, 'hd': 5.741687376816751}.
[13:23:02.690] Evaluation Finished!⏹️
[13:23:02.842] iteration 14701: total_loss: 0.316143, loss_sup: 0.076246, loss_mps: 0.087701, loss_cps: 0.152196
[13:23:02.990] iteration 14702: total_loss: 0.580068, loss_sup: 0.039147, loss_mps: 0.170644, loss_cps: 0.370278
[13:23:03.136] iteration 14703: total_loss: 1.165295, loss_sup: 0.198314, loss_mps: 0.297269, loss_cps: 0.669711
[13:23:03.281] iteration 14704: total_loss: 0.447544, loss_sup: 0.041461, loss_mps: 0.130936, loss_cps: 0.275147
[13:23:03.427] iteration 14705: total_loss: 0.260458, loss_sup: 0.021893, loss_mps: 0.084627, loss_cps: 0.153937
[13:23:03.572] iteration 14706: total_loss: 0.424187, loss_sup: 0.041460, loss_mps: 0.132420, loss_cps: 0.250307
[13:23:03.718] iteration 14707: total_loss: 0.472718, loss_sup: 0.118797, loss_mps: 0.122114, loss_cps: 0.231807
[13:23:03.863] iteration 14708: total_loss: 0.344389, loss_sup: 0.049242, loss_mps: 0.104988, loss_cps: 0.190159
[13:23:04.008] iteration 14709: total_loss: 0.383893, loss_sup: 0.010239, loss_mps: 0.127084, loss_cps: 0.246570
[13:23:04.153] iteration 14710: total_loss: 0.359916, loss_sup: 0.096789, loss_mps: 0.092414, loss_cps: 0.170713
[13:23:04.301] iteration 14711: total_loss: 0.424926, loss_sup: 0.054003, loss_mps: 0.125382, loss_cps: 0.245542
[13:23:04.447] iteration 14712: total_loss: 0.498774, loss_sup: 0.147215, loss_mps: 0.112453, loss_cps: 0.239105
[13:23:04.593] iteration 14713: total_loss: 0.612477, loss_sup: 0.202131, loss_mps: 0.140313, loss_cps: 0.270034
[13:23:04.738] iteration 14714: total_loss: 0.562128, loss_sup: 0.188623, loss_mps: 0.124014, loss_cps: 0.249491
[13:23:04.883] iteration 14715: total_loss: 0.597003, loss_sup: 0.123482, loss_mps: 0.150522, loss_cps: 0.322999
[13:23:05.033] iteration 14716: total_loss: 0.324470, loss_sup: 0.022099, loss_mps: 0.107563, loss_cps: 0.194808
[13:23:05.180] iteration 14717: total_loss: 0.351585, loss_sup: 0.073599, loss_mps: 0.093937, loss_cps: 0.184048
[13:23:05.325] iteration 14718: total_loss: 0.262089, loss_sup: 0.019502, loss_mps: 0.089811, loss_cps: 0.152776
[13:23:05.471] iteration 14719: total_loss: 0.347613, loss_sup: 0.087928, loss_mps: 0.090390, loss_cps: 0.169294
[13:23:05.616] iteration 14720: total_loss: 0.460092, loss_sup: 0.098804, loss_mps: 0.123233, loss_cps: 0.238055
[13:23:05.762] iteration 14721: total_loss: 0.423772, loss_sup: 0.050783, loss_mps: 0.129734, loss_cps: 0.243254
[13:23:05.907] iteration 14722: total_loss: 0.270301, loss_sup: 0.031492, loss_mps: 0.086874, loss_cps: 0.151935
[13:23:06.052] iteration 14723: total_loss: 0.369086, loss_sup: 0.048648, loss_mps: 0.109909, loss_cps: 0.210530
[13:23:06.200] iteration 14724: total_loss: 0.284437, loss_sup: 0.056569, loss_mps: 0.081302, loss_cps: 0.146566
[13:23:06.346] iteration 14725: total_loss: 0.481991, loss_sup: 0.030271, loss_mps: 0.152236, loss_cps: 0.299485
[13:23:06.491] iteration 14726: total_loss: 0.466108, loss_sup: 0.139970, loss_mps: 0.107906, loss_cps: 0.218232
[13:23:06.637] iteration 14727: total_loss: 0.382860, loss_sup: 0.116256, loss_mps: 0.093163, loss_cps: 0.173440
[13:23:06.782] iteration 14728: total_loss: 0.484717, loss_sup: 0.093954, loss_mps: 0.136531, loss_cps: 0.254232
[13:23:06.927] iteration 14729: total_loss: 0.279555, loss_sup: 0.023535, loss_mps: 0.091144, loss_cps: 0.164877
[13:23:07.073] iteration 14730: total_loss: 0.463223, loss_sup: 0.208826, loss_mps: 0.089346, loss_cps: 0.165051
[13:23:07.219] iteration 14731: total_loss: 0.499022, loss_sup: 0.154559, loss_mps: 0.113036, loss_cps: 0.231427
[13:23:07.364] iteration 14732: total_loss: 0.933259, loss_sup: 0.292440, loss_mps: 0.198617, loss_cps: 0.442203
[13:23:07.510] iteration 14733: total_loss: 0.561620, loss_sup: 0.147078, loss_mps: 0.137533, loss_cps: 0.277009
[13:23:07.656] iteration 14734: total_loss: 0.406612, loss_sup: 0.098899, loss_mps: 0.105614, loss_cps: 0.202099
[13:23:07.802] iteration 14735: total_loss: 0.652859, loss_sup: 0.094652, loss_mps: 0.178448, loss_cps: 0.379759
[13:23:07.950] iteration 14736: total_loss: 0.604290, loss_sup: 0.252026, loss_mps: 0.118370, loss_cps: 0.233893
[13:23:08.096] iteration 14737: total_loss: 0.445922, loss_sup: 0.064061, loss_mps: 0.127663, loss_cps: 0.254199
[13:23:08.248] iteration 14738: total_loss: 0.351300, loss_sup: 0.031524, loss_mps: 0.108596, loss_cps: 0.211180
[13:23:08.395] iteration 14739: total_loss: 0.397596, loss_sup: 0.107336, loss_mps: 0.101498, loss_cps: 0.188762
[13:23:08.541] iteration 14740: total_loss: 0.553680, loss_sup: 0.132661, loss_mps: 0.142644, loss_cps: 0.278376
[13:23:08.687] iteration 14741: total_loss: 0.486110, loss_sup: 0.095354, loss_mps: 0.132445, loss_cps: 0.258310
[13:23:08.833] iteration 14742: total_loss: 0.358613, loss_sup: 0.027618, loss_mps: 0.114371, loss_cps: 0.216624
[13:23:08.980] iteration 14743: total_loss: 0.335370, loss_sup: 0.043479, loss_mps: 0.103231, loss_cps: 0.188660
[13:23:09.125] iteration 14744: total_loss: 0.601563, loss_sup: 0.109796, loss_mps: 0.166826, loss_cps: 0.324941
[13:23:09.270] iteration 14745: total_loss: 0.561675, loss_sup: 0.090046, loss_mps: 0.164238, loss_cps: 0.307391
[13:23:09.416] iteration 14746: total_loss: 0.672314, loss_sup: 0.272760, loss_mps: 0.134961, loss_cps: 0.264592
[13:23:09.561] iteration 14747: total_loss: 0.446535, loss_sup: 0.099809, loss_mps: 0.120410, loss_cps: 0.226316
[13:23:09.707] iteration 14748: total_loss: 0.376454, loss_sup: 0.067692, loss_mps: 0.113456, loss_cps: 0.195306
[13:23:09.852] iteration 14749: total_loss: 0.491867, loss_sup: 0.101665, loss_mps: 0.130184, loss_cps: 0.260017
[13:23:09.998] iteration 14750: total_loss: 0.548131, loss_sup: 0.052414, loss_mps: 0.169852, loss_cps: 0.325865
[13:23:10.145] iteration 14751: total_loss: 0.428756, loss_sup: 0.036816, loss_mps: 0.134348, loss_cps: 0.257593
[13:23:10.291] iteration 14752: total_loss: 0.311391, loss_sup: 0.036336, loss_mps: 0.103312, loss_cps: 0.171743
[13:23:10.439] iteration 14753: total_loss: 0.403697, loss_sup: 0.032511, loss_mps: 0.127070, loss_cps: 0.244115
[13:23:10.588] iteration 14754: total_loss: 0.327252, loss_sup: 0.038999, loss_mps: 0.103932, loss_cps: 0.184320
[13:23:10.734] iteration 14755: total_loss: 0.274529, loss_sup: 0.006206, loss_mps: 0.096208, loss_cps: 0.172114
[13:23:10.880] iteration 14756: total_loss: 0.674504, loss_sup: 0.039533, loss_mps: 0.204829, loss_cps: 0.430142
[13:23:11.027] iteration 14757: total_loss: 0.456484, loss_sup: 0.051134, loss_mps: 0.140388, loss_cps: 0.264962
[13:23:11.173] iteration 14758: total_loss: 0.212627, loss_sup: 0.015552, loss_mps: 0.071480, loss_cps: 0.125596
[13:23:11.320] iteration 14759: total_loss: 0.761742, loss_sup: 0.132803, loss_mps: 0.207670, loss_cps: 0.421268
[13:23:11.467] iteration 14760: total_loss: 0.632368, loss_sup: 0.046907, loss_mps: 0.182954, loss_cps: 0.402508
[13:23:11.613] iteration 14761: total_loss: 0.554171, loss_sup: 0.064588, loss_mps: 0.160613, loss_cps: 0.328970
[13:23:11.758] iteration 14762: total_loss: 0.318109, loss_sup: 0.102593, loss_mps: 0.076747, loss_cps: 0.138769
[13:23:11.904] iteration 14763: total_loss: 0.358067, loss_sup: 0.096855, loss_mps: 0.093762, loss_cps: 0.167450
[13:23:12.050] iteration 14764: total_loss: 0.647782, loss_sup: 0.363452, loss_mps: 0.099743, loss_cps: 0.184588
[13:23:12.196] iteration 14765: total_loss: 0.281270, loss_sup: 0.074335, loss_mps: 0.072639, loss_cps: 0.134296
[13:23:12.341] iteration 14766: total_loss: 0.384981, loss_sup: 0.124851, loss_mps: 0.092230, loss_cps: 0.167900
[13:23:12.488] iteration 14767: total_loss: 0.590312, loss_sup: 0.079299, loss_mps: 0.161157, loss_cps: 0.349856
[13:23:12.635] iteration 14768: total_loss: 0.415550, loss_sup: 0.053420, loss_mps: 0.116538, loss_cps: 0.245592
[13:23:12.780] iteration 14769: total_loss: 0.347769, loss_sup: 0.089747, loss_mps: 0.087245, loss_cps: 0.170776
[13:23:12.928] iteration 14770: total_loss: 0.453625, loss_sup: 0.179874, loss_mps: 0.096503, loss_cps: 0.177248
[13:23:13.074] iteration 14771: total_loss: 0.495980, loss_sup: 0.137028, loss_mps: 0.121705, loss_cps: 0.237247
[13:23:13.220] iteration 14772: total_loss: 0.518431, loss_sup: 0.090585, loss_mps: 0.140265, loss_cps: 0.287582
[13:23:13.366] iteration 14773: total_loss: 0.456352, loss_sup: 0.031996, loss_mps: 0.136349, loss_cps: 0.288007
[13:23:13.511] iteration 14774: total_loss: 0.530731, loss_sup: 0.036554, loss_mps: 0.159954, loss_cps: 0.334224
[13:23:13.658] iteration 14775: total_loss: 0.511598, loss_sup: 0.037419, loss_mps: 0.153174, loss_cps: 0.321004
[13:23:13.804] iteration 14776: total_loss: 0.279585, loss_sup: 0.021544, loss_mps: 0.089094, loss_cps: 0.168947
[13:23:13.951] iteration 14777: total_loss: 0.307922, loss_sup: 0.079811, loss_mps: 0.086554, loss_cps: 0.141556
[13:23:14.096] iteration 14778: total_loss: 0.688442, loss_sup: 0.421276, loss_mps: 0.095360, loss_cps: 0.171806
[13:23:14.242] iteration 14779: total_loss: 0.177845, loss_sup: 0.013812, loss_mps: 0.063310, loss_cps: 0.100723
[13:23:14.387] iteration 14780: total_loss: 0.265292, loss_sup: 0.040349, loss_mps: 0.084383, loss_cps: 0.140560
[13:23:14.533] iteration 14781: total_loss: 0.240787, loss_sup: 0.033373, loss_mps: 0.074006, loss_cps: 0.133408
[13:23:14.679] iteration 14782: total_loss: 0.573498, loss_sup: 0.058630, loss_mps: 0.169305, loss_cps: 0.345563
[13:23:14.830] iteration 14783: total_loss: 0.199719, loss_sup: 0.012193, loss_mps: 0.072423, loss_cps: 0.115104
[13:23:14.977] iteration 14784: total_loss: 0.679199, loss_sup: 0.061503, loss_mps: 0.196076, loss_cps: 0.421620
[13:23:15.122] iteration 14785: total_loss: 0.292215, loss_sup: 0.017581, loss_mps: 0.092774, loss_cps: 0.181860
[13:23:15.268] iteration 14786: total_loss: 0.541360, loss_sup: 0.148374, loss_mps: 0.136142, loss_cps: 0.256844
[13:23:15.414] iteration 14787: total_loss: 0.349923, loss_sup: 0.022623, loss_mps: 0.117074, loss_cps: 0.210227
[13:23:15.560] iteration 14788: total_loss: 0.451204, loss_sup: 0.153619, loss_mps: 0.104875, loss_cps: 0.192710
[13:23:15.706] iteration 14789: total_loss: 0.628600, loss_sup: 0.175779, loss_mps: 0.154721, loss_cps: 0.298100
[13:23:15.853] iteration 14790: total_loss: 0.440938, loss_sup: 0.148501, loss_mps: 0.109926, loss_cps: 0.182511
[13:23:15.999] iteration 14791: total_loss: 0.686201, loss_sup: 0.054144, loss_mps: 0.203116, loss_cps: 0.428940
[13:23:16.144] iteration 14792: total_loss: 0.275101, loss_sup: 0.014107, loss_mps: 0.098452, loss_cps: 0.162542
[13:23:16.290] iteration 14793: total_loss: 0.466070, loss_sup: 0.074341, loss_mps: 0.130027, loss_cps: 0.261701
[13:23:16.436] iteration 14794: total_loss: 0.377699, loss_sup: 0.046525, loss_mps: 0.113479, loss_cps: 0.217694
[13:23:16.583] iteration 14795: total_loss: 0.303166, loss_sup: 0.038962, loss_mps: 0.093108, loss_cps: 0.171096
[13:23:16.729] iteration 14796: total_loss: 0.715402, loss_sup: 0.102774, loss_mps: 0.202780, loss_cps: 0.409848
[13:23:16.875] iteration 14797: total_loss: 0.440074, loss_sup: 0.087819, loss_mps: 0.123647, loss_cps: 0.228608
[13:23:17.024] iteration 14798: total_loss: 0.357195, loss_sup: 0.096282, loss_mps: 0.098814, loss_cps: 0.162098
[13:23:17.170] iteration 14799: total_loss: 0.426131, loss_sup: 0.156704, loss_mps: 0.096699, loss_cps: 0.172729
[13:23:17.317] iteration 14800: total_loss: 0.443745, loss_sup: 0.069973, loss_mps: 0.125339, loss_cps: 0.248433
[13:23:17.317] Evaluation Started ==>
[13:23:28.726] ==> valid iteration 14800: unet metrics: {'dc': 0.676083221523014, 'jc': 0.5541744236219979, 'pre': 0.733059115026092, 'hd': 5.904768951490941}, ynet metrics: {'dc': 0.5435236762263032, 'jc': 0.4297513621374911, 'pre': 0.7795460170182178, 'hd': 5.82977249363829}.
[13:23:28.728] Evaluation Finished!⏹️
[13:23:28.883] iteration 14801: total_loss: 0.356625, loss_sup: 0.032204, loss_mps: 0.112010, loss_cps: 0.212411
[13:23:29.030] iteration 14802: total_loss: 0.356196, loss_sup: 0.042874, loss_mps: 0.103485, loss_cps: 0.209836
[13:23:29.176] iteration 14803: total_loss: 0.575993, loss_sup: 0.062181, loss_mps: 0.167315, loss_cps: 0.346497
[13:23:29.322] iteration 14804: total_loss: 0.249095, loss_sup: 0.047978, loss_mps: 0.071757, loss_cps: 0.129360
[13:23:29.468] iteration 14805: total_loss: 0.314001, loss_sup: 0.016908, loss_mps: 0.098506, loss_cps: 0.198587
[13:23:29.614] iteration 14806: total_loss: 0.259711, loss_sup: 0.068156, loss_mps: 0.067569, loss_cps: 0.123987
[13:23:29.760] iteration 14807: total_loss: 0.497820, loss_sup: 0.125957, loss_mps: 0.128983, loss_cps: 0.242881
[13:23:29.907] iteration 14808: total_loss: 0.524064, loss_sup: 0.139871, loss_mps: 0.125974, loss_cps: 0.258218
[13:23:30.055] iteration 14809: total_loss: 0.579155, loss_sup: 0.164073, loss_mps: 0.138383, loss_cps: 0.276698
[13:23:30.200] iteration 14810: total_loss: 0.778835, loss_sup: 0.244075, loss_mps: 0.175080, loss_cps: 0.359681
[13:23:30.346] iteration 14811: total_loss: 0.401870, loss_sup: 0.070553, loss_mps: 0.113997, loss_cps: 0.217319
[13:23:30.492] iteration 14812: total_loss: 0.291381, loss_sup: 0.067464, loss_mps: 0.076151, loss_cps: 0.147766
[13:23:30.638] iteration 14813: total_loss: 0.294554, loss_sup: 0.011001, loss_mps: 0.094466, loss_cps: 0.189087
[13:23:30.784] iteration 14814: total_loss: 0.376172, loss_sup: 0.039981, loss_mps: 0.109142, loss_cps: 0.227049
[13:23:30.931] iteration 14815: total_loss: 0.447922, loss_sup: 0.037181, loss_mps: 0.129639, loss_cps: 0.281101
[13:23:31.077] iteration 14816: total_loss: 0.667172, loss_sup: 0.056746, loss_mps: 0.178128, loss_cps: 0.432297
[13:23:31.222] iteration 14817: total_loss: 0.510701, loss_sup: 0.140230, loss_mps: 0.122296, loss_cps: 0.248175
[13:23:31.372] iteration 14818: total_loss: 0.836874, loss_sup: 0.073255, loss_mps: 0.228763, loss_cps: 0.534856
[13:23:31.519] iteration 14819: total_loss: 0.454657, loss_sup: 0.098029, loss_mps: 0.118294, loss_cps: 0.238334
[13:23:31.667] iteration 14820: total_loss: 0.426909, loss_sup: 0.050005, loss_mps: 0.127083, loss_cps: 0.249821
[13:23:31.813] iteration 14821: total_loss: 0.413203, loss_sup: 0.119060, loss_mps: 0.102698, loss_cps: 0.191445
[13:23:31.959] iteration 14822: total_loss: 0.295586, loss_sup: 0.029908, loss_mps: 0.093102, loss_cps: 0.172575
[13:23:32.105] iteration 14823: total_loss: 0.520776, loss_sup: 0.082983, loss_mps: 0.145581, loss_cps: 0.292211
[13:23:32.251] iteration 14824: total_loss: 0.766660, loss_sup: 0.165439, loss_mps: 0.196301, loss_cps: 0.404921
[13:23:32.397] iteration 14825: total_loss: 0.554437, loss_sup: 0.126427, loss_mps: 0.140607, loss_cps: 0.287402
[13:23:32.543] iteration 14826: total_loss: 0.532522, loss_sup: 0.062666, loss_mps: 0.160542, loss_cps: 0.309314
[13:23:32.688] iteration 14827: total_loss: 0.471277, loss_sup: 0.101417, loss_mps: 0.127221, loss_cps: 0.242639
[13:23:32.834] iteration 14828: total_loss: 0.485716, loss_sup: 0.088789, loss_mps: 0.134625, loss_cps: 0.262303
[13:23:32.979] iteration 14829: total_loss: 0.388729, loss_sup: 0.038473, loss_mps: 0.117944, loss_cps: 0.232312
[13:23:33.126] iteration 14830: total_loss: 0.347534, loss_sup: 0.098176, loss_mps: 0.089036, loss_cps: 0.160322
[13:23:33.273] iteration 14831: total_loss: 0.407079, loss_sup: 0.113508, loss_mps: 0.102253, loss_cps: 0.191318
[13:23:33.418] iteration 14832: total_loss: 0.394900, loss_sup: 0.064554, loss_mps: 0.120673, loss_cps: 0.209672
[13:23:33.564] iteration 14833: total_loss: 0.335903, loss_sup: 0.119247, loss_mps: 0.085816, loss_cps: 0.130840
[13:23:33.710] iteration 14834: total_loss: 0.374554, loss_sup: 0.070910, loss_mps: 0.100492, loss_cps: 0.203151
[13:23:33.855] iteration 14835: total_loss: 0.814925, loss_sup: 0.135707, loss_mps: 0.214045, loss_cps: 0.465173
[13:23:34.001] iteration 14836: total_loss: 0.537598, loss_sup: 0.103083, loss_mps: 0.146844, loss_cps: 0.287671
[13:23:34.146] iteration 14837: total_loss: 0.364069, loss_sup: 0.025718, loss_mps: 0.118780, loss_cps: 0.219571
[13:23:34.292] iteration 14838: total_loss: 0.484918, loss_sup: 0.036639, loss_mps: 0.149957, loss_cps: 0.298322
[13:23:34.438] iteration 14839: total_loss: 0.927163, loss_sup: 0.271595, loss_mps: 0.200376, loss_cps: 0.455193
[13:23:34.584] iteration 14840: total_loss: 0.381002, loss_sup: 0.058570, loss_mps: 0.111190, loss_cps: 0.211242
[13:23:34.729] iteration 14841: total_loss: 0.385170, loss_sup: 0.025412, loss_mps: 0.120875, loss_cps: 0.238884
[13:23:34.875] iteration 14842: total_loss: 0.563647, loss_sup: 0.100244, loss_mps: 0.150552, loss_cps: 0.312851
[13:23:35.020] iteration 14843: total_loss: 0.626111, loss_sup: 0.426464, loss_mps: 0.076145, loss_cps: 0.123502
[13:23:35.165] iteration 14844: total_loss: 0.658941, loss_sup: 0.050252, loss_mps: 0.191804, loss_cps: 0.416885
[13:23:35.312] iteration 14845: total_loss: 0.606386, loss_sup: 0.133793, loss_mps: 0.159097, loss_cps: 0.313497
[13:23:35.457] iteration 14846: total_loss: 0.419589, loss_sup: 0.062374, loss_mps: 0.122954, loss_cps: 0.234262
[13:23:35.605] iteration 14847: total_loss: 0.347727, loss_sup: 0.124243, loss_mps: 0.085107, loss_cps: 0.138377
[13:23:35.751] iteration 14848: total_loss: 0.629888, loss_sup: 0.014971, loss_mps: 0.192837, loss_cps: 0.422080
[13:23:35.896] iteration 14849: total_loss: 0.499418, loss_sup: 0.116856, loss_mps: 0.128750, loss_cps: 0.253812
[13:23:36.042] iteration 14850: total_loss: 0.455117, loss_sup: 0.065742, loss_mps: 0.128365, loss_cps: 0.261010
[13:23:36.187] iteration 14851: total_loss: 0.344501, loss_sup: 0.048637, loss_mps: 0.101329, loss_cps: 0.194536
[13:23:36.334] iteration 14852: total_loss: 0.278999, loss_sup: 0.011258, loss_mps: 0.101852, loss_cps: 0.165889
[13:23:36.480] iteration 14853: total_loss: 0.535361, loss_sup: 0.167959, loss_mps: 0.123781, loss_cps: 0.243621
[13:23:36.626] iteration 14854: total_loss: 0.397599, loss_sup: 0.096848, loss_mps: 0.103353, loss_cps: 0.197398
[13:23:36.772] iteration 14855: total_loss: 0.240554, loss_sup: 0.006577, loss_mps: 0.083864, loss_cps: 0.150113
[13:23:36.918] iteration 14856: total_loss: 0.398610, loss_sup: 0.079629, loss_mps: 0.112470, loss_cps: 0.206511
[13:23:37.065] iteration 14857: total_loss: 0.580033, loss_sup: 0.063253, loss_mps: 0.171371, loss_cps: 0.345409
[13:23:37.210] iteration 14858: total_loss: 0.311491, loss_sup: 0.011004, loss_mps: 0.102041, loss_cps: 0.198446
[13:23:37.356] iteration 14859: total_loss: 0.526237, loss_sup: 0.112725, loss_mps: 0.140710, loss_cps: 0.272801
[13:23:37.502] iteration 14860: total_loss: 0.724222, loss_sup: 0.234234, loss_mps: 0.156453, loss_cps: 0.333536
[13:23:37.649] iteration 14861: total_loss: 0.314419, loss_sup: 0.014064, loss_mps: 0.109442, loss_cps: 0.190913
[13:23:37.797] iteration 14862: total_loss: 0.299946, loss_sup: 0.048706, loss_mps: 0.091281, loss_cps: 0.159958
[13:23:37.945] iteration 14863: total_loss: 0.445307, loss_sup: 0.111929, loss_mps: 0.110436, loss_cps: 0.222942
[13:23:38.091] iteration 14864: total_loss: 0.256923, loss_sup: 0.057068, loss_mps: 0.075581, loss_cps: 0.124274
[13:23:38.237] iteration 14865: total_loss: 0.372394, loss_sup: 0.105071, loss_mps: 0.095330, loss_cps: 0.171993
[13:23:38.383] iteration 14866: total_loss: 0.401287, loss_sup: 0.195716, loss_mps: 0.075864, loss_cps: 0.129708
[13:23:38.528] iteration 14867: total_loss: 0.194477, loss_sup: 0.006366, loss_mps: 0.067617, loss_cps: 0.120494
[13:23:38.674] iteration 14868: total_loss: 0.614036, loss_sup: 0.332664, loss_mps: 0.101677, loss_cps: 0.179696
[13:23:38.820] iteration 14869: total_loss: 0.351269, loss_sup: 0.153782, loss_mps: 0.072539, loss_cps: 0.124948
[13:23:38.966] iteration 14870: total_loss: 0.279242, loss_sup: 0.054416, loss_mps: 0.083242, loss_cps: 0.141584
[13:23:39.112] iteration 14871: total_loss: 0.308274, loss_sup: 0.021386, loss_mps: 0.102085, loss_cps: 0.184803
[13:23:39.258] iteration 14872: total_loss: 0.677086, loss_sup: 0.261337, loss_mps: 0.146272, loss_cps: 0.269477
[13:23:39.403] iteration 14873: total_loss: 0.416926, loss_sup: 0.098245, loss_mps: 0.112455, loss_cps: 0.206226
[13:23:39.550] iteration 14874: total_loss: 0.477666, loss_sup: 0.125118, loss_mps: 0.126113, loss_cps: 0.226436
[13:23:39.697] iteration 14875: total_loss: 0.313472, loss_sup: 0.015137, loss_mps: 0.107981, loss_cps: 0.190354
[13:23:39.842] iteration 14876: total_loss: 0.377557, loss_sup: 0.155791, loss_mps: 0.082582, loss_cps: 0.139184
[13:23:39.988] iteration 14877: total_loss: 0.132143, loss_sup: 0.006242, loss_mps: 0.048833, loss_cps: 0.077068
[13:23:40.135] iteration 14878: total_loss: 0.334365, loss_sup: 0.087030, loss_mps: 0.085625, loss_cps: 0.161710
[13:23:40.280] iteration 14879: total_loss: 0.611572, loss_sup: 0.094626, loss_mps: 0.170659, loss_cps: 0.346287
[13:23:40.426] iteration 14880: total_loss: 0.406290, loss_sup: 0.025592, loss_mps: 0.128037, loss_cps: 0.252661
[13:23:40.572] iteration 14881: total_loss: 0.378021, loss_sup: 0.032342, loss_mps: 0.118169, loss_cps: 0.227510
[13:23:40.719] iteration 14882: total_loss: 0.567404, loss_sup: 0.101525, loss_mps: 0.153491, loss_cps: 0.312388
[13:23:40.865] iteration 14883: total_loss: 0.653927, loss_sup: 0.317267, loss_mps: 0.117577, loss_cps: 0.219083
[13:23:41.011] iteration 14884: total_loss: 0.389132, loss_sup: 0.121613, loss_mps: 0.099070, loss_cps: 0.168449
[13:23:41.157] iteration 14885: total_loss: 0.600845, loss_sup: 0.170539, loss_mps: 0.146219, loss_cps: 0.284087
[13:23:41.303] iteration 14886: total_loss: 0.232135, loss_sup: 0.008181, loss_mps: 0.081258, loss_cps: 0.142696
[13:23:41.449] iteration 14887: total_loss: 0.244741, loss_sup: 0.019556, loss_mps: 0.079242, loss_cps: 0.145942
[13:23:41.596] iteration 14888: total_loss: 0.623796, loss_sup: 0.110951, loss_mps: 0.165914, loss_cps: 0.346932
[13:23:41.743] iteration 14889: total_loss: 0.449576, loss_sup: 0.015094, loss_mps: 0.142933, loss_cps: 0.291549
[13:23:41.890] iteration 14890: total_loss: 0.473800, loss_sup: 0.053500, loss_mps: 0.137460, loss_cps: 0.282841
[13:23:42.036] iteration 14891: total_loss: 0.392032, loss_sup: 0.016757, loss_mps: 0.128620, loss_cps: 0.246655
[13:23:42.182] iteration 14892: total_loss: 0.579423, loss_sup: 0.105192, loss_mps: 0.157580, loss_cps: 0.316651
[13:23:42.328] iteration 14893: total_loss: 0.315146, loss_sup: 0.019260, loss_mps: 0.103718, loss_cps: 0.192168
[13:23:42.475] iteration 14894: total_loss: 0.364003, loss_sup: 0.070060, loss_mps: 0.102765, loss_cps: 0.191178
[13:23:42.621] iteration 14895: total_loss: 0.492297, loss_sup: 0.016953, loss_mps: 0.159029, loss_cps: 0.316315
[13:23:42.773] iteration 14896: total_loss: 0.698027, loss_sup: 0.121707, loss_mps: 0.187413, loss_cps: 0.388907
[13:23:42.919] iteration 14897: total_loss: 0.347665, loss_sup: 0.009444, loss_mps: 0.117868, loss_cps: 0.220353
[13:23:43.065] iteration 14898: total_loss: 0.524194, loss_sup: 0.118848, loss_mps: 0.137460, loss_cps: 0.267886
[13:23:43.211] iteration 14899: total_loss: 0.683495, loss_sup: 0.182489, loss_mps: 0.167062, loss_cps: 0.333944
[13:23:43.357] iteration 14900: total_loss: 0.399717, loss_sup: 0.072944, loss_mps: 0.110427, loss_cps: 0.216346
[13:23:43.357] Evaluation Started ==>
[13:23:54.714] ==> valid iteration 14900: unet metrics: {'dc': 0.6653475746579525, 'jc': 0.5444024952081341, 'pre': 0.7537387176455262, 'hd': 5.803624848340234}, ynet metrics: {'dc': 0.614451753752853, 'jc': 0.502406222732522, 'pre': 0.7657179587696998, 'hd': 5.569254732503954}.
[13:23:54.716] Evaluation Finished!⏹️
[13:23:54.865] iteration 14901: total_loss: 0.238262, loss_sup: 0.011149, loss_mps: 0.080302, loss_cps: 0.146810
[13:23:55.012] iteration 14902: total_loss: 0.305090, loss_sup: 0.018779, loss_mps: 0.096479, loss_cps: 0.189832
[13:23:55.157] iteration 14903: total_loss: 0.741769, loss_sup: 0.076705, loss_mps: 0.205515, loss_cps: 0.459549
[13:23:55.302] iteration 14904: total_loss: 0.404963, loss_sup: 0.063869, loss_mps: 0.115805, loss_cps: 0.225289
[13:23:55.447] iteration 14905: total_loss: 0.708885, loss_sup: 0.125179, loss_mps: 0.187817, loss_cps: 0.395889
[13:23:55.592] iteration 14906: total_loss: 0.253546, loss_sup: 0.062493, loss_mps: 0.073326, loss_cps: 0.117728
[13:23:55.737] iteration 14907: total_loss: 0.424881, loss_sup: 0.059876, loss_mps: 0.123833, loss_cps: 0.241172
[13:23:55.882] iteration 14908: total_loss: 0.918176, loss_sup: 0.311040, loss_mps: 0.196086, loss_cps: 0.411049
[13:23:56.026] iteration 14909: total_loss: 0.296379, loss_sup: 0.034619, loss_mps: 0.090969, loss_cps: 0.170791
[13:23:56.171] iteration 14910: total_loss: 0.679025, loss_sup: 0.250631, loss_mps: 0.143384, loss_cps: 0.285010
[13:23:56.318] iteration 14911: total_loss: 0.492418, loss_sup: 0.124418, loss_mps: 0.127140, loss_cps: 0.240861
[13:23:56.465] iteration 14912: total_loss: 0.416107, loss_sup: 0.041761, loss_mps: 0.123768, loss_cps: 0.250577
[13:23:56.610] iteration 14913: total_loss: 0.534348, loss_sup: 0.043806, loss_mps: 0.149397, loss_cps: 0.341145
[13:23:56.755] iteration 14914: total_loss: 0.407747, loss_sup: 0.046846, loss_mps: 0.118606, loss_cps: 0.242296
[13:23:56.903] iteration 14915: total_loss: 0.336814, loss_sup: 0.042732, loss_mps: 0.102102, loss_cps: 0.191979
[13:23:57.055] iteration 14916: total_loss: 0.486673, loss_sup: 0.120063, loss_mps: 0.122469, loss_cps: 0.244142
[13:23:57.202] iteration 14917: total_loss: 0.320273, loss_sup: 0.009198, loss_mps: 0.109223, loss_cps: 0.201852
[13:23:57.348] iteration 14918: total_loss: 0.390047, loss_sup: 0.074386, loss_mps: 0.109661, loss_cps: 0.206001
[13:23:57.495] iteration 14919: total_loss: 0.624794, loss_sup: 0.147082, loss_mps: 0.147962, loss_cps: 0.329750
[13:23:57.641] iteration 14920: total_loss: 0.609514, loss_sup: 0.097371, loss_mps: 0.167486, loss_cps: 0.344657
[13:23:57.786] iteration 14921: total_loss: 0.316790, loss_sup: 0.008392, loss_mps: 0.112707, loss_cps: 0.195691
[13:23:57.931] iteration 14922: total_loss: 0.416998, loss_sup: 0.105218, loss_mps: 0.112620, loss_cps: 0.199160
[13:23:58.078] iteration 14923: total_loss: 0.340290, loss_sup: 0.029117, loss_mps: 0.114263, loss_cps: 0.196909
[13:23:58.224] iteration 14924: total_loss: 0.474910, loss_sup: 0.052523, loss_mps: 0.145366, loss_cps: 0.277021
[13:23:58.371] iteration 14925: total_loss: 0.483291, loss_sup: 0.045516, loss_mps: 0.139082, loss_cps: 0.298693
[13:23:58.517] iteration 14926: total_loss: 0.600577, loss_sup: 0.063678, loss_mps: 0.170841, loss_cps: 0.366058
[13:23:58.664] iteration 14927: total_loss: 0.326527, loss_sup: 0.029848, loss_mps: 0.103865, loss_cps: 0.192814
[13:23:58.809] iteration 14928: total_loss: 0.478445, loss_sup: 0.160822, loss_mps: 0.103055, loss_cps: 0.214569
[13:23:58.955] iteration 14929: total_loss: 0.395273, loss_sup: 0.065586, loss_mps: 0.113784, loss_cps: 0.215904
[13:23:59.100] iteration 14930: total_loss: 0.381208, loss_sup: 0.106041, loss_mps: 0.093341, loss_cps: 0.181825
[13:23:59.248] iteration 14931: total_loss: 0.636225, loss_sup: 0.252291, loss_mps: 0.130394, loss_cps: 0.253539
[13:23:59.394] iteration 14932: total_loss: 0.378586, loss_sup: 0.041524, loss_mps: 0.116105, loss_cps: 0.220957
[13:23:59.540] iteration 14933: total_loss: 0.371485, loss_sup: 0.106175, loss_mps: 0.091472, loss_cps: 0.173838
[13:23:59.686] iteration 14934: total_loss: 0.506528, loss_sup: 0.130506, loss_mps: 0.122221, loss_cps: 0.253801
[13:23:59.831] iteration 14935: total_loss: 0.188484, loss_sup: 0.027889, loss_mps: 0.058510, loss_cps: 0.102085
[13:23:59.977] iteration 14936: total_loss: 0.362796, loss_sup: 0.012846, loss_mps: 0.113360, loss_cps: 0.236590
[13:24:00.123] iteration 14937: total_loss: 0.300581, loss_sup: 0.010654, loss_mps: 0.095004, loss_cps: 0.194924
[13:24:00.269] iteration 14938: total_loss: 0.438643, loss_sup: 0.182655, loss_mps: 0.093822, loss_cps: 0.162166
[13:24:00.417] iteration 14939: total_loss: 0.186507, loss_sup: 0.007109, loss_mps: 0.066357, loss_cps: 0.113041
[13:24:00.563] iteration 14940: total_loss: 0.450438, loss_sup: 0.189637, loss_mps: 0.091846, loss_cps: 0.168955
[13:24:00.709] iteration 14941: total_loss: 0.534669, loss_sup: 0.280149, loss_mps: 0.089451, loss_cps: 0.165069
[13:24:00.856] iteration 14942: total_loss: 0.310238, loss_sup: 0.082689, loss_mps: 0.080847, loss_cps: 0.146702
[13:24:01.001] iteration 14943: total_loss: 0.577923, loss_sup: 0.229953, loss_mps: 0.117769, loss_cps: 0.230201
[13:24:01.147] iteration 14944: total_loss: 0.540653, loss_sup: 0.141493, loss_mps: 0.131885, loss_cps: 0.267275
[13:24:01.292] iteration 14945: total_loss: 0.456136, loss_sup: 0.032946, loss_mps: 0.137285, loss_cps: 0.285905
[13:24:01.437] iteration 14946: total_loss: 0.241091, loss_sup: 0.013731, loss_mps: 0.082553, loss_cps: 0.144807
[13:24:01.583] iteration 14947: total_loss: 0.541808, loss_sup: 0.038802, loss_mps: 0.159104, loss_cps: 0.343902
[13:24:01.729] iteration 14948: total_loss: 0.317955, loss_sup: 0.089727, loss_mps: 0.089411, loss_cps: 0.138817
[13:24:01.874] iteration 14949: total_loss: 0.396025, loss_sup: 0.116314, loss_mps: 0.105950, loss_cps: 0.173761
[13:24:02.023] iteration 14950: total_loss: 0.248728, loss_sup: 0.021232, loss_mps: 0.088083, loss_cps: 0.139412
[13:24:02.168] iteration 14951: total_loss: 0.300332, loss_sup: 0.012547, loss_mps: 0.106558, loss_cps: 0.181226
[13:24:02.314] iteration 14952: total_loss: 0.401966, loss_sup: 0.068262, loss_mps: 0.118120, loss_cps: 0.215583
[13:24:02.460] iteration 14953: total_loss: 0.317677, loss_sup: 0.026525, loss_mps: 0.100786, loss_cps: 0.190366
[13:24:02.606] iteration 14954: total_loss: 0.529503, loss_sup: 0.091468, loss_mps: 0.145459, loss_cps: 0.292576
[13:24:02.752] iteration 14955: total_loss: 0.231848, loss_sup: 0.019909, loss_mps: 0.075186, loss_cps: 0.136753
[13:24:02.898] iteration 14956: total_loss: 0.726425, loss_sup: 0.162609, loss_mps: 0.191346, loss_cps: 0.372471
[13:24:03.043] iteration 14957: total_loss: 0.249195, loss_sup: 0.034026, loss_mps: 0.081843, loss_cps: 0.133326
[13:24:03.191] iteration 14958: total_loss: 0.511030, loss_sup: 0.048564, loss_mps: 0.156715, loss_cps: 0.305751
[13:24:03.337] iteration 14959: total_loss: 0.424453, loss_sup: 0.073598, loss_mps: 0.125597, loss_cps: 0.225258
[13:24:03.482] iteration 14960: total_loss: 0.540966, loss_sup: 0.082752, loss_mps: 0.158559, loss_cps: 0.299655
[13:24:03.627] iteration 14961: total_loss: 0.314209, loss_sup: 0.064696, loss_mps: 0.089444, loss_cps: 0.160069
[13:24:03.773] iteration 14962: total_loss: 0.378148, loss_sup: 0.119134, loss_mps: 0.095190, loss_cps: 0.163825
[13:24:03.918] iteration 14963: total_loss: 0.547162, loss_sup: 0.165309, loss_mps: 0.125523, loss_cps: 0.256331
[13:24:04.064] iteration 14964: total_loss: 0.887834, loss_sup: 0.338464, loss_mps: 0.173947, loss_cps: 0.375423
[13:24:04.211] iteration 14965: total_loss: 0.434951, loss_sup: 0.057631, loss_mps: 0.127286, loss_cps: 0.250033
[13:24:04.356] iteration 14966: total_loss: 0.742239, loss_sup: 0.086036, loss_mps: 0.204063, loss_cps: 0.452140
[13:24:04.502] iteration 14967: total_loss: 0.276157, loss_sup: 0.086741, loss_mps: 0.070314, loss_cps: 0.119103
[13:24:04.648] iteration 14968: total_loss: 0.301454, loss_sup: 0.068406, loss_mps: 0.084280, loss_cps: 0.148767
[13:24:04.794] iteration 14969: total_loss: 0.162079, loss_sup: 0.005437, loss_mps: 0.061293, loss_cps: 0.095349
[13:24:04.940] iteration 14970: total_loss: 0.468077, loss_sup: 0.090959, loss_mps: 0.118677, loss_cps: 0.258441
[13:24:05.085] iteration 14971: total_loss: 0.293948, loss_sup: 0.025799, loss_mps: 0.090765, loss_cps: 0.177385
[13:24:05.231] iteration 14972: total_loss: 0.456462, loss_sup: 0.128982, loss_mps: 0.104811, loss_cps: 0.222669
[13:24:05.377] iteration 14973: total_loss: 0.499711, loss_sup: 0.091584, loss_mps: 0.130977, loss_cps: 0.277151
[13:24:05.523] iteration 14974: total_loss: 0.351256, loss_sup: 0.061941, loss_mps: 0.099278, loss_cps: 0.190037
[13:24:05.668] iteration 14975: total_loss: 0.398626, loss_sup: 0.044839, loss_mps: 0.115513, loss_cps: 0.238274
[13:24:05.814] iteration 14976: total_loss: 0.536384, loss_sup: 0.150054, loss_mps: 0.136820, loss_cps: 0.249510
[13:24:05.959] iteration 14977: total_loss: 0.435192, loss_sup: 0.023756, loss_mps: 0.134498, loss_cps: 0.276938
[13:24:06.105] iteration 14978: total_loss: 0.311784, loss_sup: 0.068692, loss_mps: 0.088009, loss_cps: 0.155083
[13:24:06.251] iteration 14979: total_loss: 0.363679, loss_sup: 0.063903, loss_mps: 0.103280, loss_cps: 0.196496
[13:24:06.397] iteration 14980: total_loss: 0.324959, loss_sup: 0.030655, loss_mps: 0.101879, loss_cps: 0.192425
[13:24:06.543] iteration 14981: total_loss: 0.242750, loss_sup: 0.052732, loss_mps: 0.068603, loss_cps: 0.121415
[13:24:06.689] iteration 14982: total_loss: 0.309731, loss_sup: 0.022995, loss_mps: 0.096442, loss_cps: 0.190294
[13:24:06.837] iteration 14983: total_loss: 0.528011, loss_sup: 0.010903, loss_mps: 0.161623, loss_cps: 0.355485
[13:24:06.983] iteration 14984: total_loss: 0.688497, loss_sup: 0.088561, loss_mps: 0.193251, loss_cps: 0.406685
[13:24:07.129] iteration 14985: total_loss: 0.395465, loss_sup: 0.048482, loss_mps: 0.116133, loss_cps: 0.230851
[13:24:07.275] iteration 14986: total_loss: 0.371788, loss_sup: 0.034655, loss_mps: 0.111566, loss_cps: 0.225567
[13:24:07.422] iteration 14987: total_loss: 0.466567, loss_sup: 0.043287, loss_mps: 0.142245, loss_cps: 0.281035
[13:24:07.568] iteration 14988: total_loss: 0.485761, loss_sup: 0.047402, loss_mps: 0.145967, loss_cps: 0.292392
[13:24:07.716] iteration 14989: total_loss: 0.558328, loss_sup: 0.026116, loss_mps: 0.168721, loss_cps: 0.363491
[13:24:07.863] iteration 14990: total_loss: 0.674790, loss_sup: 0.045478, loss_mps: 0.194009, loss_cps: 0.435303
[13:24:08.012] iteration 14991: total_loss: 0.533010, loss_sup: 0.151084, loss_mps: 0.126030, loss_cps: 0.255896
[13:24:08.158] iteration 14992: total_loss: 0.236261, loss_sup: 0.021427, loss_mps: 0.079425, loss_cps: 0.135408
[13:24:08.304] iteration 14993: total_loss: 0.440708, loss_sup: 0.049055, loss_mps: 0.136269, loss_cps: 0.255384
[13:24:08.452] iteration 14994: total_loss: 0.449605, loss_sup: 0.129430, loss_mps: 0.111101, loss_cps: 0.209075
[13:24:08.599] iteration 14995: total_loss: 0.309288, loss_sup: 0.074028, loss_mps: 0.090141, loss_cps: 0.145119
[13:24:08.745] iteration 14996: total_loss: 0.391959, loss_sup: 0.044613, loss_mps: 0.120561, loss_cps: 0.226785
[13:24:08.891] iteration 14997: total_loss: 0.533653, loss_sup: 0.106695, loss_mps: 0.148603, loss_cps: 0.278355
[13:24:09.040] iteration 14998: total_loss: 0.235488, loss_sup: 0.036548, loss_mps: 0.071509, loss_cps: 0.127431
[13:24:09.187] iteration 14999: total_loss: 0.406791, loss_sup: 0.184318, loss_mps: 0.080486, loss_cps: 0.141988
[13:24:09.334] iteration 15000: total_loss: 0.319013, loss_sup: 0.069225, loss_mps: 0.084918, loss_cps: 0.164870
[13:24:09.334] Evaluation Started ==>
[13:24:20.720] ==> valid iteration 15000: unet metrics: {'dc': 0.625937478858798, 'jc': 0.5045960778131978, 'pre': 0.7904122324516235, 'hd': 5.668914103953532}, ynet metrics: {'dc': 0.5827336665555183, 'jc': 0.46453317348482315, 'pre': 0.7943614331563437, 'hd': 5.724537917387536}.
[13:24:20.722] Evaluation Finished!⏹️
[13:24:20.875] iteration 15001: total_loss: 0.445955, loss_sup: 0.120416, loss_mps: 0.110500, loss_cps: 0.215039
[13:24:21.024] iteration 15002: total_loss: 0.743037, loss_sup: 0.160562, loss_mps: 0.189645, loss_cps: 0.392831
[13:24:21.170] iteration 15003: total_loss: 0.373635, loss_sup: 0.041465, loss_mps: 0.110104, loss_cps: 0.222066
[13:24:21.315] iteration 15004: total_loss: 0.472320, loss_sup: 0.116563, loss_mps: 0.129398, loss_cps: 0.226359
[13:24:21.460] iteration 15005: total_loss: 0.289815, loss_sup: 0.038438, loss_mps: 0.093406, loss_cps: 0.157971
[13:24:21.606] iteration 15006: total_loss: 0.527907, loss_sup: 0.145680, loss_mps: 0.125253, loss_cps: 0.256974
[13:24:21.751] iteration 15007: total_loss: 0.518396, loss_sup: 0.115213, loss_mps: 0.134169, loss_cps: 0.269014
[13:24:21.896] iteration 15008: total_loss: 1.082210, loss_sup: 0.164951, loss_mps: 0.280984, loss_cps: 0.636275
[13:24:22.041] iteration 15009: total_loss: 0.416134, loss_sup: 0.142179, loss_mps: 0.100429, loss_cps: 0.173527
[13:24:22.187] iteration 15010: total_loss: 0.324237, loss_sup: 0.085560, loss_mps: 0.081814, loss_cps: 0.156862
[13:24:22.332] iteration 15011: total_loss: 0.893179, loss_sup: 0.406549, loss_mps: 0.157789, loss_cps: 0.328841
[13:24:22.477] iteration 15012: total_loss: 0.361168, loss_sup: 0.038992, loss_mps: 0.111774, loss_cps: 0.210402
[13:24:22.622] iteration 15013: total_loss: 0.321402, loss_sup: 0.021415, loss_mps: 0.108954, loss_cps: 0.191033
[13:24:22.767] iteration 15014: total_loss: 0.576032, loss_sup: 0.101461, loss_mps: 0.158419, loss_cps: 0.316152
[13:24:22.915] iteration 15015: total_loss: 0.317883, loss_sup: 0.033479, loss_mps: 0.100382, loss_cps: 0.184023
[13:24:23.060] iteration 15016: total_loss: 0.901851, loss_sup: 0.325566, loss_mps: 0.185571, loss_cps: 0.390714
[13:24:23.205] iteration 15017: total_loss: 0.738130, loss_sup: 0.127784, loss_mps: 0.196723, loss_cps: 0.413623
[13:24:23.351] iteration 15018: total_loss: 0.844079, loss_sup: 0.290154, loss_mps: 0.176897, loss_cps: 0.377028
[13:24:23.498] iteration 15019: total_loss: 0.428150, loss_sup: 0.084292, loss_mps: 0.119164, loss_cps: 0.224694
[13:24:23.650] iteration 15020: total_loss: 0.320016, loss_sup: 0.074455, loss_mps: 0.085052, loss_cps: 0.160510
[13:24:23.796] iteration 15021: total_loss: 0.468548, loss_sup: 0.182318, loss_mps: 0.109995, loss_cps: 0.176235
[13:24:23.944] iteration 15022: total_loss: 0.877605, loss_sup: 0.102449, loss_mps: 0.245596, loss_cps: 0.529560
[13:24:24.090] iteration 15023: total_loss: 0.305572, loss_sup: 0.017765, loss_mps: 0.106887, loss_cps: 0.180921
[13:24:24.236] iteration 15024: total_loss: 0.381002, loss_sup: 0.033047, loss_mps: 0.120347, loss_cps: 0.227607
[13:24:24.381] iteration 15025: total_loss: 0.419632, loss_sup: 0.072859, loss_mps: 0.119348, loss_cps: 0.227425
[13:24:24.527] iteration 15026: total_loss: 0.264813, loss_sup: 0.027891, loss_mps: 0.094321, loss_cps: 0.142602
[13:24:24.673] iteration 15027: total_loss: 0.487180, loss_sup: 0.122292, loss_mps: 0.129282, loss_cps: 0.235606
[13:24:24.820] iteration 15028: total_loss: 0.806762, loss_sup: 0.146234, loss_mps: 0.206293, loss_cps: 0.454235
[13:24:24.966] iteration 15029: total_loss: 0.478610, loss_sup: 0.035423, loss_mps: 0.144809, loss_cps: 0.298377
[13:24:25.114] iteration 15030: total_loss: 0.312649, loss_sup: 0.026619, loss_mps: 0.102978, loss_cps: 0.183052
[13:24:25.261] iteration 15031: total_loss: 0.484973, loss_sup: 0.045562, loss_mps: 0.153577, loss_cps: 0.285834
[13:24:25.406] iteration 15032: total_loss: 0.542537, loss_sup: 0.104754, loss_mps: 0.153986, loss_cps: 0.283797
[13:24:25.552] iteration 15033: total_loss: 0.532300, loss_sup: 0.108083, loss_mps: 0.141175, loss_cps: 0.283042
[13:24:25.697] iteration 15034: total_loss: 0.502393, loss_sup: 0.015109, loss_mps: 0.156309, loss_cps: 0.330975
[13:24:25.842] iteration 15035: total_loss: 0.498805, loss_sup: 0.067929, loss_mps: 0.141627, loss_cps: 0.289248
[13:24:25.987] iteration 15036: total_loss: 0.396699, loss_sup: 0.068154, loss_mps: 0.108042, loss_cps: 0.220503
[13:24:26.133] iteration 15037: total_loss: 0.541445, loss_sup: 0.036703, loss_mps: 0.166632, loss_cps: 0.338110
[13:24:26.279] iteration 15038: total_loss: 0.741352, loss_sup: 0.245735, loss_mps: 0.168637, loss_cps: 0.326980
[13:24:26.425] iteration 15039: total_loss: 0.472461, loss_sup: 0.057708, loss_mps: 0.145892, loss_cps: 0.268861
[13:24:26.570] iteration 15040: total_loss: 0.254098, loss_sup: 0.067535, loss_mps: 0.068693, loss_cps: 0.117869
[13:24:26.716] iteration 15041: total_loss: 0.287634, loss_sup: 0.029572, loss_mps: 0.096092, loss_cps: 0.161971
[13:24:26.863] iteration 15042: total_loss: 0.269373, loss_sup: 0.016547, loss_mps: 0.091051, loss_cps: 0.161775
[13:24:27.009] iteration 15043: total_loss: 0.711266, loss_sup: 0.094523, loss_mps: 0.200778, loss_cps: 0.415965
[13:24:27.155] iteration 15044: total_loss: 0.436955, loss_sup: 0.064785, loss_mps: 0.124423, loss_cps: 0.247747
[13:24:27.300] iteration 15045: total_loss: 0.421406, loss_sup: 0.193595, loss_mps: 0.086925, loss_cps: 0.140885
[13:24:27.446] iteration 15046: total_loss: 0.325870, loss_sup: 0.057421, loss_mps: 0.094378, loss_cps: 0.174071
[13:24:27.591] iteration 15047: total_loss: 0.378256, loss_sup: 0.125663, loss_mps: 0.094248, loss_cps: 0.158345
[13:24:27.656] iteration 15048: total_loss: 1.053196, loss_sup: 0.479118, loss_mps: 0.181488, loss_cps: 0.392589
[13:24:28.845] iteration 15049: total_loss: 0.684024, loss_sup: 0.297442, loss_mps: 0.128780, loss_cps: 0.257802
[13:24:28.993] iteration 15050: total_loss: 0.349333, loss_sup: 0.028500, loss_mps: 0.120962, loss_cps: 0.199872
[13:24:29.139] iteration 15051: total_loss: 0.960296, loss_sup: 0.184773, loss_mps: 0.241695, loss_cps: 0.533828
[13:24:29.285] iteration 15052: total_loss: 0.509771, loss_sup: 0.078457, loss_mps: 0.156685, loss_cps: 0.274630
[13:24:29.433] iteration 15053: total_loss: 0.262151, loss_sup: 0.032714, loss_mps: 0.082103, loss_cps: 0.147335
[13:24:29.578] iteration 15054: total_loss: 0.795685, loss_sup: 0.136023, loss_mps: 0.215306, loss_cps: 0.444357
[13:24:29.724] iteration 15055: total_loss: 0.565108, loss_sup: 0.181881, loss_mps: 0.131791, loss_cps: 0.251436
[13:24:29.870] iteration 15056: total_loss: 0.593891, loss_sup: 0.049799, loss_mps: 0.168479, loss_cps: 0.375613
[13:24:30.016] iteration 15057: total_loss: 0.855984, loss_sup: 0.224255, loss_mps: 0.200900, loss_cps: 0.430830
[13:24:30.162] iteration 15058: total_loss: 0.314225, loss_sup: 0.018054, loss_mps: 0.108779, loss_cps: 0.187391
[13:24:30.307] iteration 15059: total_loss: 0.205730, loss_sup: 0.010686, loss_mps: 0.075944, loss_cps: 0.119100
[13:24:30.453] iteration 15060: total_loss: 0.444467, loss_sup: 0.079972, loss_mps: 0.120766, loss_cps: 0.243729
[13:24:30.598] iteration 15061: total_loss: 0.548779, loss_sup: 0.115174, loss_mps: 0.148018, loss_cps: 0.285587
[13:24:30.743] iteration 15062: total_loss: 0.430919, loss_sup: 0.049384, loss_mps: 0.126363, loss_cps: 0.255172
[13:24:30.890] iteration 15063: total_loss: 0.930805, loss_sup: 0.105851, loss_mps: 0.253324, loss_cps: 0.571630
[13:24:31.036] iteration 15064: total_loss: 0.458207, loss_sup: 0.107128, loss_mps: 0.116267, loss_cps: 0.234813
[13:24:31.183] iteration 15065: total_loss: 0.419459, loss_sup: 0.086375, loss_mps: 0.115266, loss_cps: 0.217818
[13:24:31.329] iteration 15066: total_loss: 0.841352, loss_sup: 0.112347, loss_mps: 0.235528, loss_cps: 0.493477
[13:24:31.475] iteration 15067: total_loss: 0.670537, loss_sup: 0.248857, loss_mps: 0.140654, loss_cps: 0.281026
[13:24:31.621] iteration 15068: total_loss: 0.482871, loss_sup: 0.093410, loss_mps: 0.134217, loss_cps: 0.255245
[13:24:31.767] iteration 15069: total_loss: 0.326207, loss_sup: 0.091836, loss_mps: 0.087669, loss_cps: 0.146702
[13:24:31.914] iteration 15070: total_loss: 0.286963, loss_sup: 0.028033, loss_mps: 0.093880, loss_cps: 0.165050
[13:24:32.059] iteration 15071: total_loss: 0.375736, loss_sup: 0.016915, loss_mps: 0.126902, loss_cps: 0.231919
[13:24:32.208] iteration 15072: total_loss: 0.374136, loss_sup: 0.026887, loss_mps: 0.119489, loss_cps: 0.227759
[13:24:32.354] iteration 15073: total_loss: 0.252299, loss_sup: 0.020998, loss_mps: 0.088693, loss_cps: 0.142608
[13:24:32.499] iteration 15074: total_loss: 0.497367, loss_sup: 0.098722, loss_mps: 0.136074, loss_cps: 0.262571
[13:24:32.645] iteration 15075: total_loss: 0.400107, loss_sup: 0.163653, loss_mps: 0.085888, loss_cps: 0.150566
[13:24:32.791] iteration 15076: total_loss: 0.280998, loss_sup: 0.014217, loss_mps: 0.098651, loss_cps: 0.168130
[13:24:32.937] iteration 15077: total_loss: 0.387736, loss_sup: 0.061579, loss_mps: 0.112578, loss_cps: 0.213579
[13:24:33.082] iteration 15078: total_loss: 0.549068, loss_sup: 0.159141, loss_mps: 0.133448, loss_cps: 0.256479
[13:24:33.228] iteration 15079: total_loss: 0.387073, loss_sup: 0.086876, loss_mps: 0.102029, loss_cps: 0.198168
[13:24:33.373] iteration 15080: total_loss: 0.461733, loss_sup: 0.074606, loss_mps: 0.133827, loss_cps: 0.253300
[13:24:33.518] iteration 15081: total_loss: 0.281500, loss_sup: 0.011694, loss_mps: 0.103986, loss_cps: 0.165821
[13:24:33.663] iteration 15082: total_loss: 0.584925, loss_sup: 0.027510, loss_mps: 0.173323, loss_cps: 0.384091
[13:24:33.810] iteration 15083: total_loss: 0.348242, loss_sup: 0.033828, loss_mps: 0.105828, loss_cps: 0.208586
[13:24:33.956] iteration 15084: total_loss: 0.386696, loss_sup: 0.012392, loss_mps: 0.126234, loss_cps: 0.248070
[13:24:34.104] iteration 15085: total_loss: 0.310539, loss_sup: 0.064571, loss_mps: 0.087515, loss_cps: 0.158454
[13:24:34.253] iteration 15086: total_loss: 0.328054, loss_sup: 0.024066, loss_mps: 0.104813, loss_cps: 0.199175
[13:24:34.398] iteration 15087: total_loss: 0.598138, loss_sup: 0.109794, loss_mps: 0.164170, loss_cps: 0.324175
[13:24:34.545] iteration 15088: total_loss: 0.475654, loss_sup: 0.027942, loss_mps: 0.144450, loss_cps: 0.303261
[13:24:34.691] iteration 15089: total_loss: 0.323755, loss_sup: 0.015536, loss_mps: 0.104654, loss_cps: 0.203565
[13:24:34.837] iteration 15090: total_loss: 0.202426, loss_sup: 0.015766, loss_mps: 0.070048, loss_cps: 0.116612
[13:24:34.985] iteration 15091: total_loss: 0.267778, loss_sup: 0.005565, loss_mps: 0.090948, loss_cps: 0.171265
[13:24:35.132] iteration 15092: total_loss: 0.386773, loss_sup: 0.122330, loss_mps: 0.092787, loss_cps: 0.171656
[13:24:35.278] iteration 15093: total_loss: 0.364005, loss_sup: 0.045943, loss_mps: 0.109964, loss_cps: 0.208099
[13:24:35.424] iteration 15094: total_loss: 0.582331, loss_sup: 0.252765, loss_mps: 0.110705, loss_cps: 0.218861
[13:24:35.570] iteration 15095: total_loss: 0.437473, loss_sup: 0.199024, loss_mps: 0.078940, loss_cps: 0.159508
[13:24:35.717] iteration 15096: total_loss: 0.434204, loss_sup: 0.117521, loss_mps: 0.107661, loss_cps: 0.209022
[13:24:35.862] iteration 15097: total_loss: 0.362720, loss_sup: 0.080590, loss_mps: 0.096333, loss_cps: 0.185797
[13:24:36.008] iteration 15098: total_loss: 0.183067, loss_sup: 0.014083, loss_mps: 0.067025, loss_cps: 0.101959
[13:24:36.154] iteration 15099: total_loss: 0.305341, loss_sup: 0.039630, loss_mps: 0.094391, loss_cps: 0.171319
[13:24:36.301] iteration 15100: total_loss: 0.295866, loss_sup: 0.062382, loss_mps: 0.085487, loss_cps: 0.147997
[13:24:36.301] Evaluation Started ==>
[13:24:47.619] ==> valid iteration 15100: unet metrics: {'dc': 0.6546550067941587, 'jc': 0.5353420787253781, 'pre': 0.7783869366197622, 'hd': 5.590141220377057}, ynet metrics: {'dc': 0.5626940628397645, 'jc': 0.4500568748621613, 'pre': 0.7646878111928798, 'hd': 5.762269817146375}.
[13:24:47.620] Evaluation Finished!⏹️
[13:24:47.769] iteration 15101: total_loss: 0.208383, loss_sup: 0.009184, loss_mps: 0.074437, loss_cps: 0.124763
[13:24:47.918] iteration 15102: total_loss: 0.754255, loss_sup: 0.125979, loss_mps: 0.211222, loss_cps: 0.417054
[13:24:48.063] iteration 15103: total_loss: 0.413383, loss_sup: 0.047606, loss_mps: 0.123716, loss_cps: 0.242061
[13:24:48.209] iteration 15104: total_loss: 0.331436, loss_sup: 0.094256, loss_mps: 0.092288, loss_cps: 0.144892
[13:24:48.354] iteration 15105: total_loss: 0.553518, loss_sup: 0.072012, loss_mps: 0.156898, loss_cps: 0.324609
[13:24:48.500] iteration 15106: total_loss: 0.340208, loss_sup: 0.059460, loss_mps: 0.092261, loss_cps: 0.188487
[13:24:48.645] iteration 15107: total_loss: 0.469238, loss_sup: 0.052495, loss_mps: 0.129260, loss_cps: 0.287482
[13:24:48.796] iteration 15108: total_loss: 0.404302, loss_sup: 0.180795, loss_mps: 0.081645, loss_cps: 0.141862
[13:24:48.941] iteration 15109: total_loss: 0.782228, loss_sup: 0.338729, loss_mps: 0.144808, loss_cps: 0.298691
[13:24:49.087] iteration 15110: total_loss: 0.545945, loss_sup: 0.038060, loss_mps: 0.168986, loss_cps: 0.338900
[13:24:49.234] iteration 15111: total_loss: 0.178420, loss_sup: 0.016560, loss_mps: 0.064297, loss_cps: 0.097564
[13:24:49.379] iteration 15112: total_loss: 0.456483, loss_sup: 0.193913, loss_mps: 0.088566, loss_cps: 0.174004
[13:24:49.527] iteration 15113: total_loss: 0.310957, loss_sup: 0.029484, loss_mps: 0.097009, loss_cps: 0.184464
[13:24:49.672] iteration 15114: total_loss: 0.495483, loss_sup: 0.058218, loss_mps: 0.141114, loss_cps: 0.296151
[13:24:49.818] iteration 15115: total_loss: 0.293439, loss_sup: 0.078501, loss_mps: 0.079416, loss_cps: 0.135523
[13:24:49.967] iteration 15116: total_loss: 0.491679, loss_sup: 0.078719, loss_mps: 0.143918, loss_cps: 0.269042
[13:24:50.112] iteration 15117: total_loss: 0.635189, loss_sup: 0.138031, loss_mps: 0.169265, loss_cps: 0.327893
[13:24:50.258] iteration 15118: total_loss: 0.348917, loss_sup: 0.120529, loss_mps: 0.089135, loss_cps: 0.139252
[13:24:50.404] iteration 15119: total_loss: 0.232640, loss_sup: 0.027507, loss_mps: 0.075941, loss_cps: 0.129192
[13:24:50.550] iteration 15120: total_loss: 0.348719, loss_sup: 0.061982, loss_mps: 0.100324, loss_cps: 0.186412
[13:24:50.695] iteration 15121: total_loss: 0.579275, loss_sup: 0.177772, loss_mps: 0.135102, loss_cps: 0.266401
[13:24:50.841] iteration 15122: total_loss: 0.352517, loss_sup: 0.024421, loss_mps: 0.118381, loss_cps: 0.209715
[13:24:50.988] iteration 15123: total_loss: 0.337002, loss_sup: 0.054789, loss_mps: 0.100490, loss_cps: 0.181724
[13:24:51.133] iteration 15124: total_loss: 0.321214, loss_sup: 0.095481, loss_mps: 0.086024, loss_cps: 0.139709
[13:24:51.278] iteration 15125: total_loss: 0.263846, loss_sup: 0.075959, loss_mps: 0.071625, loss_cps: 0.116261
[13:24:51.424] iteration 15126: total_loss: 0.404262, loss_sup: 0.072412, loss_mps: 0.114562, loss_cps: 0.217287
[13:24:51.572] iteration 15127: total_loss: 0.581719, loss_sup: 0.162329, loss_mps: 0.141226, loss_cps: 0.278163
[13:24:51.719] iteration 15128: total_loss: 0.654263, loss_sup: 0.058561, loss_mps: 0.187323, loss_cps: 0.408378
[13:24:51.867] iteration 15129: total_loss: 0.194567, loss_sup: 0.045728, loss_mps: 0.057255, loss_cps: 0.091584
[13:24:52.013] iteration 15130: total_loss: 0.569495, loss_sup: 0.068946, loss_mps: 0.168417, loss_cps: 0.332132
[13:24:52.159] iteration 15131: total_loss: 0.315736, loss_sup: 0.010093, loss_mps: 0.106327, loss_cps: 0.199316
[13:24:52.305] iteration 15132: total_loss: 0.241642, loss_sup: 0.031716, loss_mps: 0.079019, loss_cps: 0.130908
[13:24:52.451] iteration 15133: total_loss: 0.349857, loss_sup: 0.030598, loss_mps: 0.105309, loss_cps: 0.213950
[13:24:52.598] iteration 15134: total_loss: 0.623017, loss_sup: 0.084477, loss_mps: 0.178798, loss_cps: 0.359742
[13:24:52.744] iteration 15135: total_loss: 0.368356, loss_sup: 0.032838, loss_mps: 0.107693, loss_cps: 0.227826
[13:24:52.891] iteration 15136: total_loss: 0.432649, loss_sup: 0.180484, loss_mps: 0.091396, loss_cps: 0.160769
[13:24:53.038] iteration 15137: total_loss: 0.405764, loss_sup: 0.206554, loss_mps: 0.070800, loss_cps: 0.128410
[13:24:53.183] iteration 15138: total_loss: 0.527681, loss_sup: 0.063834, loss_mps: 0.149313, loss_cps: 0.314534
[13:24:53.330] iteration 15139: total_loss: 0.238356, loss_sup: 0.014001, loss_mps: 0.083506, loss_cps: 0.140849
[13:24:53.475] iteration 15140: total_loss: 0.197364, loss_sup: 0.009410, loss_mps: 0.066123, loss_cps: 0.121831
[13:24:53.621] iteration 15141: total_loss: 0.444038, loss_sup: 0.075340, loss_mps: 0.125955, loss_cps: 0.242743
[13:24:53.768] iteration 15142: total_loss: 0.373903, loss_sup: 0.031255, loss_mps: 0.119607, loss_cps: 0.223041
[13:24:53.913] iteration 15143: total_loss: 0.355311, loss_sup: 0.078676, loss_mps: 0.091439, loss_cps: 0.185195
[13:24:54.058] iteration 15144: total_loss: 0.591681, loss_sup: 0.063911, loss_mps: 0.173366, loss_cps: 0.354404
[13:24:54.203] iteration 15145: total_loss: 0.306470, loss_sup: 0.051733, loss_mps: 0.095718, loss_cps: 0.159018
[13:24:54.350] iteration 15146: total_loss: 0.661952, loss_sup: 0.341871, loss_mps: 0.114025, loss_cps: 0.206055
[13:24:54.497] iteration 15147: total_loss: 0.221803, loss_sup: 0.032446, loss_mps: 0.070420, loss_cps: 0.118938
[13:24:54.646] iteration 15148: total_loss: 0.326164, loss_sup: 0.067818, loss_mps: 0.089038, loss_cps: 0.169308
[13:24:54.792] iteration 15149: total_loss: 0.391221, loss_sup: 0.011891, loss_mps: 0.126500, loss_cps: 0.252830
[13:24:54.942] iteration 15150: total_loss: 0.326256, loss_sup: 0.112659, loss_mps: 0.077573, loss_cps: 0.136023
[13:24:55.088] iteration 15151: total_loss: 0.925551, loss_sup: 0.507055, loss_mps: 0.141648, loss_cps: 0.276849
[13:24:55.233] iteration 15152: total_loss: 0.316968, loss_sup: 0.080909, loss_mps: 0.086179, loss_cps: 0.149881
[13:24:55.381] iteration 15153: total_loss: 0.346016, loss_sup: 0.060805, loss_mps: 0.097852, loss_cps: 0.187359
[13:24:55.527] iteration 15154: total_loss: 0.449106, loss_sup: 0.115931, loss_mps: 0.120004, loss_cps: 0.213172
[13:24:55.673] iteration 15155: total_loss: 0.656504, loss_sup: 0.142416, loss_mps: 0.166913, loss_cps: 0.347175
[13:24:55.819] iteration 15156: total_loss: 0.405238, loss_sup: 0.187961, loss_mps: 0.078805, loss_cps: 0.138473
[13:24:55.967] iteration 15157: total_loss: 0.331084, loss_sup: 0.013836, loss_mps: 0.110792, loss_cps: 0.206456
[13:24:56.117] iteration 15158: total_loss: 0.313435, loss_sup: 0.031599, loss_mps: 0.095098, loss_cps: 0.186739
[13:24:56.263] iteration 15159: total_loss: 0.353343, loss_sup: 0.030346, loss_mps: 0.107395, loss_cps: 0.215602
[13:24:56.409] iteration 15160: total_loss: 0.600744, loss_sup: 0.205403, loss_mps: 0.129975, loss_cps: 0.265365
[13:24:56.560] iteration 15161: total_loss: 0.576096, loss_sup: 0.071473, loss_mps: 0.161975, loss_cps: 0.342649
[13:24:56.707] iteration 15162: total_loss: 0.384868, loss_sup: 0.063977, loss_mps: 0.105944, loss_cps: 0.214947
[13:24:56.853] iteration 15163: total_loss: 0.568303, loss_sup: 0.081567, loss_mps: 0.160207, loss_cps: 0.326529
[13:24:57.000] iteration 15164: total_loss: 0.260102, loss_sup: 0.019734, loss_mps: 0.085857, loss_cps: 0.154512
[13:24:57.146] iteration 15165: total_loss: 0.619471, loss_sup: 0.101490, loss_mps: 0.163750, loss_cps: 0.354230
[13:24:57.292] iteration 15166: total_loss: 0.579380, loss_sup: 0.098022, loss_mps: 0.156967, loss_cps: 0.324391
[13:24:57.438] iteration 15167: total_loss: 0.458339, loss_sup: 0.102566, loss_mps: 0.125033, loss_cps: 0.230740
[13:24:57.584] iteration 15168: total_loss: 0.488412, loss_sup: 0.052394, loss_mps: 0.147387, loss_cps: 0.288631
[13:24:57.730] iteration 15169: total_loss: 0.595746, loss_sup: 0.207944, loss_mps: 0.138215, loss_cps: 0.249587
[13:24:57.876] iteration 15170: total_loss: 0.410605, loss_sup: 0.035514, loss_mps: 0.129749, loss_cps: 0.245341
[13:24:58.022] iteration 15171: total_loss: 0.687889, loss_sup: 0.141225, loss_mps: 0.174041, loss_cps: 0.372623
[13:24:58.167] iteration 15172: total_loss: 0.869996, loss_sup: 0.165124, loss_mps: 0.220663, loss_cps: 0.484209
[13:24:58.313] iteration 15173: total_loss: 0.317491, loss_sup: 0.021843, loss_mps: 0.100553, loss_cps: 0.195096
[13:24:58.458] iteration 15174: total_loss: 0.440572, loss_sup: 0.055801, loss_mps: 0.147366, loss_cps: 0.237405
[13:24:58.606] iteration 15175: total_loss: 0.251952, loss_sup: 0.012643, loss_mps: 0.085089, loss_cps: 0.154220
[13:24:58.752] iteration 15176: total_loss: 0.811062, loss_sup: 0.254949, loss_mps: 0.176970, loss_cps: 0.379143
[13:24:58.900] iteration 15177: total_loss: 0.284377, loss_sup: 0.009358, loss_mps: 0.097877, loss_cps: 0.177143
[13:24:59.046] iteration 15178: total_loss: 0.395150, loss_sup: 0.027402, loss_mps: 0.122548, loss_cps: 0.245200
[13:24:59.191] iteration 15179: total_loss: 0.574507, loss_sup: 0.091023, loss_mps: 0.152321, loss_cps: 0.331163
[13:24:59.338] iteration 15180: total_loss: 0.357055, loss_sup: 0.035373, loss_mps: 0.112303, loss_cps: 0.209379
[13:24:59.484] iteration 15181: total_loss: 0.600019, loss_sup: 0.076628, loss_mps: 0.161115, loss_cps: 0.362276
[13:24:59.630] iteration 15182: total_loss: 0.716740, loss_sup: 0.059307, loss_mps: 0.208604, loss_cps: 0.448829
[13:24:59.776] iteration 15183: total_loss: 0.477440, loss_sup: 0.191739, loss_mps: 0.102325, loss_cps: 0.183375
[13:24:59.924] iteration 15184: total_loss: 0.451218, loss_sup: 0.062027, loss_mps: 0.128150, loss_cps: 0.261041
[13:25:00.070] iteration 15185: total_loss: 0.792507, loss_sup: 0.120858, loss_mps: 0.222042, loss_cps: 0.449607
[13:25:00.216] iteration 15186: total_loss: 0.270559, loss_sup: 0.031955, loss_mps: 0.086691, loss_cps: 0.151913
[13:25:00.362] iteration 15187: total_loss: 0.219893, loss_sup: 0.034772, loss_mps: 0.069999, loss_cps: 0.115123
[13:25:00.507] iteration 15188: total_loss: 0.536078, loss_sup: 0.081385, loss_mps: 0.152861, loss_cps: 0.301832
[13:25:00.653] iteration 15189: total_loss: 0.434739, loss_sup: 0.024870, loss_mps: 0.133978, loss_cps: 0.275891
[13:25:00.803] iteration 15190: total_loss: 0.519257, loss_sup: 0.223002, loss_mps: 0.103404, loss_cps: 0.192851
[13:25:00.948] iteration 15191: total_loss: 0.317369, loss_sup: 0.074224, loss_mps: 0.086837, loss_cps: 0.156308
[13:25:01.094] iteration 15192: total_loss: 0.449306, loss_sup: 0.094475, loss_mps: 0.117988, loss_cps: 0.236843
[13:25:01.240] iteration 15193: total_loss: 0.653912, loss_sup: 0.092181, loss_mps: 0.182743, loss_cps: 0.378988
[13:25:01.386] iteration 15194: total_loss: 0.405016, loss_sup: 0.174363, loss_mps: 0.087747, loss_cps: 0.142907
[13:25:01.531] iteration 15195: total_loss: 0.566336, loss_sup: 0.140475, loss_mps: 0.144048, loss_cps: 0.281813
[13:25:01.678] iteration 15196: total_loss: 0.425178, loss_sup: 0.046910, loss_mps: 0.125948, loss_cps: 0.252320
[13:25:01.824] iteration 15197: total_loss: 0.218042, loss_sup: 0.011098, loss_mps: 0.075285, loss_cps: 0.131659
[13:25:01.970] iteration 15198: total_loss: 0.238542, loss_sup: 0.054094, loss_mps: 0.067747, loss_cps: 0.116701
[13:25:02.118] iteration 15199: total_loss: 0.465974, loss_sup: 0.136743, loss_mps: 0.119400, loss_cps: 0.209832
[13:25:02.264] iteration 15200: total_loss: 0.551672, loss_sup: 0.191709, loss_mps: 0.121594, loss_cps: 0.238369
[13:25:02.264] Evaluation Started ==>
[13:25:13.587] ==> valid iteration 15200: unet metrics: {'dc': 0.6410798702760737, 'jc': 0.5230949468811847, 'pre': 0.7646301339322051, 'hd': 5.620497754622888}, ynet metrics: {'dc': 0.5670124688238349, 'jc': 0.4543704084551721, 'pre': 0.7835499072707826, 'hd': 5.72444225545421}.
[13:25:13.589] Evaluation Finished!⏹️
[13:25:13.739] iteration 15201: total_loss: 0.252480, loss_sup: 0.018817, loss_mps: 0.082647, loss_cps: 0.151017
[13:25:13.886] iteration 15202: total_loss: 0.442693, loss_sup: 0.184101, loss_mps: 0.091600, loss_cps: 0.166991
[13:25:14.032] iteration 15203: total_loss: 0.693241, loss_sup: 0.262926, loss_mps: 0.143013, loss_cps: 0.287303
[13:25:14.178] iteration 15204: total_loss: 0.321756, loss_sup: 0.012538, loss_mps: 0.108201, loss_cps: 0.201016
[13:25:14.324] iteration 15205: total_loss: 0.266712, loss_sup: 0.031202, loss_mps: 0.088136, loss_cps: 0.147374
[13:25:14.473] iteration 15206: total_loss: 0.494760, loss_sup: 0.155750, loss_mps: 0.113934, loss_cps: 0.225076
[13:25:14.623] iteration 15207: total_loss: 0.529532, loss_sup: 0.033863, loss_mps: 0.163179, loss_cps: 0.332490
[13:25:14.768] iteration 15208: total_loss: 0.539953, loss_sup: 0.133826, loss_mps: 0.140501, loss_cps: 0.265625
[13:25:14.913] iteration 15209: total_loss: 0.504947, loss_sup: 0.237401, loss_mps: 0.092967, loss_cps: 0.174580
[13:25:15.059] iteration 15210: total_loss: 0.444135, loss_sup: 0.127052, loss_mps: 0.112640, loss_cps: 0.204443
[13:25:15.205] iteration 15211: total_loss: 0.650706, loss_sup: 0.193070, loss_mps: 0.157717, loss_cps: 0.299919
[13:25:15.351] iteration 15212: total_loss: 0.548900, loss_sup: 0.096391, loss_mps: 0.145182, loss_cps: 0.307327
[13:25:15.497] iteration 15213: total_loss: 0.352578, loss_sup: 0.075015, loss_mps: 0.100532, loss_cps: 0.177032
[13:25:15.641] iteration 15214: total_loss: 0.336289, loss_sup: 0.036495, loss_mps: 0.106857, loss_cps: 0.192938
[13:25:15.786] iteration 15215: total_loss: 0.362889, loss_sup: 0.034773, loss_mps: 0.119512, loss_cps: 0.208605
[13:25:15.932] iteration 15216: total_loss: 0.774929, loss_sup: 0.063593, loss_mps: 0.220929, loss_cps: 0.490407
[13:25:16.077] iteration 15217: total_loss: 0.540916, loss_sup: 0.087287, loss_mps: 0.150333, loss_cps: 0.303295
[13:25:16.223] iteration 15218: total_loss: 0.475256, loss_sup: 0.036412, loss_mps: 0.138934, loss_cps: 0.299910
[13:25:16.368] iteration 15219: total_loss: 0.393683, loss_sup: 0.165804, loss_mps: 0.084977, loss_cps: 0.142902
[13:25:16.513] iteration 15220: total_loss: 0.350287, loss_sup: 0.034485, loss_mps: 0.107167, loss_cps: 0.208635
[13:25:16.659] iteration 15221: total_loss: 0.368959, loss_sup: 0.037494, loss_mps: 0.111450, loss_cps: 0.220015
[13:25:16.804] iteration 15222: total_loss: 0.779077, loss_sup: 0.081762, loss_mps: 0.222283, loss_cps: 0.475032
[13:25:16.950] iteration 15223: total_loss: 0.626644, loss_sup: 0.086589, loss_mps: 0.173108, loss_cps: 0.366947
[13:25:17.095] iteration 15224: total_loss: 0.371416, loss_sup: 0.067333, loss_mps: 0.108870, loss_cps: 0.195213
[13:25:17.242] iteration 15225: total_loss: 0.660123, loss_sup: 0.054125, loss_mps: 0.195330, loss_cps: 0.410669
[13:25:17.388] iteration 15226: total_loss: 0.422840, loss_sup: 0.109233, loss_mps: 0.108109, loss_cps: 0.205499
[13:25:17.538] iteration 15227: total_loss: 0.384671, loss_sup: 0.077672, loss_mps: 0.110693, loss_cps: 0.196305
[13:25:17.683] iteration 15228: total_loss: 0.385770, loss_sup: 0.089843, loss_mps: 0.101950, loss_cps: 0.193977
[13:25:17.829] iteration 15229: total_loss: 0.515435, loss_sup: 0.084312, loss_mps: 0.142875, loss_cps: 0.288248
[13:25:17.975] iteration 15230: total_loss: 0.494474, loss_sup: 0.211625, loss_mps: 0.105122, loss_cps: 0.177727
[13:25:18.121] iteration 15231: total_loss: 0.377627, loss_sup: 0.090937, loss_mps: 0.105097, loss_cps: 0.181594
[13:25:18.268] iteration 15232: total_loss: 0.456271, loss_sup: 0.067100, loss_mps: 0.129822, loss_cps: 0.259350
[13:25:18.413] iteration 15233: total_loss: 0.678108, loss_sup: 0.112381, loss_mps: 0.176441, loss_cps: 0.389286
[13:25:18.558] iteration 15234: total_loss: 0.641503, loss_sup: 0.089813, loss_mps: 0.171251, loss_cps: 0.380439
[13:25:18.703] iteration 15235: total_loss: 0.918143, loss_sup: 0.353248, loss_mps: 0.178653, loss_cps: 0.386241
[13:25:18.848] iteration 15236: total_loss: 0.570953, loss_sup: 0.176969, loss_mps: 0.130938, loss_cps: 0.263046
[13:25:18.994] iteration 15237: total_loss: 0.484561, loss_sup: 0.148128, loss_mps: 0.117526, loss_cps: 0.218908
[13:25:19.142] iteration 15238: total_loss: 0.446986, loss_sup: 0.009172, loss_mps: 0.145950, loss_cps: 0.291863
[13:25:19.288] iteration 15239: total_loss: 0.301377, loss_sup: 0.034239, loss_mps: 0.094712, loss_cps: 0.172426
[13:25:19.433] iteration 15240: total_loss: 0.411122, loss_sup: 0.114459, loss_mps: 0.107929, loss_cps: 0.188733
[13:25:19.578] iteration 15241: total_loss: 0.414682, loss_sup: 0.116892, loss_mps: 0.105230, loss_cps: 0.192560
[13:25:19.724] iteration 15242: total_loss: 0.719629, loss_sup: 0.110307, loss_mps: 0.190396, loss_cps: 0.418926
[13:25:19.870] iteration 15243: total_loss: 0.341513, loss_sup: 0.013703, loss_mps: 0.114185, loss_cps: 0.213626
[13:25:20.015] iteration 15244: total_loss: 0.556757, loss_sup: 0.200920, loss_mps: 0.123546, loss_cps: 0.232290
[13:25:20.161] iteration 15245: total_loss: 0.781817, loss_sup: 0.117773, loss_mps: 0.218661, loss_cps: 0.445383
[13:25:20.308] iteration 15246: total_loss: 0.528900, loss_sup: 0.062041, loss_mps: 0.153752, loss_cps: 0.313108
[13:25:20.456] iteration 15247: total_loss: 0.640392, loss_sup: 0.036589, loss_mps: 0.195603, loss_cps: 0.408200
[13:25:20.601] iteration 15248: total_loss: 0.453220, loss_sup: 0.132499, loss_mps: 0.110849, loss_cps: 0.209872
[13:25:20.747] iteration 15249: total_loss: 0.279931, loss_sup: 0.062575, loss_mps: 0.076865, loss_cps: 0.140491
[13:25:20.893] iteration 15250: total_loss: 1.256150, loss_sup: 0.028411, loss_mps: 0.376438, loss_cps: 0.851301
[13:25:21.040] iteration 15251: total_loss: 0.416215, loss_sup: 0.038245, loss_mps: 0.128842, loss_cps: 0.249128
[13:25:21.185] iteration 15252: total_loss: 0.328664, loss_sup: 0.087684, loss_mps: 0.086813, loss_cps: 0.154168
[13:25:21.331] iteration 15253: total_loss: 0.396705, loss_sup: 0.007758, loss_mps: 0.133763, loss_cps: 0.255184
[13:25:21.477] iteration 15254: total_loss: 0.459406, loss_sup: 0.051695, loss_mps: 0.141077, loss_cps: 0.266634
[13:25:21.623] iteration 15255: total_loss: 0.357734, loss_sup: 0.055748, loss_mps: 0.105605, loss_cps: 0.196381
[13:25:21.769] iteration 15256: total_loss: 0.434353, loss_sup: 0.058816, loss_mps: 0.127690, loss_cps: 0.247847
[13:25:21.914] iteration 15257: total_loss: 0.400792, loss_sup: 0.045152, loss_mps: 0.118211, loss_cps: 0.237428
[13:25:22.060] iteration 15258: total_loss: 0.256461, loss_sup: 0.044061, loss_mps: 0.077007, loss_cps: 0.135394
[13:25:22.205] iteration 15259: total_loss: 0.547378, loss_sup: 0.057534, loss_mps: 0.160523, loss_cps: 0.329321
[13:25:22.350] iteration 15260: total_loss: 0.406776, loss_sup: 0.048565, loss_mps: 0.117186, loss_cps: 0.241025
[13:25:22.495] iteration 15261: total_loss: 0.829383, loss_sup: 0.120057, loss_mps: 0.214141, loss_cps: 0.495184
[13:25:22.642] iteration 15262: total_loss: 0.513630, loss_sup: 0.064450, loss_mps: 0.141050, loss_cps: 0.308131
[13:25:22.790] iteration 15263: total_loss: 0.363420, loss_sup: 0.053837, loss_mps: 0.109280, loss_cps: 0.200303
[13:25:22.936] iteration 15264: total_loss: 0.556485, loss_sup: 0.170387, loss_mps: 0.129553, loss_cps: 0.256545
[13:25:23.082] iteration 15265: total_loss: 0.327929, loss_sup: 0.031548, loss_mps: 0.105430, loss_cps: 0.190951
[13:25:23.227] iteration 15266: total_loss: 0.900022, loss_sup: 0.213637, loss_mps: 0.208537, loss_cps: 0.477847
[13:25:23.373] iteration 15267: total_loss: 0.465569, loss_sup: 0.055871, loss_mps: 0.142157, loss_cps: 0.267540
[13:25:23.519] iteration 15268: total_loss: 0.740499, loss_sup: 0.305775, loss_mps: 0.149351, loss_cps: 0.285373
[13:25:23.664] iteration 15269: total_loss: 0.534497, loss_sup: 0.092167, loss_mps: 0.137407, loss_cps: 0.304923
[13:25:23.813] iteration 15270: total_loss: 0.499925, loss_sup: 0.127654, loss_mps: 0.128942, loss_cps: 0.243329
[13:25:23.959] iteration 15271: total_loss: 0.716239, loss_sup: 0.073306, loss_mps: 0.206525, loss_cps: 0.436408
[13:25:24.105] iteration 15272: total_loss: 0.534329, loss_sup: 0.150474, loss_mps: 0.130754, loss_cps: 0.253100
[13:25:24.255] iteration 15273: total_loss: 0.525651, loss_sup: 0.157686, loss_mps: 0.119888, loss_cps: 0.248078
[13:25:24.403] iteration 15274: total_loss: 0.537307, loss_sup: 0.161242, loss_mps: 0.138102, loss_cps: 0.237963
[13:25:24.551] iteration 15275: total_loss: 0.234267, loss_sup: 0.023682, loss_mps: 0.074993, loss_cps: 0.135592
[13:25:24.697] iteration 15276: total_loss: 0.507834, loss_sup: 0.197367, loss_mps: 0.113772, loss_cps: 0.196696
[13:25:24.845] iteration 15277: total_loss: 0.352744, loss_sup: 0.129857, loss_mps: 0.079170, loss_cps: 0.143716
[13:25:24.990] iteration 15278: total_loss: 0.588293, loss_sup: 0.130178, loss_mps: 0.150914, loss_cps: 0.307201
[13:25:25.137] iteration 15279: total_loss: 0.605020, loss_sup: 0.259702, loss_mps: 0.125313, loss_cps: 0.220005
[13:25:25.282] iteration 15280: total_loss: 0.661372, loss_sup: 0.174837, loss_mps: 0.162569, loss_cps: 0.323966
[13:25:25.428] iteration 15281: total_loss: 0.437351, loss_sup: 0.051532, loss_mps: 0.135920, loss_cps: 0.249899
[13:25:25.574] iteration 15282: total_loss: 0.301192, loss_sup: 0.004781, loss_mps: 0.108493, loss_cps: 0.187917
[13:25:25.720] iteration 15283: total_loss: 0.555000, loss_sup: 0.162047, loss_mps: 0.138237, loss_cps: 0.254715
[13:25:25.866] iteration 15284: total_loss: 0.575530, loss_sup: 0.091434, loss_mps: 0.152355, loss_cps: 0.331741
[13:25:26.012] iteration 15285: total_loss: 0.405815, loss_sup: 0.205318, loss_mps: 0.078364, loss_cps: 0.122134
[13:25:26.158] iteration 15286: total_loss: 0.454912, loss_sup: 0.058921, loss_mps: 0.144501, loss_cps: 0.251490
[13:25:26.303] iteration 15287: total_loss: 0.240870, loss_sup: 0.019447, loss_mps: 0.088107, loss_cps: 0.133317
[13:25:26.449] iteration 15288: total_loss: 0.304631, loss_sup: 0.015718, loss_mps: 0.106098, loss_cps: 0.182815
[13:25:26.596] iteration 15289: total_loss: 0.643112, loss_sup: 0.215643, loss_mps: 0.152219, loss_cps: 0.275249
[13:25:26.741] iteration 15290: total_loss: 0.371795, loss_sup: 0.139001, loss_mps: 0.086132, loss_cps: 0.146661
[13:25:26.887] iteration 15291: total_loss: 0.584803, loss_sup: 0.171847, loss_mps: 0.135005, loss_cps: 0.277951
[13:25:27.033] iteration 15292: total_loss: 0.213384, loss_sup: 0.007923, loss_mps: 0.077666, loss_cps: 0.127795
[13:25:27.179] iteration 15293: total_loss: 0.274111, loss_sup: 0.013525, loss_mps: 0.100331, loss_cps: 0.160255
[13:25:27.326] iteration 15294: total_loss: 0.353488, loss_sup: 0.039835, loss_mps: 0.111852, loss_cps: 0.201802
[13:25:27.472] iteration 15295: total_loss: 0.450517, loss_sup: 0.099185, loss_mps: 0.128988, loss_cps: 0.222343
[13:25:27.619] iteration 15296: total_loss: 0.517180, loss_sup: 0.010137, loss_mps: 0.170321, loss_cps: 0.336722
[13:25:27.765] iteration 15297: total_loss: 0.740094, loss_sup: 0.087264, loss_mps: 0.212364, loss_cps: 0.440465
[13:25:27.911] iteration 15298: total_loss: 0.356053, loss_sup: 0.071609, loss_mps: 0.102105, loss_cps: 0.182339
[13:25:28.057] iteration 15299: total_loss: 0.336173, loss_sup: 0.045539, loss_mps: 0.103642, loss_cps: 0.186992
[13:25:28.202] iteration 15300: total_loss: 0.552698, loss_sup: 0.076897, loss_mps: 0.157447, loss_cps: 0.318354
[13:25:28.202] Evaluation Started ==>
[13:25:39.551] ==> valid iteration 15300: unet metrics: {'dc': 0.6585383904809649, 'jc': 0.5402338271025935, 'pre': 0.781408075179485, 'hd': 5.54388766133226}, ynet metrics: {'dc': 0.5215466188010445, 'jc': 0.4134168295617103, 'pre': 0.7801412418773025, 'hd': 5.807502420531784}.
[13:25:39.552] Evaluation Finished!⏹️
[13:25:39.700] iteration 15301: total_loss: 0.385656, loss_sup: 0.063908, loss_mps: 0.113070, loss_cps: 0.208678
[13:25:39.848] iteration 15302: total_loss: 0.343400, loss_sup: 0.073689, loss_mps: 0.102199, loss_cps: 0.167511
[13:25:39.993] iteration 15303: total_loss: 0.290845, loss_sup: 0.018829, loss_mps: 0.100885, loss_cps: 0.171131
[13:25:40.138] iteration 15304: total_loss: 0.947316, loss_sup: 0.299675, loss_mps: 0.208003, loss_cps: 0.439638
[13:25:40.284] iteration 15305: total_loss: 0.550977, loss_sup: 0.036093, loss_mps: 0.157788, loss_cps: 0.357096
[13:25:40.430] iteration 15306: total_loss: 0.327465, loss_sup: 0.059812, loss_mps: 0.101762, loss_cps: 0.165892
[13:25:40.576] iteration 15307: total_loss: 0.498144, loss_sup: 0.054898, loss_mps: 0.149547, loss_cps: 0.293699
[13:25:40.721] iteration 15308: total_loss: 0.414304, loss_sup: 0.173832, loss_mps: 0.084805, loss_cps: 0.155667
[13:25:40.868] iteration 15309: total_loss: 0.309061, loss_sup: 0.096831, loss_mps: 0.077931, loss_cps: 0.134299
[13:25:41.014] iteration 15310: total_loss: 0.322010, loss_sup: 0.075616, loss_mps: 0.086942, loss_cps: 0.159453
[13:25:41.159] iteration 15311: total_loss: 0.456659, loss_sup: 0.044074, loss_mps: 0.137417, loss_cps: 0.275169
[13:25:41.304] iteration 15312: total_loss: 0.421010, loss_sup: 0.084151, loss_mps: 0.117583, loss_cps: 0.219275
[13:25:41.450] iteration 15313: total_loss: 0.337118, loss_sup: 0.022652, loss_mps: 0.113409, loss_cps: 0.201056
[13:25:41.596] iteration 15314: total_loss: 0.252532, loss_sup: 0.009053, loss_mps: 0.090499, loss_cps: 0.152979
[13:25:41.742] iteration 15315: total_loss: 0.486706, loss_sup: 0.101594, loss_mps: 0.130656, loss_cps: 0.254456
[13:25:41.888] iteration 15316: total_loss: 0.492167, loss_sup: 0.038492, loss_mps: 0.153128, loss_cps: 0.300547
[13:25:42.035] iteration 15317: total_loss: 0.580334, loss_sup: 0.188058, loss_mps: 0.123886, loss_cps: 0.268389
[13:25:42.181] iteration 15318: total_loss: 0.387246, loss_sup: 0.027684, loss_mps: 0.124962, loss_cps: 0.234600
[13:25:42.329] iteration 15319: total_loss: 0.312231, loss_sup: 0.027487, loss_mps: 0.097465, loss_cps: 0.187279
[13:25:42.474] iteration 15320: total_loss: 0.293097, loss_sup: 0.055307, loss_mps: 0.090758, loss_cps: 0.147031
[13:25:42.621] iteration 15321: total_loss: 0.315642, loss_sup: 0.074183, loss_mps: 0.087297, loss_cps: 0.154161
[13:25:42.767] iteration 15322: total_loss: 0.442150, loss_sup: 0.053591, loss_mps: 0.132078, loss_cps: 0.256480
[13:25:42.913] iteration 15323: total_loss: 0.451512, loss_sup: 0.030108, loss_mps: 0.140377, loss_cps: 0.281027
[13:25:43.058] iteration 15324: total_loss: 0.454151, loss_sup: 0.190432, loss_mps: 0.093676, loss_cps: 0.170043
[13:25:43.205] iteration 15325: total_loss: 0.527611, loss_sup: 0.198237, loss_mps: 0.120392, loss_cps: 0.208983
[13:25:43.350] iteration 15326: total_loss: 0.393203, loss_sup: 0.095784, loss_mps: 0.101967, loss_cps: 0.195452
[13:25:43.495] iteration 15327: total_loss: 0.269478, loss_sup: 0.045370, loss_mps: 0.083431, loss_cps: 0.140677
[13:25:43.642] iteration 15328: total_loss: 0.384511, loss_sup: 0.048287, loss_mps: 0.106611, loss_cps: 0.229613
[13:25:43.787] iteration 15329: total_loss: 0.249103, loss_sup: 0.039425, loss_mps: 0.075098, loss_cps: 0.134580
[13:25:43.932] iteration 15330: total_loss: 0.535444, loss_sup: 0.032666, loss_mps: 0.159933, loss_cps: 0.342844
[13:25:44.079] iteration 15331: total_loss: 0.349447, loss_sup: 0.054818, loss_mps: 0.110002, loss_cps: 0.184627
[13:25:44.225] iteration 15332: total_loss: 0.269981, loss_sup: 0.015603, loss_mps: 0.092794, loss_cps: 0.161584
[13:25:44.373] iteration 15333: total_loss: 0.552591, loss_sup: 0.084609, loss_mps: 0.157978, loss_cps: 0.310005
[13:25:44.518] iteration 15334: total_loss: 0.405639, loss_sup: 0.012969, loss_mps: 0.129868, loss_cps: 0.262803
[13:25:44.667] iteration 15335: total_loss: 0.313068, loss_sup: 0.026461, loss_mps: 0.097753, loss_cps: 0.188854
[13:25:44.813] iteration 15336: total_loss: 0.460471, loss_sup: 0.082801, loss_mps: 0.123878, loss_cps: 0.253792
[13:25:44.959] iteration 15337: total_loss: 0.214697, loss_sup: 0.041144, loss_mps: 0.064824, loss_cps: 0.108729
[13:25:45.104] iteration 15338: total_loss: 0.503803, loss_sup: 0.065685, loss_mps: 0.141330, loss_cps: 0.296789
[13:25:45.250] iteration 15339: total_loss: 0.375741, loss_sup: 0.055294, loss_mps: 0.113937, loss_cps: 0.206511
[13:25:45.396] iteration 15340: total_loss: 0.287343, loss_sup: 0.015235, loss_mps: 0.097295, loss_cps: 0.174812
[13:25:45.542] iteration 15341: total_loss: 0.389227, loss_sup: 0.128950, loss_mps: 0.095235, loss_cps: 0.165041
[13:25:45.689] iteration 15342: total_loss: 0.441893, loss_sup: 0.134160, loss_mps: 0.102309, loss_cps: 0.205424
[13:25:45.836] iteration 15343: total_loss: 0.577904, loss_sup: 0.077862, loss_mps: 0.162190, loss_cps: 0.337851
[13:25:45.982] iteration 15344: total_loss: 0.300008, loss_sup: 0.058670, loss_mps: 0.088462, loss_cps: 0.152876
[13:25:46.128] iteration 15345: total_loss: 0.344289, loss_sup: 0.014519, loss_mps: 0.116381, loss_cps: 0.213389
[13:25:46.274] iteration 15346: total_loss: 0.340952, loss_sup: 0.017714, loss_mps: 0.113892, loss_cps: 0.209347
[13:25:46.419] iteration 15347: total_loss: 0.600269, loss_sup: 0.152428, loss_mps: 0.150694, loss_cps: 0.297147
[13:25:46.566] iteration 15348: total_loss: 0.511681, loss_sup: 0.157920, loss_mps: 0.121712, loss_cps: 0.232049
[13:25:46.712] iteration 15349: total_loss: 0.352087, loss_sup: 0.154937, loss_mps: 0.072668, loss_cps: 0.124482
[13:25:46.857] iteration 15350: total_loss: 0.356421, loss_sup: 0.091337, loss_mps: 0.093462, loss_cps: 0.171623
[13:25:47.004] iteration 15351: total_loss: 0.690375, loss_sup: 0.101182, loss_mps: 0.186866, loss_cps: 0.402327
[13:25:47.152] iteration 15352: total_loss: 0.182891, loss_sup: 0.010775, loss_mps: 0.065221, loss_cps: 0.106895
[13:25:47.297] iteration 15353: total_loss: 0.320803, loss_sup: 0.076218, loss_mps: 0.088806, loss_cps: 0.155779
[13:25:47.443] iteration 15354: total_loss: 0.417328, loss_sup: 0.193733, loss_mps: 0.074926, loss_cps: 0.148668
[13:25:47.591] iteration 15355: total_loss: 0.359285, loss_sup: 0.034263, loss_mps: 0.107030, loss_cps: 0.217991
[13:25:47.737] iteration 15356: total_loss: 0.198852, loss_sup: 0.009911, loss_mps: 0.068583, loss_cps: 0.120359
[13:25:47.886] iteration 15357: total_loss: 0.364273, loss_sup: 0.054215, loss_mps: 0.103038, loss_cps: 0.207020
[13:25:48.032] iteration 15358: total_loss: 0.446944, loss_sup: 0.020226, loss_mps: 0.134234, loss_cps: 0.292484
[13:25:48.179] iteration 15359: total_loss: 0.392472, loss_sup: 0.014821, loss_mps: 0.118493, loss_cps: 0.259159
[13:25:48.325] iteration 15360: total_loss: 0.460133, loss_sup: 0.071950, loss_mps: 0.123440, loss_cps: 0.264743
[13:25:48.471] iteration 15361: total_loss: 0.425446, loss_sup: 0.053271, loss_mps: 0.127521, loss_cps: 0.244654
[13:25:48.616] iteration 15362: total_loss: 0.421133, loss_sup: 0.039271, loss_mps: 0.121118, loss_cps: 0.260743
[13:25:48.764] iteration 15363: total_loss: 0.439905, loss_sup: 0.069360, loss_mps: 0.125709, loss_cps: 0.244836
[13:25:48.910] iteration 15364: total_loss: 0.232008, loss_sup: 0.062720, loss_mps: 0.060815, loss_cps: 0.108473
[13:25:49.060] iteration 15365: total_loss: 0.431178, loss_sup: 0.048805, loss_mps: 0.130906, loss_cps: 0.251467
[13:25:49.207] iteration 15366: total_loss: 0.391270, loss_sup: 0.031468, loss_mps: 0.122778, loss_cps: 0.237024
[13:25:49.357] iteration 15367: total_loss: 0.811451, loss_sup: 0.177960, loss_mps: 0.198539, loss_cps: 0.434952
[13:25:49.504] iteration 15368: total_loss: 0.316452, loss_sup: 0.054658, loss_mps: 0.088525, loss_cps: 0.173269
[13:25:49.652] iteration 15369: total_loss: 0.347005, loss_sup: 0.037771, loss_mps: 0.101514, loss_cps: 0.207720
[13:25:49.800] iteration 15370: total_loss: 0.511386, loss_sup: 0.043727, loss_mps: 0.152641, loss_cps: 0.315017
[13:25:49.947] iteration 15371: total_loss: 0.399922, loss_sup: 0.125919, loss_mps: 0.101311, loss_cps: 0.172692
[13:25:50.094] iteration 15372: total_loss: 0.616644, loss_sup: 0.121408, loss_mps: 0.156157, loss_cps: 0.339079
[13:25:50.240] iteration 15373: total_loss: 0.437292, loss_sup: 0.013306, loss_mps: 0.136643, loss_cps: 0.287342
[13:25:50.386] iteration 15374: total_loss: 0.407371, loss_sup: 0.094474, loss_mps: 0.111470, loss_cps: 0.201427
[13:25:50.532] iteration 15375: total_loss: 0.473956, loss_sup: 0.271771, loss_mps: 0.075054, loss_cps: 0.127130
[13:25:50.678] iteration 15376: total_loss: 0.347494, loss_sup: 0.054106, loss_mps: 0.097208, loss_cps: 0.196179
[13:25:50.826] iteration 15377: total_loss: 0.294175, loss_sup: 0.021201, loss_mps: 0.096245, loss_cps: 0.176728
[13:25:50.973] iteration 15378: total_loss: 0.250529, loss_sup: 0.042185, loss_mps: 0.074822, loss_cps: 0.133522
[13:25:51.119] iteration 15379: total_loss: 0.474881, loss_sup: 0.168744, loss_mps: 0.111512, loss_cps: 0.194625
[13:25:51.267] iteration 15380: total_loss: 0.361276, loss_sup: 0.063676, loss_mps: 0.105278, loss_cps: 0.192322
[13:25:51.413] iteration 15381: total_loss: 0.270948, loss_sup: 0.018090, loss_mps: 0.092782, loss_cps: 0.160077
[13:25:51.559] iteration 15382: total_loss: 0.223306, loss_sup: 0.040093, loss_mps: 0.064721, loss_cps: 0.118492
[13:25:51.705] iteration 15383: total_loss: 0.605893, loss_sup: 0.306239, loss_mps: 0.107393, loss_cps: 0.192262
[13:25:51.851] iteration 15384: total_loss: 0.459061, loss_sup: 0.191716, loss_mps: 0.099396, loss_cps: 0.167949
[13:25:51.997] iteration 15385: total_loss: 0.281631, loss_sup: 0.041494, loss_mps: 0.090435, loss_cps: 0.149703
[13:25:52.143] iteration 15386: total_loss: 0.382648, loss_sup: 0.189536, loss_mps: 0.069372, loss_cps: 0.123739
[13:25:52.289] iteration 15387: total_loss: 0.492202, loss_sup: 0.044444, loss_mps: 0.147192, loss_cps: 0.300566
[13:25:52.434] iteration 15388: total_loss: 0.711476, loss_sup: 0.300310, loss_mps: 0.142316, loss_cps: 0.268849
[13:25:52.580] iteration 15389: total_loss: 0.253402, loss_sup: 0.067141, loss_mps: 0.072764, loss_cps: 0.113496
[13:25:52.726] iteration 15390: total_loss: 0.239088, loss_sup: 0.005445, loss_mps: 0.084476, loss_cps: 0.149167
[13:25:52.874] iteration 15391: total_loss: 0.230892, loss_sup: 0.006440, loss_mps: 0.082881, loss_cps: 0.141571
[13:25:53.021] iteration 15392: total_loss: 0.481170, loss_sup: 0.031741, loss_mps: 0.155957, loss_cps: 0.293471
[13:25:53.167] iteration 15393: total_loss: 0.283008, loss_sup: 0.028504, loss_mps: 0.094070, loss_cps: 0.160434
[13:25:53.312] iteration 15394: total_loss: 0.423229, loss_sup: 0.070279, loss_mps: 0.120240, loss_cps: 0.232711
[13:25:53.458] iteration 15395: total_loss: 0.405417, loss_sup: 0.165958, loss_mps: 0.085441, loss_cps: 0.154018
[13:25:53.604] iteration 15396: total_loss: 0.665968, loss_sup: 0.053321, loss_mps: 0.191932, loss_cps: 0.420715
[13:25:53.751] iteration 15397: total_loss: 0.374669, loss_sup: 0.028169, loss_mps: 0.121634, loss_cps: 0.224866
[13:25:53.898] iteration 15398: total_loss: 0.366015, loss_sup: 0.030409, loss_mps: 0.114702, loss_cps: 0.220905
[13:25:54.044] iteration 15399: total_loss: 0.527875, loss_sup: 0.097767, loss_mps: 0.147961, loss_cps: 0.282147
[13:25:54.191] iteration 15400: total_loss: 0.509011, loss_sup: 0.269599, loss_mps: 0.092170, loss_cps: 0.147242
[13:25:54.192] Evaluation Started ==>
[13:26:05.587] ==> valid iteration 15400: unet metrics: {'dc': 0.6260128893496197, 'jc': 0.510044339317743, 'pre': 0.7932111758348334, 'hd': 5.40545962613466}, ynet metrics: {'dc': 0.6133117540549426, 'jc': 0.4982553049289102, 'pre': 0.7714479599662428, 'hd': 5.619880238908494}.
[13:26:05.589] Evaluation Finished!⏹️
[13:26:05.742] iteration 15401: total_loss: 0.277359, loss_sup: 0.138770, loss_mps: 0.053918, loss_cps: 0.084671
[13:26:05.889] iteration 15402: total_loss: 0.278745, loss_sup: 0.013348, loss_mps: 0.094145, loss_cps: 0.171251
[13:26:06.036] iteration 15403: total_loss: 0.273640, loss_sup: 0.033772, loss_mps: 0.089870, loss_cps: 0.149998
[13:26:06.183] iteration 15404: total_loss: 0.296457, loss_sup: 0.012824, loss_mps: 0.100812, loss_cps: 0.182820
[13:26:06.330] iteration 15405: total_loss: 0.467449, loss_sup: 0.149416, loss_mps: 0.108100, loss_cps: 0.209933
[13:26:06.477] iteration 15406: total_loss: 0.436629, loss_sup: 0.079971, loss_mps: 0.115133, loss_cps: 0.241525
[13:26:06.622] iteration 15407: total_loss: 0.463910, loss_sup: 0.088722, loss_mps: 0.118788, loss_cps: 0.256400
[13:26:06.769] iteration 15408: total_loss: 0.715319, loss_sup: 0.062390, loss_mps: 0.207485, loss_cps: 0.445444
[13:26:06.914] iteration 15409: total_loss: 0.451352, loss_sup: 0.018095, loss_mps: 0.138570, loss_cps: 0.294687
[13:26:07.062] iteration 15410: total_loss: 0.312569, loss_sup: 0.046590, loss_mps: 0.091679, loss_cps: 0.174299
[13:26:07.209] iteration 15411: total_loss: 0.628520, loss_sup: 0.049000, loss_mps: 0.188134, loss_cps: 0.391386
[13:26:07.356] iteration 15412: total_loss: 0.546835, loss_sup: 0.076219, loss_mps: 0.157515, loss_cps: 0.313101
[13:26:07.503] iteration 15413: total_loss: 0.375150, loss_sup: 0.096912, loss_mps: 0.094383, loss_cps: 0.183855
[13:26:07.650] iteration 15414: total_loss: 0.266868, loss_sup: 0.013239, loss_mps: 0.091515, loss_cps: 0.162113
[13:26:07.796] iteration 15415: total_loss: 0.308000, loss_sup: 0.029147, loss_mps: 0.099321, loss_cps: 0.179532
[13:26:07.942] iteration 15416: total_loss: 0.336240, loss_sup: 0.104323, loss_mps: 0.086794, loss_cps: 0.145123
[13:26:08.088] iteration 15417: total_loss: 0.319050, loss_sup: 0.062663, loss_mps: 0.090346, loss_cps: 0.166041
[13:26:08.234] iteration 15418: total_loss: 0.543225, loss_sup: 0.059541, loss_mps: 0.156803, loss_cps: 0.326882
[13:26:08.380] iteration 15419: total_loss: 0.339201, loss_sup: 0.065179, loss_mps: 0.097309, loss_cps: 0.176714
[13:26:08.526] iteration 15420: total_loss: 0.685869, loss_sup: 0.064362, loss_mps: 0.198008, loss_cps: 0.423499
[13:26:08.673] iteration 15421: total_loss: 0.367528, loss_sup: 0.024587, loss_mps: 0.116338, loss_cps: 0.226603
[13:26:08.819] iteration 15422: total_loss: 0.343271, loss_sup: 0.030651, loss_mps: 0.109352, loss_cps: 0.203268
[13:26:08.965] iteration 15423: total_loss: 0.366132, loss_sup: 0.012781, loss_mps: 0.118623, loss_cps: 0.234728
[13:26:09.110] iteration 15424: total_loss: 0.588575, loss_sup: 0.236704, loss_mps: 0.126527, loss_cps: 0.225345
[13:26:09.258] iteration 15425: total_loss: 0.296831, loss_sup: 0.010738, loss_mps: 0.098496, loss_cps: 0.187597
[13:26:09.403] iteration 15426: total_loss: 0.425374, loss_sup: 0.071317, loss_mps: 0.117106, loss_cps: 0.236952
[13:26:09.549] iteration 15427: total_loss: 0.427820, loss_sup: 0.044185, loss_mps: 0.132356, loss_cps: 0.251279
[13:26:09.695] iteration 15428: total_loss: 0.338269, loss_sup: 0.073388, loss_mps: 0.095054, loss_cps: 0.169827
[13:26:09.840] iteration 15429: total_loss: 0.479494, loss_sup: 0.198118, loss_mps: 0.107427, loss_cps: 0.173950
[13:26:09.986] iteration 15430: total_loss: 0.356543, loss_sup: 0.079593, loss_mps: 0.100140, loss_cps: 0.176811
[13:26:10.131] iteration 15431: total_loss: 0.256208, loss_sup: 0.016192, loss_mps: 0.083929, loss_cps: 0.156087
[13:26:10.280] iteration 15432: total_loss: 0.294561, loss_sup: 0.021772, loss_mps: 0.099330, loss_cps: 0.173459
[13:26:10.427] iteration 15433: total_loss: 0.268208, loss_sup: 0.005252, loss_mps: 0.090513, loss_cps: 0.172443
[13:26:10.573] iteration 15434: total_loss: 0.419204, loss_sup: 0.115780, loss_mps: 0.110461, loss_cps: 0.192964
[13:26:10.721] iteration 15435: total_loss: 0.451935, loss_sup: 0.016699, loss_mps: 0.144466, loss_cps: 0.290770
[13:26:10.873] iteration 15436: total_loss: 0.602018, loss_sup: 0.067436, loss_mps: 0.163905, loss_cps: 0.370677
[13:26:11.019] iteration 15437: total_loss: 0.470410, loss_sup: 0.025424, loss_mps: 0.142504, loss_cps: 0.302482
[13:26:11.165] iteration 15438: total_loss: 0.734350, loss_sup: 0.102938, loss_mps: 0.196939, loss_cps: 0.434473
[13:26:11.313] iteration 15439: total_loss: 0.489658, loss_sup: 0.034168, loss_mps: 0.151042, loss_cps: 0.304448
[13:26:11.460] iteration 15440: total_loss: 0.532429, loss_sup: 0.153662, loss_mps: 0.132048, loss_cps: 0.246719
[13:26:11.606] iteration 15441: total_loss: 0.404803, loss_sup: 0.099366, loss_mps: 0.103890, loss_cps: 0.201548
[13:26:11.752] iteration 15442: total_loss: 0.347893, loss_sup: 0.044212, loss_mps: 0.104966, loss_cps: 0.198715
[13:26:11.901] iteration 15443: total_loss: 0.282338, loss_sup: 0.060445, loss_mps: 0.084018, loss_cps: 0.137876
[13:26:12.047] iteration 15444: total_loss: 0.534510, loss_sup: 0.119248, loss_mps: 0.136308, loss_cps: 0.278953
[13:26:12.193] iteration 15445: total_loss: 0.667743, loss_sup: 0.135991, loss_mps: 0.175928, loss_cps: 0.355824
[13:26:12.341] iteration 15446: total_loss: 0.369001, loss_sup: 0.137782, loss_mps: 0.084463, loss_cps: 0.146757
[13:26:12.487] iteration 15447: total_loss: 0.330033, loss_sup: 0.055200, loss_mps: 0.095329, loss_cps: 0.179503
[13:26:12.633] iteration 15448: total_loss: 0.171350, loss_sup: 0.014578, loss_mps: 0.056056, loss_cps: 0.100716
[13:26:12.780] iteration 15449: total_loss: 1.114154, loss_sup: 0.225873, loss_mps: 0.263734, loss_cps: 0.624547
[13:26:12.926] iteration 15450: total_loss: 0.404982, loss_sup: 0.025902, loss_mps: 0.124187, loss_cps: 0.254892
[13:26:13.071] iteration 15451: total_loss: 0.382257, loss_sup: 0.023020, loss_mps: 0.121527, loss_cps: 0.237711
[13:26:13.217] iteration 15452: total_loss: 0.500821, loss_sup: 0.171872, loss_mps: 0.111492, loss_cps: 0.217456
[13:26:13.365] iteration 15453: total_loss: 0.617101, loss_sup: 0.207111, loss_mps: 0.138529, loss_cps: 0.271460
[13:26:13.511] iteration 15454: total_loss: 0.483627, loss_sup: 0.057136, loss_mps: 0.136474, loss_cps: 0.290017
[13:26:13.656] iteration 15455: total_loss: 0.662515, loss_sup: 0.264555, loss_mps: 0.136178, loss_cps: 0.261782
[13:26:13.802] iteration 15456: total_loss: 0.599707, loss_sup: 0.056050, loss_mps: 0.169890, loss_cps: 0.373766
[13:26:13.948] iteration 15457: total_loss: 0.800698, loss_sup: 0.183748, loss_mps: 0.196157, loss_cps: 0.420792
[13:26:14.094] iteration 15458: total_loss: 0.499559, loss_sup: 0.012635, loss_mps: 0.153023, loss_cps: 0.333900
[13:26:14.239] iteration 15459: total_loss: 0.520731, loss_sup: 0.036497, loss_mps: 0.152726, loss_cps: 0.331509
[13:26:14.385] iteration 15460: total_loss: 0.482592, loss_sup: 0.073075, loss_mps: 0.133355, loss_cps: 0.276162
[13:26:14.531] iteration 15461: total_loss: 0.379273, loss_sup: 0.077595, loss_mps: 0.108930, loss_cps: 0.192748
[13:26:14.677] iteration 15462: total_loss: 0.441092, loss_sup: 0.043764, loss_mps: 0.137245, loss_cps: 0.260083
[13:26:14.823] iteration 15463: total_loss: 0.475429, loss_sup: 0.115097, loss_mps: 0.120945, loss_cps: 0.239387
[13:26:14.969] iteration 15464: total_loss: 0.567529, loss_sup: 0.083333, loss_mps: 0.149636, loss_cps: 0.334560
[13:26:15.115] iteration 15465: total_loss: 0.421822, loss_sup: 0.101257, loss_mps: 0.105831, loss_cps: 0.214734
[13:26:15.177] iteration 15466: total_loss: 0.690867, loss_sup: 0.113929, loss_mps: 0.183633, loss_cps: 0.393305
[13:26:16.404] iteration 15467: total_loss: 0.709309, loss_sup: 0.026187, loss_mps: 0.220332, loss_cps: 0.462790
[13:26:16.552] iteration 15468: total_loss: 0.557167, loss_sup: 0.083116, loss_mps: 0.154990, loss_cps: 0.319061
[13:26:16.699] iteration 15469: total_loss: 0.447453, loss_sup: 0.108515, loss_mps: 0.120313, loss_cps: 0.218624
[13:26:16.847] iteration 15470: total_loss: 0.318793, loss_sup: 0.052119, loss_mps: 0.098320, loss_cps: 0.168354
[13:26:16.993] iteration 15471: total_loss: 0.549610, loss_sup: 0.011704, loss_mps: 0.165268, loss_cps: 0.372638
[13:26:17.139] iteration 15472: total_loss: 0.725690, loss_sup: 0.102443, loss_mps: 0.199760, loss_cps: 0.423487
[13:26:17.285] iteration 15473: total_loss: 0.446202, loss_sup: 0.018530, loss_mps: 0.141095, loss_cps: 0.286578
[13:26:17.431] iteration 15474: total_loss: 0.288076, loss_sup: 0.056345, loss_mps: 0.087854, loss_cps: 0.143877
[13:26:17.577] iteration 15475: total_loss: 0.603420, loss_sup: 0.077532, loss_mps: 0.172284, loss_cps: 0.353604
[13:26:17.722] iteration 15476: total_loss: 0.481894, loss_sup: 0.069123, loss_mps: 0.138133, loss_cps: 0.274638
[13:26:17.868] iteration 15477: total_loss: 0.578322, loss_sup: 0.082182, loss_mps: 0.170120, loss_cps: 0.326020
[13:26:18.014] iteration 15478: total_loss: 0.417658, loss_sup: 0.071279, loss_mps: 0.123606, loss_cps: 0.222773
[13:26:18.160] iteration 15479: total_loss: 0.510593, loss_sup: 0.165514, loss_mps: 0.126350, loss_cps: 0.218729
[13:26:18.306] iteration 15480: total_loss: 0.761718, loss_sup: 0.232319, loss_mps: 0.178739, loss_cps: 0.350661
[13:26:18.454] iteration 15481: total_loss: 0.511624, loss_sup: 0.049587, loss_mps: 0.152767, loss_cps: 0.309271
[13:26:18.600] iteration 15482: total_loss: 0.523729, loss_sup: 0.081997, loss_mps: 0.148709, loss_cps: 0.293022
[13:26:18.745] iteration 15483: total_loss: 0.640095, loss_sup: 0.198970, loss_mps: 0.146520, loss_cps: 0.294605
[13:26:18.891] iteration 15484: total_loss: 0.503068, loss_sup: 0.028976, loss_mps: 0.158630, loss_cps: 0.315463
[13:26:19.040] iteration 15485: total_loss: 0.349819, loss_sup: 0.076929, loss_mps: 0.095540, loss_cps: 0.177351
[13:26:19.186] iteration 15486: total_loss: 0.459890, loss_sup: 0.083197, loss_mps: 0.136260, loss_cps: 0.240433
[13:26:19.332] iteration 15487: total_loss: 0.518742, loss_sup: 0.079831, loss_mps: 0.144135, loss_cps: 0.294776
[13:26:19.479] iteration 15488: total_loss: 1.027707, loss_sup: 0.207740, loss_mps: 0.255653, loss_cps: 0.564314
[13:26:19.629] iteration 15489: total_loss: 0.494299, loss_sup: 0.117816, loss_mps: 0.129738, loss_cps: 0.246745
[13:26:19.775] iteration 15490: total_loss: 0.905770, loss_sup: 0.196648, loss_mps: 0.221116, loss_cps: 0.488006
[13:26:19.921] iteration 15491: total_loss: 0.447322, loss_sup: 0.070710, loss_mps: 0.128091, loss_cps: 0.248521
[13:26:20.068] iteration 15492: total_loss: 0.609677, loss_sup: 0.101536, loss_mps: 0.172207, loss_cps: 0.335934
[13:26:20.214] iteration 15493: total_loss: 0.532690, loss_sup: 0.138231, loss_mps: 0.139184, loss_cps: 0.255275
[13:26:20.362] iteration 15494: total_loss: 0.337136, loss_sup: 0.065863, loss_mps: 0.101870, loss_cps: 0.169403
[13:26:20.510] iteration 15495: total_loss: 0.315703, loss_sup: 0.021330, loss_mps: 0.097357, loss_cps: 0.197016
[13:26:20.656] iteration 15496: total_loss: 0.644322, loss_sup: 0.242835, loss_mps: 0.133023, loss_cps: 0.268464
[13:26:20.803] iteration 15497: total_loss: 0.586447, loss_sup: 0.088741, loss_mps: 0.166323, loss_cps: 0.331382
[13:26:20.949] iteration 15498: total_loss: 0.682059, loss_sup: 0.210882, loss_mps: 0.168117, loss_cps: 0.303060
[13:26:21.096] iteration 15499: total_loss: 0.678879, loss_sup: 0.161643, loss_mps: 0.173815, loss_cps: 0.343421
[13:26:21.241] iteration 15500: total_loss: 0.461037, loss_sup: 0.107675, loss_mps: 0.122747, loss_cps: 0.230615
[13:26:21.242] Evaluation Started ==>
[13:26:32.583] ==> valid iteration 15500: unet metrics: {'dc': 0.6389534666912952, 'jc': 0.5213665664085328, 'pre': 0.7894672077011519, 'hd': 5.643217096474207}, ynet metrics: {'dc': 0.5487875772040507, 'jc': 0.4407699588930325, 'pre': 0.782053863167878, 'hd': 5.705045248060641}.
[13:26:32.586] Evaluation Finished!⏹️
[13:26:32.740] iteration 15501: total_loss: 0.469431, loss_sup: 0.113661, loss_mps: 0.127696, loss_cps: 0.228073
[13:26:32.888] iteration 15502: total_loss: 0.429511, loss_sup: 0.079864, loss_mps: 0.118071, loss_cps: 0.231576
[13:26:33.034] iteration 15503: total_loss: 0.702645, loss_sup: 0.058148, loss_mps: 0.200343, loss_cps: 0.444154
[13:26:33.179] iteration 15504: total_loss: 0.526178, loss_sup: 0.113782, loss_mps: 0.143834, loss_cps: 0.268562
[13:26:33.324] iteration 15505: total_loss: 0.863178, loss_sup: 0.154505, loss_mps: 0.237276, loss_cps: 0.471398
[13:26:33.470] iteration 15506: total_loss: 0.332570, loss_sup: 0.046509, loss_mps: 0.102798, loss_cps: 0.183262
[13:26:33.615] iteration 15507: total_loss: 0.614114, loss_sup: 0.152830, loss_mps: 0.154527, loss_cps: 0.306757
[13:26:33.761] iteration 15508: total_loss: 0.647991, loss_sup: 0.085447, loss_mps: 0.177140, loss_cps: 0.385404
[13:26:33.911] iteration 15509: total_loss: 0.643593, loss_sup: 0.244157, loss_mps: 0.141688, loss_cps: 0.257748
[13:26:34.058] iteration 15510: total_loss: 0.352939, loss_sup: 0.016040, loss_mps: 0.120295, loss_cps: 0.216604
[13:26:34.208] iteration 15511: total_loss: 0.412078, loss_sup: 0.049381, loss_mps: 0.126148, loss_cps: 0.236549
[13:26:34.354] iteration 15512: total_loss: 0.500633, loss_sup: 0.094874, loss_mps: 0.123958, loss_cps: 0.281801
[13:26:34.500] iteration 15513: total_loss: 0.275634, loss_sup: 0.025870, loss_mps: 0.093016, loss_cps: 0.156748
[13:26:34.645] iteration 15514: total_loss: 0.369166, loss_sup: 0.041494, loss_mps: 0.118611, loss_cps: 0.209061
[13:26:34.793] iteration 15515: total_loss: 0.307839, loss_sup: 0.033043, loss_mps: 0.097295, loss_cps: 0.177502
[13:26:34.944] iteration 15516: total_loss: 0.384285, loss_sup: 0.018286, loss_mps: 0.126541, loss_cps: 0.239458
[13:26:35.090] iteration 15517: total_loss: 0.341224, loss_sup: 0.026766, loss_mps: 0.111624, loss_cps: 0.202834
[13:26:35.237] iteration 15518: total_loss: 0.435522, loss_sup: 0.036787, loss_mps: 0.132374, loss_cps: 0.266360
[13:26:35.382] iteration 15519: total_loss: 0.487571, loss_sup: 0.111173, loss_mps: 0.135832, loss_cps: 0.240565
[13:26:35.528] iteration 15520: total_loss: 0.458559, loss_sup: 0.112060, loss_mps: 0.120258, loss_cps: 0.226241
[13:26:35.674] iteration 15521: total_loss: 0.648575, loss_sup: 0.151217, loss_mps: 0.160561, loss_cps: 0.336797
[13:26:35.821] iteration 15522: total_loss: 0.424005, loss_sup: 0.101684, loss_mps: 0.106642, loss_cps: 0.215678
[13:26:35.967] iteration 15523: total_loss: 0.663885, loss_sup: 0.079454, loss_mps: 0.186555, loss_cps: 0.397875
[13:26:36.117] iteration 15524: total_loss: 0.273170, loss_sup: 0.017783, loss_mps: 0.089069, loss_cps: 0.166318
[13:26:36.263] iteration 15525: total_loss: 0.601373, loss_sup: 0.064783, loss_mps: 0.170059, loss_cps: 0.366531
[13:26:36.409] iteration 15526: total_loss: 0.481422, loss_sup: 0.096155, loss_mps: 0.125122, loss_cps: 0.260146
[13:26:36.555] iteration 15527: total_loss: 0.387352, loss_sup: 0.091560, loss_mps: 0.102094, loss_cps: 0.193699
[13:26:36.700] iteration 15528: total_loss: 0.598248, loss_sup: 0.035621, loss_mps: 0.173714, loss_cps: 0.388913
[13:26:36.846] iteration 15529: total_loss: 0.327564, loss_sup: 0.038860, loss_mps: 0.102456, loss_cps: 0.186248
[13:26:36.992] iteration 15530: total_loss: 0.379973, loss_sup: 0.031881, loss_mps: 0.123201, loss_cps: 0.224890
[13:26:37.138] iteration 15531: total_loss: 0.309295, loss_sup: 0.050363, loss_mps: 0.094920, loss_cps: 0.164012
[13:26:37.283] iteration 15532: total_loss: 0.284118, loss_sup: 0.012285, loss_mps: 0.095666, loss_cps: 0.176167
[13:26:37.429] iteration 15533: total_loss: 0.462769, loss_sup: 0.028243, loss_mps: 0.142842, loss_cps: 0.291683
[13:26:37.574] iteration 15534: total_loss: 0.242248, loss_sup: 0.010248, loss_mps: 0.084186, loss_cps: 0.147814
[13:26:37.720] iteration 15535: total_loss: 0.198409, loss_sup: 0.015186, loss_mps: 0.065468, loss_cps: 0.117755
[13:26:37.865] iteration 15536: total_loss: 0.385191, loss_sup: 0.039151, loss_mps: 0.113526, loss_cps: 0.232514
[13:26:38.012] iteration 15537: total_loss: 0.431467, loss_sup: 0.103165, loss_mps: 0.114713, loss_cps: 0.213589
[13:26:38.157] iteration 15538: total_loss: 0.481359, loss_sup: 0.150675, loss_mps: 0.112215, loss_cps: 0.218470
[13:26:38.302] iteration 15539: total_loss: 0.784511, loss_sup: 0.328626, loss_mps: 0.144262, loss_cps: 0.311622
[13:26:38.449] iteration 15540: total_loss: 0.281647, loss_sup: 0.017529, loss_mps: 0.093281, loss_cps: 0.170837
[13:26:38.598] iteration 15541: total_loss: 0.426044, loss_sup: 0.046559, loss_mps: 0.124336, loss_cps: 0.255149
[13:26:38.743] iteration 15542: total_loss: 0.769014, loss_sup: 0.014069, loss_mps: 0.240878, loss_cps: 0.514067
[13:26:38.889] iteration 15543: total_loss: 0.567411, loss_sup: 0.109343, loss_mps: 0.143004, loss_cps: 0.315063
[13:26:39.035] iteration 15544: total_loss: 0.564025, loss_sup: 0.054670, loss_mps: 0.168509, loss_cps: 0.340846
[13:26:39.180] iteration 15545: total_loss: 0.497588, loss_sup: 0.027373, loss_mps: 0.150501, loss_cps: 0.319714
[13:26:39.326] iteration 15546: total_loss: 0.509674, loss_sup: 0.070681, loss_mps: 0.139684, loss_cps: 0.299309
[13:26:39.474] iteration 15547: total_loss: 0.461270, loss_sup: 0.014776, loss_mps: 0.139850, loss_cps: 0.306644
[13:26:39.620] iteration 15548: total_loss: 0.481697, loss_sup: 0.023386, loss_mps: 0.145527, loss_cps: 0.312784
[13:26:39.768] iteration 15549: total_loss: 0.479896, loss_sup: 0.082789, loss_mps: 0.135108, loss_cps: 0.262000
[13:26:39.914] iteration 15550: total_loss: 0.276080, loss_sup: 0.061211, loss_mps: 0.077981, loss_cps: 0.136888
[13:26:40.059] iteration 15551: total_loss: 0.590804, loss_sup: 0.078596, loss_mps: 0.160873, loss_cps: 0.351336
[13:26:40.205] iteration 15552: total_loss: 0.328642, loss_sup: 0.035566, loss_mps: 0.101076, loss_cps: 0.192000
[13:26:40.350] iteration 15553: total_loss: 0.270359, loss_sup: 0.058731, loss_mps: 0.075907, loss_cps: 0.135722
[13:26:40.497] iteration 15554: total_loss: 0.459516, loss_sup: 0.086727, loss_mps: 0.119057, loss_cps: 0.253732
[13:26:40.644] iteration 15555: total_loss: 0.360507, loss_sup: 0.044872, loss_mps: 0.115173, loss_cps: 0.200462
[13:26:40.791] iteration 15556: total_loss: 0.393271, loss_sup: 0.063004, loss_mps: 0.106570, loss_cps: 0.223697
[13:26:40.937] iteration 15557: total_loss: 0.446350, loss_sup: 0.122893, loss_mps: 0.107866, loss_cps: 0.215591
[13:26:41.083] iteration 15558: total_loss: 0.597722, loss_sup: 0.090228, loss_mps: 0.175308, loss_cps: 0.332186
[13:26:41.230] iteration 15559: total_loss: 0.656277, loss_sup: 0.120733, loss_mps: 0.168515, loss_cps: 0.367029
[13:26:41.376] iteration 15560: total_loss: 0.704377, loss_sup: 0.258712, loss_mps: 0.143105, loss_cps: 0.302560
[13:26:41.522] iteration 15561: total_loss: 0.392178, loss_sup: 0.183469, loss_mps: 0.076544, loss_cps: 0.132165
[13:26:41.668] iteration 15562: total_loss: 0.449936, loss_sup: 0.128392, loss_mps: 0.112265, loss_cps: 0.209279
[13:26:41.815] iteration 15563: total_loss: 0.780244, loss_sup: 0.404656, loss_mps: 0.133797, loss_cps: 0.241791
[13:26:41.962] iteration 15564: total_loss: 0.761045, loss_sup: 0.221304, loss_mps: 0.180024, loss_cps: 0.359717
[13:26:42.108] iteration 15565: total_loss: 0.375873, loss_sup: 0.035554, loss_mps: 0.119049, loss_cps: 0.221269
[13:26:42.254] iteration 15566: total_loss: 0.352479, loss_sup: 0.042158, loss_mps: 0.103225, loss_cps: 0.207096
[13:26:42.402] iteration 15567: total_loss: 0.441573, loss_sup: 0.081841, loss_mps: 0.125530, loss_cps: 0.234202
[13:26:42.550] iteration 15568: total_loss: 0.431623, loss_sup: 0.087190, loss_mps: 0.112562, loss_cps: 0.231871
[13:26:42.696] iteration 15569: total_loss: 0.889434, loss_sup: 0.464646, loss_mps: 0.148244, loss_cps: 0.276544
[13:26:42.843] iteration 15570: total_loss: 0.516576, loss_sup: 0.121561, loss_mps: 0.143448, loss_cps: 0.251566
[13:26:42.990] iteration 15571: total_loss: 0.387804, loss_sup: 0.010198, loss_mps: 0.128408, loss_cps: 0.249198
[13:26:43.137] iteration 15572: total_loss: 0.337580, loss_sup: 0.023608, loss_mps: 0.112669, loss_cps: 0.201303
[13:26:43.284] iteration 15573: total_loss: 0.284531, loss_sup: 0.027469, loss_mps: 0.096107, loss_cps: 0.160955
[13:26:43.433] iteration 15574: total_loss: 0.308031, loss_sup: 0.033178, loss_mps: 0.106779, loss_cps: 0.168073
[13:26:43.579] iteration 15575: total_loss: 0.383094, loss_sup: 0.022239, loss_mps: 0.124188, loss_cps: 0.236666
[13:26:43.726] iteration 15576: total_loss: 0.328812, loss_sup: 0.057498, loss_mps: 0.097162, loss_cps: 0.174153
[13:26:43.873] iteration 15577: total_loss: 0.296815, loss_sup: 0.062554, loss_mps: 0.093329, loss_cps: 0.140932
[13:26:44.020] iteration 15578: total_loss: 0.520100, loss_sup: 0.010435, loss_mps: 0.176728, loss_cps: 0.332938
[13:26:44.166] iteration 15579: total_loss: 0.345347, loss_sup: 0.060820, loss_mps: 0.106706, loss_cps: 0.177820
[13:26:44.314] iteration 15580: total_loss: 0.595894, loss_sup: 0.068920, loss_mps: 0.174477, loss_cps: 0.352497
[13:26:44.460] iteration 15581: total_loss: 0.275647, loss_sup: 0.022322, loss_mps: 0.093967, loss_cps: 0.159358
[13:26:44.606] iteration 15582: total_loss: 0.503582, loss_sup: 0.039579, loss_mps: 0.155957, loss_cps: 0.308047
[13:26:44.753] iteration 15583: total_loss: 0.614085, loss_sup: 0.152657, loss_mps: 0.147723, loss_cps: 0.313705
[13:26:44.900] iteration 15584: total_loss: 0.567459, loss_sup: 0.152039, loss_mps: 0.141799, loss_cps: 0.273622
[13:26:45.050] iteration 15585: total_loss: 0.308530, loss_sup: 0.080741, loss_mps: 0.092490, loss_cps: 0.135299
[13:26:45.196] iteration 15586: total_loss: 0.249335, loss_sup: 0.008027, loss_mps: 0.089923, loss_cps: 0.151385
[13:26:45.345] iteration 15587: total_loss: 0.912629, loss_sup: 0.183789, loss_mps: 0.222879, loss_cps: 0.505961
[13:26:45.492] iteration 15588: total_loss: 0.473873, loss_sup: 0.134810, loss_mps: 0.116026, loss_cps: 0.223037
[13:26:45.638] iteration 15589: total_loss: 0.564078, loss_sup: 0.085733, loss_mps: 0.161706, loss_cps: 0.316639
[13:26:45.784] iteration 15590: total_loss: 0.238902, loss_sup: 0.014739, loss_mps: 0.083631, loss_cps: 0.140532
[13:26:45.931] iteration 15591: total_loss: 0.356488, loss_sup: 0.047700, loss_mps: 0.106222, loss_cps: 0.202566
[13:26:46.077] iteration 15592: total_loss: 0.235893, loss_sup: 0.037360, loss_mps: 0.077057, loss_cps: 0.121475
[13:26:46.224] iteration 15593: total_loss: 0.483770, loss_sup: 0.033021, loss_mps: 0.153302, loss_cps: 0.297447
[13:26:46.372] iteration 15594: total_loss: 0.477193, loss_sup: 0.127889, loss_mps: 0.120803, loss_cps: 0.228501
[13:26:46.519] iteration 15595: total_loss: 0.348202, loss_sup: 0.026642, loss_mps: 0.117820, loss_cps: 0.203740
[13:26:46.666] iteration 15596: total_loss: 0.407090, loss_sup: 0.045481, loss_mps: 0.126877, loss_cps: 0.234732
[13:26:46.812] iteration 15597: total_loss: 0.336478, loss_sup: 0.009350, loss_mps: 0.111828, loss_cps: 0.215300
[13:26:46.959] iteration 15598: total_loss: 0.618420, loss_sup: 0.093128, loss_mps: 0.175921, loss_cps: 0.349372
[13:26:47.105] iteration 15599: total_loss: 0.777009, loss_sup: 0.314810, loss_mps: 0.149439, loss_cps: 0.312760
[13:26:47.251] iteration 15600: total_loss: 0.370934, loss_sup: 0.077834, loss_mps: 0.100360, loss_cps: 0.192740
[13:26:47.252] Evaluation Started ==>
[13:26:58.598] ==> valid iteration 15600: unet metrics: {'dc': 0.6142555792409043, 'jc': 0.501094191283163, 'pre': 0.7606861435920225, 'hd': 5.528292495396964}, ynet metrics: {'dc': 0.5710358871435738, 'jc': 0.4589320091533097, 'pre': 0.7951551699228607, 'hd': 5.6200100431006}.
[13:26:58.600] Evaluation Finished!⏹️
[13:26:58.752] iteration 15601: total_loss: 0.493533, loss_sup: 0.080462, loss_mps: 0.138781, loss_cps: 0.274290
[13:26:58.900] iteration 15602: total_loss: 0.325669, loss_sup: 0.006703, loss_mps: 0.114405, loss_cps: 0.204561
[13:26:59.046] iteration 15603: total_loss: 0.584646, loss_sup: 0.208970, loss_mps: 0.128340, loss_cps: 0.247336
[13:26:59.191] iteration 15604: total_loss: 0.614648, loss_sup: 0.174282, loss_mps: 0.137966, loss_cps: 0.302400
[13:26:59.337] iteration 15605: total_loss: 0.568414, loss_sup: 0.102603, loss_mps: 0.157560, loss_cps: 0.308252
[13:26:59.484] iteration 15606: total_loss: 0.269492, loss_sup: 0.038725, loss_mps: 0.086186, loss_cps: 0.144581
[13:26:59.629] iteration 15607: total_loss: 0.432669, loss_sup: 0.093600, loss_mps: 0.113922, loss_cps: 0.225147
[13:26:59.774] iteration 15608: total_loss: 0.204585, loss_sup: 0.018756, loss_mps: 0.067302, loss_cps: 0.118527
[13:26:59.920] iteration 15609: total_loss: 0.385860, loss_sup: 0.131642, loss_mps: 0.088873, loss_cps: 0.165346
[13:27:00.066] iteration 15610: total_loss: 1.045155, loss_sup: 0.155952, loss_mps: 0.268063, loss_cps: 0.621140
[13:27:00.212] iteration 15611: total_loss: 0.533021, loss_sup: 0.112933, loss_mps: 0.136947, loss_cps: 0.283140
[13:27:00.360] iteration 15612: total_loss: 0.301925, loss_sup: 0.026617, loss_mps: 0.101505, loss_cps: 0.173803
[13:27:00.508] iteration 15613: total_loss: 0.431866, loss_sup: 0.053686, loss_mps: 0.126038, loss_cps: 0.252142
[13:27:00.653] iteration 15614: total_loss: 0.286139, loss_sup: 0.077443, loss_mps: 0.079128, loss_cps: 0.129567
[13:27:00.798] iteration 15615: total_loss: 0.459421, loss_sup: 0.112466, loss_mps: 0.115387, loss_cps: 0.231568
[13:27:00.944] iteration 15616: total_loss: 0.366049, loss_sup: 0.049949, loss_mps: 0.109633, loss_cps: 0.206467
[13:27:01.089] iteration 15617: total_loss: 0.531221, loss_sup: 0.248438, loss_mps: 0.102664, loss_cps: 0.180120
[13:27:01.237] iteration 15618: total_loss: 0.980062, loss_sup: 0.139841, loss_mps: 0.245901, loss_cps: 0.594319
[13:27:01.383] iteration 15619: total_loss: 0.412623, loss_sup: 0.075828, loss_mps: 0.113358, loss_cps: 0.223437
[13:27:01.530] iteration 15620: total_loss: 0.735301, loss_sup: 0.066120, loss_mps: 0.208854, loss_cps: 0.460328
[13:27:01.676] iteration 15621: total_loss: 0.288847, loss_sup: 0.052317, loss_mps: 0.091517, loss_cps: 0.145013
[13:27:01.822] iteration 15622: total_loss: 0.453778, loss_sup: 0.036787, loss_mps: 0.145049, loss_cps: 0.271942
[13:27:01.968] iteration 15623: total_loss: 0.462016, loss_sup: 0.113597, loss_mps: 0.116676, loss_cps: 0.231744
[13:27:02.113] iteration 15624: total_loss: 0.418753, loss_sup: 0.045727, loss_mps: 0.121850, loss_cps: 0.251176
[13:27:02.260] iteration 15625: total_loss: 0.502059, loss_sup: 0.047284, loss_mps: 0.155892, loss_cps: 0.298883
[13:27:02.406] iteration 15626: total_loss: 0.238998, loss_sup: 0.040579, loss_mps: 0.073427, loss_cps: 0.124992
[13:27:02.552] iteration 15627: total_loss: 0.460674, loss_sup: 0.109876, loss_mps: 0.114805, loss_cps: 0.235993
[13:27:02.698] iteration 15628: total_loss: 0.312957, loss_sup: 0.024974, loss_mps: 0.102214, loss_cps: 0.185769
[13:27:02.848] iteration 15629: total_loss: 0.308129, loss_sup: 0.038934, loss_mps: 0.102223, loss_cps: 0.166972
[13:27:02.994] iteration 15630: total_loss: 0.438898, loss_sup: 0.043046, loss_mps: 0.138519, loss_cps: 0.257333
[13:27:03.140] iteration 15631: total_loss: 0.439792, loss_sup: 0.022132, loss_mps: 0.137287, loss_cps: 0.280373
[13:27:03.286] iteration 15632: total_loss: 0.290409, loss_sup: 0.024046, loss_mps: 0.095951, loss_cps: 0.170412
[13:27:03.431] iteration 15633: total_loss: 0.405833, loss_sup: 0.035860, loss_mps: 0.130942, loss_cps: 0.239030
[13:27:03.578] iteration 15634: total_loss: 0.498555, loss_sup: 0.129383, loss_mps: 0.125802, loss_cps: 0.243371
[13:27:03.724] iteration 15635: total_loss: 0.485564, loss_sup: 0.173652, loss_mps: 0.109246, loss_cps: 0.202666
[13:27:03.869] iteration 15636: total_loss: 0.381136, loss_sup: 0.087552, loss_mps: 0.109951, loss_cps: 0.183633
[13:27:04.019] iteration 15637: total_loss: 0.745718, loss_sup: 0.247240, loss_mps: 0.167280, loss_cps: 0.331198
[13:27:04.169] iteration 15638: total_loss: 0.316358, loss_sup: 0.053661, loss_mps: 0.091990, loss_cps: 0.170707
[13:27:04.317] iteration 15639: total_loss: 0.256009, loss_sup: 0.007390, loss_mps: 0.086132, loss_cps: 0.162487
[13:27:04.463] iteration 15640: total_loss: 0.338918, loss_sup: 0.004447, loss_mps: 0.117054, loss_cps: 0.217417
[13:27:04.609] iteration 15641: total_loss: 0.416696, loss_sup: 0.023737, loss_mps: 0.129319, loss_cps: 0.263639
[13:27:04.755] iteration 15642: total_loss: 0.485886, loss_sup: 0.114128, loss_mps: 0.121896, loss_cps: 0.249861
[13:27:04.900] iteration 15643: total_loss: 0.333317, loss_sup: 0.097891, loss_mps: 0.086617, loss_cps: 0.148809
[13:27:05.045] iteration 15644: total_loss: 0.438224, loss_sup: 0.180540, loss_mps: 0.087519, loss_cps: 0.170166
[13:27:05.192] iteration 15645: total_loss: 0.222464, loss_sup: 0.015422, loss_mps: 0.079493, loss_cps: 0.127548
[13:27:05.340] iteration 15646: total_loss: 0.472298, loss_sup: 0.174796, loss_mps: 0.110402, loss_cps: 0.187100
[13:27:05.486] iteration 15647: total_loss: 0.300280, loss_sup: 0.033899, loss_mps: 0.092203, loss_cps: 0.174178
[13:27:05.632] iteration 15648: total_loss: 0.692367, loss_sup: 0.025915, loss_mps: 0.214028, loss_cps: 0.452423
[13:27:05.777] iteration 15649: total_loss: 0.667798, loss_sup: 0.122596, loss_mps: 0.167624, loss_cps: 0.377578
[13:27:05.922] iteration 15650: total_loss: 0.450238, loss_sup: 0.073587, loss_mps: 0.126563, loss_cps: 0.250087
[13:27:06.069] iteration 15651: total_loss: 0.475707, loss_sup: 0.042168, loss_mps: 0.145323, loss_cps: 0.288217
[13:27:06.215] iteration 15652: total_loss: 0.860830, loss_sup: 0.140481, loss_mps: 0.226709, loss_cps: 0.493640
[13:27:06.360] iteration 15653: total_loss: 0.459352, loss_sup: 0.180309, loss_mps: 0.099370, loss_cps: 0.179674
[13:27:06.505] iteration 15654: total_loss: 0.585521, loss_sup: 0.085938, loss_mps: 0.162754, loss_cps: 0.336829
[13:27:06.651] iteration 15655: total_loss: 0.506095, loss_sup: 0.106234, loss_mps: 0.140500, loss_cps: 0.259361
[13:27:06.799] iteration 15656: total_loss: 0.666712, loss_sup: 0.311401, loss_mps: 0.123839, loss_cps: 0.231472
[13:27:06.945] iteration 15657: total_loss: 0.545205, loss_sup: 0.169784, loss_mps: 0.123371, loss_cps: 0.252050
[13:27:07.092] iteration 15658: total_loss: 0.327894, loss_sup: 0.048800, loss_mps: 0.096565, loss_cps: 0.182529
[13:27:07.238] iteration 15659: total_loss: 0.503835, loss_sup: 0.196358, loss_mps: 0.102599, loss_cps: 0.204878
[13:27:07.384] iteration 15660: total_loss: 0.321635, loss_sup: 0.047989, loss_mps: 0.096922, loss_cps: 0.176724
[13:27:07.529] iteration 15661: total_loss: 0.643801, loss_sup: 0.213087, loss_mps: 0.144644, loss_cps: 0.286069
[13:27:07.676] iteration 15662: total_loss: 0.392955, loss_sup: 0.029731, loss_mps: 0.129638, loss_cps: 0.233586
[13:27:07.821] iteration 15663: total_loss: 0.682270, loss_sup: 0.203880, loss_mps: 0.158479, loss_cps: 0.319911
[13:27:07.968] iteration 15664: total_loss: 0.516840, loss_sup: 0.079018, loss_mps: 0.142410, loss_cps: 0.295412
[13:27:08.114] iteration 15665: total_loss: 0.361356, loss_sup: 0.008723, loss_mps: 0.123253, loss_cps: 0.229380
[13:27:08.261] iteration 15666: total_loss: 0.609536, loss_sup: 0.034088, loss_mps: 0.189365, loss_cps: 0.386083
[13:27:08.409] iteration 15667: total_loss: 0.350293, loss_sup: 0.065459, loss_mps: 0.103709, loss_cps: 0.181126
[13:27:08.555] iteration 15668: total_loss: 0.936634, loss_sup: 0.479012, loss_mps: 0.156446, loss_cps: 0.301176
[13:27:08.703] iteration 15669: total_loss: 0.289291, loss_sup: 0.022574, loss_mps: 0.094184, loss_cps: 0.172533
[13:27:08.852] iteration 15670: total_loss: 0.593167, loss_sup: 0.198401, loss_mps: 0.140496, loss_cps: 0.254270
[13:27:08.998] iteration 15671: total_loss: 0.924260, loss_sup: 0.095147, loss_mps: 0.260139, loss_cps: 0.568973
[13:27:09.144] iteration 15672: total_loss: 0.540957, loss_sup: 0.158337, loss_mps: 0.132203, loss_cps: 0.250417
[13:27:09.290] iteration 15673: total_loss: 0.507366, loss_sup: 0.032136, loss_mps: 0.153331, loss_cps: 0.321900
[13:27:09.437] iteration 15674: total_loss: 0.523835, loss_sup: 0.127698, loss_mps: 0.138365, loss_cps: 0.257771
[13:27:09.583] iteration 15675: total_loss: 0.301228, loss_sup: 0.033445, loss_mps: 0.093794, loss_cps: 0.173990
[13:27:09.730] iteration 15676: total_loss: 0.290375, loss_sup: 0.014259, loss_mps: 0.094410, loss_cps: 0.181706
[13:27:09.877] iteration 15677: total_loss: 0.535930, loss_sup: 0.027056, loss_mps: 0.169424, loss_cps: 0.339450
[13:27:10.025] iteration 15678: total_loss: 0.461088, loss_sup: 0.055196, loss_mps: 0.139643, loss_cps: 0.266249
[13:27:10.171] iteration 15679: total_loss: 0.543494, loss_sup: 0.138884, loss_mps: 0.140197, loss_cps: 0.264412
[13:27:10.318] iteration 15680: total_loss: 0.691511, loss_sup: 0.256219, loss_mps: 0.150492, loss_cps: 0.284800
[13:27:10.464] iteration 15681: total_loss: 0.479550, loss_sup: 0.049587, loss_mps: 0.149851, loss_cps: 0.280112
[13:27:10.611] iteration 15682: total_loss: 0.234874, loss_sup: 0.012899, loss_mps: 0.083668, loss_cps: 0.138308
[13:27:10.758] iteration 15683: total_loss: 0.533406, loss_sup: 0.226854, loss_mps: 0.110941, loss_cps: 0.195611
[13:27:10.906] iteration 15684: total_loss: 0.357239, loss_sup: 0.085409, loss_mps: 0.101697, loss_cps: 0.170133
[13:27:11.052] iteration 15685: total_loss: 0.373796, loss_sup: 0.039692, loss_mps: 0.111195, loss_cps: 0.222908
[13:27:11.199] iteration 15686: total_loss: 0.446456, loss_sup: 0.119729, loss_mps: 0.113210, loss_cps: 0.213517
[13:27:11.345] iteration 15687: total_loss: 0.892076, loss_sup: 0.279770, loss_mps: 0.202960, loss_cps: 0.409346
[13:27:11.494] iteration 15688: total_loss: 0.574349, loss_sup: 0.170381, loss_mps: 0.148899, loss_cps: 0.255069
[13:27:11.640] iteration 15689: total_loss: 0.598385, loss_sup: 0.174271, loss_mps: 0.147713, loss_cps: 0.276401
[13:27:11.787] iteration 15690: total_loss: 0.392686, loss_sup: 0.065185, loss_mps: 0.114777, loss_cps: 0.212724
[13:27:11.934] iteration 15691: total_loss: 0.488352, loss_sup: 0.027419, loss_mps: 0.159306, loss_cps: 0.301627
[13:27:12.081] iteration 15692: total_loss: 0.473762, loss_sup: 0.092168, loss_mps: 0.129619, loss_cps: 0.251976
[13:27:12.229] iteration 15693: total_loss: 0.988234, loss_sup: 0.030462, loss_mps: 0.285790, loss_cps: 0.671982
[13:27:12.377] iteration 15694: total_loss: 0.329480, loss_sup: 0.077271, loss_mps: 0.094371, loss_cps: 0.157839
[13:27:12.524] iteration 15695: total_loss: 0.332487, loss_sup: 0.026666, loss_mps: 0.108145, loss_cps: 0.197676
[13:27:12.671] iteration 15696: total_loss: 0.408433, loss_sup: 0.076825, loss_mps: 0.124403, loss_cps: 0.207204
[13:27:12.818] iteration 15697: total_loss: 0.434365, loss_sup: 0.014197, loss_mps: 0.139737, loss_cps: 0.280431
[13:27:12.965] iteration 15698: total_loss: 0.407490, loss_sup: 0.125630, loss_mps: 0.103499, loss_cps: 0.178360
[13:27:13.112] iteration 15699: total_loss: 0.289525, loss_sup: 0.043940, loss_mps: 0.091448, loss_cps: 0.154137
[13:27:13.258] iteration 15700: total_loss: 0.463829, loss_sup: 0.016870, loss_mps: 0.145552, loss_cps: 0.301408
[13:27:13.258] Evaluation Started ==>
[13:27:24.625] ==> valid iteration 15700: unet metrics: {'dc': 0.6540643019200866, 'jc': 0.5359756377223323, 'pre': 0.7705555072762934, 'hd': 5.76205110816278}, ynet metrics: {'dc': 0.5563309076675267, 'jc': 0.44483911124835795, 'pre': 0.7864133930065323, 'hd': 5.685111671929998}.
[13:27:24.627] Evaluation Finished!⏹️
[13:27:24.778] iteration 15701: total_loss: 0.492285, loss_sup: 0.075590, loss_mps: 0.133090, loss_cps: 0.283605
[13:27:24.925] iteration 15702: total_loss: 0.482978, loss_sup: 0.107662, loss_mps: 0.127741, loss_cps: 0.247576
[13:27:25.070] iteration 15703: total_loss: 0.491885, loss_sup: 0.057117, loss_mps: 0.148764, loss_cps: 0.286004
[13:27:25.217] iteration 15704: total_loss: 0.348206, loss_sup: 0.008801, loss_mps: 0.119032, loss_cps: 0.220373
[13:27:25.362] iteration 15705: total_loss: 0.307011, loss_sup: 0.031422, loss_mps: 0.098688, loss_cps: 0.176901
[13:27:25.507] iteration 15706: total_loss: 0.423474, loss_sup: 0.043305, loss_mps: 0.131171, loss_cps: 0.248998
[13:27:25.653] iteration 15707: total_loss: 0.719864, loss_sup: 0.134603, loss_mps: 0.190345, loss_cps: 0.394915
[13:27:25.801] iteration 15708: total_loss: 0.403191, loss_sup: 0.063941, loss_mps: 0.121400, loss_cps: 0.217850
[13:27:25.946] iteration 15709: total_loss: 0.492233, loss_sup: 0.099109, loss_mps: 0.130934, loss_cps: 0.262190
[13:27:26.092] iteration 15710: total_loss: 0.292593, loss_sup: 0.033109, loss_mps: 0.091609, loss_cps: 0.167875
[13:27:26.238] iteration 15711: total_loss: 1.025689, loss_sup: 0.415310, loss_mps: 0.192152, loss_cps: 0.418226
[13:27:26.385] iteration 15712: total_loss: 0.394212, loss_sup: 0.142804, loss_mps: 0.090447, loss_cps: 0.160961
[13:27:26.533] iteration 15713: total_loss: 0.612299, loss_sup: 0.091142, loss_mps: 0.166785, loss_cps: 0.354372
[13:27:26.679] iteration 15714: total_loss: 0.827904, loss_sup: 0.272246, loss_mps: 0.181195, loss_cps: 0.374463
[13:27:26.825] iteration 15715: total_loss: 0.306746, loss_sup: 0.014837, loss_mps: 0.097478, loss_cps: 0.194432
[13:27:26.971] iteration 15716: total_loss: 0.378944, loss_sup: 0.013309, loss_mps: 0.121566, loss_cps: 0.244069
[13:27:27.117] iteration 15717: total_loss: 0.590757, loss_sup: 0.128102, loss_mps: 0.155356, loss_cps: 0.307299
[13:27:27.262] iteration 15718: total_loss: 0.507469, loss_sup: 0.189943, loss_mps: 0.105623, loss_cps: 0.211902
[13:27:27.411] iteration 15719: total_loss: 0.400731, loss_sup: 0.086070, loss_mps: 0.107631, loss_cps: 0.207030
[13:27:27.556] iteration 15720: total_loss: 0.696555, loss_sup: 0.044958, loss_mps: 0.199321, loss_cps: 0.452276
[13:27:27.701] iteration 15721: total_loss: 0.463247, loss_sup: 0.117837, loss_mps: 0.120910, loss_cps: 0.224500
[13:27:27.846] iteration 15722: total_loss: 0.364927, loss_sup: 0.040239, loss_mps: 0.110486, loss_cps: 0.214202
[13:27:27.995] iteration 15723: total_loss: 0.609358, loss_sup: 0.122834, loss_mps: 0.171704, loss_cps: 0.314820
[13:27:28.141] iteration 15724: total_loss: 1.329689, loss_sup: 0.436706, loss_mps: 0.280432, loss_cps: 0.612551
[13:27:28.286] iteration 15725: total_loss: 0.588408, loss_sup: 0.168320, loss_mps: 0.142969, loss_cps: 0.277119
[13:27:28.433] iteration 15726: total_loss: 0.376632, loss_sup: 0.078237, loss_mps: 0.107363, loss_cps: 0.191032
[13:27:28.579] iteration 15727: total_loss: 0.452899, loss_sup: 0.076560, loss_mps: 0.132786, loss_cps: 0.243553
[13:27:28.724] iteration 15728: total_loss: 0.572098, loss_sup: 0.280147, loss_mps: 0.113798, loss_cps: 0.178153
[13:27:28.870] iteration 15729: total_loss: 0.577301, loss_sup: 0.049922, loss_mps: 0.171893, loss_cps: 0.355486
[13:27:29.015] iteration 15730: total_loss: 0.168228, loss_sup: 0.005084, loss_mps: 0.063502, loss_cps: 0.099642
[13:27:29.163] iteration 15731: total_loss: 0.639360, loss_sup: 0.278369, loss_mps: 0.127873, loss_cps: 0.233118
[13:27:29.309] iteration 15732: total_loss: 0.336650, loss_sup: 0.061078, loss_mps: 0.101953, loss_cps: 0.173619
[13:27:29.454] iteration 15733: total_loss: 0.505029, loss_sup: 0.064940, loss_mps: 0.145001, loss_cps: 0.295089
[13:27:29.601] iteration 15734: total_loss: 0.510410, loss_sup: 0.063228, loss_mps: 0.151494, loss_cps: 0.295688
[13:27:29.746] iteration 15735: total_loss: 0.308310, loss_sup: 0.067979, loss_mps: 0.087362, loss_cps: 0.152969
[13:27:29.892] iteration 15736: total_loss: 0.404458, loss_sup: 0.075312, loss_mps: 0.115555, loss_cps: 0.213591
[13:27:30.037] iteration 15737: total_loss: 0.347208, loss_sup: 0.043595, loss_mps: 0.114617, loss_cps: 0.188996
[13:27:30.183] iteration 15738: total_loss: 0.449282, loss_sup: 0.088072, loss_mps: 0.121252, loss_cps: 0.239958
[13:27:30.328] iteration 15739: total_loss: 0.580325, loss_sup: 0.089894, loss_mps: 0.158035, loss_cps: 0.332396
[13:27:30.473] iteration 15740: total_loss: 0.428230, loss_sup: 0.014168, loss_mps: 0.139592, loss_cps: 0.274470
[13:27:30.619] iteration 15741: total_loss: 0.481723, loss_sup: 0.077493, loss_mps: 0.142347, loss_cps: 0.261883
[13:27:30.766] iteration 15742: total_loss: 0.323930, loss_sup: 0.062482, loss_mps: 0.095995, loss_cps: 0.165453
[13:27:30.913] iteration 15743: total_loss: 0.501060, loss_sup: 0.120254, loss_mps: 0.135575, loss_cps: 0.245231
[13:27:31.058] iteration 15744: total_loss: 0.251881, loss_sup: 0.035287, loss_mps: 0.079956, loss_cps: 0.136638
[13:27:31.203] iteration 15745: total_loss: 0.345692, loss_sup: 0.045029, loss_mps: 0.110758, loss_cps: 0.189905
[13:27:31.349] iteration 15746: total_loss: 0.554134, loss_sup: 0.158687, loss_mps: 0.133221, loss_cps: 0.262226
[13:27:31.496] iteration 15747: total_loss: 0.408867, loss_sup: 0.071127, loss_mps: 0.124718, loss_cps: 0.213022
[13:27:31.642] iteration 15748: total_loss: 0.401106, loss_sup: 0.039753, loss_mps: 0.120768, loss_cps: 0.240585
[13:27:31.787] iteration 15749: total_loss: 0.300431, loss_sup: 0.056818, loss_mps: 0.087871, loss_cps: 0.155741
[13:27:31.932] iteration 15750: total_loss: 0.742861, loss_sup: 0.080970, loss_mps: 0.208400, loss_cps: 0.453490
[13:27:32.080] iteration 15751: total_loss: 0.227833, loss_sup: 0.015259, loss_mps: 0.079003, loss_cps: 0.133571
[13:27:32.225] iteration 15752: total_loss: 0.386974, loss_sup: 0.017827, loss_mps: 0.131367, loss_cps: 0.237780
[13:27:32.371] iteration 15753: total_loss: 0.258442, loss_sup: 0.011501, loss_mps: 0.091377, loss_cps: 0.155564
[13:27:32.519] iteration 15754: total_loss: 0.260437, loss_sup: 0.013176, loss_mps: 0.090004, loss_cps: 0.157258
[13:27:32.664] iteration 15755: total_loss: 0.307998, loss_sup: 0.080754, loss_mps: 0.082247, loss_cps: 0.144996
[13:27:32.809] iteration 15756: total_loss: 0.382416, loss_sup: 0.130955, loss_mps: 0.087454, loss_cps: 0.164007
[13:27:32.955] iteration 15757: total_loss: 0.361105, loss_sup: 0.098223, loss_mps: 0.091695, loss_cps: 0.171186
[13:27:33.101] iteration 15758: total_loss: 0.311102, loss_sup: 0.006051, loss_mps: 0.106484, loss_cps: 0.198566
[13:27:33.247] iteration 15759: total_loss: 0.403736, loss_sup: 0.067975, loss_mps: 0.118282, loss_cps: 0.217479
[13:27:33.397] iteration 15760: total_loss: 0.272234, loss_sup: 0.030480, loss_mps: 0.089670, loss_cps: 0.152083
[13:27:33.542] iteration 15761: total_loss: 0.240834, loss_sup: 0.061827, loss_mps: 0.070273, loss_cps: 0.108735
[13:27:33.688] iteration 15762: total_loss: 0.183149, loss_sup: 0.014833, loss_mps: 0.063923, loss_cps: 0.104393
[13:27:33.834] iteration 15763: total_loss: 0.436306, loss_sup: 0.068914, loss_mps: 0.124197, loss_cps: 0.243195
[13:27:33.979] iteration 15764: total_loss: 0.372779, loss_sup: 0.064607, loss_mps: 0.107383, loss_cps: 0.200789
[13:27:34.126] iteration 15765: total_loss: 0.313414, loss_sup: 0.041226, loss_mps: 0.096319, loss_cps: 0.175869
[13:27:34.272] iteration 15766: total_loss: 0.306546, loss_sup: 0.099194, loss_mps: 0.072927, loss_cps: 0.134425
[13:27:34.419] iteration 15767: total_loss: 0.390785, loss_sup: 0.097670, loss_mps: 0.102721, loss_cps: 0.190393
[13:27:34.568] iteration 15768: total_loss: 0.344195, loss_sup: 0.057290, loss_mps: 0.100333, loss_cps: 0.186571
[13:27:34.713] iteration 15769: total_loss: 0.453277, loss_sup: 0.016749, loss_mps: 0.138241, loss_cps: 0.298288
[13:27:34.859] iteration 15770: total_loss: 0.744398, loss_sup: 0.161788, loss_mps: 0.176357, loss_cps: 0.406253
[13:27:35.008] iteration 15771: total_loss: 0.511866, loss_sup: 0.110705, loss_mps: 0.134753, loss_cps: 0.266408
[13:27:35.154] iteration 15772: total_loss: 0.320858, loss_sup: 0.083828, loss_mps: 0.082562, loss_cps: 0.154468
[13:27:35.301] iteration 15773: total_loss: 0.203205, loss_sup: 0.036850, loss_mps: 0.060632, loss_cps: 0.105723
[13:27:35.449] iteration 15774: total_loss: 0.247826, loss_sup: 0.042397, loss_mps: 0.069921, loss_cps: 0.135508
[13:27:35.596] iteration 15775: total_loss: 0.151363, loss_sup: 0.001903, loss_mps: 0.056167, loss_cps: 0.093294
[13:27:35.742] iteration 15776: total_loss: 0.518465, loss_sup: 0.067692, loss_mps: 0.146581, loss_cps: 0.304192
[13:27:35.893] iteration 15777: total_loss: 0.701136, loss_sup: 0.198114, loss_mps: 0.160134, loss_cps: 0.342888
[13:27:36.039] iteration 15778: total_loss: 0.333685, loss_sup: 0.086846, loss_mps: 0.087094, loss_cps: 0.159745
[13:27:36.186] iteration 15779: total_loss: 0.470694, loss_sup: 0.093718, loss_mps: 0.129834, loss_cps: 0.247141
[13:27:36.332] iteration 15780: total_loss: 0.461384, loss_sup: 0.124230, loss_mps: 0.107492, loss_cps: 0.229663
[13:27:36.477] iteration 15781: total_loss: 0.932876, loss_sup: 0.198873, loss_mps: 0.232859, loss_cps: 0.501145
[13:27:36.623] iteration 15782: total_loss: 1.168284, loss_sup: 0.150834, loss_mps: 0.314147, loss_cps: 0.703304
[13:27:36.769] iteration 15783: total_loss: 0.501305, loss_sup: 0.053050, loss_mps: 0.152957, loss_cps: 0.295298
[13:27:36.915] iteration 15784: total_loss: 0.543429, loss_sup: 0.124425, loss_mps: 0.141298, loss_cps: 0.277707
[13:27:37.061] iteration 15785: total_loss: 0.353306, loss_sup: 0.041495, loss_mps: 0.104950, loss_cps: 0.206860
[13:27:37.207] iteration 15786: total_loss: 0.574192, loss_sup: 0.176798, loss_mps: 0.131877, loss_cps: 0.265517
[13:27:37.358] iteration 15787: total_loss: 0.614429, loss_sup: 0.031150, loss_mps: 0.191039, loss_cps: 0.392240
[13:27:37.504] iteration 15788: total_loss: 0.418318, loss_sup: 0.113436, loss_mps: 0.105938, loss_cps: 0.198944
[13:27:37.650] iteration 15789: total_loss: 0.860796, loss_sup: 0.239945, loss_mps: 0.198506, loss_cps: 0.422345
[13:27:37.798] iteration 15790: total_loss: 0.334156, loss_sup: 0.059234, loss_mps: 0.096432, loss_cps: 0.178490
[13:27:37.945] iteration 15791: total_loss: 0.655919, loss_sup: 0.129255, loss_mps: 0.170863, loss_cps: 0.355801
[13:27:38.093] iteration 15792: total_loss: 0.271364, loss_sup: 0.020296, loss_mps: 0.092302, loss_cps: 0.158766
[13:27:38.239] iteration 15793: total_loss: 0.332393, loss_sup: 0.016102, loss_mps: 0.118176, loss_cps: 0.198115
[13:27:38.385] iteration 15794: total_loss: 0.495057, loss_sup: 0.042854, loss_mps: 0.147681, loss_cps: 0.304521
[13:27:38.531] iteration 15795: total_loss: 0.561127, loss_sup: 0.232419, loss_mps: 0.116915, loss_cps: 0.211793
[13:27:38.678] iteration 15796: total_loss: 0.784357, loss_sup: 0.188650, loss_mps: 0.197122, loss_cps: 0.398584
[13:27:38.824] iteration 15797: total_loss: 0.421340, loss_sup: 0.119174, loss_mps: 0.112030, loss_cps: 0.190136
[13:27:38.970] iteration 15798: total_loss: 0.314641, loss_sup: 0.035729, loss_mps: 0.100956, loss_cps: 0.177956
[13:27:39.117] iteration 15799: total_loss: 0.922983, loss_sup: 0.111199, loss_mps: 0.253029, loss_cps: 0.558755
[13:27:39.263] iteration 15800: total_loss: 0.482087, loss_sup: 0.050473, loss_mps: 0.148343, loss_cps: 0.283271
[13:27:39.263] Evaluation Started ==>
[13:27:50.623] ==> valid iteration 15800: unet metrics: {'dc': 0.6239386570435594, 'jc': 0.505864054163563, 'pre': 0.7815353698499532, 'hd': 5.663946563928127}, ynet metrics: {'dc': 0.6200715668096347, 'jc': 0.5047927560126109, 'pre': 0.7644517899587923, 'hd': 5.683563520607052}.
[13:27:50.625] Evaluation Finished!⏹️
[13:27:50.774] iteration 15801: total_loss: 0.451951, loss_sup: 0.071012, loss_mps: 0.132696, loss_cps: 0.248242
[13:27:50.921] iteration 15802: total_loss: 0.465713, loss_sup: 0.048175, loss_mps: 0.149234, loss_cps: 0.268304
[13:27:51.067] iteration 15803: total_loss: 0.401723, loss_sup: 0.036670, loss_mps: 0.127339, loss_cps: 0.237715
[13:27:51.212] iteration 15804: total_loss: 0.323742, loss_sup: 0.055932, loss_mps: 0.098626, loss_cps: 0.169185
[13:27:51.357] iteration 15805: total_loss: 0.337091, loss_sup: 0.134858, loss_mps: 0.075525, loss_cps: 0.126708
[13:27:51.503] iteration 15806: total_loss: 0.465927, loss_sup: 0.058130, loss_mps: 0.144242, loss_cps: 0.263556
[13:27:51.649] iteration 15807: total_loss: 0.527184, loss_sup: 0.080011, loss_mps: 0.158653, loss_cps: 0.288519
[13:27:51.794] iteration 15808: total_loss: 0.498823, loss_sup: 0.085353, loss_mps: 0.137811, loss_cps: 0.275659
[13:27:51.941] iteration 15809: total_loss: 0.562399, loss_sup: 0.070917, loss_mps: 0.171230, loss_cps: 0.320252
[13:27:52.088] iteration 15810: total_loss: 0.453984, loss_sup: 0.114696, loss_mps: 0.128467, loss_cps: 0.210821
[13:27:52.236] iteration 15811: total_loss: 0.451978, loss_sup: 0.053712, loss_mps: 0.132639, loss_cps: 0.265627
[13:27:52.383] iteration 15812: total_loss: 0.323978, loss_sup: 0.067737, loss_mps: 0.096144, loss_cps: 0.160097
[13:27:52.529] iteration 15813: total_loss: 0.666206, loss_sup: 0.120293, loss_mps: 0.185979, loss_cps: 0.359934
[13:27:52.674] iteration 15814: total_loss: 0.537657, loss_sup: 0.092859, loss_mps: 0.151954, loss_cps: 0.292843
[13:27:52.822] iteration 15815: total_loss: 0.245718, loss_sup: 0.025351, loss_mps: 0.082682, loss_cps: 0.137686
[13:27:52.969] iteration 15816: total_loss: 0.695592, loss_sup: 0.063348, loss_mps: 0.191492, loss_cps: 0.440752
[13:27:53.114] iteration 15817: total_loss: 0.389373, loss_sup: 0.032422, loss_mps: 0.117499, loss_cps: 0.239452
[13:27:53.259] iteration 15818: total_loss: 0.442021, loss_sup: 0.031598, loss_mps: 0.141950, loss_cps: 0.268473
[13:27:53.405] iteration 15819: total_loss: 0.285859, loss_sup: 0.041113, loss_mps: 0.093232, loss_cps: 0.151514
[13:27:53.551] iteration 15820: total_loss: 0.438609, loss_sup: 0.075180, loss_mps: 0.122635, loss_cps: 0.240794
[13:27:53.696] iteration 15821: total_loss: 0.623539, loss_sup: 0.168867, loss_mps: 0.144913, loss_cps: 0.309759
[13:27:53.842] iteration 15822: total_loss: 0.539868, loss_sup: 0.089067, loss_mps: 0.148939, loss_cps: 0.301863
[13:27:53.987] iteration 15823: total_loss: 0.314447, loss_sup: 0.005012, loss_mps: 0.110882, loss_cps: 0.198553
[13:27:54.133] iteration 15824: total_loss: 0.554705, loss_sup: 0.063809, loss_mps: 0.160425, loss_cps: 0.330471
[13:27:54.278] iteration 15825: total_loss: 0.471496, loss_sup: 0.024892, loss_mps: 0.151131, loss_cps: 0.295473
[13:27:54.424] iteration 15826: total_loss: 0.401466, loss_sup: 0.072472, loss_mps: 0.113557, loss_cps: 0.215437
[13:27:54.569] iteration 15827: total_loss: 0.496843, loss_sup: 0.058804, loss_mps: 0.145856, loss_cps: 0.292183
[13:27:54.714] iteration 15828: total_loss: 0.402780, loss_sup: 0.074091, loss_mps: 0.112117, loss_cps: 0.216572
[13:27:54.859] iteration 15829: total_loss: 0.323240, loss_sup: 0.008523, loss_mps: 0.104638, loss_cps: 0.210079
[13:27:55.006] iteration 15830: total_loss: 0.577725, loss_sup: 0.155661, loss_mps: 0.134453, loss_cps: 0.287612
[13:27:55.151] iteration 15831: total_loss: 0.526389, loss_sup: 0.047385, loss_mps: 0.158390, loss_cps: 0.320615
[13:27:55.301] iteration 15832: total_loss: 0.641131, loss_sup: 0.176060, loss_mps: 0.154426, loss_cps: 0.310646
[13:27:55.447] iteration 15833: total_loss: 0.640164, loss_sup: 0.204594, loss_mps: 0.149820, loss_cps: 0.285751
[13:27:55.593] iteration 15834: total_loss: 0.614454, loss_sup: 0.193490, loss_mps: 0.144424, loss_cps: 0.276540
[13:27:55.738] iteration 15835: total_loss: 0.432479, loss_sup: 0.083320, loss_mps: 0.116998, loss_cps: 0.232160
[13:27:55.883] iteration 15836: total_loss: 0.433484, loss_sup: 0.035126, loss_mps: 0.136706, loss_cps: 0.261652
[13:27:56.029] iteration 15837: total_loss: 0.937207, loss_sup: 0.132476, loss_mps: 0.246679, loss_cps: 0.558051
[13:27:56.178] iteration 15838: total_loss: 0.219493, loss_sup: 0.010880, loss_mps: 0.079455, loss_cps: 0.129157
[13:27:56.323] iteration 15839: total_loss: 0.357696, loss_sup: 0.050495, loss_mps: 0.110235, loss_cps: 0.196966
[13:27:56.468] iteration 15840: total_loss: 0.508045, loss_sup: 0.054513, loss_mps: 0.145626, loss_cps: 0.307906
[13:27:56.614] iteration 15841: total_loss: 0.331169, loss_sup: 0.015223, loss_mps: 0.109490, loss_cps: 0.206455
[13:27:56.759] iteration 15842: total_loss: 0.643091, loss_sup: 0.318153, loss_mps: 0.112814, loss_cps: 0.212124
[13:27:56.905] iteration 15843: total_loss: 1.408861, loss_sup: 0.173575, loss_mps: 0.372816, loss_cps: 0.862470
[13:27:57.052] iteration 15844: total_loss: 0.397374, loss_sup: 0.016630, loss_mps: 0.119964, loss_cps: 0.260780
[13:27:57.197] iteration 15845: total_loss: 0.665691, loss_sup: 0.205600, loss_mps: 0.149212, loss_cps: 0.310878
[13:27:57.342] iteration 15846: total_loss: 0.349110, loss_sup: 0.052011, loss_mps: 0.109842, loss_cps: 0.187257
[13:27:57.488] iteration 15847: total_loss: 0.341022, loss_sup: 0.087564, loss_mps: 0.098461, loss_cps: 0.154997
[13:27:57.633] iteration 15848: total_loss: 0.316470, loss_sup: 0.014305, loss_mps: 0.109132, loss_cps: 0.193033
[13:27:57.781] iteration 15849: total_loss: 0.428576, loss_sup: 0.092923, loss_mps: 0.114965, loss_cps: 0.220687
[13:27:57.928] iteration 15850: total_loss: 0.404121, loss_sup: 0.042531, loss_mps: 0.120534, loss_cps: 0.241056
[13:27:58.074] iteration 15851: total_loss: 0.386185, loss_sup: 0.088474, loss_mps: 0.102263, loss_cps: 0.195449
[13:27:58.220] iteration 15852: total_loss: 0.382645, loss_sup: 0.028438, loss_mps: 0.122103, loss_cps: 0.232104
[13:27:58.366] iteration 15853: total_loss: 0.300320, loss_sup: 0.006916, loss_mps: 0.104231, loss_cps: 0.189173
[13:27:58.512] iteration 15854: total_loss: 0.389391, loss_sup: 0.048022, loss_mps: 0.119343, loss_cps: 0.222026
[13:27:58.658] iteration 15855: total_loss: 0.329664, loss_sup: 0.063786, loss_mps: 0.097622, loss_cps: 0.168257
[13:27:58.803] iteration 15856: total_loss: 0.658558, loss_sup: 0.129095, loss_mps: 0.175118, loss_cps: 0.354345
[13:27:58.950] iteration 15857: total_loss: 0.812235, loss_sup: 0.100015, loss_mps: 0.222946, loss_cps: 0.489274
[13:27:59.099] iteration 15858: total_loss: 0.488022, loss_sup: 0.050587, loss_mps: 0.139394, loss_cps: 0.298041
[13:27:59.245] iteration 15859: total_loss: 0.259467, loss_sup: 0.036317, loss_mps: 0.082431, loss_cps: 0.140719
[13:27:59.391] iteration 15860: total_loss: 0.646373, loss_sup: 0.144679, loss_mps: 0.169555, loss_cps: 0.332139
[13:27:59.538] iteration 15861: total_loss: 0.501016, loss_sup: 0.081281, loss_mps: 0.142066, loss_cps: 0.277669
[13:27:59.684] iteration 15862: total_loss: 0.278922, loss_sup: 0.048549, loss_mps: 0.084707, loss_cps: 0.145665
[13:27:59.831] iteration 15863: total_loss: 0.571709, loss_sup: 0.196155, loss_mps: 0.133543, loss_cps: 0.242011
[13:27:59.977] iteration 15864: total_loss: 0.421482, loss_sup: 0.036001, loss_mps: 0.134598, loss_cps: 0.250883
[13:28:00.126] iteration 15865: total_loss: 0.154150, loss_sup: 0.013953, loss_mps: 0.054395, loss_cps: 0.085801
[13:28:00.273] iteration 15866: total_loss: 0.538930, loss_sup: 0.194308, loss_mps: 0.113692, loss_cps: 0.230931
[13:28:00.421] iteration 15867: total_loss: 0.678752, loss_sup: 0.073547, loss_mps: 0.186796, loss_cps: 0.418409
[13:28:00.567] iteration 15868: total_loss: 0.434467, loss_sup: 0.050205, loss_mps: 0.127973, loss_cps: 0.256289
[13:28:00.713] iteration 15869: total_loss: 0.548582, loss_sup: 0.018501, loss_mps: 0.165308, loss_cps: 0.364773
[13:28:00.859] iteration 15870: total_loss: 0.384173, loss_sup: 0.073209, loss_mps: 0.109418, loss_cps: 0.201546
[13:28:01.005] iteration 15871: total_loss: 0.545956, loss_sup: 0.192011, loss_mps: 0.124070, loss_cps: 0.229876
[13:28:01.151] iteration 15872: total_loss: 0.440344, loss_sup: 0.172146, loss_mps: 0.099220, loss_cps: 0.168978
[13:28:01.296] iteration 15873: total_loss: 0.747411, loss_sup: 0.172338, loss_mps: 0.194918, loss_cps: 0.380155
[13:28:01.442] iteration 15874: total_loss: 0.562055, loss_sup: 0.224446, loss_mps: 0.111195, loss_cps: 0.226414
[13:28:01.588] iteration 15875: total_loss: 0.313957, loss_sup: 0.041239, loss_mps: 0.101530, loss_cps: 0.171189
[13:28:01.734] iteration 15876: total_loss: 0.594599, loss_sup: 0.044995, loss_mps: 0.179565, loss_cps: 0.370038
[13:28:01.882] iteration 15877: total_loss: 0.620738, loss_sup: 0.146629, loss_mps: 0.156301, loss_cps: 0.317808
[13:28:02.029] iteration 15878: total_loss: 0.907223, loss_sup: 0.186770, loss_mps: 0.226392, loss_cps: 0.494061
[13:28:02.175] iteration 15879: total_loss: 0.246258, loss_sup: 0.027190, loss_mps: 0.082401, loss_cps: 0.136668
[13:28:02.321] iteration 15880: total_loss: 0.535679, loss_sup: 0.203208, loss_mps: 0.110709, loss_cps: 0.221762
[13:28:02.467] iteration 15881: total_loss: 0.719847, loss_sup: 0.164424, loss_mps: 0.186266, loss_cps: 0.369157
[13:28:02.613] iteration 15882: total_loss: 0.579321, loss_sup: 0.041559, loss_mps: 0.176543, loss_cps: 0.361219
[13:28:02.759] iteration 15883: total_loss: 0.688108, loss_sup: 0.119243, loss_mps: 0.187476, loss_cps: 0.381389
[13:28:02.823] iteration 15884: total_loss: 0.641051, loss_sup: 0.009969, loss_mps: 0.204326, loss_cps: 0.426756
[13:28:04.033] iteration 15885: total_loss: 0.410501, loss_sup: 0.043408, loss_mps: 0.132674, loss_cps: 0.234419
[13:28:04.181] iteration 15886: total_loss: 0.496845, loss_sup: 0.098658, loss_mps: 0.137801, loss_cps: 0.260386
[13:28:04.327] iteration 15887: total_loss: 0.542573, loss_sup: 0.109305, loss_mps: 0.148269, loss_cps: 0.284999
[13:28:04.472] iteration 15888: total_loss: 0.360829, loss_sup: 0.035472, loss_mps: 0.119387, loss_cps: 0.205970
[13:28:04.618] iteration 15889: total_loss: 0.818105, loss_sup: 0.137982, loss_mps: 0.224410, loss_cps: 0.455713
[13:28:04.764] iteration 15890: total_loss: 0.585176, loss_sup: 0.079628, loss_mps: 0.171906, loss_cps: 0.333642
[13:28:04.912] iteration 15891: total_loss: 0.434847, loss_sup: 0.083885, loss_mps: 0.119263, loss_cps: 0.231699
[13:28:05.060] iteration 15892: total_loss: 0.534352, loss_sup: 0.083589, loss_mps: 0.155497, loss_cps: 0.295266
[13:28:05.206] iteration 15893: total_loss: 0.386480, loss_sup: 0.064239, loss_mps: 0.121243, loss_cps: 0.200998
[13:28:05.351] iteration 15894: total_loss: 0.302518, loss_sup: 0.050247, loss_mps: 0.089933, loss_cps: 0.162338
[13:28:05.498] iteration 15895: total_loss: 0.530147, loss_sup: 0.067203, loss_mps: 0.155232, loss_cps: 0.307712
[13:28:05.645] iteration 15896: total_loss: 0.621647, loss_sup: 0.151832, loss_mps: 0.161049, loss_cps: 0.308766
[13:28:05.791] iteration 15897: total_loss: 0.386275, loss_sup: 0.055921, loss_mps: 0.116073, loss_cps: 0.214282
[13:28:05.938] iteration 15898: total_loss: 0.352347, loss_sup: 0.040130, loss_mps: 0.117379, loss_cps: 0.194838
[13:28:06.085] iteration 15899: total_loss: 0.530798, loss_sup: 0.106203, loss_mps: 0.147384, loss_cps: 0.277210
[13:28:06.231] iteration 15900: total_loss: 0.625962, loss_sup: 0.052679, loss_mps: 0.196951, loss_cps: 0.376333
[13:28:06.231] Evaluation Started ==>
[13:28:17.574] ==> valid iteration 15900: unet metrics: {'dc': 0.6225998886334662, 'jc': 0.5082595880529805, 'pre': 0.7704825535073637, 'hd': 5.767225859605801}, ynet metrics: {'dc': 0.6085632673362092, 'jc': 0.49102032121129935, 'pre': 0.7772015209470149, 'hd': 5.6492817963186095}.
[13:28:17.576] Evaluation Finished!⏹️
[13:28:17.726] iteration 15901: total_loss: 0.494272, loss_sup: 0.053829, loss_mps: 0.150607, loss_cps: 0.289836
[13:28:17.873] iteration 15902: total_loss: 0.512069, loss_sup: 0.086902, loss_mps: 0.146400, loss_cps: 0.278766
[13:28:18.019] iteration 15903: total_loss: 1.012288, loss_sup: 0.291073, loss_mps: 0.227078, loss_cps: 0.494138
[13:28:18.165] iteration 15904: total_loss: 0.605957, loss_sup: 0.173539, loss_mps: 0.144833, loss_cps: 0.287585
[13:28:18.312] iteration 15905: total_loss: 0.582613, loss_sup: 0.283496, loss_mps: 0.107712, loss_cps: 0.191406
[13:28:18.459] iteration 15906: total_loss: 0.561676, loss_sup: 0.051503, loss_mps: 0.159275, loss_cps: 0.350898
[13:28:18.606] iteration 15907: total_loss: 0.492050, loss_sup: 0.171764, loss_mps: 0.112141, loss_cps: 0.208145
[13:28:18.752] iteration 15908: total_loss: 0.350321, loss_sup: 0.070765, loss_mps: 0.101998, loss_cps: 0.177558
[13:28:18.898] iteration 15909: total_loss: 0.420874, loss_sup: 0.094068, loss_mps: 0.117984, loss_cps: 0.208822
[13:28:19.043] iteration 15910: total_loss: 0.361995, loss_sup: 0.060480, loss_mps: 0.108834, loss_cps: 0.192681
[13:28:19.189] iteration 15911: total_loss: 0.282946, loss_sup: 0.027045, loss_mps: 0.095967, loss_cps: 0.159933
[13:28:19.334] iteration 15912: total_loss: 0.472977, loss_sup: 0.086290, loss_mps: 0.134822, loss_cps: 0.251866
[13:28:19.481] iteration 15913: total_loss: 0.478383, loss_sup: 0.028676, loss_mps: 0.156552, loss_cps: 0.293155
[13:28:19.627] iteration 15914: total_loss: 0.700599, loss_sup: 0.199847, loss_mps: 0.161151, loss_cps: 0.339601
[13:28:19.772] iteration 15915: total_loss: 0.399885, loss_sup: 0.061112, loss_mps: 0.118167, loss_cps: 0.220605
[13:28:19.918] iteration 15916: total_loss: 0.541088, loss_sup: 0.123596, loss_mps: 0.140397, loss_cps: 0.277095
[13:28:20.065] iteration 15917: total_loss: 0.249983, loss_sup: 0.004396, loss_mps: 0.088961, loss_cps: 0.156626
[13:28:20.211] iteration 15918: total_loss: 0.467381, loss_sup: 0.004571, loss_mps: 0.154363, loss_cps: 0.308447
[13:28:20.357] iteration 15919: total_loss: 0.526035, loss_sup: 0.041935, loss_mps: 0.161207, loss_cps: 0.322894
[13:28:20.503] iteration 15920: total_loss: 0.375957, loss_sup: 0.016953, loss_mps: 0.122242, loss_cps: 0.236762
[13:28:20.648] iteration 15921: total_loss: 0.440519, loss_sup: 0.105118, loss_mps: 0.126734, loss_cps: 0.208667
[13:28:20.794] iteration 15922: total_loss: 0.494641, loss_sup: 0.146679, loss_mps: 0.137912, loss_cps: 0.210050
[13:28:20.942] iteration 15923: total_loss: 0.238760, loss_sup: 0.020033, loss_mps: 0.084399, loss_cps: 0.134328
[13:28:21.090] iteration 15924: total_loss: 0.432261, loss_sup: 0.024928, loss_mps: 0.138370, loss_cps: 0.268963
[13:28:21.236] iteration 15925: total_loss: 0.410060, loss_sup: 0.029398, loss_mps: 0.131662, loss_cps: 0.249000
[13:28:21.381] iteration 15926: total_loss: 0.383388, loss_sup: 0.060442, loss_mps: 0.117336, loss_cps: 0.205610
[13:28:21.527] iteration 15927: total_loss: 0.510495, loss_sup: 0.191248, loss_mps: 0.116581, loss_cps: 0.202665
[13:28:21.675] iteration 15928: total_loss: 0.390179, loss_sup: 0.020543, loss_mps: 0.119255, loss_cps: 0.250381
[13:28:21.821] iteration 15929: total_loss: 0.324661, loss_sup: 0.062782, loss_mps: 0.095272, loss_cps: 0.166607
[13:28:21.966] iteration 15930: total_loss: 0.362994, loss_sup: 0.144026, loss_mps: 0.078452, loss_cps: 0.140516
[13:28:22.112] iteration 15931: total_loss: 0.363961, loss_sup: 0.061519, loss_mps: 0.105544, loss_cps: 0.196898
[13:28:22.257] iteration 15932: total_loss: 1.059562, loss_sup: 0.228468, loss_mps: 0.265251, loss_cps: 0.565843
[13:28:22.404] iteration 15933: total_loss: 0.263033, loss_sup: 0.033564, loss_mps: 0.080017, loss_cps: 0.149452
[13:28:22.551] iteration 15934: total_loss: 0.448283, loss_sup: 0.143578, loss_mps: 0.103892, loss_cps: 0.200814
[13:28:22.699] iteration 15935: total_loss: 0.367249, loss_sup: 0.023692, loss_mps: 0.111273, loss_cps: 0.232284
[13:28:22.844] iteration 15936: total_loss: 0.512921, loss_sup: 0.129743, loss_mps: 0.132545, loss_cps: 0.250632
[13:28:22.991] iteration 15937: total_loss: 0.475091, loss_sup: 0.047737, loss_mps: 0.143054, loss_cps: 0.284300
[13:28:23.136] iteration 15938: total_loss: 0.309572, loss_sup: 0.074504, loss_mps: 0.087260, loss_cps: 0.147808
[13:28:23.282] iteration 15939: total_loss: 0.476160, loss_sup: 0.052072, loss_mps: 0.139788, loss_cps: 0.284299
[13:28:23.428] iteration 15940: total_loss: 0.523634, loss_sup: 0.107896, loss_mps: 0.137821, loss_cps: 0.277917
[13:28:23.575] iteration 15941: total_loss: 0.305701, loss_sup: 0.011410, loss_mps: 0.101344, loss_cps: 0.192948
[13:28:23.723] iteration 15942: total_loss: 0.352926, loss_sup: 0.007944, loss_mps: 0.122929, loss_cps: 0.222053
[13:28:23.869] iteration 15943: total_loss: 0.286601, loss_sup: 0.048348, loss_mps: 0.084115, loss_cps: 0.154138
[13:28:24.015] iteration 15944: total_loss: 0.405815, loss_sup: 0.101023, loss_mps: 0.102532, loss_cps: 0.202260
[13:28:24.162] iteration 15945: total_loss: 0.510433, loss_sup: 0.188972, loss_mps: 0.119529, loss_cps: 0.201931
[13:28:24.307] iteration 15946: total_loss: 0.319056, loss_sup: 0.049541, loss_mps: 0.094706, loss_cps: 0.174810
[13:28:24.453] iteration 15947: total_loss: 0.270947, loss_sup: 0.052966, loss_mps: 0.078632, loss_cps: 0.139349
[13:28:24.600] iteration 15948: total_loss: 0.606406, loss_sup: 0.151190, loss_mps: 0.144246, loss_cps: 0.310970
[13:28:24.747] iteration 15949: total_loss: 0.723497, loss_sup: 0.383217, loss_mps: 0.114402, loss_cps: 0.225878
[13:28:24.893] iteration 15950: total_loss: 0.724053, loss_sup: 0.036676, loss_mps: 0.221610, loss_cps: 0.465767
[13:28:25.039] iteration 15951: total_loss: 0.554297, loss_sup: 0.017552, loss_mps: 0.171404, loss_cps: 0.365341
[13:28:25.186] iteration 15952: total_loss: 0.252291, loss_sup: 0.025455, loss_mps: 0.083569, loss_cps: 0.143268
[13:28:25.334] iteration 15953: total_loss: 0.384535, loss_sup: 0.014983, loss_mps: 0.128429, loss_cps: 0.241123
[13:28:25.483] iteration 15954: total_loss: 0.555275, loss_sup: 0.137707, loss_mps: 0.136550, loss_cps: 0.281018
[13:28:25.629] iteration 15955: total_loss: 0.830965, loss_sup: 0.052837, loss_mps: 0.241101, loss_cps: 0.537026
[13:28:25.776] iteration 15956: total_loss: 0.410616, loss_sup: 0.081525, loss_mps: 0.111942, loss_cps: 0.217149
[13:28:25.921] iteration 15957: total_loss: 0.366132, loss_sup: 0.035886, loss_mps: 0.117139, loss_cps: 0.213108
[13:28:26.067] iteration 15958: total_loss: 0.930590, loss_sup: 0.114808, loss_mps: 0.247086, loss_cps: 0.568695
[13:28:26.212] iteration 15959: total_loss: 0.509846, loss_sup: 0.060545, loss_mps: 0.157063, loss_cps: 0.292237
[13:28:26.359] iteration 15960: total_loss: 0.286237, loss_sup: 0.083930, loss_mps: 0.073287, loss_cps: 0.129020
[13:28:26.504] iteration 15961: total_loss: 0.331782, loss_sup: 0.083875, loss_mps: 0.091551, loss_cps: 0.156356
[13:28:26.650] iteration 15962: total_loss: 0.422239, loss_sup: 0.028074, loss_mps: 0.138558, loss_cps: 0.255607
[13:28:26.796] iteration 15963: total_loss: 0.512719, loss_sup: 0.065786, loss_mps: 0.148086, loss_cps: 0.298847
[13:28:26.944] iteration 15964: total_loss: 0.972250, loss_sup: 0.113522, loss_mps: 0.265386, loss_cps: 0.593343
[13:28:27.090] iteration 15965: total_loss: 0.625252, loss_sup: 0.114954, loss_mps: 0.169714, loss_cps: 0.340583
[13:28:27.236] iteration 15966: total_loss: 0.700909, loss_sup: 0.094787, loss_mps: 0.187826, loss_cps: 0.418296
[13:28:27.383] iteration 15967: total_loss: 0.406143, loss_sup: 0.059780, loss_mps: 0.117016, loss_cps: 0.229347
[13:28:27.529] iteration 15968: total_loss: 0.445136, loss_sup: 0.117882, loss_mps: 0.111309, loss_cps: 0.215945
[13:28:27.675] iteration 15969: total_loss: 0.281656, loss_sup: 0.036614, loss_mps: 0.086772, loss_cps: 0.158270
[13:28:27.822] iteration 15970: total_loss: 0.316834, loss_sup: 0.028632, loss_mps: 0.105477, loss_cps: 0.182725
[13:28:27.967] iteration 15971: total_loss: 0.503460, loss_sup: 0.025179, loss_mps: 0.156375, loss_cps: 0.321906
[13:28:28.115] iteration 15972: total_loss: 0.684513, loss_sup: 0.144507, loss_mps: 0.181234, loss_cps: 0.358773
[13:28:28.261] iteration 15973: total_loss: 0.280532, loss_sup: 0.018652, loss_mps: 0.091229, loss_cps: 0.170651
[13:28:28.407] iteration 15974: total_loss: 0.335204, loss_sup: 0.048594, loss_mps: 0.097680, loss_cps: 0.188931
[13:28:28.554] iteration 15975: total_loss: 0.519217, loss_sup: 0.054255, loss_mps: 0.158359, loss_cps: 0.306604
[13:28:28.703] iteration 15976: total_loss: 0.771404, loss_sup: 0.317800, loss_mps: 0.153317, loss_cps: 0.300287
[13:28:28.851] iteration 15977: total_loss: 0.441966, loss_sup: 0.054071, loss_mps: 0.129724, loss_cps: 0.258172
[13:28:28.999] iteration 15978: total_loss: 0.633817, loss_sup: 0.091790, loss_mps: 0.172868, loss_cps: 0.369160
[13:28:29.145] iteration 15979: total_loss: 0.412152, loss_sup: 0.044667, loss_mps: 0.127227, loss_cps: 0.240258
[13:28:29.292] iteration 15980: total_loss: 0.425913, loss_sup: 0.054498, loss_mps: 0.131229, loss_cps: 0.240186
[13:28:29.444] iteration 15981: total_loss: 0.943355, loss_sup: 0.070157, loss_mps: 0.260427, loss_cps: 0.612771
[13:28:29.591] iteration 15982: total_loss: 0.317057, loss_sup: 0.029147, loss_mps: 0.101219, loss_cps: 0.186691
[13:28:29.738] iteration 15983: total_loss: 0.608982, loss_sup: 0.086580, loss_mps: 0.171048, loss_cps: 0.351353
[13:28:29.884] iteration 15984: total_loss: 0.559475, loss_sup: 0.128240, loss_mps: 0.140615, loss_cps: 0.290620
[13:28:30.030] iteration 15985: total_loss: 0.414667, loss_sup: 0.010119, loss_mps: 0.134578, loss_cps: 0.269970
[13:28:30.177] iteration 15986: total_loss: 0.339148, loss_sup: 0.086858, loss_mps: 0.090860, loss_cps: 0.161429
[13:28:30.324] iteration 15987: total_loss: 0.592872, loss_sup: 0.038409, loss_mps: 0.176809, loss_cps: 0.377654
[13:28:30.471] iteration 15988: total_loss: 0.513274, loss_sup: 0.218781, loss_mps: 0.108461, loss_cps: 0.186032
[13:28:30.619] iteration 15989: total_loss: 0.574290, loss_sup: 0.107484, loss_mps: 0.160404, loss_cps: 0.306402
[13:28:30.765] iteration 15990: total_loss: 0.387013, loss_sup: 0.138761, loss_mps: 0.095979, loss_cps: 0.152274
[13:28:30.911] iteration 15991: total_loss: 0.631788, loss_sup: 0.075214, loss_mps: 0.182814, loss_cps: 0.373760
[13:28:31.058] iteration 15992: total_loss: 0.962022, loss_sup: 0.093131, loss_mps: 0.274940, loss_cps: 0.593950
[13:28:31.204] iteration 15993: total_loss: 0.671899, loss_sup: 0.031355, loss_mps: 0.202590, loss_cps: 0.437953
[13:28:31.353] iteration 15994: total_loss: 0.418413, loss_sup: 0.080108, loss_mps: 0.119347, loss_cps: 0.218958
[13:28:31.500] iteration 15995: total_loss: 0.241055, loss_sup: 0.010026, loss_mps: 0.088602, loss_cps: 0.142428
[13:28:31.648] iteration 15996: total_loss: 0.927629, loss_sup: 0.475682, loss_mps: 0.157156, loss_cps: 0.294791
[13:28:31.794] iteration 15997: total_loss: 0.404216, loss_sup: 0.126314, loss_mps: 0.106695, loss_cps: 0.171207
[13:28:31.941] iteration 15998: total_loss: 0.466539, loss_sup: 0.063099, loss_mps: 0.140349, loss_cps: 0.263091
[13:28:32.088] iteration 15999: total_loss: 0.735469, loss_sup: 0.068117, loss_mps: 0.218748, loss_cps: 0.448605
[13:28:32.235] iteration 16000: total_loss: 0.351142, loss_sup: 0.061186, loss_mps: 0.105476, loss_cps: 0.184480
[13:28:32.235] Evaluation Started ==>
[13:28:43.578] ==> valid iteration 16000: unet metrics: {'dc': 0.6592781251076327, 'jc': 0.5419887444171272, 'pre': 0.7908829475532715, 'hd': 5.546155975068308}, ynet metrics: {'dc': 0.5985075939186962, 'jc': 0.4805697713953535, 'pre': 0.7951308607883616, 'hd': 5.62752870060103}.
[13:28:43.580] Evaluation Finished!⏹️
[13:28:43.730] iteration 16001: total_loss: 0.556269, loss_sup: 0.154186, loss_mps: 0.140494, loss_cps: 0.261590
[13:28:43.877] iteration 16002: total_loss: 0.282041, loss_sup: 0.080983, loss_mps: 0.076696, loss_cps: 0.124362
[13:28:44.022] iteration 16003: total_loss: 0.509271, loss_sup: 0.220639, loss_mps: 0.107311, loss_cps: 0.181320
[13:28:44.167] iteration 16004: total_loss: 0.339369, loss_sup: 0.069318, loss_mps: 0.099828, loss_cps: 0.170223
[13:28:44.316] iteration 16005: total_loss: 0.384974, loss_sup: 0.135256, loss_mps: 0.089048, loss_cps: 0.160670
[13:28:44.461] iteration 16006: total_loss: 0.397839, loss_sup: 0.011676, loss_mps: 0.136110, loss_cps: 0.250053
[13:28:44.606] iteration 16007: total_loss: 0.317851, loss_sup: 0.075264, loss_mps: 0.091570, loss_cps: 0.151017
[13:28:44.753] iteration 16008: total_loss: 0.442083, loss_sup: 0.052598, loss_mps: 0.131958, loss_cps: 0.257526
[13:28:44.898] iteration 16009: total_loss: 0.205975, loss_sup: 0.055157, loss_mps: 0.062149, loss_cps: 0.088669
[13:28:45.044] iteration 16010: total_loss: 0.295981, loss_sup: 0.047383, loss_mps: 0.095872, loss_cps: 0.152726
[13:28:45.192] iteration 16011: total_loss: 0.402191, loss_sup: 0.118510, loss_mps: 0.105758, loss_cps: 0.177924
[13:28:45.340] iteration 16012: total_loss: 0.481039, loss_sup: 0.110945, loss_mps: 0.132565, loss_cps: 0.237529
[13:28:45.487] iteration 16013: total_loss: 0.505894, loss_sup: 0.037853, loss_mps: 0.159891, loss_cps: 0.308149
[13:28:45.634] iteration 16014: total_loss: 0.271258, loss_sup: 0.065690, loss_mps: 0.081784, loss_cps: 0.123785
[13:28:45.780] iteration 16015: total_loss: 0.749276, loss_sup: 0.181020, loss_mps: 0.190410, loss_cps: 0.377846
[13:28:45.928] iteration 16016: total_loss: 0.466481, loss_sup: 0.161878, loss_mps: 0.111503, loss_cps: 0.193101
[13:28:46.074] iteration 16017: total_loss: 0.599350, loss_sup: 0.152480, loss_mps: 0.140101, loss_cps: 0.306769
[13:28:46.220] iteration 16018: total_loss: 0.686462, loss_sup: 0.226022, loss_mps: 0.150651, loss_cps: 0.309788
[13:28:46.365] iteration 16019: total_loss: 0.378789, loss_sup: 0.012849, loss_mps: 0.127230, loss_cps: 0.238709
[13:28:46.514] iteration 16020: total_loss: 0.691878, loss_sup: 0.066730, loss_mps: 0.201394, loss_cps: 0.423754
[13:28:46.659] iteration 16021: total_loss: 0.515570, loss_sup: 0.142063, loss_mps: 0.123896, loss_cps: 0.249611
[13:28:46.804] iteration 16022: total_loss: 0.644436, loss_sup: 0.162915, loss_mps: 0.157626, loss_cps: 0.323895
[13:28:46.952] iteration 16023: total_loss: 0.887446, loss_sup: 0.126569, loss_mps: 0.232398, loss_cps: 0.528480
[13:28:47.098] iteration 16024: total_loss: 0.516524, loss_sup: 0.142239, loss_mps: 0.127326, loss_cps: 0.246959
[13:28:47.245] iteration 16025: total_loss: 0.677854, loss_sup: 0.186499, loss_mps: 0.162939, loss_cps: 0.328416
[13:28:47.394] iteration 16026: total_loss: 0.478519, loss_sup: 0.038210, loss_mps: 0.142804, loss_cps: 0.297504
[13:28:47.540] iteration 16027: total_loss: 0.343990, loss_sup: 0.035901, loss_mps: 0.105209, loss_cps: 0.202880
[13:28:47.686] iteration 16028: total_loss: 0.526078, loss_sup: 0.090714, loss_mps: 0.145470, loss_cps: 0.289894
[13:28:47.833] iteration 16029: total_loss: 0.342531, loss_sup: 0.081781, loss_mps: 0.091036, loss_cps: 0.169714
[13:28:47.979] iteration 16030: total_loss: 0.616010, loss_sup: 0.095782, loss_mps: 0.161294, loss_cps: 0.358933
[13:28:48.125] iteration 16031: total_loss: 0.485677, loss_sup: 0.200501, loss_mps: 0.096584, loss_cps: 0.188591
[13:28:48.273] iteration 16032: total_loss: 0.387865, loss_sup: 0.071314, loss_mps: 0.112312, loss_cps: 0.204239
[13:28:48.420] iteration 16033: total_loss: 0.641029, loss_sup: 0.111989, loss_mps: 0.167562, loss_cps: 0.361478
[13:28:48.570] iteration 16034: total_loss: 0.461108, loss_sup: 0.046171, loss_mps: 0.136809, loss_cps: 0.278128
[13:28:48.716] iteration 16035: total_loss: 0.374673, loss_sup: 0.052681, loss_mps: 0.105594, loss_cps: 0.216397
[13:28:48.869] iteration 16036: total_loss: 0.413711, loss_sup: 0.063461, loss_mps: 0.122870, loss_cps: 0.227380
[13:28:49.014] iteration 16037: total_loss: 0.287886, loss_sup: 0.078543, loss_mps: 0.073594, loss_cps: 0.135750
[13:28:49.161] iteration 16038: total_loss: 0.663569, loss_sup: 0.174666, loss_mps: 0.157186, loss_cps: 0.331717
[13:28:49.306] iteration 16039: total_loss: 0.708053, loss_sup: 0.319838, loss_mps: 0.125636, loss_cps: 0.262579
[13:28:49.454] iteration 16040: total_loss: 0.343265, loss_sup: 0.032565, loss_mps: 0.107899, loss_cps: 0.202800
[13:28:49.600] iteration 16041: total_loss: 0.493711, loss_sup: 0.055707, loss_mps: 0.146500, loss_cps: 0.291504
[13:28:49.746] iteration 16042: total_loss: 0.642378, loss_sup: 0.298048, loss_mps: 0.118369, loss_cps: 0.225961
[13:28:49.892] iteration 16043: total_loss: 0.424788, loss_sup: 0.202866, loss_mps: 0.080326, loss_cps: 0.141596
[13:28:50.038] iteration 16044: total_loss: 0.836820, loss_sup: 0.234977, loss_mps: 0.190019, loss_cps: 0.411824
[13:28:50.185] iteration 16045: total_loss: 0.625774, loss_sup: 0.091622, loss_mps: 0.173547, loss_cps: 0.360605
[13:28:50.332] iteration 16046: total_loss: 0.660123, loss_sup: 0.057948, loss_mps: 0.208495, loss_cps: 0.393679
[13:28:50.478] iteration 16047: total_loss: 0.481910, loss_sup: 0.046158, loss_mps: 0.150295, loss_cps: 0.285457
[13:28:50.623] iteration 16048: total_loss: 0.408934, loss_sup: 0.019333, loss_mps: 0.143916, loss_cps: 0.245685
[13:28:50.769] iteration 16049: total_loss: 0.410136, loss_sup: 0.021538, loss_mps: 0.132487, loss_cps: 0.256112
[13:28:50.915] iteration 16050: total_loss: 0.325878, loss_sup: 0.036882, loss_mps: 0.109838, loss_cps: 0.179158
[13:28:51.060] iteration 16051: total_loss: 0.464150, loss_sup: 0.013665, loss_mps: 0.150503, loss_cps: 0.299981
[13:28:51.206] iteration 16052: total_loss: 0.590554, loss_sup: 0.255236, loss_mps: 0.123267, loss_cps: 0.212051
[13:28:51.351] iteration 16053: total_loss: 0.388400, loss_sup: 0.102938, loss_mps: 0.109913, loss_cps: 0.175549
[13:28:51.498] iteration 16054: total_loss: 1.065586, loss_sup: 0.228147, loss_mps: 0.262110, loss_cps: 0.575329
[13:28:51.643] iteration 16055: total_loss: 0.306109, loss_sup: 0.017267, loss_mps: 0.106564, loss_cps: 0.182278
[13:28:51.789] iteration 16056: total_loss: 0.416066, loss_sup: 0.040069, loss_mps: 0.129139, loss_cps: 0.246858
[13:28:51.935] iteration 16057: total_loss: 0.475560, loss_sup: 0.053597, loss_mps: 0.145004, loss_cps: 0.276959
[13:28:52.080] iteration 16058: total_loss: 0.370475, loss_sup: 0.055895, loss_mps: 0.109728, loss_cps: 0.204852
[13:28:52.226] iteration 16059: total_loss: 0.895952, loss_sup: 0.245072, loss_mps: 0.204149, loss_cps: 0.446732
[13:28:52.372] iteration 16060: total_loss: 0.399242, loss_sup: 0.037516, loss_mps: 0.119487, loss_cps: 0.242240
[13:28:52.519] iteration 16061: total_loss: 0.978336, loss_sup: 0.384088, loss_mps: 0.194472, loss_cps: 0.399777
[13:28:52.666] iteration 16062: total_loss: 0.416355, loss_sup: 0.031299, loss_mps: 0.130676, loss_cps: 0.254381
[13:28:52.813] iteration 16063: total_loss: 0.338173, loss_sup: 0.056384, loss_mps: 0.104432, loss_cps: 0.177357
[13:28:52.958] iteration 16064: total_loss: 0.355936, loss_sup: 0.086894, loss_mps: 0.093451, loss_cps: 0.175591
[13:28:53.105] iteration 16065: total_loss: 0.422867, loss_sup: 0.058094, loss_mps: 0.118394, loss_cps: 0.246379
[13:28:53.254] iteration 16066: total_loss: 0.371532, loss_sup: 0.044722, loss_mps: 0.116464, loss_cps: 0.210346
[13:28:53.400] iteration 16067: total_loss: 0.524624, loss_sup: 0.097760, loss_mps: 0.145643, loss_cps: 0.281222
[13:28:53.546] iteration 16068: total_loss: 0.168387, loss_sup: 0.025969, loss_mps: 0.055695, loss_cps: 0.086722
[13:28:53.692] iteration 16069: total_loss: 0.355824, loss_sup: 0.050147, loss_mps: 0.108865, loss_cps: 0.196811
[13:28:53.838] iteration 16070: total_loss: 0.517049, loss_sup: 0.089217, loss_mps: 0.146202, loss_cps: 0.281629
[13:28:53.984] iteration 16071: total_loss: 0.720039, loss_sup: 0.307109, loss_mps: 0.146297, loss_cps: 0.266633
[13:28:54.131] iteration 16072: total_loss: 0.495033, loss_sup: 0.171120, loss_mps: 0.116495, loss_cps: 0.207418
[13:28:54.277] iteration 16073: total_loss: 0.355362, loss_sup: 0.061624, loss_mps: 0.103740, loss_cps: 0.189999
[13:28:54.423] iteration 16074: total_loss: 0.595422, loss_sup: 0.112259, loss_mps: 0.160379, loss_cps: 0.322784
[13:28:54.569] iteration 16075: total_loss: 0.350454, loss_sup: 0.015234, loss_mps: 0.114026, loss_cps: 0.221194
[13:28:54.717] iteration 16076: total_loss: 0.393811, loss_sup: 0.037853, loss_mps: 0.122022, loss_cps: 0.233936
[13:28:54.863] iteration 16077: total_loss: 0.578256, loss_sup: 0.148873, loss_mps: 0.150237, loss_cps: 0.279146
[13:28:55.009] iteration 16078: total_loss: 0.460334, loss_sup: 0.076969, loss_mps: 0.137370, loss_cps: 0.245996
[13:28:55.155] iteration 16079: total_loss: 0.490606, loss_sup: 0.084778, loss_mps: 0.136823, loss_cps: 0.269004
[13:28:55.301] iteration 16080: total_loss: 0.768252, loss_sup: 0.065606, loss_mps: 0.229696, loss_cps: 0.472950
[13:28:55.447] iteration 16081: total_loss: 0.399922, loss_sup: 0.080331, loss_mps: 0.115962, loss_cps: 0.203628
[13:28:55.592] iteration 16082: total_loss: 0.359978, loss_sup: 0.056855, loss_mps: 0.111366, loss_cps: 0.191757
[13:28:55.738] iteration 16083: total_loss: 0.370516, loss_sup: 0.010822, loss_mps: 0.121549, loss_cps: 0.238145
[13:28:55.884] iteration 16084: total_loss: 0.582066, loss_sup: 0.052308, loss_mps: 0.180039, loss_cps: 0.349719
[13:28:56.030] iteration 16085: total_loss: 0.336785, loss_sup: 0.102879, loss_mps: 0.091256, loss_cps: 0.142650
[13:28:56.176] iteration 16086: total_loss: 0.352715, loss_sup: 0.026364, loss_mps: 0.111635, loss_cps: 0.214716
[13:28:56.323] iteration 16087: total_loss: 0.322337, loss_sup: 0.095835, loss_mps: 0.083444, loss_cps: 0.143058
[13:28:56.469] iteration 16088: total_loss: 0.390587, loss_sup: 0.017545, loss_mps: 0.129916, loss_cps: 0.243125
[13:28:56.615] iteration 16089: total_loss: 0.281725, loss_sup: 0.051973, loss_mps: 0.085281, loss_cps: 0.144470
[13:28:56.761] iteration 16090: total_loss: 0.357415, loss_sup: 0.063464, loss_mps: 0.108872, loss_cps: 0.185079
[13:28:56.908] iteration 16091: total_loss: 0.379953, loss_sup: 0.019512, loss_mps: 0.117343, loss_cps: 0.243098
[13:28:57.053] iteration 16092: total_loss: 0.393052, loss_sup: 0.087647, loss_mps: 0.113831, loss_cps: 0.191574
[13:28:57.199] iteration 16093: total_loss: 0.370947, loss_sup: 0.038823, loss_mps: 0.113039, loss_cps: 0.219085
[13:28:57.345] iteration 16094: total_loss: 0.469454, loss_sup: 0.104036, loss_mps: 0.122232, loss_cps: 0.243186
[13:28:57.491] iteration 16095: total_loss: 0.377083, loss_sup: 0.054742, loss_mps: 0.111444, loss_cps: 0.210897
[13:28:57.637] iteration 16096: total_loss: 0.493204, loss_sup: 0.078271, loss_mps: 0.139596, loss_cps: 0.275338
[13:28:57.783] iteration 16097: total_loss: 0.377684, loss_sup: 0.092602, loss_mps: 0.103359, loss_cps: 0.181724
[13:28:57.930] iteration 16098: total_loss: 0.397159, loss_sup: 0.026430, loss_mps: 0.122291, loss_cps: 0.248438
[13:28:58.076] iteration 16099: total_loss: 0.395565, loss_sup: 0.030483, loss_mps: 0.125146, loss_cps: 0.239936
[13:28:58.222] iteration 16100: total_loss: 0.410728, loss_sup: 0.039935, loss_mps: 0.123814, loss_cps: 0.246979
[13:28:58.223] Evaluation Started ==>
[13:29:09.584] ==> valid iteration 16100: unet metrics: {'dc': 0.6596184775625634, 'jc': 0.538601210960542, 'pre': 0.7519784461977572, 'hd': 5.832605383165646}, ynet metrics: {'dc': 0.6026738579888445, 'jc': 0.48774991520528005, 'pre': 0.7839951829207554, 'hd': 5.594880834065337}.
[13:29:09.587] Evaluation Finished!⏹️
[13:29:09.738] iteration 16101: total_loss: 0.312001, loss_sup: 0.014338, loss_mps: 0.103975, loss_cps: 0.193688
[13:29:09.885] iteration 16102: total_loss: 0.623210, loss_sup: 0.149015, loss_mps: 0.161823, loss_cps: 0.312371
[13:29:10.031] iteration 16103: total_loss: 0.300139, loss_sup: 0.011467, loss_mps: 0.105130, loss_cps: 0.183542
[13:29:10.177] iteration 16104: total_loss: 0.511473, loss_sup: 0.103926, loss_mps: 0.137887, loss_cps: 0.269660
[13:29:10.322] iteration 16105: total_loss: 0.691127, loss_sup: 0.225150, loss_mps: 0.159357, loss_cps: 0.306620
[13:29:10.468] iteration 16106: total_loss: 0.863041, loss_sup: 0.047169, loss_mps: 0.258923, loss_cps: 0.556949
[13:29:10.616] iteration 16107: total_loss: 0.729690, loss_sup: 0.127395, loss_mps: 0.179547, loss_cps: 0.422748
[13:29:10.762] iteration 16108: total_loss: 0.229286, loss_sup: 0.025245, loss_mps: 0.075408, loss_cps: 0.128633
[13:29:10.907] iteration 16109: total_loss: 0.467051, loss_sup: 0.121577, loss_mps: 0.118527, loss_cps: 0.226947
[13:29:11.052] iteration 16110: total_loss: 0.663492, loss_sup: 0.139914, loss_mps: 0.178395, loss_cps: 0.345184
[13:29:11.198] iteration 16111: total_loss: 0.294831, loss_sup: 0.028933, loss_mps: 0.090594, loss_cps: 0.175304
[13:29:11.343] iteration 16112: total_loss: 0.400471, loss_sup: 0.094118, loss_mps: 0.102655, loss_cps: 0.203698
[13:29:11.488] iteration 16113: total_loss: 0.391580, loss_sup: 0.031028, loss_mps: 0.125248, loss_cps: 0.235304
[13:29:11.636] iteration 16114: total_loss: 0.394510, loss_sup: 0.043219, loss_mps: 0.120615, loss_cps: 0.230675
[13:29:11.785] iteration 16115: total_loss: 0.556958, loss_sup: 0.134121, loss_mps: 0.143802, loss_cps: 0.279035
[13:29:11.932] iteration 16116: total_loss: 0.778236, loss_sup: 0.174085, loss_mps: 0.187722, loss_cps: 0.416428
[13:29:12.077] iteration 16117: total_loss: 0.519338, loss_sup: 0.120326, loss_mps: 0.133447, loss_cps: 0.265565
[13:29:12.223] iteration 16118: total_loss: 0.323073, loss_sup: 0.068230, loss_mps: 0.087778, loss_cps: 0.167066
[13:29:12.369] iteration 16119: total_loss: 0.568141, loss_sup: 0.045238, loss_mps: 0.162821, loss_cps: 0.360082
[13:29:12.515] iteration 16120: total_loss: 0.381688, loss_sup: 0.015080, loss_mps: 0.121556, loss_cps: 0.245052
[13:29:12.663] iteration 16121: total_loss: 0.388091, loss_sup: 0.028345, loss_mps: 0.123835, loss_cps: 0.235911
[13:29:12.809] iteration 16122: total_loss: 0.390824, loss_sup: 0.050933, loss_mps: 0.117157, loss_cps: 0.222733
[13:29:12.954] iteration 16123: total_loss: 0.253900, loss_sup: 0.019141, loss_mps: 0.084298, loss_cps: 0.150462
[13:29:13.100] iteration 16124: total_loss: 0.304507, loss_sup: 0.057118, loss_mps: 0.090177, loss_cps: 0.157211
[13:29:13.245] iteration 16125: total_loss: 0.545318, loss_sup: 0.240750, loss_mps: 0.105283, loss_cps: 0.199285
[13:29:13.390] iteration 16126: total_loss: 0.504585, loss_sup: 0.103354, loss_mps: 0.131245, loss_cps: 0.269986
[13:29:13.538] iteration 16127: total_loss: 0.388464, loss_sup: 0.020155, loss_mps: 0.125374, loss_cps: 0.242935
[13:29:13.685] iteration 16128: total_loss: 0.450594, loss_sup: 0.036133, loss_mps: 0.138056, loss_cps: 0.276405
[13:29:13.830] iteration 16129: total_loss: 0.353281, loss_sup: 0.055455, loss_mps: 0.112168, loss_cps: 0.185658
[13:29:13.976] iteration 16130: total_loss: 0.206820, loss_sup: 0.015594, loss_mps: 0.069244, loss_cps: 0.121983
[13:29:14.122] iteration 16131: total_loss: 0.358365, loss_sup: 0.008229, loss_mps: 0.115105, loss_cps: 0.235031
[13:29:14.270] iteration 16132: total_loss: 1.021001, loss_sup: 0.434842, loss_mps: 0.176192, loss_cps: 0.409968
[13:29:14.416] iteration 16133: total_loss: 0.450547, loss_sup: 0.150181, loss_mps: 0.106901, loss_cps: 0.193465
[13:29:14.561] iteration 16134: total_loss: 0.510932, loss_sup: 0.165738, loss_mps: 0.119101, loss_cps: 0.226093
[13:29:14.708] iteration 16135: total_loss: 0.409483, loss_sup: 0.028561, loss_mps: 0.136476, loss_cps: 0.244446
[13:29:14.854] iteration 16136: total_loss: 0.374178, loss_sup: 0.119922, loss_mps: 0.091142, loss_cps: 0.163114
[13:29:14.999] iteration 16137: total_loss: 0.309960, loss_sup: 0.043150, loss_mps: 0.103653, loss_cps: 0.163156
[13:29:15.145] iteration 16138: total_loss: 0.566411, loss_sup: 0.188420, loss_mps: 0.129736, loss_cps: 0.248255
[13:29:15.291] iteration 16139: total_loss: 0.515671, loss_sup: 0.093512, loss_mps: 0.140234, loss_cps: 0.281925
[13:29:15.437] iteration 16140: total_loss: 0.428771, loss_sup: 0.032441, loss_mps: 0.137309, loss_cps: 0.259021
[13:29:15.585] iteration 16141: total_loss: 0.295567, loss_sup: 0.008294, loss_mps: 0.106134, loss_cps: 0.181139
[13:29:15.732] iteration 16142: total_loss: 0.359006, loss_sup: 0.074723, loss_mps: 0.099099, loss_cps: 0.185183
[13:29:15.878] iteration 16143: total_loss: 0.589736, loss_sup: 0.078383, loss_mps: 0.170402, loss_cps: 0.340952
[13:29:16.025] iteration 16144: total_loss: 0.397509, loss_sup: 0.041096, loss_mps: 0.122367, loss_cps: 0.234045
[13:29:16.170] iteration 16145: total_loss: 0.319841, loss_sup: 0.045958, loss_mps: 0.098041, loss_cps: 0.175842
[13:29:16.317] iteration 16146: total_loss: 0.304482, loss_sup: 0.046019, loss_mps: 0.093073, loss_cps: 0.165390
[13:29:16.463] iteration 16147: total_loss: 0.232108, loss_sup: 0.037686, loss_mps: 0.075311, loss_cps: 0.119110
[13:29:16.609] iteration 16148: total_loss: 0.441949, loss_sup: 0.085936, loss_mps: 0.119641, loss_cps: 0.236372
[13:29:16.760] iteration 16149: total_loss: 0.464754, loss_sup: 0.034258, loss_mps: 0.146278, loss_cps: 0.284218
[13:29:16.906] iteration 16150: total_loss: 0.344404, loss_sup: 0.014141, loss_mps: 0.111987, loss_cps: 0.218276
[13:29:17.053] iteration 16151: total_loss: 0.407948, loss_sup: 0.072796, loss_mps: 0.121311, loss_cps: 0.213840
[13:29:17.200] iteration 16152: total_loss: 0.523034, loss_sup: 0.231184, loss_mps: 0.103030, loss_cps: 0.188819
[13:29:17.346] iteration 16153: total_loss: 0.402647, loss_sup: 0.011094, loss_mps: 0.135261, loss_cps: 0.256292
[13:29:17.493] iteration 16154: total_loss: 0.489469, loss_sup: 0.071551, loss_mps: 0.141174, loss_cps: 0.276744
[13:29:17.639] iteration 16155: total_loss: 0.585050, loss_sup: 0.156743, loss_mps: 0.147090, loss_cps: 0.281218
[13:29:17.785] iteration 16156: total_loss: 0.419620, loss_sup: 0.083414, loss_mps: 0.115182, loss_cps: 0.221024
[13:29:17.930] iteration 16157: total_loss: 0.387243, loss_sup: 0.057956, loss_mps: 0.118488, loss_cps: 0.210799
[13:29:18.076] iteration 16158: total_loss: 0.455324, loss_sup: 0.117864, loss_mps: 0.111923, loss_cps: 0.225537
[13:29:18.224] iteration 16159: total_loss: 0.486456, loss_sup: 0.085001, loss_mps: 0.133562, loss_cps: 0.267893
[13:29:18.370] iteration 16160: total_loss: 0.383175, loss_sup: 0.013252, loss_mps: 0.122750, loss_cps: 0.247174
[13:29:18.516] iteration 16161: total_loss: 0.427529, loss_sup: 0.062989, loss_mps: 0.121206, loss_cps: 0.243333
[13:29:18.662] iteration 16162: total_loss: 0.235960, loss_sup: 0.044531, loss_mps: 0.067241, loss_cps: 0.124187
[13:29:18.812] iteration 16163: total_loss: 0.471377, loss_sup: 0.019931, loss_mps: 0.143931, loss_cps: 0.307515
[13:29:18.958] iteration 16164: total_loss: 0.404142, loss_sup: 0.048626, loss_mps: 0.118669, loss_cps: 0.236847
[13:29:19.104] iteration 16165: total_loss: 0.523346, loss_sup: 0.148426, loss_mps: 0.124731, loss_cps: 0.250189
[13:29:19.250] iteration 16166: total_loss: 0.747264, loss_sup: 0.033109, loss_mps: 0.225887, loss_cps: 0.488267
[13:29:19.397] iteration 16167: total_loss: 0.323124, loss_sup: 0.092076, loss_mps: 0.084768, loss_cps: 0.146280
[13:29:19.543] iteration 16168: total_loss: 0.468977, loss_sup: 0.135221, loss_mps: 0.120162, loss_cps: 0.213594
[13:29:19.689] iteration 16169: total_loss: 0.460935, loss_sup: 0.163226, loss_mps: 0.106626, loss_cps: 0.191083
[13:29:19.836] iteration 16170: total_loss: 0.468350, loss_sup: 0.078279, loss_mps: 0.137461, loss_cps: 0.252611
[13:29:19.983] iteration 16171: total_loss: 0.666635, loss_sup: 0.068009, loss_mps: 0.193810, loss_cps: 0.404816
[13:29:20.129] iteration 16172: total_loss: 0.442401, loss_sup: 0.097899, loss_mps: 0.123324, loss_cps: 0.221178
[13:29:20.275] iteration 16173: total_loss: 0.410186, loss_sup: 0.038155, loss_mps: 0.129187, loss_cps: 0.242844
[13:29:20.422] iteration 16174: total_loss: 0.360009, loss_sup: 0.084901, loss_mps: 0.098089, loss_cps: 0.177018
[13:29:20.568] iteration 16175: total_loss: 0.942270, loss_sup: 0.288114, loss_mps: 0.208286, loss_cps: 0.445869
[13:29:20.714] iteration 16176: total_loss: 0.270310, loss_sup: 0.070841, loss_mps: 0.076246, loss_cps: 0.123223
[13:29:20.862] iteration 16177: total_loss: 0.361956, loss_sup: 0.038626, loss_mps: 0.113361, loss_cps: 0.209969
[13:29:21.011] iteration 16178: total_loss: 0.382545, loss_sup: 0.060763, loss_mps: 0.112631, loss_cps: 0.209151
[13:29:21.157] iteration 16179: total_loss: 0.592042, loss_sup: 0.058829, loss_mps: 0.177145, loss_cps: 0.356068
[13:29:21.302] iteration 16180: total_loss: 0.873446, loss_sup: 0.102756, loss_mps: 0.222828, loss_cps: 0.547862
[13:29:21.447] iteration 16181: total_loss: 0.779750, loss_sup: 0.049560, loss_mps: 0.228879, loss_cps: 0.501311
[13:29:21.594] iteration 16182: total_loss: 0.261688, loss_sup: 0.018584, loss_mps: 0.085908, loss_cps: 0.157196
[13:29:21.740] iteration 16183: total_loss: 0.896886, loss_sup: 0.241812, loss_mps: 0.202500, loss_cps: 0.452574
[13:29:21.886] iteration 16184: total_loss: 1.076605, loss_sup: 0.220916, loss_mps: 0.264866, loss_cps: 0.590823
[13:29:22.032] iteration 16185: total_loss: 0.558572, loss_sup: 0.225169, loss_mps: 0.119258, loss_cps: 0.214145
[13:29:22.179] iteration 16186: total_loss: 0.410637, loss_sup: 0.035365, loss_mps: 0.131249, loss_cps: 0.244023
[13:29:22.327] iteration 16187: total_loss: 0.482113, loss_sup: 0.043497, loss_mps: 0.143887, loss_cps: 0.294729
[13:29:22.476] iteration 16188: total_loss: 0.940891, loss_sup: 0.145948, loss_mps: 0.259119, loss_cps: 0.535824
[13:29:22.622] iteration 16189: total_loss: 0.205376, loss_sup: 0.003947, loss_mps: 0.075365, loss_cps: 0.126064
[13:29:22.769] iteration 16190: total_loss: 0.377453, loss_sup: 0.018024, loss_mps: 0.124111, loss_cps: 0.235318
[13:29:22.915] iteration 16191: total_loss: 0.401309, loss_sup: 0.107028, loss_mps: 0.104514, loss_cps: 0.189767
[13:29:23.062] iteration 16192: total_loss: 0.504039, loss_sup: 0.082055, loss_mps: 0.147802, loss_cps: 0.274181
[13:29:23.208] iteration 16193: total_loss: 0.578788, loss_sup: 0.031500, loss_mps: 0.178132, loss_cps: 0.369156
[13:29:23.354] iteration 16194: total_loss: 0.399166, loss_sup: 0.074852, loss_mps: 0.120310, loss_cps: 0.204005
[13:29:23.501] iteration 16195: total_loss: 0.764410, loss_sup: 0.129362, loss_mps: 0.202693, loss_cps: 0.432355
[13:29:23.646] iteration 16196: total_loss: 0.482697, loss_sup: 0.043202, loss_mps: 0.145379, loss_cps: 0.294115
[13:29:23.792] iteration 16197: total_loss: 0.309357, loss_sup: 0.031730, loss_mps: 0.104973, loss_cps: 0.172654
[13:29:23.939] iteration 16198: total_loss: 0.458392, loss_sup: 0.069468, loss_mps: 0.133459, loss_cps: 0.255466
[13:29:24.084] iteration 16199: total_loss: 0.425350, loss_sup: 0.109369, loss_mps: 0.110128, loss_cps: 0.205853
[13:29:24.231] iteration 16200: total_loss: 0.449297, loss_sup: 0.041599, loss_mps: 0.139714, loss_cps: 0.267984
[13:29:24.231] Evaluation Started ==>
[13:29:35.569] ==> valid iteration 16200: unet metrics: {'dc': 0.6094956334261222, 'jc': 0.4922283042913018, 'pre': 0.7632443940415299, 'hd': 5.630708477892058}, ynet metrics: {'dc': 0.6227864568534971, 'jc': 0.5062384189372299, 'pre': 0.7573656985045343, 'hd': 5.6189949523856075}.
[13:29:35.571] Evaluation Finished!⏹️
[13:29:35.722] iteration 16201: total_loss: 0.546170, loss_sup: 0.206867, loss_mps: 0.111474, loss_cps: 0.227829
[13:29:35.868] iteration 16202: total_loss: 0.970179, loss_sup: 0.240288, loss_mps: 0.233266, loss_cps: 0.496625
[13:29:36.017] iteration 16203: total_loss: 0.314262, loss_sup: 0.032666, loss_mps: 0.106167, loss_cps: 0.175430
[13:29:36.163] iteration 16204: total_loss: 0.307245, loss_sup: 0.037460, loss_mps: 0.098943, loss_cps: 0.170842
[13:29:36.308] iteration 16205: total_loss: 0.427543, loss_sup: 0.160933, loss_mps: 0.098524, loss_cps: 0.168086
[13:29:36.453] iteration 16206: total_loss: 1.202797, loss_sup: 0.269964, loss_mps: 0.284889, loss_cps: 0.647944
[13:29:36.599] iteration 16207: total_loss: 0.587756, loss_sup: 0.239519, loss_mps: 0.127254, loss_cps: 0.220983
[13:29:36.744] iteration 16208: total_loss: 0.681959, loss_sup: 0.049704, loss_mps: 0.206341, loss_cps: 0.425914
[13:29:36.889] iteration 16209: total_loss: 0.357128, loss_sup: 0.027908, loss_mps: 0.120531, loss_cps: 0.208689
[13:29:37.034] iteration 16210: total_loss: 0.369187, loss_sup: 0.033455, loss_mps: 0.121666, loss_cps: 0.214066
[13:29:37.179] iteration 16211: total_loss: 0.484355, loss_sup: 0.051039, loss_mps: 0.151015, loss_cps: 0.282300
[13:29:37.324] iteration 16212: total_loss: 0.485507, loss_sup: 0.094626, loss_mps: 0.139027, loss_cps: 0.251855
[13:29:37.469] iteration 16213: total_loss: 1.111342, loss_sup: 0.224919, loss_mps: 0.278733, loss_cps: 0.607690
[13:29:37.616] iteration 16214: total_loss: 0.478175, loss_sup: 0.006972, loss_mps: 0.156430, loss_cps: 0.314773
[13:29:37.762] iteration 16215: total_loss: 0.463139, loss_sup: 0.084349, loss_mps: 0.129657, loss_cps: 0.249133
[13:29:37.909] iteration 16216: total_loss: 0.269464, loss_sup: 0.004704, loss_mps: 0.096518, loss_cps: 0.168242
[13:29:38.056] iteration 16217: total_loss: 0.462003, loss_sup: 0.034242, loss_mps: 0.141693, loss_cps: 0.286068
[13:29:38.201] iteration 16218: total_loss: 0.608316, loss_sup: 0.116236, loss_mps: 0.167443, loss_cps: 0.324637
[13:29:38.347] iteration 16219: total_loss: 0.541964, loss_sup: 0.127100, loss_mps: 0.142737, loss_cps: 0.272126
[13:29:38.492] iteration 16220: total_loss: 0.393707, loss_sup: 0.117065, loss_mps: 0.100471, loss_cps: 0.176171
[13:29:38.637] iteration 16221: total_loss: 0.342929, loss_sup: 0.046902, loss_mps: 0.105471, loss_cps: 0.190557
[13:29:38.783] iteration 16222: total_loss: 0.352762, loss_sup: 0.081627, loss_mps: 0.095967, loss_cps: 0.175169
[13:29:38.928] iteration 16223: total_loss: 0.418927, loss_sup: 0.051364, loss_mps: 0.130467, loss_cps: 0.237096
[13:29:39.074] iteration 16224: total_loss: 0.356112, loss_sup: 0.087657, loss_mps: 0.097760, loss_cps: 0.170695
[13:29:39.219] iteration 16225: total_loss: 0.607658, loss_sup: 0.044734, loss_mps: 0.191488, loss_cps: 0.371436
[13:29:39.364] iteration 16226: total_loss: 0.916560, loss_sup: 0.110962, loss_mps: 0.257494, loss_cps: 0.548103
[13:29:39.513] iteration 16227: total_loss: 0.501725, loss_sup: 0.017423, loss_mps: 0.165248, loss_cps: 0.319054
[13:29:39.659] iteration 16228: total_loss: 0.591253, loss_sup: 0.189184, loss_mps: 0.138654, loss_cps: 0.263415
[13:29:39.804] iteration 16229: total_loss: 0.367323, loss_sup: 0.031656, loss_mps: 0.112836, loss_cps: 0.222832
[13:29:39.953] iteration 16230: total_loss: 0.941970, loss_sup: 0.325729, loss_mps: 0.194522, loss_cps: 0.421720
[13:29:40.098] iteration 16231: total_loss: 0.377739, loss_sup: 0.062231, loss_mps: 0.107936, loss_cps: 0.207573
[13:29:40.243] iteration 16232: total_loss: 0.422352, loss_sup: 0.027363, loss_mps: 0.128697, loss_cps: 0.266293
[13:29:40.389] iteration 16233: total_loss: 0.693833, loss_sup: 0.102536, loss_mps: 0.189403, loss_cps: 0.401894
[13:29:40.535] iteration 16234: total_loss: 0.374311, loss_sup: 0.104524, loss_mps: 0.102384, loss_cps: 0.167403
[13:29:40.680] iteration 16235: total_loss: 0.447392, loss_sup: 0.086832, loss_mps: 0.129794, loss_cps: 0.230766
[13:29:40.826] iteration 16236: total_loss: 0.455058, loss_sup: 0.092515, loss_mps: 0.128090, loss_cps: 0.234454
[13:29:40.972] iteration 16237: total_loss: 0.545350, loss_sup: 0.130412, loss_mps: 0.144426, loss_cps: 0.270511
[13:29:41.117] iteration 16238: total_loss: 0.342272, loss_sup: 0.019673, loss_mps: 0.115696, loss_cps: 0.206903
[13:29:41.263] iteration 16239: total_loss: 0.479985, loss_sup: 0.056299, loss_mps: 0.143234, loss_cps: 0.280452
[13:29:41.410] iteration 16240: total_loss: 0.411862, loss_sup: 0.022695, loss_mps: 0.134454, loss_cps: 0.254714
[13:29:41.557] iteration 16241: total_loss: 0.898162, loss_sup: 0.021773, loss_mps: 0.271217, loss_cps: 0.605172
[13:29:41.702] iteration 16242: total_loss: 0.496121, loss_sup: 0.092605, loss_mps: 0.130362, loss_cps: 0.273154
[13:29:41.848] iteration 16243: total_loss: 1.186445, loss_sup: 0.181967, loss_mps: 0.302812, loss_cps: 0.701666
[13:29:41.997] iteration 16244: total_loss: 0.434406, loss_sup: 0.036143, loss_mps: 0.139603, loss_cps: 0.258659
[13:29:42.142] iteration 16245: total_loss: 0.403677, loss_sup: 0.112176, loss_mps: 0.108011, loss_cps: 0.183490
[13:29:42.292] iteration 16246: total_loss: 0.421567, loss_sup: 0.011106, loss_mps: 0.140900, loss_cps: 0.269561
[13:29:42.438] iteration 16247: total_loss: 0.338677, loss_sup: 0.051250, loss_mps: 0.101371, loss_cps: 0.186057
[13:29:42.588] iteration 16248: total_loss: 0.754204, loss_sup: 0.144672, loss_mps: 0.193263, loss_cps: 0.416269
[13:29:42.734] iteration 16249: total_loss: 0.612739, loss_sup: 0.106330, loss_mps: 0.168486, loss_cps: 0.337923
[13:29:42.880] iteration 16250: total_loss: 0.921614, loss_sup: 0.292294, loss_mps: 0.216643, loss_cps: 0.412677
[13:29:43.029] iteration 16251: total_loss: 0.293772, loss_sup: 0.004770, loss_mps: 0.103354, loss_cps: 0.185648
[13:29:43.175] iteration 16252: total_loss: 0.593784, loss_sup: 0.147020, loss_mps: 0.158014, loss_cps: 0.288751
[13:29:43.321] iteration 16253: total_loss: 0.618693, loss_sup: 0.194164, loss_mps: 0.153063, loss_cps: 0.271466
[13:29:43.468] iteration 16254: total_loss: 0.523367, loss_sup: 0.088744, loss_mps: 0.149515, loss_cps: 0.285108
[13:29:43.614] iteration 16255: total_loss: 0.450220, loss_sup: 0.123060, loss_mps: 0.114547, loss_cps: 0.212612
[13:29:43.760] iteration 16256: total_loss: 0.880683, loss_sup: 0.240990, loss_mps: 0.213798, loss_cps: 0.425896
[13:29:43.905] iteration 16257: total_loss: 0.518925, loss_sup: 0.070024, loss_mps: 0.151553, loss_cps: 0.297348
[13:29:44.052] iteration 16258: total_loss: 0.525840, loss_sup: 0.132487, loss_mps: 0.132039, loss_cps: 0.261313
[13:29:44.198] iteration 16259: total_loss: 0.451349, loss_sup: 0.036431, loss_mps: 0.144804, loss_cps: 0.270114
[13:29:44.345] iteration 16260: total_loss: 0.343381, loss_sup: 0.040400, loss_mps: 0.109897, loss_cps: 0.193084
[13:29:44.491] iteration 16261: total_loss: 0.234095, loss_sup: 0.028523, loss_mps: 0.075228, loss_cps: 0.130344
[13:29:44.636] iteration 16262: total_loss: 0.416632, loss_sup: 0.108433, loss_mps: 0.106475, loss_cps: 0.201724
[13:29:44.782] iteration 16263: total_loss: 0.403515, loss_sup: 0.058295, loss_mps: 0.124549, loss_cps: 0.220671
[13:29:44.928] iteration 16264: total_loss: 0.471157, loss_sup: 0.103639, loss_mps: 0.122033, loss_cps: 0.245485
[13:29:45.075] iteration 16265: total_loss: 0.508280, loss_sup: 0.183225, loss_mps: 0.119191, loss_cps: 0.205864
[13:29:45.222] iteration 16266: total_loss: 0.415180, loss_sup: 0.093871, loss_mps: 0.113453, loss_cps: 0.207856
[13:29:45.371] iteration 16267: total_loss: 0.316964, loss_sup: 0.083661, loss_mps: 0.087101, loss_cps: 0.146202
[13:29:45.518] iteration 16268: total_loss: 0.541551, loss_sup: 0.132415, loss_mps: 0.141026, loss_cps: 0.268110
[13:29:45.664] iteration 16269: total_loss: 0.508402, loss_sup: 0.156983, loss_mps: 0.121207, loss_cps: 0.230212
[13:29:45.811] iteration 16270: total_loss: 0.523463, loss_sup: 0.117663, loss_mps: 0.139941, loss_cps: 0.265860
[13:29:45.956] iteration 16271: total_loss: 0.580224, loss_sup: 0.084946, loss_mps: 0.160650, loss_cps: 0.334628
[13:29:46.102] iteration 16272: total_loss: 0.656100, loss_sup: 0.096678, loss_mps: 0.187049, loss_cps: 0.372373
[13:29:46.248] iteration 16273: total_loss: 0.337484, loss_sup: 0.077443, loss_mps: 0.092232, loss_cps: 0.167809
[13:29:46.394] iteration 16274: total_loss: 0.381495, loss_sup: 0.028098, loss_mps: 0.130605, loss_cps: 0.222792
[13:29:46.542] iteration 16275: total_loss: 0.519829, loss_sup: 0.095116, loss_mps: 0.140687, loss_cps: 0.284026
[13:29:46.690] iteration 16276: total_loss: 0.443931, loss_sup: 0.073700, loss_mps: 0.130007, loss_cps: 0.240224
[13:29:46.836] iteration 16277: total_loss: 0.515554, loss_sup: 0.074708, loss_mps: 0.142382, loss_cps: 0.298463
[13:29:46.981] iteration 16278: total_loss: 0.377020, loss_sup: 0.086976, loss_mps: 0.110660, loss_cps: 0.179384
[13:29:47.129] iteration 16279: total_loss: 0.466770, loss_sup: 0.132840, loss_mps: 0.117612, loss_cps: 0.216318
[13:29:47.275] iteration 16280: total_loss: 0.364289, loss_sup: 0.040250, loss_mps: 0.113896, loss_cps: 0.210144
[13:29:47.421] iteration 16281: total_loss: 0.746419, loss_sup: 0.163341, loss_mps: 0.189997, loss_cps: 0.393081
[13:29:47.567] iteration 16282: total_loss: 0.515343, loss_sup: 0.143758, loss_mps: 0.129044, loss_cps: 0.242541
[13:29:47.714] iteration 16283: total_loss: 0.449401, loss_sup: 0.127013, loss_mps: 0.114428, loss_cps: 0.207960
[13:29:47.860] iteration 16284: total_loss: 0.462153, loss_sup: 0.100596, loss_mps: 0.133826, loss_cps: 0.227731
[13:29:48.006] iteration 16285: total_loss: 0.359266, loss_sup: 0.043085, loss_mps: 0.112419, loss_cps: 0.203762
[13:29:48.153] iteration 16286: total_loss: 0.308816, loss_sup: 0.113532, loss_mps: 0.075277, loss_cps: 0.120007
[13:29:48.299] iteration 16287: total_loss: 0.337388, loss_sup: 0.057202, loss_mps: 0.099742, loss_cps: 0.180445
[13:29:48.445] iteration 16288: total_loss: 0.371786, loss_sup: 0.041412, loss_mps: 0.117849, loss_cps: 0.212525
[13:29:48.591] iteration 16289: total_loss: 0.500093, loss_sup: 0.078175, loss_mps: 0.145640, loss_cps: 0.276278
[13:29:48.736] iteration 16290: total_loss: 0.364147, loss_sup: 0.047167, loss_mps: 0.118566, loss_cps: 0.198414
[13:29:48.882] iteration 16291: total_loss: 0.493467, loss_sup: 0.044628, loss_mps: 0.158830, loss_cps: 0.290009
[13:29:49.028] iteration 16292: total_loss: 0.454025, loss_sup: 0.205177, loss_mps: 0.092020, loss_cps: 0.156827
[13:29:49.174] iteration 16293: total_loss: 0.598559, loss_sup: 0.022451, loss_mps: 0.189558, loss_cps: 0.386550
[13:29:49.319] iteration 16294: total_loss: 0.245891, loss_sup: 0.008294, loss_mps: 0.088445, loss_cps: 0.149152
[13:29:49.465] iteration 16295: total_loss: 0.700512, loss_sup: 0.103629, loss_mps: 0.197055, loss_cps: 0.399827
[13:29:49.611] iteration 16296: total_loss: 0.324267, loss_sup: 0.037811, loss_mps: 0.103322, loss_cps: 0.183135
[13:29:49.756] iteration 16297: total_loss: 0.432538, loss_sup: 0.110194, loss_mps: 0.119265, loss_cps: 0.203079
[13:29:49.903] iteration 16298: total_loss: 0.270582, loss_sup: 0.013373, loss_mps: 0.096138, loss_cps: 0.161070
[13:29:50.049] iteration 16299: total_loss: 0.250925, loss_sup: 0.003665, loss_mps: 0.092267, loss_cps: 0.154993
[13:29:50.195] iteration 16300: total_loss: 0.430539, loss_sup: 0.032917, loss_mps: 0.130549, loss_cps: 0.267073
[13:29:50.195] Evaluation Started ==>
[13:30:01.552] ==> valid iteration 16300: unet metrics: {'dc': 0.6599679988406414, 'jc': 0.540240977336192, 'pre': 0.7805392295238295, 'hd': 5.5390945916187695}, ynet metrics: {'dc': 0.6136590283844685, 'jc': 0.4943876868117377, 'pre': 0.8044283004479011, 'hd': 5.598734107939843}.
[13:30:01.554] Evaluation Finished!⏹️
[13:30:01.706] iteration 16301: total_loss: 0.648787, loss_sup: 0.277381, loss_mps: 0.134017, loss_cps: 0.237390
[13:30:01.776] iteration 16302: total_loss: 1.346519, loss_sup: 0.005056, loss_mps: 0.386328, loss_cps: 0.955134
[13:30:02.993] iteration 16303: total_loss: 0.513032, loss_sup: 0.164152, loss_mps: 0.127805, loss_cps: 0.221075
[13:30:03.141] iteration 16304: total_loss: 0.407600, loss_sup: 0.036497, loss_mps: 0.121830, loss_cps: 0.249273
[13:30:03.287] iteration 16305: total_loss: 0.351730, loss_sup: 0.054851, loss_mps: 0.100543, loss_cps: 0.196336
[13:30:03.434] iteration 16306: total_loss: 0.868378, loss_sup: 0.083555, loss_mps: 0.241651, loss_cps: 0.543173
[13:30:03.583] iteration 16307: total_loss: 0.354564, loss_sup: 0.040227, loss_mps: 0.106723, loss_cps: 0.207615
[13:30:03.729] iteration 16308: total_loss: 0.546901, loss_sup: 0.044859, loss_mps: 0.163382, loss_cps: 0.338660
[13:30:03.874] iteration 16309: total_loss: 0.482895, loss_sup: 0.056257, loss_mps: 0.142199, loss_cps: 0.284439
[13:30:04.020] iteration 16310: total_loss: 0.705134, loss_sup: 0.090950, loss_mps: 0.188938, loss_cps: 0.425246
[13:30:04.165] iteration 16311: total_loss: 0.560445, loss_sup: 0.078581, loss_mps: 0.149310, loss_cps: 0.332554
[13:30:04.311] iteration 16312: total_loss: 0.380153, loss_sup: 0.037592, loss_mps: 0.118345, loss_cps: 0.224216
[13:30:04.456] iteration 16313: total_loss: 0.738374, loss_sup: 0.220174, loss_mps: 0.160219, loss_cps: 0.357981
[13:30:04.601] iteration 16314: total_loss: 0.346318, loss_sup: 0.038178, loss_mps: 0.102161, loss_cps: 0.205979
[13:30:04.747] iteration 16315: total_loss: 0.662402, loss_sup: 0.035197, loss_mps: 0.187252, loss_cps: 0.439954
[13:30:04.891] iteration 16316: total_loss: 0.546358, loss_sup: 0.148684, loss_mps: 0.131129, loss_cps: 0.266545
[13:30:05.036] iteration 16317: total_loss: 0.298688, loss_sup: 0.023033, loss_mps: 0.092575, loss_cps: 0.183080
[13:30:05.182] iteration 16318: total_loss: 0.470088, loss_sup: 0.137671, loss_mps: 0.113576, loss_cps: 0.218841
[13:30:05.330] iteration 16319: total_loss: 0.922846, loss_sup: 0.147173, loss_mps: 0.250701, loss_cps: 0.524972
[13:30:05.474] iteration 16320: total_loss: 1.065283, loss_sup: 0.277433, loss_mps: 0.235159, loss_cps: 0.552691
[13:30:05.620] iteration 16321: total_loss: 0.681240, loss_sup: 0.221133, loss_mps: 0.154549, loss_cps: 0.305558
[13:30:05.765] iteration 16322: total_loss: 0.685010, loss_sup: 0.169683, loss_mps: 0.161665, loss_cps: 0.353661
[13:30:05.910] iteration 16323: total_loss: 0.662829, loss_sup: 0.134594, loss_mps: 0.165016, loss_cps: 0.363220
[13:30:06.055] iteration 16324: total_loss: 0.583349, loss_sup: 0.102326, loss_mps: 0.156273, loss_cps: 0.324751
[13:30:06.200] iteration 16325: total_loss: 0.335802, loss_sup: 0.028738, loss_mps: 0.106712, loss_cps: 0.200351
[13:30:06.346] iteration 16326: total_loss: 0.537627, loss_sup: 0.061736, loss_mps: 0.156437, loss_cps: 0.319453
[13:30:06.492] iteration 16327: total_loss: 0.368353, loss_sup: 0.012820, loss_mps: 0.120488, loss_cps: 0.235046
[13:30:06.637] iteration 16328: total_loss: 0.466307, loss_sup: 0.148805, loss_mps: 0.111248, loss_cps: 0.206254
[13:30:06.782] iteration 16329: total_loss: 0.513348, loss_sup: 0.190889, loss_mps: 0.115045, loss_cps: 0.207414
[13:30:06.928] iteration 16330: total_loss: 0.891510, loss_sup: 0.037525, loss_mps: 0.274437, loss_cps: 0.579548
[13:30:07.073] iteration 16331: total_loss: 0.431532, loss_sup: 0.047952, loss_mps: 0.139233, loss_cps: 0.244347
[13:30:07.218] iteration 16332: total_loss: 0.496522, loss_sup: 0.064866, loss_mps: 0.152637, loss_cps: 0.279019
[13:30:07.363] iteration 16333: total_loss: 0.734435, loss_sup: 0.073086, loss_mps: 0.204620, loss_cps: 0.456729
[13:30:07.509] iteration 16334: total_loss: 0.395647, loss_sup: 0.075371, loss_mps: 0.113185, loss_cps: 0.207091
[13:30:07.654] iteration 16335: total_loss: 0.498010, loss_sup: 0.028717, loss_mps: 0.147063, loss_cps: 0.322230
[13:30:07.801] iteration 16336: total_loss: 0.436977, loss_sup: 0.179192, loss_mps: 0.097109, loss_cps: 0.160677
[13:30:07.946] iteration 16337: total_loss: 0.457091, loss_sup: 0.048885, loss_mps: 0.139562, loss_cps: 0.268645
[13:30:08.093] iteration 16338: total_loss: 0.282320, loss_sup: 0.023393, loss_mps: 0.092555, loss_cps: 0.166373
[13:30:08.238] iteration 16339: total_loss: 0.341425, loss_sup: 0.089006, loss_mps: 0.100612, loss_cps: 0.151807
[13:30:08.386] iteration 16340: total_loss: 0.961888, loss_sup: 0.176855, loss_mps: 0.250761, loss_cps: 0.534272
[13:30:08.531] iteration 16341: total_loss: 0.864502, loss_sup: 0.202616, loss_mps: 0.222114, loss_cps: 0.439771
[13:30:08.677] iteration 16342: total_loss: 0.399094, loss_sup: 0.025761, loss_mps: 0.128803, loss_cps: 0.244529
[13:30:08.822] iteration 16343: total_loss: 0.473035, loss_sup: 0.089210, loss_mps: 0.132406, loss_cps: 0.251420
[13:30:08.968] iteration 16344: total_loss: 0.285128, loss_sup: 0.022436, loss_mps: 0.096214, loss_cps: 0.166478
[13:30:09.113] iteration 16345: total_loss: 0.541527, loss_sup: 0.125976, loss_mps: 0.138952, loss_cps: 0.276599
[13:30:09.259] iteration 16346: total_loss: 0.455420, loss_sup: 0.191488, loss_mps: 0.094369, loss_cps: 0.169563
[13:30:09.406] iteration 16347: total_loss: 0.858134, loss_sup: 0.173142, loss_mps: 0.224967, loss_cps: 0.460025
[13:30:09.552] iteration 16348: total_loss: 0.454159, loss_sup: 0.066446, loss_mps: 0.129467, loss_cps: 0.258245
[13:30:09.698] iteration 16349: total_loss: 0.471790, loss_sup: 0.102341, loss_mps: 0.127829, loss_cps: 0.241621
[13:30:09.845] iteration 16350: total_loss: 0.522997, loss_sup: 0.045365, loss_mps: 0.161095, loss_cps: 0.316537
[13:30:09.991] iteration 16351: total_loss: 0.310357, loss_sup: 0.015906, loss_mps: 0.105138, loss_cps: 0.189313
[13:30:10.141] iteration 16352: total_loss: 0.436554, loss_sup: 0.042851, loss_mps: 0.133277, loss_cps: 0.260426
[13:30:10.287] iteration 16353: total_loss: 0.353610, loss_sup: 0.069638, loss_mps: 0.103796, loss_cps: 0.180176
[13:30:10.432] iteration 16354: total_loss: 0.606094, loss_sup: 0.057169, loss_mps: 0.186283, loss_cps: 0.362642
[13:30:10.579] iteration 16355: total_loss: 0.349598, loss_sup: 0.038529, loss_mps: 0.109444, loss_cps: 0.201626
[13:30:10.725] iteration 16356: total_loss: 0.354673, loss_sup: 0.055223, loss_mps: 0.104058, loss_cps: 0.195393
[13:30:10.870] iteration 16357: total_loss: 0.656945, loss_sup: 0.158795, loss_mps: 0.164473, loss_cps: 0.333676
[13:30:11.016] iteration 16358: total_loss: 0.408101, loss_sup: 0.041282, loss_mps: 0.126789, loss_cps: 0.240030
[13:30:11.162] iteration 16359: total_loss: 0.884731, loss_sup: 0.274673, loss_mps: 0.196000, loss_cps: 0.414058
[13:30:11.310] iteration 16360: total_loss: 0.419636, loss_sup: 0.023859, loss_mps: 0.138482, loss_cps: 0.257295
[13:30:11.458] iteration 16361: total_loss: 0.355676, loss_sup: 0.087034, loss_mps: 0.100752, loss_cps: 0.167891
[13:30:11.607] iteration 16362: total_loss: 0.628549, loss_sup: 0.115380, loss_mps: 0.166509, loss_cps: 0.346661
[13:30:11.753] iteration 16363: total_loss: 0.525982, loss_sup: 0.058001, loss_mps: 0.159697, loss_cps: 0.308284
[13:30:11.899] iteration 16364: total_loss: 0.530128, loss_sup: 0.083969, loss_mps: 0.150468, loss_cps: 0.295691
[13:30:12.045] iteration 16365: total_loss: 0.518763, loss_sup: 0.082428, loss_mps: 0.143523, loss_cps: 0.292813
[13:30:12.191] iteration 16366: total_loss: 1.154648, loss_sup: 0.094865, loss_mps: 0.326149, loss_cps: 0.733635
[13:30:12.337] iteration 16367: total_loss: 0.361573, loss_sup: 0.035676, loss_mps: 0.115838, loss_cps: 0.210059
[13:30:12.484] iteration 16368: total_loss: 0.443765, loss_sup: 0.035935, loss_mps: 0.138798, loss_cps: 0.269033
[13:30:12.630] iteration 16369: total_loss: 0.816216, loss_sup: 0.065045, loss_mps: 0.241219, loss_cps: 0.509952
[13:30:12.777] iteration 16370: total_loss: 0.495339, loss_sup: 0.045699, loss_mps: 0.154589, loss_cps: 0.295051
[13:30:12.923] iteration 16371: total_loss: 0.440481, loss_sup: 0.121655, loss_mps: 0.107847, loss_cps: 0.210979
[13:30:13.069] iteration 16372: total_loss: 0.540242, loss_sup: 0.012687, loss_mps: 0.159541, loss_cps: 0.368013
[13:30:13.215] iteration 16373: total_loss: 0.538469, loss_sup: 0.130754, loss_mps: 0.139583, loss_cps: 0.268132
[13:30:13.361] iteration 16374: total_loss: 1.007706, loss_sup: 0.181204, loss_mps: 0.253622, loss_cps: 0.572881
[13:30:13.508] iteration 16375: total_loss: 0.662344, loss_sup: 0.081758, loss_mps: 0.193838, loss_cps: 0.386747
[13:30:13.655] iteration 16376: total_loss: 0.531219, loss_sup: 0.200499, loss_mps: 0.117347, loss_cps: 0.213373
[13:30:13.803] iteration 16377: total_loss: 0.577386, loss_sup: 0.192122, loss_mps: 0.136815, loss_cps: 0.248449
[13:30:13.949] iteration 16378: total_loss: 0.656712, loss_sup: 0.074127, loss_mps: 0.193058, loss_cps: 0.389526
[13:30:14.097] iteration 16379: total_loss: 0.492452, loss_sup: 0.142114, loss_mps: 0.124244, loss_cps: 0.226094
[13:30:14.244] iteration 16380: total_loss: 0.585641, loss_sup: 0.045368, loss_mps: 0.180526, loss_cps: 0.359746
[13:30:14.390] iteration 16381: total_loss: 0.424062, loss_sup: 0.100205, loss_mps: 0.112793, loss_cps: 0.211064
[13:30:14.536] iteration 16382: total_loss: 0.556338, loss_sup: 0.117244, loss_mps: 0.140225, loss_cps: 0.298870
[13:30:14.683] iteration 16383: total_loss: 0.915063, loss_sup: 0.290191, loss_mps: 0.197978, loss_cps: 0.426893
[13:30:14.829] iteration 16384: total_loss: 0.398618, loss_sup: 0.056497, loss_mps: 0.118244, loss_cps: 0.223878
[13:30:14.976] iteration 16385: total_loss: 0.424091, loss_sup: 0.019944, loss_mps: 0.141423, loss_cps: 0.262724
[13:30:15.122] iteration 16386: total_loss: 0.710062, loss_sup: 0.086097, loss_mps: 0.212034, loss_cps: 0.411931
[13:30:15.269] iteration 16387: total_loss: 0.637324, loss_sup: 0.204911, loss_mps: 0.144741, loss_cps: 0.287672
[13:30:15.416] iteration 16388: total_loss: 0.494835, loss_sup: 0.132048, loss_mps: 0.126467, loss_cps: 0.236321
[13:30:15.563] iteration 16389: total_loss: 0.841954, loss_sup: 0.337663, loss_mps: 0.190425, loss_cps: 0.313866
[13:30:15.709] iteration 16390: total_loss: 0.420241, loss_sup: 0.128215, loss_mps: 0.108384, loss_cps: 0.183642
[13:30:15.855] iteration 16391: total_loss: 0.711856, loss_sup: 0.079631, loss_mps: 0.212068, loss_cps: 0.420157
[13:30:16.002] iteration 16392: total_loss: 0.474629, loss_sup: 0.044058, loss_mps: 0.145596, loss_cps: 0.284975
[13:30:16.149] iteration 16393: total_loss: 0.467265, loss_sup: 0.067299, loss_mps: 0.139010, loss_cps: 0.260956
[13:30:16.296] iteration 16394: total_loss: 0.370362, loss_sup: 0.015486, loss_mps: 0.121107, loss_cps: 0.233769
[13:30:16.446] iteration 16395: total_loss: 0.628812, loss_sup: 0.102858, loss_mps: 0.179047, loss_cps: 0.346907
[13:30:16.594] iteration 16396: total_loss: 1.076922, loss_sup: 0.148376, loss_mps: 0.297517, loss_cps: 0.631030
[13:30:16.744] iteration 16397: total_loss: 0.520104, loss_sup: 0.088685, loss_mps: 0.151946, loss_cps: 0.279473
[13:30:16.890] iteration 16398: total_loss: 0.848640, loss_sup: 0.098176, loss_mps: 0.232019, loss_cps: 0.518445
[13:30:17.036] iteration 16399: total_loss: 0.537820, loss_sup: 0.088305, loss_mps: 0.159691, loss_cps: 0.289823
[13:30:17.183] iteration 16400: total_loss: 0.333722, loss_sup: 0.070917, loss_mps: 0.098459, loss_cps: 0.164346
[13:30:17.183] Evaluation Started ==>
[13:30:28.515] ==> valid iteration 16400: unet metrics: {'dc': 0.6305155315755556, 'jc': 0.5127721859114517, 'pre': 0.7884998298987691, 'hd': 5.571605632897072}, ynet metrics: {'dc': 0.5995818532409551, 'jc': 0.4855105380167093, 'pre': 0.7979670746744314, 'hd': 5.451586900395043}.
[13:30:28.517] Evaluation Finished!⏹️
[13:30:28.667] iteration 16401: total_loss: 0.731904, loss_sup: 0.218135, loss_mps: 0.172014, loss_cps: 0.341755
[13:30:28.814] iteration 16402: total_loss: 0.579878, loss_sup: 0.061718, loss_mps: 0.176847, loss_cps: 0.341313
[13:30:28.962] iteration 16403: total_loss: 0.513797, loss_sup: 0.067591, loss_mps: 0.158588, loss_cps: 0.287618
[13:30:29.108] iteration 16404: total_loss: 0.703576, loss_sup: 0.347150, loss_mps: 0.129054, loss_cps: 0.227372
[13:30:29.254] iteration 16405: total_loss: 0.431548, loss_sup: 0.033539, loss_mps: 0.138873, loss_cps: 0.259136
[13:30:29.400] iteration 16406: total_loss: 0.313117, loss_sup: 0.060815, loss_mps: 0.094242, loss_cps: 0.158061
[13:30:29.549] iteration 16407: total_loss: 0.470119, loss_sup: 0.077863, loss_mps: 0.134835, loss_cps: 0.257421
[13:30:29.695] iteration 16408: total_loss: 0.344262, loss_sup: 0.023400, loss_mps: 0.111088, loss_cps: 0.209774
[13:30:29.840] iteration 16409: total_loss: 0.726583, loss_sup: 0.042186, loss_mps: 0.217926, loss_cps: 0.466470
[13:30:29.987] iteration 16410: total_loss: 0.737284, loss_sup: 0.099383, loss_mps: 0.198722, loss_cps: 0.439179
[13:30:30.134] iteration 16411: total_loss: 0.383819, loss_sup: 0.046508, loss_mps: 0.116358, loss_cps: 0.220953
[13:30:30.279] iteration 16412: total_loss: 0.639487, loss_sup: 0.196207, loss_mps: 0.156244, loss_cps: 0.287036
[13:30:30.424] iteration 16413: total_loss: 0.590355, loss_sup: 0.092688, loss_mps: 0.164601, loss_cps: 0.333066
[13:30:30.571] iteration 16414: total_loss: 0.457431, loss_sup: 0.080700, loss_mps: 0.139470, loss_cps: 0.237261
[13:30:30.716] iteration 16415: total_loss: 0.513521, loss_sup: 0.099047, loss_mps: 0.130029, loss_cps: 0.284445
[13:30:30.862] iteration 16416: total_loss: 0.363325, loss_sup: 0.082292, loss_mps: 0.104755, loss_cps: 0.176278
[13:30:31.007] iteration 16417: total_loss: 0.571037, loss_sup: 0.113610, loss_mps: 0.157807, loss_cps: 0.299620
[13:30:31.153] iteration 16418: total_loss: 0.515329, loss_sup: 0.148223, loss_mps: 0.133703, loss_cps: 0.233404
[13:30:31.300] iteration 16419: total_loss: 0.522723, loss_sup: 0.161935, loss_mps: 0.133371, loss_cps: 0.227417
[13:30:31.445] iteration 16420: total_loss: 0.550415, loss_sup: 0.063141, loss_mps: 0.155615, loss_cps: 0.331660
[13:30:31.591] iteration 16421: total_loss: 0.629457, loss_sup: 0.142653, loss_mps: 0.169268, loss_cps: 0.317536
[13:30:31.741] iteration 16422: total_loss: 0.476048, loss_sup: 0.060833, loss_mps: 0.141281, loss_cps: 0.273934
[13:30:31.887] iteration 16423: total_loss: 0.443012, loss_sup: 0.174058, loss_mps: 0.099834, loss_cps: 0.169119
[13:30:32.032] iteration 16424: total_loss: 0.565486, loss_sup: 0.228675, loss_mps: 0.120082, loss_cps: 0.216728
[13:30:32.182] iteration 16425: total_loss: 0.265349, loss_sup: 0.006062, loss_mps: 0.097252, loss_cps: 0.162035
[13:30:32.328] iteration 16426: total_loss: 0.607671, loss_sup: 0.083007, loss_mps: 0.176974, loss_cps: 0.347691
[13:30:32.474] iteration 16427: total_loss: 0.751394, loss_sup: 0.226684, loss_mps: 0.182962, loss_cps: 0.341748
[13:30:32.620] iteration 16428: total_loss: 0.276535, loss_sup: 0.051633, loss_mps: 0.087030, loss_cps: 0.137871
[13:30:32.765] iteration 16429: total_loss: 0.626845, loss_sup: 0.026545, loss_mps: 0.195573, loss_cps: 0.404727
[13:30:32.912] iteration 16430: total_loss: 0.593522, loss_sup: 0.075323, loss_mps: 0.176087, loss_cps: 0.342112
[13:30:33.059] iteration 16431: total_loss: 0.333046, loss_sup: 0.032658, loss_mps: 0.113326, loss_cps: 0.187062
[13:30:33.206] iteration 16432: total_loss: 0.511364, loss_sup: 0.127418, loss_mps: 0.142191, loss_cps: 0.241756
[13:30:33.351] iteration 16433: total_loss: 0.392913, loss_sup: 0.060468, loss_mps: 0.110401, loss_cps: 0.222043
[13:30:33.498] iteration 16434: total_loss: 0.361381, loss_sup: 0.084602, loss_mps: 0.100014, loss_cps: 0.176765
[13:30:33.645] iteration 16435: total_loss: 0.526898, loss_sup: 0.014194, loss_mps: 0.167892, loss_cps: 0.344811
[13:30:33.792] iteration 16436: total_loss: 0.659540, loss_sup: 0.254770, loss_mps: 0.136744, loss_cps: 0.268027
[13:30:33.938] iteration 16437: total_loss: 0.465105, loss_sup: 0.019900, loss_mps: 0.145731, loss_cps: 0.299474
[13:30:34.084] iteration 16438: total_loss: 0.356396, loss_sup: 0.008149, loss_mps: 0.117903, loss_cps: 0.230344
[13:30:34.230] iteration 16439: total_loss: 0.564185, loss_sup: 0.140792, loss_mps: 0.146911, loss_cps: 0.276481
[13:30:34.376] iteration 16440: total_loss: 1.007921, loss_sup: 0.282425, loss_mps: 0.225495, loss_cps: 0.500001
[13:30:34.523] iteration 16441: total_loss: 0.463251, loss_sup: 0.179238, loss_mps: 0.105607, loss_cps: 0.178407
[13:30:34.669] iteration 16442: total_loss: 0.487986, loss_sup: 0.093308, loss_mps: 0.134976, loss_cps: 0.259703
[13:30:34.816] iteration 16443: total_loss: 0.428266, loss_sup: 0.110761, loss_mps: 0.112171, loss_cps: 0.205334
[13:30:34.962] iteration 16444: total_loss: 0.471273, loss_sup: 0.028889, loss_mps: 0.148891, loss_cps: 0.293493
[13:30:35.108] iteration 16445: total_loss: 0.614458, loss_sup: 0.093908, loss_mps: 0.162662, loss_cps: 0.357888
[13:30:35.255] iteration 16446: total_loss: 0.522675, loss_sup: 0.059313, loss_mps: 0.150196, loss_cps: 0.313166
[13:30:35.402] iteration 16447: total_loss: 1.301262, loss_sup: 0.030436, loss_mps: 0.389023, loss_cps: 0.881803
[13:30:35.548] iteration 16448: total_loss: 0.646612, loss_sup: 0.101573, loss_mps: 0.175464, loss_cps: 0.369574
[13:30:35.695] iteration 16449: total_loss: 0.427240, loss_sup: 0.022192, loss_mps: 0.131507, loss_cps: 0.273541
[13:30:35.843] iteration 16450: total_loss: 0.760583, loss_sup: 0.061703, loss_mps: 0.208777, loss_cps: 0.490104
[13:30:35.990] iteration 16451: total_loss: 0.524633, loss_sup: 0.120928, loss_mps: 0.136354, loss_cps: 0.267350
[13:30:36.136] iteration 16452: total_loss: 0.506283, loss_sup: 0.167453, loss_mps: 0.119149, loss_cps: 0.219681
[13:30:36.281] iteration 16453: total_loss: 0.677767, loss_sup: 0.033878, loss_mps: 0.209668, loss_cps: 0.434221
[13:30:36.429] iteration 16454: total_loss: 0.484163, loss_sup: 0.106565, loss_mps: 0.130718, loss_cps: 0.246879
[13:30:36.576] iteration 16455: total_loss: 0.573736, loss_sup: 0.244193, loss_mps: 0.109715, loss_cps: 0.219827
[13:30:36.722] iteration 16456: total_loss: 1.060691, loss_sup: 0.316097, loss_mps: 0.236198, loss_cps: 0.508396
[13:30:36.868] iteration 16457: total_loss: 0.333604, loss_sup: 0.056781, loss_mps: 0.100579, loss_cps: 0.176244
[13:30:37.016] iteration 16458: total_loss: 0.877967, loss_sup: 0.138496, loss_mps: 0.223482, loss_cps: 0.515989
[13:30:37.163] iteration 16459: total_loss: 0.617810, loss_sup: 0.118046, loss_mps: 0.162394, loss_cps: 0.337371
[13:30:37.309] iteration 16460: total_loss: 0.634279, loss_sup: 0.128982, loss_mps: 0.160774, loss_cps: 0.344523
[13:30:37.455] iteration 16461: total_loss: 0.617952, loss_sup: 0.057190, loss_mps: 0.187444, loss_cps: 0.373318
[13:30:37.602] iteration 16462: total_loss: 0.435244, loss_sup: 0.120682, loss_mps: 0.113256, loss_cps: 0.201306
[13:30:37.749] iteration 16463: total_loss: 0.454538, loss_sup: 0.026446, loss_mps: 0.148627, loss_cps: 0.279465
[13:30:37.895] iteration 16464: total_loss: 0.379748, loss_sup: 0.001623, loss_mps: 0.127340, loss_cps: 0.250785
[13:30:38.041] iteration 16465: total_loss: 0.567700, loss_sup: 0.059012, loss_mps: 0.164850, loss_cps: 0.343837
[13:30:38.188] iteration 16466: total_loss: 0.580603, loss_sup: 0.272937, loss_mps: 0.104736, loss_cps: 0.202930
[13:30:38.334] iteration 16467: total_loss: 0.584321, loss_sup: 0.044604, loss_mps: 0.182537, loss_cps: 0.357180
[13:30:38.480] iteration 16468: total_loss: 0.438647, loss_sup: 0.064048, loss_mps: 0.132464, loss_cps: 0.242135
[13:30:38.627] iteration 16469: total_loss: 0.574359, loss_sup: 0.041754, loss_mps: 0.175038, loss_cps: 0.357567
[13:30:38.774] iteration 16470: total_loss: 0.279761, loss_sup: 0.024216, loss_mps: 0.100060, loss_cps: 0.155485
[13:30:38.921] iteration 16471: total_loss: 0.427090, loss_sup: 0.070042, loss_mps: 0.124478, loss_cps: 0.232570
[13:30:39.067] iteration 16472: total_loss: 0.577983, loss_sup: 0.140744, loss_mps: 0.157131, loss_cps: 0.280107
[13:30:39.213] iteration 16473: total_loss: 0.727776, loss_sup: 0.099677, loss_mps: 0.209330, loss_cps: 0.418769
[13:30:39.359] iteration 16474: total_loss: 0.597496, loss_sup: 0.075315, loss_mps: 0.175367, loss_cps: 0.346815
[13:30:39.505] iteration 16475: total_loss: 0.362163, loss_sup: 0.021353, loss_mps: 0.120285, loss_cps: 0.220525
[13:30:39.651] iteration 16476: total_loss: 0.463991, loss_sup: 0.060623, loss_mps: 0.138505, loss_cps: 0.264863
[13:30:39.798] iteration 16477: total_loss: 0.430862, loss_sup: 0.027065, loss_mps: 0.140335, loss_cps: 0.263463
[13:30:39.944] iteration 16478: total_loss: 0.679390, loss_sup: 0.077356, loss_mps: 0.213205, loss_cps: 0.388829
[13:30:40.090] iteration 16479: total_loss: 0.541063, loss_sup: 0.062709, loss_mps: 0.157446, loss_cps: 0.320908
[13:30:40.240] iteration 16480: total_loss: 0.492151, loss_sup: 0.032494, loss_mps: 0.159707, loss_cps: 0.299950
[13:30:40.391] iteration 16481: total_loss: 0.414071, loss_sup: 0.050377, loss_mps: 0.119212, loss_cps: 0.244483
[13:30:40.538] iteration 16482: total_loss: 0.657105, loss_sup: 0.052974, loss_mps: 0.191822, loss_cps: 0.412308
[13:30:40.687] iteration 16483: total_loss: 0.748209, loss_sup: 0.118551, loss_mps: 0.210131, loss_cps: 0.419527
[13:30:40.834] iteration 16484: total_loss: 0.340930, loss_sup: 0.096673, loss_mps: 0.091699, loss_cps: 0.152558
[13:30:40.981] iteration 16485: total_loss: 0.494278, loss_sup: 0.070632, loss_mps: 0.143993, loss_cps: 0.279653
[13:30:41.129] iteration 16486: total_loss: 0.373615, loss_sup: 0.080527, loss_mps: 0.097796, loss_cps: 0.195293
[13:30:41.276] iteration 16487: total_loss: 0.589659, loss_sup: 0.169580, loss_mps: 0.145639, loss_cps: 0.274440
[13:30:41.422] iteration 16488: total_loss: 0.287285, loss_sup: 0.052869, loss_mps: 0.085131, loss_cps: 0.149284
[13:30:41.570] iteration 16489: total_loss: 0.614350, loss_sup: 0.125303, loss_mps: 0.169046, loss_cps: 0.320001
[13:30:41.716] iteration 16490: total_loss: 0.296156, loss_sup: 0.028898, loss_mps: 0.097284, loss_cps: 0.169974
[13:30:41.863] iteration 16491: total_loss: 0.273130, loss_sup: 0.022425, loss_mps: 0.089053, loss_cps: 0.161653
[13:30:42.010] iteration 16492: total_loss: 0.556978, loss_sup: 0.146952, loss_mps: 0.139189, loss_cps: 0.270837
[13:30:42.156] iteration 16493: total_loss: 0.400808, loss_sup: 0.011124, loss_mps: 0.128347, loss_cps: 0.261337
[13:30:42.304] iteration 16494: total_loss: 0.390447, loss_sup: 0.046421, loss_mps: 0.115616, loss_cps: 0.228410
[13:30:42.451] iteration 16495: total_loss: 0.319858, loss_sup: 0.077320, loss_mps: 0.088270, loss_cps: 0.154268
[13:30:42.597] iteration 16496: total_loss: 0.477594, loss_sup: 0.073947, loss_mps: 0.137003, loss_cps: 0.266644
[13:30:42.743] iteration 16497: total_loss: 1.237976, loss_sup: 0.280607, loss_mps: 0.298912, loss_cps: 0.658457
[13:30:42.889] iteration 16498: total_loss: 0.556304, loss_sup: 0.081776, loss_mps: 0.155370, loss_cps: 0.319159
[13:30:43.036] iteration 16499: total_loss: 0.262303, loss_sup: 0.011364, loss_mps: 0.086807, loss_cps: 0.164132
[13:30:43.182] iteration 16500: total_loss: 0.582057, loss_sup: 0.145269, loss_mps: 0.144104, loss_cps: 0.292684
[13:30:43.182] Evaluation Started ==>
[13:30:54.517] ==> valid iteration 16500: unet metrics: {'dc': 0.6510511585327264, 'jc': 0.5314552814043648, 'pre': 0.8073540475457356, 'hd': 5.554017635534505}, ynet metrics: {'dc': 0.6205959165933458, 'jc': 0.5037442082261647, 'pre': 0.7884709845263037, 'hd': 5.580831548076565}.
[13:30:54.519] Evaluation Finished!⏹️
[13:30:54.670] iteration 16501: total_loss: 0.458544, loss_sup: 0.060722, loss_mps: 0.137021, loss_cps: 0.260801
[13:30:54.819] iteration 16502: total_loss: 0.363038, loss_sup: 0.042254, loss_mps: 0.113886, loss_cps: 0.206898
[13:30:54.965] iteration 16503: total_loss: 0.402762, loss_sup: 0.017555, loss_mps: 0.134029, loss_cps: 0.251178
[13:30:55.111] iteration 16504: total_loss: 0.579315, loss_sup: 0.161224, loss_mps: 0.149476, loss_cps: 0.268616
[13:30:55.256] iteration 16505: total_loss: 0.409836, loss_sup: 0.008843, loss_mps: 0.131446, loss_cps: 0.269547
[13:30:55.402] iteration 16506: total_loss: 0.666828, loss_sup: 0.152682, loss_mps: 0.167438, loss_cps: 0.346708
[13:30:55.547] iteration 16507: total_loss: 0.378209, loss_sup: 0.026962, loss_mps: 0.126498, loss_cps: 0.224749
[13:30:55.693] iteration 16508: total_loss: 0.258029, loss_sup: 0.015774, loss_mps: 0.090263, loss_cps: 0.151992
[13:30:55.839] iteration 16509: total_loss: 0.500360, loss_sup: 0.083195, loss_mps: 0.138471, loss_cps: 0.278694
[13:30:55.985] iteration 16510: total_loss: 0.285415, loss_sup: 0.052904, loss_mps: 0.086453, loss_cps: 0.146058
[13:30:56.131] iteration 16511: total_loss: 0.441404, loss_sup: 0.029770, loss_mps: 0.140440, loss_cps: 0.271195
[13:30:56.280] iteration 16512: total_loss: 0.509613, loss_sup: 0.097417, loss_mps: 0.139358, loss_cps: 0.272838
[13:30:56.426] iteration 16513: total_loss: 0.365631, loss_sup: 0.089663, loss_mps: 0.103363, loss_cps: 0.172604
[13:30:56.574] iteration 16514: total_loss: 0.387982, loss_sup: 0.129109, loss_mps: 0.092748, loss_cps: 0.166126
[13:30:56.719] iteration 16515: total_loss: 0.418416, loss_sup: 0.037954, loss_mps: 0.129667, loss_cps: 0.250794
[13:30:56.865] iteration 16516: total_loss: 0.479553, loss_sup: 0.010451, loss_mps: 0.152743, loss_cps: 0.316359
[13:30:57.011] iteration 16517: total_loss: 0.387244, loss_sup: 0.072007, loss_mps: 0.113365, loss_cps: 0.201872
[13:30:57.159] iteration 16518: total_loss: 0.483373, loss_sup: 0.083955, loss_mps: 0.151234, loss_cps: 0.248184
[13:30:57.304] iteration 16519: total_loss: 0.600563, loss_sup: 0.005519, loss_mps: 0.186604, loss_cps: 0.408439
[13:30:57.451] iteration 16520: total_loss: 0.293454, loss_sup: 0.055807, loss_mps: 0.084892, loss_cps: 0.152755
[13:30:57.597] iteration 16521: total_loss: 0.364189, loss_sup: 0.014338, loss_mps: 0.121704, loss_cps: 0.228147
[13:30:57.743] iteration 16522: total_loss: 0.298308, loss_sup: 0.022164, loss_mps: 0.092974, loss_cps: 0.183170
[13:30:57.892] iteration 16523: total_loss: 0.554354, loss_sup: 0.049879, loss_mps: 0.163223, loss_cps: 0.341252
[13:30:58.037] iteration 16524: total_loss: 0.731159, loss_sup: 0.237861, loss_mps: 0.163140, loss_cps: 0.330158
[13:30:58.184] iteration 16525: total_loss: 0.627940, loss_sup: 0.042336, loss_mps: 0.185853, loss_cps: 0.399751
[13:30:58.331] iteration 16526: total_loss: 0.641391, loss_sup: 0.277346, loss_mps: 0.124466, loss_cps: 0.239579
[13:30:58.477] iteration 16527: total_loss: 0.399330, loss_sup: 0.024272, loss_mps: 0.129719, loss_cps: 0.245340
[13:30:58.623] iteration 16528: total_loss: 0.295318, loss_sup: 0.093796, loss_mps: 0.072640, loss_cps: 0.128881
[13:30:58.768] iteration 16529: total_loss: 0.248697, loss_sup: 0.002209, loss_mps: 0.089823, loss_cps: 0.156664
[13:30:58.915] iteration 16530: total_loss: 0.434379, loss_sup: 0.040289, loss_mps: 0.135090, loss_cps: 0.259000
[13:30:59.061] iteration 16531: total_loss: 0.479491, loss_sup: 0.051596, loss_mps: 0.141890, loss_cps: 0.286005
[13:30:59.209] iteration 16532: total_loss: 0.631484, loss_sup: 0.038526, loss_mps: 0.195069, loss_cps: 0.397889
[13:30:59.355] iteration 16533: total_loss: 0.599946, loss_sup: 0.078386, loss_mps: 0.172374, loss_cps: 0.349186
[13:30:59.504] iteration 16534: total_loss: 0.453090, loss_sup: 0.084814, loss_mps: 0.128140, loss_cps: 0.240136
[13:30:59.650] iteration 16535: total_loss: 0.658887, loss_sup: 0.316256, loss_mps: 0.114066, loss_cps: 0.228565
[13:30:59.796] iteration 16536: total_loss: 0.265472, loss_sup: 0.027726, loss_mps: 0.085562, loss_cps: 0.152184
[13:30:59.942] iteration 16537: total_loss: 0.516592, loss_sup: 0.199761, loss_mps: 0.110331, loss_cps: 0.206500
[13:31:00.088] iteration 16538: total_loss: 0.311417, loss_sup: 0.066811, loss_mps: 0.090752, loss_cps: 0.153855
[13:31:00.235] iteration 16539: total_loss: 0.394478, loss_sup: 0.041026, loss_mps: 0.122677, loss_cps: 0.230775
[13:31:00.381] iteration 16540: total_loss: 0.716728, loss_sup: 0.179022, loss_mps: 0.178497, loss_cps: 0.359209
[13:31:00.527] iteration 16541: total_loss: 0.523711, loss_sup: 0.278998, loss_mps: 0.086794, loss_cps: 0.157918
[13:31:00.673] iteration 16542: total_loss: 0.321320, loss_sup: 0.017261, loss_mps: 0.107072, loss_cps: 0.196987
[13:31:00.819] iteration 16543: total_loss: 0.561396, loss_sup: 0.210286, loss_mps: 0.127049, loss_cps: 0.224061
[13:31:00.965] iteration 16544: total_loss: 0.619779, loss_sup: 0.076265, loss_mps: 0.178929, loss_cps: 0.364585
[13:31:01.112] iteration 16545: total_loss: 0.449739, loss_sup: 0.024067, loss_mps: 0.150568, loss_cps: 0.275103
[13:31:01.260] iteration 16546: total_loss: 0.545270, loss_sup: 0.084639, loss_mps: 0.150958, loss_cps: 0.309672
[13:31:01.407] iteration 16547: total_loss: 0.465832, loss_sup: 0.047571, loss_mps: 0.150353, loss_cps: 0.267908
[13:31:01.552] iteration 16548: total_loss: 0.388702, loss_sup: 0.060653, loss_mps: 0.119983, loss_cps: 0.208067
[13:31:01.699] iteration 16549: total_loss: 0.333815, loss_sup: 0.046239, loss_mps: 0.109603, loss_cps: 0.177972
[13:31:01.845] iteration 16550: total_loss: 0.561924, loss_sup: 0.105264, loss_mps: 0.154243, loss_cps: 0.302417
[13:31:01.991] iteration 16551: total_loss: 0.840877, loss_sup: 0.089228, loss_mps: 0.249905, loss_cps: 0.501744
[13:31:02.137] iteration 16552: total_loss: 0.447055, loss_sup: 0.080076, loss_mps: 0.129742, loss_cps: 0.237236
[13:31:02.286] iteration 16553: total_loss: 0.392473, loss_sup: 0.040390, loss_mps: 0.124181, loss_cps: 0.227901
[13:31:02.432] iteration 16554: total_loss: 0.501615, loss_sup: 0.042245, loss_mps: 0.150400, loss_cps: 0.308969
[13:31:02.578] iteration 16555: total_loss: 0.430920, loss_sup: 0.047045, loss_mps: 0.140043, loss_cps: 0.243832
[13:31:02.724] iteration 16556: total_loss: 0.359390, loss_sup: 0.038321, loss_mps: 0.114238, loss_cps: 0.206832
[13:31:02.871] iteration 16557: total_loss: 0.649986, loss_sup: 0.040484, loss_mps: 0.175784, loss_cps: 0.433717
[13:31:03.016] iteration 16558: total_loss: 0.438569, loss_sup: 0.040726, loss_mps: 0.137923, loss_cps: 0.259920
[13:31:03.162] iteration 16559: total_loss: 1.278720, loss_sup: 0.120972, loss_mps: 0.347928, loss_cps: 0.809819
[13:31:03.308] iteration 16560: total_loss: 0.221198, loss_sup: 0.003120, loss_mps: 0.082793, loss_cps: 0.135286
[13:31:03.455] iteration 16561: total_loss: 0.522295, loss_sup: 0.038921, loss_mps: 0.155667, loss_cps: 0.327707
[13:31:03.600] iteration 16562: total_loss: 0.740109, loss_sup: 0.103844, loss_mps: 0.199869, loss_cps: 0.436396
[13:31:03.748] iteration 16563: total_loss: 0.589912, loss_sup: 0.049357, loss_mps: 0.170389, loss_cps: 0.370166
[13:31:03.896] iteration 16564: total_loss: 0.838102, loss_sup: 0.072560, loss_mps: 0.237234, loss_cps: 0.528307
[13:31:04.043] iteration 16565: total_loss: 0.568300, loss_sup: 0.040695, loss_mps: 0.175340, loss_cps: 0.352265
[13:31:04.189] iteration 16566: total_loss: 0.652609, loss_sup: 0.195387, loss_mps: 0.154507, loss_cps: 0.302715
[13:31:04.336] iteration 16567: total_loss: 0.890476, loss_sup: 0.388033, loss_mps: 0.162515, loss_cps: 0.339928
[13:31:04.482] iteration 16568: total_loss: 0.474887, loss_sup: 0.073478, loss_mps: 0.134781, loss_cps: 0.266629
[13:31:04.632] iteration 16569: total_loss: 0.441777, loss_sup: 0.037507, loss_mps: 0.139959, loss_cps: 0.264311
[13:31:04.779] iteration 16570: total_loss: 0.483442, loss_sup: 0.053502, loss_mps: 0.146662, loss_cps: 0.283279
[13:31:04.926] iteration 16571: total_loss: 0.501485, loss_sup: 0.012668, loss_mps: 0.162214, loss_cps: 0.326602
[13:31:05.072] iteration 16572: total_loss: 0.484819, loss_sup: 0.084130, loss_mps: 0.133260, loss_cps: 0.267429
[13:31:05.218] iteration 16573: total_loss: 0.319308, loss_sup: 0.054799, loss_mps: 0.095024, loss_cps: 0.169485
[13:31:05.365] iteration 16574: total_loss: 0.616449, loss_sup: 0.070296, loss_mps: 0.180778, loss_cps: 0.365375
[13:31:05.511] iteration 16575: total_loss: 0.624204, loss_sup: 0.096711, loss_mps: 0.175920, loss_cps: 0.351573
[13:31:05.656] iteration 16576: total_loss: 0.905908, loss_sup: 0.171329, loss_mps: 0.245106, loss_cps: 0.489473
[13:31:05.803] iteration 16577: total_loss: 0.715414, loss_sup: 0.230303, loss_mps: 0.165215, loss_cps: 0.319896
[13:31:05.950] iteration 16578: total_loss: 0.631205, loss_sup: 0.038892, loss_mps: 0.189662, loss_cps: 0.402651
[13:31:06.100] iteration 16579: total_loss: 0.435856, loss_sup: 0.067601, loss_mps: 0.124526, loss_cps: 0.243729
[13:31:06.246] iteration 16580: total_loss: 0.579807, loss_sup: 0.084816, loss_mps: 0.170163, loss_cps: 0.324828
[13:31:06.392] iteration 16581: total_loss: 0.535158, loss_sup: 0.106003, loss_mps: 0.140014, loss_cps: 0.289142
[13:31:06.538] iteration 16582: total_loss: 0.266006, loss_sup: 0.012078, loss_mps: 0.091093, loss_cps: 0.162835
[13:31:06.687] iteration 16583: total_loss: 0.415908, loss_sup: 0.051394, loss_mps: 0.127313, loss_cps: 0.237200
[13:31:06.834] iteration 16584: total_loss: 0.788250, loss_sup: 0.127690, loss_mps: 0.224533, loss_cps: 0.436026
[13:31:06.981] iteration 16585: total_loss: 0.370370, loss_sup: 0.042835, loss_mps: 0.116140, loss_cps: 0.211395
[13:31:07.128] iteration 16586: total_loss: 0.276625, loss_sup: 0.064149, loss_mps: 0.082605, loss_cps: 0.129870
[13:31:07.274] iteration 16587: total_loss: 0.683048, loss_sup: 0.235514, loss_mps: 0.150660, loss_cps: 0.296873
[13:31:07.421] iteration 16588: total_loss: 0.561529, loss_sup: 0.134604, loss_mps: 0.151883, loss_cps: 0.275042
[13:31:07.568] iteration 16589: total_loss: 0.345360, loss_sup: 0.068850, loss_mps: 0.105276, loss_cps: 0.171233
[13:31:07.713] iteration 16590: total_loss: 0.564973, loss_sup: 0.074483, loss_mps: 0.167548, loss_cps: 0.322943
[13:31:07.860] iteration 16591: total_loss: 0.288068, loss_sup: 0.011183, loss_mps: 0.097756, loss_cps: 0.179130
[13:31:08.011] iteration 16592: total_loss: 0.220171, loss_sup: 0.006043, loss_mps: 0.081614, loss_cps: 0.132514
[13:31:08.158] iteration 16593: total_loss: 0.726261, loss_sup: 0.183962, loss_mps: 0.190090, loss_cps: 0.352210
[13:31:08.305] iteration 16594: total_loss: 0.380674, loss_sup: 0.043984, loss_mps: 0.120809, loss_cps: 0.215881
[13:31:08.451] iteration 16595: total_loss: 0.419586, loss_sup: 0.137030, loss_mps: 0.107799, loss_cps: 0.174758
[13:31:08.598] iteration 16596: total_loss: 0.592463, loss_sup: 0.078882, loss_mps: 0.168320, loss_cps: 0.345261
[13:31:08.744] iteration 16597: total_loss: 0.431866, loss_sup: 0.069774, loss_mps: 0.124652, loss_cps: 0.237439
[13:31:08.890] iteration 16598: total_loss: 0.714113, loss_sup: 0.196121, loss_mps: 0.180779, loss_cps: 0.337213
[13:31:09.036] iteration 16599: total_loss: 0.422885, loss_sup: 0.097607, loss_mps: 0.112693, loss_cps: 0.212585
[13:31:09.184] iteration 16600: total_loss: 0.507984, loss_sup: 0.016982, loss_mps: 0.165328, loss_cps: 0.325673
[13:31:09.184] Evaluation Started ==>
[13:31:20.519] ==> valid iteration 16600: unet metrics: {'dc': 0.6589338630822471, 'jc': 0.5395223932438037, 'pre': 0.7888050606146834, 'hd': 5.5808456442187975}, ynet metrics: {'dc': 0.5903479467661541, 'jc': 0.4702537481015801, 'pre': 0.7978400760760055, 'hd': 5.620372245538097}.
[13:31:20.521] Evaluation Finished!⏹️
[13:31:20.670] iteration 16601: total_loss: 0.342406, loss_sup: 0.068483, loss_mps: 0.103425, loss_cps: 0.170498
[13:31:20.818] iteration 16602: total_loss: 0.433153, loss_sup: 0.006302, loss_mps: 0.140926, loss_cps: 0.285926
[13:31:20.964] iteration 16603: total_loss: 0.565018, loss_sup: 0.066257, loss_mps: 0.166582, loss_cps: 0.332180
[13:31:21.108] iteration 16604: total_loss: 1.583515, loss_sup: 0.146252, loss_mps: 0.431970, loss_cps: 1.005292
[13:31:21.253] iteration 16605: total_loss: 0.558031, loss_sup: 0.281294, loss_mps: 0.101804, loss_cps: 0.174933
[13:31:21.399] iteration 16606: total_loss: 0.438792, loss_sup: 0.026098, loss_mps: 0.136222, loss_cps: 0.276472
[13:31:21.544] iteration 16607: total_loss: 0.754096, loss_sup: 0.138593, loss_mps: 0.204656, loss_cps: 0.410847
[13:31:21.689] iteration 16608: total_loss: 0.451196, loss_sup: 0.148564, loss_mps: 0.105584, loss_cps: 0.197049
[13:31:21.835] iteration 16609: total_loss: 0.389202, loss_sup: 0.055960, loss_mps: 0.118043, loss_cps: 0.215199
[13:31:21.980] iteration 16610: total_loss: 0.431295, loss_sup: 0.057695, loss_mps: 0.125622, loss_cps: 0.247978
[13:31:22.125] iteration 16611: total_loss: 0.373964, loss_sup: 0.034031, loss_mps: 0.120007, loss_cps: 0.219927
[13:31:22.273] iteration 16612: total_loss: 0.461009, loss_sup: 0.037474, loss_mps: 0.135468, loss_cps: 0.288068
[13:31:22.419] iteration 16613: total_loss: 0.810108, loss_sup: 0.177720, loss_mps: 0.202619, loss_cps: 0.429769
[13:31:22.566] iteration 16614: total_loss: 0.420285, loss_sup: 0.048520, loss_mps: 0.127992, loss_cps: 0.243773
[13:31:22.712] iteration 16615: total_loss: 0.615106, loss_sup: 0.031139, loss_mps: 0.189178, loss_cps: 0.394790
[13:31:22.857] iteration 16616: total_loss: 0.350749, loss_sup: 0.096645, loss_mps: 0.093767, loss_cps: 0.160337
[13:31:23.002] iteration 16617: total_loss: 0.515682, loss_sup: 0.064385, loss_mps: 0.151280, loss_cps: 0.300016
[13:31:23.147] iteration 16618: total_loss: 0.229162, loss_sup: 0.008504, loss_mps: 0.082564, loss_cps: 0.138094
[13:31:23.295] iteration 16619: total_loss: 0.637140, loss_sup: 0.103245, loss_mps: 0.186002, loss_cps: 0.347894
[13:31:23.441] iteration 16620: total_loss: 0.477605, loss_sup: 0.029381, loss_mps: 0.150949, loss_cps: 0.297275
[13:31:23.588] iteration 16621: total_loss: 0.311617, loss_sup: 0.039357, loss_mps: 0.104469, loss_cps: 0.167791
[13:31:23.734] iteration 16622: total_loss: 0.235094, loss_sup: 0.060280, loss_mps: 0.066074, loss_cps: 0.108740
[13:31:23.880] iteration 16623: total_loss: 0.504652, loss_sup: 0.091542, loss_mps: 0.139018, loss_cps: 0.274092
[13:31:24.029] iteration 16624: total_loss: 0.693451, loss_sup: 0.115417, loss_mps: 0.183979, loss_cps: 0.394054
[13:31:24.175] iteration 16625: total_loss: 0.774696, loss_sup: 0.130112, loss_mps: 0.216457, loss_cps: 0.428128
[13:31:24.322] iteration 16626: total_loss: 0.338220, loss_sup: 0.080929, loss_mps: 0.093244, loss_cps: 0.164047
[13:31:24.467] iteration 16627: total_loss: 0.427507, loss_sup: 0.101070, loss_mps: 0.111317, loss_cps: 0.215120
[13:31:24.612] iteration 16628: total_loss: 0.796848, loss_sup: 0.194934, loss_mps: 0.197819, loss_cps: 0.404095
[13:31:24.758] iteration 16629: total_loss: 0.868310, loss_sup: 0.079954, loss_mps: 0.245250, loss_cps: 0.543106
[13:31:24.903] iteration 16630: total_loss: 0.531898, loss_sup: 0.212304, loss_mps: 0.112685, loss_cps: 0.206909
[13:31:25.051] iteration 16631: total_loss: 0.630241, loss_sup: 0.144165, loss_mps: 0.163830, loss_cps: 0.322246
[13:31:25.197] iteration 16632: total_loss: 0.647971, loss_sup: 0.095394, loss_mps: 0.179603, loss_cps: 0.372974
[13:31:25.343] iteration 16633: total_loss: 0.495037, loss_sup: 0.074677, loss_mps: 0.134235, loss_cps: 0.286125
[13:31:25.488] iteration 16634: total_loss: 0.887284, loss_sup: 0.290976, loss_mps: 0.192375, loss_cps: 0.403933
[13:31:25.634] iteration 16635: total_loss: 0.556158, loss_sup: 0.101181, loss_mps: 0.154847, loss_cps: 0.300130
[13:31:25.779] iteration 16636: total_loss: 0.364792, loss_sup: 0.101665, loss_mps: 0.098859, loss_cps: 0.164269
[13:31:25.925] iteration 16637: total_loss: 0.493951, loss_sup: 0.043788, loss_mps: 0.143397, loss_cps: 0.306766
[13:31:26.070] iteration 16638: total_loss: 0.548032, loss_sup: 0.250294, loss_mps: 0.107705, loss_cps: 0.190033
[13:31:26.216] iteration 16639: total_loss: 0.254044, loss_sup: 0.029382, loss_mps: 0.082390, loss_cps: 0.142271
[13:31:26.362] iteration 16640: total_loss: 0.533920, loss_sup: 0.107015, loss_mps: 0.141196, loss_cps: 0.285709
[13:31:26.507] iteration 16641: total_loss: 1.329493, loss_sup: 0.594753, loss_mps: 0.234058, loss_cps: 0.500682
[13:31:26.653] iteration 16642: total_loss: 0.394375, loss_sup: 0.071098, loss_mps: 0.109379, loss_cps: 0.213899
[13:31:26.798] iteration 16643: total_loss: 0.349960, loss_sup: 0.054675, loss_mps: 0.111732, loss_cps: 0.183553
[13:31:26.944] iteration 16644: total_loss: 0.348976, loss_sup: 0.034202, loss_mps: 0.117680, loss_cps: 0.197094
[13:31:27.089] iteration 16645: total_loss: 0.491848, loss_sup: 0.035391, loss_mps: 0.154860, loss_cps: 0.301597
[13:31:27.236] iteration 16646: total_loss: 0.451504, loss_sup: 0.101769, loss_mps: 0.116951, loss_cps: 0.232784
[13:31:27.382] iteration 16647: total_loss: 0.324959, loss_sup: 0.007607, loss_mps: 0.115992, loss_cps: 0.201361
[13:31:27.528] iteration 16648: total_loss: 0.445078, loss_sup: 0.201760, loss_mps: 0.094526, loss_cps: 0.148792
[13:31:27.673] iteration 16649: total_loss: 0.670542, loss_sup: 0.190354, loss_mps: 0.163604, loss_cps: 0.316584
[13:31:27.819] iteration 16650: total_loss: 0.385094, loss_sup: 0.026985, loss_mps: 0.131777, loss_cps: 0.226333
[13:31:27.965] iteration 16651: total_loss: 0.506931, loss_sup: 0.203727, loss_mps: 0.109367, loss_cps: 0.193837
[13:31:28.110] iteration 16652: total_loss: 0.824503, loss_sup: 0.471156, loss_mps: 0.125214, loss_cps: 0.228132
[13:31:28.256] iteration 16653: total_loss: 0.394143, loss_sup: 0.040709, loss_mps: 0.131547, loss_cps: 0.221888
[13:31:28.401] iteration 16654: total_loss: 0.471133, loss_sup: 0.049785, loss_mps: 0.144398, loss_cps: 0.276950
[13:31:28.547] iteration 16655: total_loss: 0.448300, loss_sup: 0.075479, loss_mps: 0.128674, loss_cps: 0.244147
[13:31:28.693] iteration 16656: total_loss: 0.464581, loss_sup: 0.048092, loss_mps: 0.142880, loss_cps: 0.273609
[13:31:28.839] iteration 16657: total_loss: 0.507004, loss_sup: 0.013122, loss_mps: 0.158990, loss_cps: 0.334891
[13:31:28.984] iteration 16658: total_loss: 0.317958, loss_sup: 0.037741, loss_mps: 0.105986, loss_cps: 0.174231
[13:31:29.130] iteration 16659: total_loss: 0.737282, loss_sup: 0.155551, loss_mps: 0.189102, loss_cps: 0.392629
[13:31:29.276] iteration 16660: total_loss: 0.525560, loss_sup: 0.024602, loss_mps: 0.173318, loss_cps: 0.327639
[13:31:29.422] iteration 16661: total_loss: 0.386710, loss_sup: 0.039393, loss_mps: 0.120608, loss_cps: 0.226708
[13:31:29.568] iteration 16662: total_loss: 0.452549, loss_sup: 0.076966, loss_mps: 0.142502, loss_cps: 0.233082
[13:31:29.714] iteration 16663: total_loss: 0.379246, loss_sup: 0.041403, loss_mps: 0.120804, loss_cps: 0.217040
[13:31:29.860] iteration 16664: total_loss: 0.835857, loss_sup: 0.205559, loss_mps: 0.201670, loss_cps: 0.428629
[13:31:30.009] iteration 16665: total_loss: 0.294341, loss_sup: 0.016684, loss_mps: 0.097817, loss_cps: 0.179840
[13:31:30.155] iteration 16666: total_loss: 0.400133, loss_sup: 0.087415, loss_mps: 0.110940, loss_cps: 0.201777
[13:31:30.301] iteration 16667: total_loss: 0.683928, loss_sup: 0.223552, loss_mps: 0.164014, loss_cps: 0.296361
[13:31:30.447] iteration 16668: total_loss: 0.452557, loss_sup: 0.099163, loss_mps: 0.125244, loss_cps: 0.228150
[13:31:30.593] iteration 16669: total_loss: 0.409590, loss_sup: 0.129855, loss_mps: 0.101319, loss_cps: 0.178415
[13:31:30.739] iteration 16670: total_loss: 0.528709, loss_sup: 0.161321, loss_mps: 0.128666, loss_cps: 0.238722
[13:31:30.885] iteration 16671: total_loss: 0.533265, loss_sup: 0.219412, loss_mps: 0.118675, loss_cps: 0.195178
[13:31:31.031] iteration 16672: total_loss: 0.318441, loss_sup: 0.033438, loss_mps: 0.098804, loss_cps: 0.186199
[13:31:31.178] iteration 16673: total_loss: 0.681473, loss_sup: 0.063042, loss_mps: 0.206422, loss_cps: 0.412009
[13:31:31.325] iteration 16674: total_loss: 0.631817, loss_sup: 0.149925, loss_mps: 0.158525, loss_cps: 0.323367
[13:31:31.471] iteration 16675: total_loss: 0.423093, loss_sup: 0.165608, loss_mps: 0.096914, loss_cps: 0.160571
[13:31:31.618] iteration 16676: total_loss: 0.353673, loss_sup: 0.049740, loss_mps: 0.109725, loss_cps: 0.194208
[13:31:31.764] iteration 16677: total_loss: 0.628603, loss_sup: 0.089285, loss_mps: 0.185298, loss_cps: 0.354020
[13:31:31.910] iteration 16678: total_loss: 0.449789, loss_sup: 0.095019, loss_mps: 0.126247, loss_cps: 0.228523
[13:31:32.056] iteration 16679: total_loss: 0.567967, loss_sup: 0.071090, loss_mps: 0.171893, loss_cps: 0.324984
[13:31:32.202] iteration 16680: total_loss: 0.581568, loss_sup: 0.191690, loss_mps: 0.135258, loss_cps: 0.254620
[13:31:32.348] iteration 16681: total_loss: 0.462809, loss_sup: 0.051111, loss_mps: 0.135632, loss_cps: 0.276066
[13:31:32.494] iteration 16682: total_loss: 1.090134, loss_sup: 0.096377, loss_mps: 0.321638, loss_cps: 0.672118
[13:31:32.640] iteration 16683: total_loss: 0.384429, loss_sup: 0.080576, loss_mps: 0.109356, loss_cps: 0.194497
[13:31:32.786] iteration 16684: total_loss: 0.424299, loss_sup: 0.076165, loss_mps: 0.125743, loss_cps: 0.222391
[13:31:32.932] iteration 16685: total_loss: 0.385661, loss_sup: 0.028681, loss_mps: 0.119464, loss_cps: 0.237517
[13:31:33.080] iteration 16686: total_loss: 0.519725, loss_sup: 0.030754, loss_mps: 0.163432, loss_cps: 0.325539
[13:31:33.228] iteration 16687: total_loss: 0.294638, loss_sup: 0.038470, loss_mps: 0.094174, loss_cps: 0.161994
[13:31:33.375] iteration 16688: total_loss: 0.364214, loss_sup: 0.052338, loss_mps: 0.111589, loss_cps: 0.200288
[13:31:33.521] iteration 16689: total_loss: 0.354326, loss_sup: 0.056597, loss_mps: 0.110029, loss_cps: 0.187700
[13:31:33.668] iteration 16690: total_loss: 0.851127, loss_sup: 0.143486, loss_mps: 0.228987, loss_cps: 0.478653
[13:31:33.816] iteration 16691: total_loss: 0.603388, loss_sup: 0.039424, loss_mps: 0.182814, loss_cps: 0.381151
[13:31:33.962] iteration 16692: total_loss: 0.594087, loss_sup: 0.120755, loss_mps: 0.161107, loss_cps: 0.312225
[13:31:34.108] iteration 16693: total_loss: 0.339919, loss_sup: 0.098394, loss_mps: 0.087589, loss_cps: 0.153936
[13:31:34.254] iteration 16694: total_loss: 0.317773, loss_sup: 0.070833, loss_mps: 0.091774, loss_cps: 0.155165
[13:31:34.403] iteration 16695: total_loss: 0.415812, loss_sup: 0.035967, loss_mps: 0.123142, loss_cps: 0.256703
[13:31:34.550] iteration 16696: total_loss: 1.378975, loss_sup: 0.371605, loss_mps: 0.304667, loss_cps: 0.702703
[13:31:34.696] iteration 16697: total_loss: 0.347274, loss_sup: 0.154165, loss_mps: 0.074944, loss_cps: 0.118165
[13:31:34.844] iteration 16698: total_loss: 0.405998, loss_sup: 0.114034, loss_mps: 0.104893, loss_cps: 0.187071
[13:31:34.990] iteration 16699: total_loss: 0.243068, loss_sup: 0.002493, loss_mps: 0.084544, loss_cps: 0.156031
[13:31:35.136] iteration 16700: total_loss: 0.362081, loss_sup: 0.066794, loss_mps: 0.104562, loss_cps: 0.190726
[13:31:35.137] Evaluation Started ==>
[13:31:46.549] ==> valid iteration 16700: unet metrics: {'dc': 0.6398580649805699, 'jc': 0.5229616499191736, 'pre': 0.768471290770993, 'hd': 5.533573767002831}, ynet metrics: {'dc': 0.5694478369940342, 'jc': 0.45512670583760584, 'pre': 0.7924897881662426, 'hd': 5.526229470507639}.
[13:31:46.551] Evaluation Finished!⏹️
[13:31:46.702] iteration 16701: total_loss: 1.193897, loss_sup: 0.356321, loss_mps: 0.254753, loss_cps: 0.582823
[13:31:46.849] iteration 16702: total_loss: 0.729839, loss_sup: 0.123103, loss_mps: 0.199967, loss_cps: 0.406769
[13:31:46.996] iteration 16703: total_loss: 0.647472, loss_sup: 0.038599, loss_mps: 0.199148, loss_cps: 0.409725
[13:31:47.141] iteration 16704: total_loss: 0.368921, loss_sup: 0.114778, loss_mps: 0.095688, loss_cps: 0.158455
[13:31:47.287] iteration 16705: total_loss: 0.398347, loss_sup: 0.066346, loss_mps: 0.115366, loss_cps: 0.216635
[13:31:47.433] iteration 16706: total_loss: 0.522862, loss_sup: 0.066291, loss_mps: 0.163430, loss_cps: 0.293141
[13:31:47.578] iteration 16707: total_loss: 0.330820, loss_sup: 0.042598, loss_mps: 0.103591, loss_cps: 0.184631
[13:31:47.723] iteration 16708: total_loss: 0.627655, loss_sup: 0.090517, loss_mps: 0.194560, loss_cps: 0.342577
[13:31:47.868] iteration 16709: total_loss: 0.620013, loss_sup: 0.119370, loss_mps: 0.163045, loss_cps: 0.337598
[13:31:48.013] iteration 16710: total_loss: 0.686768, loss_sup: 0.082585, loss_mps: 0.200061, loss_cps: 0.404123
[13:31:48.158] iteration 16711: total_loss: 0.513127, loss_sup: 0.051443, loss_mps: 0.152361, loss_cps: 0.309323
[13:31:48.304] iteration 16712: total_loss: 0.666466, loss_sup: 0.021635, loss_mps: 0.204723, loss_cps: 0.440108
[13:31:48.449] iteration 16713: total_loss: 0.529805, loss_sup: 0.206623, loss_mps: 0.121784, loss_cps: 0.201398
[13:31:48.595] iteration 16714: total_loss: 0.485188, loss_sup: 0.129034, loss_mps: 0.127045, loss_cps: 0.229109
[13:31:48.740] iteration 16715: total_loss: 0.464829, loss_sup: 0.079507, loss_mps: 0.141906, loss_cps: 0.243416
[13:31:48.885] iteration 16716: total_loss: 0.429748, loss_sup: 0.143372, loss_mps: 0.103259, loss_cps: 0.183117
[13:31:49.031] iteration 16717: total_loss: 0.422368, loss_sup: 0.083340, loss_mps: 0.121936, loss_cps: 0.217092
[13:31:49.176] iteration 16718: total_loss: 0.248806, loss_sup: 0.010687, loss_mps: 0.091313, loss_cps: 0.146807
[13:31:49.321] iteration 16719: total_loss: 0.426360, loss_sup: 0.036051, loss_mps: 0.137977, loss_cps: 0.252332
[13:31:49.383] iteration 16720: total_loss: 0.787515, loss_sup: 0.049586, loss_mps: 0.233456, loss_cps: 0.504473
[13:31:50.617] iteration 16721: total_loss: 0.396566, loss_sup: 0.065451, loss_mps: 0.118795, loss_cps: 0.212320
[13:31:50.765] iteration 16722: total_loss: 0.535718, loss_sup: 0.070087, loss_mps: 0.156668, loss_cps: 0.308964
[13:31:50.911] iteration 16723: total_loss: 0.316755, loss_sup: 0.034423, loss_mps: 0.099879, loss_cps: 0.182453
[13:31:51.057] iteration 16724: total_loss: 0.617025, loss_sup: 0.088365, loss_mps: 0.168458, loss_cps: 0.360203
[13:31:51.203] iteration 16725: total_loss: 0.516204, loss_sup: 0.039120, loss_mps: 0.152994, loss_cps: 0.324090
[13:31:51.349] iteration 16726: total_loss: 0.582706, loss_sup: 0.148101, loss_mps: 0.142664, loss_cps: 0.291942
[13:31:51.494] iteration 16727: total_loss: 1.034352, loss_sup: 0.139227, loss_mps: 0.276821, loss_cps: 0.618304
[13:31:51.641] iteration 16728: total_loss: 1.143412, loss_sup: 0.198783, loss_mps: 0.282733, loss_cps: 0.661895
[13:31:51.786] iteration 16729: total_loss: 0.343115, loss_sup: 0.070561, loss_mps: 0.094754, loss_cps: 0.177799
[13:31:51.932] iteration 16730: total_loss: 0.570288, loss_sup: 0.100030, loss_mps: 0.154422, loss_cps: 0.315836
[13:31:52.077] iteration 16731: total_loss: 0.533242, loss_sup: 0.054304, loss_mps: 0.156105, loss_cps: 0.322833
[13:31:52.223] iteration 16732: total_loss: 0.682244, loss_sup: 0.047112, loss_mps: 0.192831, loss_cps: 0.442301
[13:31:52.367] iteration 16733: total_loss: 0.731300, loss_sup: 0.062538, loss_mps: 0.203903, loss_cps: 0.464859
[13:31:52.513] iteration 16734: total_loss: 0.572234, loss_sup: 0.178050, loss_mps: 0.133240, loss_cps: 0.260944
[13:31:52.659] iteration 16735: total_loss: 0.440001, loss_sup: 0.021976, loss_mps: 0.139082, loss_cps: 0.278943
[13:31:52.805] iteration 16736: total_loss: 0.291230, loss_sup: 0.006422, loss_mps: 0.102808, loss_cps: 0.182000
[13:31:52.952] iteration 16737: total_loss: 0.768815, loss_sup: 0.038495, loss_mps: 0.232909, loss_cps: 0.497412
[13:31:53.098] iteration 16738: total_loss: 0.480339, loss_sup: 0.091632, loss_mps: 0.139934, loss_cps: 0.248772
[13:31:53.246] iteration 16739: total_loss: 0.431932, loss_sup: 0.077823, loss_mps: 0.125480, loss_cps: 0.228628
[13:31:53.392] iteration 16740: total_loss: 0.355259, loss_sup: 0.057312, loss_mps: 0.111083, loss_cps: 0.186865
[13:31:53.537] iteration 16741: total_loss: 0.622632, loss_sup: 0.050135, loss_mps: 0.193073, loss_cps: 0.379424
[13:31:53.683] iteration 16742: total_loss: 0.745010, loss_sup: 0.222821, loss_mps: 0.181666, loss_cps: 0.340523
[13:31:53.829] iteration 16743: total_loss: 0.522053, loss_sup: 0.014470, loss_mps: 0.169943, loss_cps: 0.337640
[13:31:53.974] iteration 16744: total_loss: 0.550541, loss_sup: 0.138164, loss_mps: 0.150601, loss_cps: 0.261776
[13:31:54.120] iteration 16745: total_loss: 0.401941, loss_sup: 0.081220, loss_mps: 0.117848, loss_cps: 0.202872
[13:31:54.267] iteration 16746: total_loss: 0.358936, loss_sup: 0.026837, loss_mps: 0.126135, loss_cps: 0.205964
[13:31:54.412] iteration 16747: total_loss: 0.588342, loss_sup: 0.164733, loss_mps: 0.160164, loss_cps: 0.263445
[13:31:54.558] iteration 16748: total_loss: 0.325246, loss_sup: 0.008291, loss_mps: 0.114195, loss_cps: 0.202759
[13:31:54.707] iteration 16749: total_loss: 0.455146, loss_sup: 0.073794, loss_mps: 0.132275, loss_cps: 0.249078
[13:31:54.854] iteration 16750: total_loss: 0.310791, loss_sup: 0.048334, loss_mps: 0.101477, loss_cps: 0.160980
[13:31:55.002] iteration 16751: total_loss: 0.448683, loss_sup: 0.116809, loss_mps: 0.114716, loss_cps: 0.217158
[13:31:55.151] iteration 16752: total_loss: 0.665938, loss_sup: 0.234705, loss_mps: 0.155758, loss_cps: 0.275475
[13:31:55.297] iteration 16753: total_loss: 0.393042, loss_sup: 0.055276, loss_mps: 0.120300, loss_cps: 0.217467
[13:31:55.443] iteration 16754: total_loss: 0.402348, loss_sup: 0.045122, loss_mps: 0.126866, loss_cps: 0.230360
[13:31:55.592] iteration 16755: total_loss: 0.392657, loss_sup: 0.042268, loss_mps: 0.122355, loss_cps: 0.228034
[13:31:55.738] iteration 16756: total_loss: 0.571216, loss_sup: 0.037762, loss_mps: 0.190003, loss_cps: 0.343452
[13:31:55.884] iteration 16757: total_loss: 0.530792, loss_sup: 0.065187, loss_mps: 0.156703, loss_cps: 0.308902
[13:31:56.031] iteration 16758: total_loss: 0.509666, loss_sup: 0.109019, loss_mps: 0.141374, loss_cps: 0.259273
[13:31:56.177] iteration 16759: total_loss: 0.759418, loss_sup: 0.185903, loss_mps: 0.184990, loss_cps: 0.388526
[13:31:56.323] iteration 16760: total_loss: 0.335435, loss_sup: 0.023063, loss_mps: 0.108539, loss_cps: 0.203833
[13:31:56.469] iteration 16761: total_loss: 0.454555, loss_sup: 0.202667, loss_mps: 0.090715, loss_cps: 0.161173
[13:31:56.618] iteration 16762: total_loss: 0.436437, loss_sup: 0.105749, loss_mps: 0.111970, loss_cps: 0.218718
[13:31:56.765] iteration 16763: total_loss: 0.405282, loss_sup: 0.070544, loss_mps: 0.128438, loss_cps: 0.206300
[13:31:56.911] iteration 16764: total_loss: 0.585270, loss_sup: 0.036804, loss_mps: 0.170206, loss_cps: 0.378260
[13:31:57.057] iteration 16765: total_loss: 0.476782, loss_sup: 0.096579, loss_mps: 0.128094, loss_cps: 0.252109
[13:31:57.204] iteration 16766: total_loss: 0.782838, loss_sup: 0.318355, loss_mps: 0.152514, loss_cps: 0.311970
[13:31:57.350] iteration 16767: total_loss: 0.998135, loss_sup: 0.179314, loss_mps: 0.260117, loss_cps: 0.558704
[13:31:57.496] iteration 16768: total_loss: 0.613754, loss_sup: 0.227133, loss_mps: 0.132214, loss_cps: 0.254407
[13:31:57.642] iteration 16769: total_loss: 0.749245, loss_sup: 0.114227, loss_mps: 0.208179, loss_cps: 0.426839
[13:31:57.789] iteration 16770: total_loss: 0.468211, loss_sup: 0.075360, loss_mps: 0.137116, loss_cps: 0.255735
[13:31:57.936] iteration 16771: total_loss: 0.417432, loss_sup: 0.153213, loss_mps: 0.090611, loss_cps: 0.173608
[13:31:58.082] iteration 16772: total_loss: 0.366507, loss_sup: 0.116168, loss_mps: 0.092532, loss_cps: 0.157807
[13:31:58.229] iteration 16773: total_loss: 0.673560, loss_sup: 0.275451, loss_mps: 0.126370, loss_cps: 0.271738
[13:31:58.374] iteration 16774: total_loss: 0.333151, loss_sup: 0.031101, loss_mps: 0.105168, loss_cps: 0.196881
[13:31:58.520] iteration 16775: total_loss: 0.731022, loss_sup: 0.157749, loss_mps: 0.179550, loss_cps: 0.393722
[13:31:58.666] iteration 16776: total_loss: 0.259182, loss_sup: 0.021633, loss_mps: 0.090413, loss_cps: 0.147136
[13:31:58.811] iteration 16777: total_loss: 0.848884, loss_sup: 0.283313, loss_mps: 0.172051, loss_cps: 0.393519
[13:31:58.957] iteration 16778: total_loss: 0.612072, loss_sup: 0.112280, loss_mps: 0.154845, loss_cps: 0.344947
[13:31:59.105] iteration 16779: total_loss: 0.644742, loss_sup: 0.090712, loss_mps: 0.176942, loss_cps: 0.377089
[13:31:59.251] iteration 16780: total_loss: 0.407660, loss_sup: 0.022024, loss_mps: 0.127462, loss_cps: 0.258174
[13:31:59.397] iteration 16781: total_loss: 0.464897, loss_sup: 0.050250, loss_mps: 0.140905, loss_cps: 0.273742
[13:31:59.544] iteration 16782: total_loss: 0.556966, loss_sup: 0.173760, loss_mps: 0.128705, loss_cps: 0.254500
[13:31:59.690] iteration 16783: total_loss: 0.397118, loss_sup: 0.037606, loss_mps: 0.128602, loss_cps: 0.230910
[13:31:59.836] iteration 16784: total_loss: 0.910789, loss_sup: 0.107406, loss_mps: 0.255242, loss_cps: 0.548141
[13:31:59.985] iteration 16785: total_loss: 0.290787, loss_sup: 0.009392, loss_mps: 0.103322, loss_cps: 0.178072
[13:32:00.131] iteration 16786: total_loss: 0.520034, loss_sup: 0.101253, loss_mps: 0.137281, loss_cps: 0.281501
[13:32:00.277] iteration 16787: total_loss: 0.399132, loss_sup: 0.072226, loss_mps: 0.124690, loss_cps: 0.202216
[13:32:00.423] iteration 16788: total_loss: 0.378261, loss_sup: 0.045473, loss_mps: 0.123067, loss_cps: 0.209721
[13:32:00.570] iteration 16789: total_loss: 0.440374, loss_sup: 0.086830, loss_mps: 0.126430, loss_cps: 0.227115
[13:32:00.716] iteration 16790: total_loss: 0.275433, loss_sup: 0.017612, loss_mps: 0.092598, loss_cps: 0.165223
[13:32:00.864] iteration 16791: total_loss: 0.497642, loss_sup: 0.270407, loss_mps: 0.088151, loss_cps: 0.139085
[13:32:01.011] iteration 16792: total_loss: 0.462532, loss_sup: 0.049776, loss_mps: 0.141372, loss_cps: 0.271384
[13:32:01.157] iteration 16793: total_loss: 0.349885, loss_sup: 0.072723, loss_mps: 0.104951, loss_cps: 0.172212
[13:32:01.303] iteration 16794: total_loss: 0.330431, loss_sup: 0.082579, loss_mps: 0.093719, loss_cps: 0.154133
[13:32:01.450] iteration 16795: total_loss: 0.269234, loss_sup: 0.026366, loss_mps: 0.090563, loss_cps: 0.152305
[13:32:01.600] iteration 16796: total_loss: 0.933887, loss_sup: 0.155245, loss_mps: 0.240407, loss_cps: 0.538235
[13:32:01.746] iteration 16797: total_loss: 0.302701, loss_sup: 0.025935, loss_mps: 0.098815, loss_cps: 0.177952
[13:32:01.892] iteration 16798: total_loss: 0.609495, loss_sup: 0.089902, loss_mps: 0.170266, loss_cps: 0.349326
[13:32:02.038] iteration 16799: total_loss: 0.521384, loss_sup: 0.116280, loss_mps: 0.138742, loss_cps: 0.266362
[13:32:02.184] iteration 16800: total_loss: 0.239101, loss_sup: 0.011739, loss_mps: 0.085065, loss_cps: 0.142297
[13:32:02.184] Evaluation Started ==>
[13:32:13.524] ==> valid iteration 16800: unet metrics: {'dc': 0.6164864732621202, 'jc': 0.5014392893808474, 'pre': 0.7646381499502661, 'hd': 5.605919619818156}, ynet metrics: {'dc': 0.4881833616028812, 'jc': 0.3814628973767928, 'pre': 0.7807028148194408, 'hd': 5.880803776317178}.
[13:32:13.526] Evaluation Finished!⏹️
[13:32:13.675] iteration 16801: total_loss: 0.563395, loss_sup: 0.065133, loss_mps: 0.174501, loss_cps: 0.323762
[13:32:13.822] iteration 16802: total_loss: 0.506112, loss_sup: 0.214118, loss_mps: 0.097042, loss_cps: 0.194952
[13:32:13.968] iteration 16803: total_loss: 0.372501, loss_sup: 0.051412, loss_mps: 0.117284, loss_cps: 0.203805
[13:32:14.114] iteration 16804: total_loss: 0.473907, loss_sup: 0.017764, loss_mps: 0.153895, loss_cps: 0.302248
[13:32:14.259] iteration 16805: total_loss: 0.322739, loss_sup: 0.125697, loss_mps: 0.072004, loss_cps: 0.125038
[13:32:14.406] iteration 16806: total_loss: 0.387203, loss_sup: 0.097690, loss_mps: 0.100974, loss_cps: 0.188539
[13:32:14.551] iteration 16807: total_loss: 0.606552, loss_sup: 0.124238, loss_mps: 0.155967, loss_cps: 0.326347
[13:32:14.701] iteration 16808: total_loss: 0.465572, loss_sup: 0.121727, loss_mps: 0.119624, loss_cps: 0.224221
[13:32:14.846] iteration 16809: total_loss: 0.364512, loss_sup: 0.035755, loss_mps: 0.114321, loss_cps: 0.214436
[13:32:14.991] iteration 16810: total_loss: 0.345530, loss_sup: 0.063848, loss_mps: 0.109066, loss_cps: 0.172616
[13:32:15.137] iteration 16811: total_loss: 0.331959, loss_sup: 0.022634, loss_mps: 0.108103, loss_cps: 0.201222
[13:32:15.282] iteration 16812: total_loss: 0.586509, loss_sup: 0.133162, loss_mps: 0.150582, loss_cps: 0.302765
[13:32:15.428] iteration 16813: total_loss: 0.632529, loss_sup: 0.119782, loss_mps: 0.163874, loss_cps: 0.348873
[13:32:15.574] iteration 16814: total_loss: 0.305308, loss_sup: 0.015202, loss_mps: 0.098897, loss_cps: 0.191209
[13:32:15.719] iteration 16815: total_loss: 0.371394, loss_sup: 0.077133, loss_mps: 0.102654, loss_cps: 0.191607
[13:32:15.865] iteration 16816: total_loss: 0.569105, loss_sup: 0.187999, loss_mps: 0.129318, loss_cps: 0.251788
[13:32:16.011] iteration 16817: total_loss: 0.650318, loss_sup: 0.221322, loss_mps: 0.143411, loss_cps: 0.285584
[13:32:16.157] iteration 16818: total_loss: 0.511997, loss_sup: 0.031360, loss_mps: 0.166311, loss_cps: 0.314326
[13:32:16.304] iteration 16819: total_loss: 0.368292, loss_sup: 0.156793, loss_mps: 0.080962, loss_cps: 0.130537
[13:32:16.450] iteration 16820: total_loss: 0.305712, loss_sup: 0.038684, loss_mps: 0.102727, loss_cps: 0.164301
[13:32:16.597] iteration 16821: total_loss: 0.313482, loss_sup: 0.105931, loss_mps: 0.080012, loss_cps: 0.127540
[13:32:16.744] iteration 16822: total_loss: 0.470545, loss_sup: 0.050209, loss_mps: 0.144555, loss_cps: 0.275781
[13:32:16.892] iteration 16823: total_loss: 0.371308, loss_sup: 0.040281, loss_mps: 0.112426, loss_cps: 0.218601
[13:32:17.040] iteration 16824: total_loss: 0.516049, loss_sup: 0.023553, loss_mps: 0.177801, loss_cps: 0.314695
[13:32:17.190] iteration 16825: total_loss: 0.581690, loss_sup: 0.096234, loss_mps: 0.166144, loss_cps: 0.319311
[13:32:17.336] iteration 16826: total_loss: 0.461436, loss_sup: 0.084187, loss_mps: 0.128789, loss_cps: 0.248460
[13:32:17.482] iteration 16827: total_loss: 0.381234, loss_sup: 0.017800, loss_mps: 0.134692, loss_cps: 0.228742
[13:32:17.629] iteration 16828: total_loss: 0.223176, loss_sup: 0.005441, loss_mps: 0.081415, loss_cps: 0.136319
[13:32:17.776] iteration 16829: total_loss: 0.921098, loss_sup: 0.097158, loss_mps: 0.262564, loss_cps: 0.561377
[13:32:17.923] iteration 16830: total_loss: 0.385067, loss_sup: 0.086649, loss_mps: 0.110585, loss_cps: 0.187833
[13:32:18.070] iteration 16831: total_loss: 0.368690, loss_sup: 0.085765, loss_mps: 0.102432, loss_cps: 0.180492
[13:32:18.216] iteration 16832: total_loss: 0.373056, loss_sup: 0.043687, loss_mps: 0.122286, loss_cps: 0.207082
[13:32:18.362] iteration 16833: total_loss: 0.567637, loss_sup: 0.030356, loss_mps: 0.164565, loss_cps: 0.372717
[13:32:18.509] iteration 16834: total_loss: 0.326631, loss_sup: 0.059325, loss_mps: 0.101641, loss_cps: 0.165664
[13:32:18.657] iteration 16835: total_loss: 0.298092, loss_sup: 0.009537, loss_mps: 0.103862, loss_cps: 0.184693
[13:32:18.807] iteration 16836: total_loss: 0.259585, loss_sup: 0.047800, loss_mps: 0.078829, loss_cps: 0.132956
[13:32:18.955] iteration 16837: total_loss: 0.588675, loss_sup: 0.176425, loss_mps: 0.132430, loss_cps: 0.279819
[13:32:19.102] iteration 16838: total_loss: 0.275730, loss_sup: 0.011199, loss_mps: 0.091571, loss_cps: 0.172960
[13:32:19.248] iteration 16839: total_loss: 0.845694, loss_sup: 0.275854, loss_mps: 0.184967, loss_cps: 0.384873
[13:32:19.394] iteration 16840: total_loss: 0.539218, loss_sup: 0.070325, loss_mps: 0.160634, loss_cps: 0.308259
[13:32:19.542] iteration 16841: total_loss: 0.609810, loss_sup: 0.036418, loss_mps: 0.187749, loss_cps: 0.385643
[13:32:19.688] iteration 16842: total_loss: 0.300299, loss_sup: 0.041687, loss_mps: 0.103423, loss_cps: 0.155189
[13:32:19.835] iteration 16843: total_loss: 0.546676, loss_sup: 0.194683, loss_mps: 0.122277, loss_cps: 0.229716
[13:32:19.981] iteration 16844: total_loss: 0.557979, loss_sup: 0.153250, loss_mps: 0.135225, loss_cps: 0.269504
[13:32:20.128] iteration 16845: total_loss: 0.324818, loss_sup: 0.032814, loss_mps: 0.104228, loss_cps: 0.187776
[13:32:20.277] iteration 16846: total_loss: 0.322855, loss_sup: 0.021797, loss_mps: 0.103647, loss_cps: 0.197411
[13:32:20.427] iteration 16847: total_loss: 0.623762, loss_sup: 0.072173, loss_mps: 0.181634, loss_cps: 0.369956
[13:32:20.573] iteration 16848: total_loss: 0.479421, loss_sup: 0.145398, loss_mps: 0.113743, loss_cps: 0.220280
[13:32:20.720] iteration 16849: total_loss: 0.373716, loss_sup: 0.106371, loss_mps: 0.099166, loss_cps: 0.168179
[13:32:20.871] iteration 16850: total_loss: 0.439449, loss_sup: 0.134768, loss_mps: 0.114490, loss_cps: 0.190190
[13:32:21.021] iteration 16851: total_loss: 0.881151, loss_sup: 0.155274, loss_mps: 0.224152, loss_cps: 0.501726
[13:32:21.168] iteration 16852: total_loss: 0.900340, loss_sup: 0.231296, loss_mps: 0.213870, loss_cps: 0.455174
[13:32:21.316] iteration 16853: total_loss: 0.562823, loss_sup: 0.034488, loss_mps: 0.175825, loss_cps: 0.352509
[13:32:21.463] iteration 16854: total_loss: 0.465696, loss_sup: 0.035633, loss_mps: 0.141954, loss_cps: 0.288108
[13:32:21.609] iteration 16855: total_loss: 0.545347, loss_sup: 0.056628, loss_mps: 0.161281, loss_cps: 0.327438
[13:32:21.755] iteration 16856: total_loss: 0.722314, loss_sup: 0.408777, loss_mps: 0.114943, loss_cps: 0.198594
[13:32:21.901] iteration 16857: total_loss: 0.231408, loss_sup: 0.049436, loss_mps: 0.071817, loss_cps: 0.110156
[13:32:22.047] iteration 16858: total_loss: 0.627285, loss_sup: 0.115455, loss_mps: 0.174958, loss_cps: 0.336873
[13:32:22.193] iteration 16859: total_loss: 0.463231, loss_sup: 0.049577, loss_mps: 0.138079, loss_cps: 0.275575
[13:32:22.341] iteration 16860: total_loss: 0.679403, loss_sup: 0.054916, loss_mps: 0.197844, loss_cps: 0.426643
[13:32:22.486] iteration 16861: total_loss: 1.214444, loss_sup: 0.143392, loss_mps: 0.321846, loss_cps: 0.749207
[13:32:22.632] iteration 16862: total_loss: 0.701329, loss_sup: 0.073520, loss_mps: 0.200355, loss_cps: 0.427455
[13:32:22.778] iteration 16863: total_loss: 0.438616, loss_sup: 0.049062, loss_mps: 0.135567, loss_cps: 0.253987
[13:32:22.924] iteration 16864: total_loss: 0.306041, loss_sup: 0.064444, loss_mps: 0.086938, loss_cps: 0.154659
[13:32:23.069] iteration 16865: total_loss: 0.389733, loss_sup: 0.025959, loss_mps: 0.118450, loss_cps: 0.245325
[13:32:23.216] iteration 16866: total_loss: 0.457859, loss_sup: 0.006573, loss_mps: 0.148125, loss_cps: 0.303161
[13:32:23.365] iteration 16867: total_loss: 0.530520, loss_sup: 0.072051, loss_mps: 0.154111, loss_cps: 0.304359
[13:32:23.511] iteration 16868: total_loss: 0.724134, loss_sup: 0.301283, loss_mps: 0.144209, loss_cps: 0.278642
[13:32:23.657] iteration 16869: total_loss: 0.514050, loss_sup: 0.027621, loss_mps: 0.156744, loss_cps: 0.329684
[13:32:23.803] iteration 16870: total_loss: 0.745505, loss_sup: 0.161664, loss_mps: 0.198815, loss_cps: 0.385026
[13:32:23.948] iteration 16871: total_loss: 0.440996, loss_sup: 0.045791, loss_mps: 0.134265, loss_cps: 0.260940
[13:32:24.094] iteration 16872: total_loss: 0.389441, loss_sup: 0.115692, loss_mps: 0.096329, loss_cps: 0.177420
[13:32:24.241] iteration 16873: total_loss: 0.601767, loss_sup: 0.160177, loss_mps: 0.156218, loss_cps: 0.285373
[13:32:24.387] iteration 16874: total_loss: 0.523414, loss_sup: 0.102123, loss_mps: 0.153412, loss_cps: 0.267879
[13:32:24.533] iteration 16875: total_loss: 0.389533, loss_sup: 0.035309, loss_mps: 0.125507, loss_cps: 0.228717
[13:32:24.681] iteration 16876: total_loss: 0.373677, loss_sup: 0.041200, loss_mps: 0.114329, loss_cps: 0.218148
[13:32:24.827] iteration 16877: total_loss: 0.423639, loss_sup: 0.108884, loss_mps: 0.112383, loss_cps: 0.202372
[13:32:24.972] iteration 16878: total_loss: 0.401834, loss_sup: 0.025485, loss_mps: 0.129321, loss_cps: 0.247029
[13:32:25.119] iteration 16879: total_loss: 0.669374, loss_sup: 0.247737, loss_mps: 0.148892, loss_cps: 0.272746
[13:32:25.266] iteration 16880: total_loss: 0.824369, loss_sup: 0.029428, loss_mps: 0.248935, loss_cps: 0.546006
[13:32:25.412] iteration 16881: total_loss: 0.270348, loss_sup: 0.023774, loss_mps: 0.092528, loss_cps: 0.154047
[13:32:25.558] iteration 16882: total_loss: 0.409427, loss_sup: 0.008384, loss_mps: 0.144639, loss_cps: 0.256405
[13:32:25.705] iteration 16883: total_loss: 0.862865, loss_sup: 0.057874, loss_mps: 0.252228, loss_cps: 0.552763
[13:32:25.852] iteration 16884: total_loss: 0.272020, loss_sup: 0.025432, loss_mps: 0.086685, loss_cps: 0.159903
[13:32:25.999] iteration 16885: total_loss: 0.496265, loss_sup: 0.134653, loss_mps: 0.120538, loss_cps: 0.241073
[13:32:26.146] iteration 16886: total_loss: 0.421410, loss_sup: 0.090225, loss_mps: 0.116759, loss_cps: 0.214426
[13:32:26.293] iteration 16887: total_loss: 0.413323, loss_sup: 0.154941, loss_mps: 0.099959, loss_cps: 0.158424
[13:32:26.439] iteration 16888: total_loss: 0.468538, loss_sup: 0.136729, loss_mps: 0.125462, loss_cps: 0.206347
[13:32:26.585] iteration 16889: total_loss: 0.980491, loss_sup: 0.178044, loss_mps: 0.247135, loss_cps: 0.555313
[13:32:26.733] iteration 16890: total_loss: 0.372530, loss_sup: 0.077124, loss_mps: 0.102804, loss_cps: 0.192602
[13:32:26.880] iteration 16891: total_loss: 0.443204, loss_sup: 0.040113, loss_mps: 0.133770, loss_cps: 0.269322
[13:32:27.027] iteration 16892: total_loss: 0.511002, loss_sup: 0.076294, loss_mps: 0.146723, loss_cps: 0.287985
[13:32:27.173] iteration 16893: total_loss: 0.409392, loss_sup: 0.147338, loss_mps: 0.093245, loss_cps: 0.168810
[13:32:27.321] iteration 16894: total_loss: 0.512488, loss_sup: 0.099320, loss_mps: 0.138372, loss_cps: 0.274796
[13:32:27.467] iteration 16895: total_loss: 0.547979, loss_sup: 0.230721, loss_mps: 0.113028, loss_cps: 0.204229
[13:32:27.616] iteration 16896: total_loss: 0.675927, loss_sup: 0.131277, loss_mps: 0.182508, loss_cps: 0.362142
[13:32:27.763] iteration 16897: total_loss: 0.387403, loss_sup: 0.038060, loss_mps: 0.130807, loss_cps: 0.218536
[13:32:27.910] iteration 16898: total_loss: 0.299248, loss_sup: 0.025849, loss_mps: 0.101728, loss_cps: 0.171671
[13:32:28.057] iteration 16899: total_loss: 1.115292, loss_sup: 0.247704, loss_mps: 0.273893, loss_cps: 0.593695
[13:32:28.203] iteration 16900: total_loss: 0.208076, loss_sup: 0.015283, loss_mps: 0.071790, loss_cps: 0.121003
[13:32:28.204] Evaluation Started ==>
[13:32:39.518] ==> valid iteration 16900: unet metrics: {'dc': 0.6425120911406208, 'jc': 0.5223061022307927, 'pre': 0.7905422376855614, 'hd': 5.46041396626293}, ynet metrics: {'dc': 0.6038476037335868, 'jc': 0.4863312095253667, 'pre': 0.7999010266559389, 'hd': 5.570086341973685}.
[13:32:39.520] Evaluation Finished!⏹️
[13:32:39.672] iteration 16901: total_loss: 0.438125, loss_sup: 0.139828, loss_mps: 0.114144, loss_cps: 0.184152
[13:32:39.819] iteration 16902: total_loss: 0.328761, loss_sup: 0.008712, loss_mps: 0.110979, loss_cps: 0.209070
[13:32:39.965] iteration 16903: total_loss: 0.625257, loss_sup: 0.251295, loss_mps: 0.128226, loss_cps: 0.245737
[13:32:40.110] iteration 16904: total_loss: 0.824727, loss_sup: 0.110426, loss_mps: 0.224565, loss_cps: 0.489737
[13:32:40.255] iteration 16905: total_loss: 0.394366, loss_sup: 0.099368, loss_mps: 0.105805, loss_cps: 0.189193
[13:32:40.402] iteration 16906: total_loss: 0.769217, loss_sup: 0.072386, loss_mps: 0.217816, loss_cps: 0.479015
[13:32:40.550] iteration 16907: total_loss: 0.344849, loss_sup: 0.009467, loss_mps: 0.119907, loss_cps: 0.215475
[13:32:40.696] iteration 16908: total_loss: 0.592030, loss_sup: 0.084394, loss_mps: 0.170664, loss_cps: 0.336972
[13:32:40.842] iteration 16909: total_loss: 0.323778, loss_sup: 0.033315, loss_mps: 0.101650, loss_cps: 0.188812
[13:32:40.988] iteration 16910: total_loss: 0.281985, loss_sup: 0.045153, loss_mps: 0.089278, loss_cps: 0.147554
[13:32:41.134] iteration 16911: total_loss: 0.515865, loss_sup: 0.037382, loss_mps: 0.151025, loss_cps: 0.327457
[13:32:41.279] iteration 16912: total_loss: 0.501714, loss_sup: 0.021965, loss_mps: 0.157055, loss_cps: 0.322693
[13:32:41.424] iteration 16913: total_loss: 0.840037, loss_sup: 0.372338, loss_mps: 0.157054, loss_cps: 0.310645
[13:32:41.570] iteration 16914: total_loss: 0.337760, loss_sup: 0.022697, loss_mps: 0.108551, loss_cps: 0.206513
[13:32:41.716] iteration 16915: total_loss: 0.377865, loss_sup: 0.028186, loss_mps: 0.119821, loss_cps: 0.229858
[13:32:41.862] iteration 16916: total_loss: 0.376214, loss_sup: 0.029045, loss_mps: 0.119645, loss_cps: 0.227524
[13:32:42.007] iteration 16917: total_loss: 0.493588, loss_sup: 0.037647, loss_mps: 0.150892, loss_cps: 0.305050
[13:32:42.155] iteration 16918: total_loss: 0.356417, loss_sup: 0.049088, loss_mps: 0.109994, loss_cps: 0.197335
[13:32:42.303] iteration 16919: total_loss: 0.349581, loss_sup: 0.021972, loss_mps: 0.113172, loss_cps: 0.214437
[13:32:42.449] iteration 16920: total_loss: 0.458458, loss_sup: 0.058233, loss_mps: 0.134575, loss_cps: 0.265649
[13:32:42.595] iteration 16921: total_loss: 0.335621, loss_sup: 0.034564, loss_mps: 0.109098, loss_cps: 0.191959
[13:32:42.740] iteration 16922: total_loss: 0.566123, loss_sup: 0.067118, loss_mps: 0.168590, loss_cps: 0.330414
[13:32:42.886] iteration 16923: total_loss: 0.627805, loss_sup: 0.173422, loss_mps: 0.158804, loss_cps: 0.295580
[13:32:43.032] iteration 16924: total_loss: 0.256140, loss_sup: 0.008539, loss_mps: 0.090476, loss_cps: 0.157125
[13:32:43.178] iteration 16925: total_loss: 0.530272, loss_sup: 0.102424, loss_mps: 0.139746, loss_cps: 0.288102
[13:32:43.323] iteration 16926: total_loss: 0.447030, loss_sup: 0.061116, loss_mps: 0.136610, loss_cps: 0.249304
[13:32:43.472] iteration 16927: total_loss: 0.574421, loss_sup: 0.111347, loss_mps: 0.149743, loss_cps: 0.313332
[13:32:43.618] iteration 16928: total_loss: 0.414222, loss_sup: 0.020371, loss_mps: 0.133835, loss_cps: 0.260015
[13:32:43.763] iteration 16929: total_loss: 0.470915, loss_sup: 0.048622, loss_mps: 0.145681, loss_cps: 0.276611
[13:32:43.909] iteration 16930: total_loss: 0.448024, loss_sup: 0.110113, loss_mps: 0.110005, loss_cps: 0.227906
[13:32:44.055] iteration 16931: total_loss: 0.550498, loss_sup: 0.033894, loss_mps: 0.173526, loss_cps: 0.343078
[13:32:44.201] iteration 16932: total_loss: 0.776646, loss_sup: 0.179937, loss_mps: 0.186155, loss_cps: 0.410554
[13:32:44.347] iteration 16933: total_loss: 0.462345, loss_sup: 0.138577, loss_mps: 0.118045, loss_cps: 0.205724
[13:32:44.493] iteration 16934: total_loss: 0.424304, loss_sup: 0.094520, loss_mps: 0.115388, loss_cps: 0.214396
[13:32:44.638] iteration 16935: total_loss: 0.692026, loss_sup: 0.057837, loss_mps: 0.202442, loss_cps: 0.431746
[13:32:44.785] iteration 16936: total_loss: 0.592057, loss_sup: 0.023996, loss_mps: 0.182587, loss_cps: 0.385473
[13:32:44.933] iteration 16937: total_loss: 0.371780, loss_sup: 0.068943, loss_mps: 0.103549, loss_cps: 0.199288
[13:32:45.078] iteration 16938: total_loss: 0.262550, loss_sup: 0.039100, loss_mps: 0.078034, loss_cps: 0.145416
[13:32:45.226] iteration 16939: total_loss: 0.659870, loss_sup: 0.127806, loss_mps: 0.171640, loss_cps: 0.360424
[13:32:45.373] iteration 16940: total_loss: 0.825925, loss_sup: 0.123375, loss_mps: 0.235286, loss_cps: 0.467264
[13:32:45.520] iteration 16941: total_loss: 0.368726, loss_sup: 0.057464, loss_mps: 0.108789, loss_cps: 0.202472
[13:32:45.666] iteration 16942: total_loss: 0.370702, loss_sup: 0.130507, loss_mps: 0.094784, loss_cps: 0.145411
[13:32:45.812] iteration 16943: total_loss: 0.535209, loss_sup: 0.014052, loss_mps: 0.168762, loss_cps: 0.352395
[13:32:45.958] iteration 16944: total_loss: 0.329985, loss_sup: 0.061886, loss_mps: 0.095891, loss_cps: 0.172208
[13:32:46.104] iteration 16945: total_loss: 0.345160, loss_sup: 0.017586, loss_mps: 0.114877, loss_cps: 0.212697
[13:32:46.252] iteration 16946: total_loss: 0.353611, loss_sup: 0.115801, loss_mps: 0.087835, loss_cps: 0.149976
[13:32:46.398] iteration 16947: total_loss: 0.415846, loss_sup: 0.063323, loss_mps: 0.121378, loss_cps: 0.231144
[13:32:46.544] iteration 16948: total_loss: 0.547235, loss_sup: 0.139620, loss_mps: 0.141993, loss_cps: 0.265622
[13:32:46.690] iteration 16949: total_loss: 0.401191, loss_sup: 0.026112, loss_mps: 0.124493, loss_cps: 0.250586
[13:32:46.836] iteration 16950: total_loss: 0.529106, loss_sup: 0.013411, loss_mps: 0.166082, loss_cps: 0.349612
[13:32:46.982] iteration 16951: total_loss: 0.703335, loss_sup: 0.048104, loss_mps: 0.211380, loss_cps: 0.443852
[13:32:47.128] iteration 16952: total_loss: 2.104470, loss_sup: 0.201295, loss_mps: 0.581436, loss_cps: 1.321740
[13:32:47.274] iteration 16953: total_loss: 0.450719, loss_sup: 0.117214, loss_mps: 0.117470, loss_cps: 0.216034
[13:32:47.420] iteration 16954: total_loss: 0.617987, loss_sup: 0.064699, loss_mps: 0.193418, loss_cps: 0.359869
[13:32:47.566] iteration 16955: total_loss: 0.471984, loss_sup: 0.020410, loss_mps: 0.152011, loss_cps: 0.299562
[13:32:47.712] iteration 16956: total_loss: 0.375577, loss_sup: 0.106599, loss_mps: 0.102281, loss_cps: 0.166697
[13:32:47.858] iteration 16957: total_loss: 0.784286, loss_sup: 0.074883, loss_mps: 0.223119, loss_cps: 0.486283
[13:32:48.004] iteration 16958: total_loss: 0.700550, loss_sup: 0.175952, loss_mps: 0.168517, loss_cps: 0.356081
[13:32:48.150] iteration 16959: total_loss: 0.749931, loss_sup: 0.103561, loss_mps: 0.197472, loss_cps: 0.448898
[13:32:48.296] iteration 16960: total_loss: 0.947972, loss_sup: 0.277427, loss_mps: 0.214386, loss_cps: 0.456158
[13:32:48.442] iteration 16961: total_loss: 0.681863, loss_sup: 0.073919, loss_mps: 0.192041, loss_cps: 0.415902
[13:32:48.589] iteration 16962: total_loss: 0.293333, loss_sup: 0.029925, loss_mps: 0.092552, loss_cps: 0.170856
[13:32:48.736] iteration 16963: total_loss: 0.669212, loss_sup: 0.131826, loss_mps: 0.183717, loss_cps: 0.353668
[13:32:48.882] iteration 16964: total_loss: 0.853347, loss_sup: 0.243509, loss_mps: 0.192568, loss_cps: 0.417271
[13:32:49.029] iteration 16965: total_loss: 0.523421, loss_sup: 0.142613, loss_mps: 0.130422, loss_cps: 0.250386
[13:32:49.178] iteration 16966: total_loss: 0.623598, loss_sup: 0.071275, loss_mps: 0.186386, loss_cps: 0.365937
[13:32:49.327] iteration 16967: total_loss: 0.562829, loss_sup: 0.123766, loss_mps: 0.159774, loss_cps: 0.279289
[13:32:49.473] iteration 16968: total_loss: 0.511086, loss_sup: 0.054699, loss_mps: 0.153875, loss_cps: 0.302512
[13:32:49.619] iteration 16969: total_loss: 0.210253, loss_sup: 0.009787, loss_mps: 0.078949, loss_cps: 0.121517
[13:32:49.765] iteration 16970: total_loss: 0.539711, loss_sup: 0.103302, loss_mps: 0.149939, loss_cps: 0.286470
[13:32:49.911] iteration 16971: total_loss: 0.903715, loss_sup: 0.331744, loss_mps: 0.186126, loss_cps: 0.385845
[13:32:50.057] iteration 16972: total_loss: 0.920011, loss_sup: 0.151982, loss_mps: 0.234622, loss_cps: 0.533407
[13:32:50.203] iteration 16973: total_loss: 0.461332, loss_sup: 0.087190, loss_mps: 0.129121, loss_cps: 0.245021
[13:32:50.352] iteration 16974: total_loss: 0.653086, loss_sup: 0.186654, loss_mps: 0.162040, loss_cps: 0.304392
[13:32:50.497] iteration 16975: total_loss: 0.611305, loss_sup: 0.108352, loss_mps: 0.174460, loss_cps: 0.328494
[13:32:50.643] iteration 16976: total_loss: 0.447346, loss_sup: 0.071707, loss_mps: 0.132898, loss_cps: 0.242741
[13:32:50.789] iteration 16977: total_loss: 0.930403, loss_sup: 0.089361, loss_mps: 0.263258, loss_cps: 0.577784
[13:32:50.935] iteration 16978: total_loss: 0.511234, loss_sup: 0.135070, loss_mps: 0.132993, loss_cps: 0.243171
[13:32:51.084] iteration 16979: total_loss: 0.441910, loss_sup: 0.055160, loss_mps: 0.135852, loss_cps: 0.250897
[13:32:51.230] iteration 16980: total_loss: 0.439752, loss_sup: 0.047593, loss_mps: 0.145494, loss_cps: 0.246665
[13:32:51.375] iteration 16981: total_loss: 0.639399, loss_sup: 0.044611, loss_mps: 0.187857, loss_cps: 0.406931
[13:32:51.521] iteration 16982: total_loss: 0.534782, loss_sup: 0.087024, loss_mps: 0.158027, loss_cps: 0.289731
[13:32:51.667] iteration 16983: total_loss: 0.844152, loss_sup: 0.079693, loss_mps: 0.251431, loss_cps: 0.513027
[13:32:51.813] iteration 16984: total_loss: 0.310499, loss_sup: 0.049652, loss_mps: 0.094324, loss_cps: 0.166523
[13:32:51.958] iteration 16985: total_loss: 0.431293, loss_sup: 0.037975, loss_mps: 0.142762, loss_cps: 0.250557
[13:32:52.104] iteration 16986: total_loss: 0.347157, loss_sup: 0.012023, loss_mps: 0.117809, loss_cps: 0.217326
[13:32:52.250] iteration 16987: total_loss: 1.028566, loss_sup: 0.147212, loss_mps: 0.282201, loss_cps: 0.599153
[13:32:52.397] iteration 16988: total_loss: 0.365974, loss_sup: 0.037676, loss_mps: 0.117616, loss_cps: 0.210682
[13:32:52.543] iteration 16989: total_loss: 0.790086, loss_sup: 0.231573, loss_mps: 0.188003, loss_cps: 0.370510
[13:32:52.690] iteration 16990: total_loss: 0.503964, loss_sup: 0.122925, loss_mps: 0.132910, loss_cps: 0.248129
[13:32:52.838] iteration 16991: total_loss: 0.273878, loss_sup: 0.066415, loss_mps: 0.084193, loss_cps: 0.123269
[13:32:52.985] iteration 16992: total_loss: 0.491118, loss_sup: 0.009614, loss_mps: 0.154000, loss_cps: 0.327504
[13:32:53.132] iteration 16993: total_loss: 0.358370, loss_sup: 0.011293, loss_mps: 0.126939, loss_cps: 0.220138
[13:32:53.278] iteration 16994: total_loss: 0.660606, loss_sup: 0.159049, loss_mps: 0.170179, loss_cps: 0.331379
[13:32:53.423] iteration 16995: total_loss: 0.752509, loss_sup: 0.098801, loss_mps: 0.215481, loss_cps: 0.438227
[13:32:53.569] iteration 16996: total_loss: 0.434045, loss_sup: 0.071408, loss_mps: 0.126943, loss_cps: 0.235694
[13:32:53.715] iteration 16997: total_loss: 0.522788, loss_sup: 0.027693, loss_mps: 0.169735, loss_cps: 0.325360
[13:32:53.862] iteration 16998: total_loss: 0.587607, loss_sup: 0.141626, loss_mps: 0.155377, loss_cps: 0.290603
[13:32:54.008] iteration 16999: total_loss: 1.070923, loss_sup: 0.435283, loss_mps: 0.210432, loss_cps: 0.425208
[13:32:54.154] iteration 17000: total_loss: 0.364070, loss_sup: 0.054561, loss_mps: 0.113017, loss_cps: 0.196491
[13:32:54.154] Evaluation Started ==>
[13:33:05.541] ==> valid iteration 17000: unet metrics: {'dc': 0.6487244086742135, 'jc': 0.5317551101796725, 'pre': 0.7972873088473629, 'hd': 5.568679323687759}, ynet metrics: {'dc': 0.6073182390038998, 'jc': 0.4865837436696539, 'pre': 0.7922102532252965, 'hd': 5.685725862729324}.
[13:33:05.543] Evaluation Finished!⏹️
[13:33:05.693] iteration 17001: total_loss: 0.679953, loss_sup: 0.206773, loss_mps: 0.166707, loss_cps: 0.306473
[13:33:05.840] iteration 17002: total_loss: 0.711069, loss_sup: 0.090620, loss_mps: 0.190326, loss_cps: 0.430123
[13:33:05.987] iteration 17003: total_loss: 0.630964, loss_sup: 0.070680, loss_mps: 0.185385, loss_cps: 0.374899
[13:33:06.134] iteration 17004: total_loss: 0.514803, loss_sup: 0.107711, loss_mps: 0.143852, loss_cps: 0.263241
[13:33:06.281] iteration 17005: total_loss: 0.462426, loss_sup: 0.085319, loss_mps: 0.132201, loss_cps: 0.244905
[13:33:06.428] iteration 17006: total_loss: 0.354620, loss_sup: 0.012942, loss_mps: 0.115830, loss_cps: 0.225848
[13:33:06.573] iteration 17007: total_loss: 0.623567, loss_sup: 0.068942, loss_mps: 0.176465, loss_cps: 0.378160
[13:33:06.718] iteration 17008: total_loss: 0.678247, loss_sup: 0.084440, loss_mps: 0.189725, loss_cps: 0.404083
[13:33:06.863] iteration 17009: total_loss: 0.287860, loss_sup: 0.079405, loss_mps: 0.077249, loss_cps: 0.131206
[13:33:07.009] iteration 17010: total_loss: 0.472674, loss_sup: 0.030131, loss_mps: 0.148585, loss_cps: 0.293958
[13:33:07.154] iteration 17011: total_loss: 0.383872, loss_sup: 0.133547, loss_mps: 0.088075, loss_cps: 0.162250
[13:33:07.303] iteration 17012: total_loss: 0.327659, loss_sup: 0.124654, loss_mps: 0.077350, loss_cps: 0.125655
[13:33:07.449] iteration 17013: total_loss: 0.624329, loss_sup: 0.055127, loss_mps: 0.186971, loss_cps: 0.382231
[13:33:07.594] iteration 17014: total_loss: 0.431205, loss_sup: 0.034696, loss_mps: 0.135863, loss_cps: 0.260646
[13:33:07.740] iteration 17015: total_loss: 1.068165, loss_sup: 0.059907, loss_mps: 0.298797, loss_cps: 0.709461
[13:33:07.887] iteration 17016: total_loss: 0.598707, loss_sup: 0.083390, loss_mps: 0.171089, loss_cps: 0.344228
[13:33:08.033] iteration 17017: total_loss: 0.904006, loss_sup: 0.534149, loss_mps: 0.132560, loss_cps: 0.237297
[13:33:08.178] iteration 17018: total_loss: 0.483395, loss_sup: 0.063654, loss_mps: 0.142929, loss_cps: 0.276812
[13:33:08.324] iteration 17019: total_loss: 0.501243, loss_sup: 0.151577, loss_mps: 0.122421, loss_cps: 0.227245
[13:33:08.471] iteration 17020: total_loss: 0.852738, loss_sup: 0.161281, loss_mps: 0.218215, loss_cps: 0.473242
[13:33:08.618] iteration 17021: total_loss: 0.407291, loss_sup: 0.173390, loss_mps: 0.086648, loss_cps: 0.147253
[13:33:08.764] iteration 17022: total_loss: 0.624131, loss_sup: 0.037828, loss_mps: 0.185526, loss_cps: 0.400777
[13:33:08.911] iteration 17023: total_loss: 0.943613, loss_sup: 0.094595, loss_mps: 0.262542, loss_cps: 0.586475
[13:33:09.058] iteration 17024: total_loss: 0.964329, loss_sup: 0.067141, loss_mps: 0.277212, loss_cps: 0.619976
[13:33:09.205] iteration 17025: total_loss: 0.831859, loss_sup: 0.118754, loss_mps: 0.232901, loss_cps: 0.480205
[13:33:09.351] iteration 17026: total_loss: 0.673043, loss_sup: 0.045551, loss_mps: 0.207265, loss_cps: 0.420226
[13:33:09.496] iteration 17027: total_loss: 0.237619, loss_sup: 0.008659, loss_mps: 0.087772, loss_cps: 0.141188
[13:33:09.642] iteration 17028: total_loss: 0.790188, loss_sup: 0.211241, loss_mps: 0.190093, loss_cps: 0.388854
[13:33:09.788] iteration 17029: total_loss: 0.925260, loss_sup: 0.157985, loss_mps: 0.250724, loss_cps: 0.516551
[13:33:09.934] iteration 17030: total_loss: 0.589615, loss_sup: 0.112347, loss_mps: 0.169929, loss_cps: 0.307338
[13:33:10.079] iteration 17031: total_loss: 0.717546, loss_sup: 0.148585, loss_mps: 0.193216, loss_cps: 0.375745
[13:33:10.225] iteration 17032: total_loss: 0.759641, loss_sup: 0.169379, loss_mps: 0.202982, loss_cps: 0.387279
[13:33:10.373] iteration 17033: total_loss: 0.582502, loss_sup: 0.205934, loss_mps: 0.134278, loss_cps: 0.242290
[13:33:10.519] iteration 17034: total_loss: 0.579352, loss_sup: 0.098720, loss_mps: 0.158062, loss_cps: 0.322570
[13:33:10.665] iteration 17035: total_loss: 0.660552, loss_sup: 0.141448, loss_mps: 0.175738, loss_cps: 0.343367
[13:33:10.811] iteration 17036: total_loss: 0.681497, loss_sup: 0.029049, loss_mps: 0.205201, loss_cps: 0.447247
[13:33:10.957] iteration 17037: total_loss: 0.824112, loss_sup: 0.123948, loss_mps: 0.234859, loss_cps: 0.465305
[13:33:11.103] iteration 17038: total_loss: 0.674673, loss_sup: 0.167017, loss_mps: 0.179608, loss_cps: 0.328048
[13:33:11.249] iteration 17039: total_loss: 0.516307, loss_sup: 0.105128, loss_mps: 0.149694, loss_cps: 0.261484
[13:33:11.395] iteration 17040: total_loss: 0.512207, loss_sup: 0.028528, loss_mps: 0.166433, loss_cps: 0.317246
[13:33:11.541] iteration 17041: total_loss: 0.567560, loss_sup: 0.109461, loss_mps: 0.162755, loss_cps: 0.295345
[13:33:11.687] iteration 17042: total_loss: 0.361139, loss_sup: 0.064912, loss_mps: 0.116732, loss_cps: 0.179496
[13:33:11.837] iteration 17043: total_loss: 0.814045, loss_sup: 0.375524, loss_mps: 0.154151, loss_cps: 0.284371
[13:33:11.983] iteration 17044: total_loss: 0.732214, loss_sup: 0.063521, loss_mps: 0.217396, loss_cps: 0.451296
[13:33:12.129] iteration 17045: total_loss: 0.494142, loss_sup: 0.182840, loss_mps: 0.116093, loss_cps: 0.195209
[13:33:12.274] iteration 17046: total_loss: 0.689948, loss_sup: 0.137935, loss_mps: 0.187780, loss_cps: 0.364233
[13:33:12.425] iteration 17047: total_loss: 0.372432, loss_sup: 0.085916, loss_mps: 0.111899, loss_cps: 0.174616
[13:33:12.573] iteration 17048: total_loss: 0.834311, loss_sup: 0.115544, loss_mps: 0.219727, loss_cps: 0.499040
[13:33:12.719] iteration 17049: total_loss: 0.351326, loss_sup: 0.064826, loss_mps: 0.111760, loss_cps: 0.174740
[13:33:12.866] iteration 17050: total_loss: 0.353879, loss_sup: 0.031855, loss_mps: 0.123703, loss_cps: 0.198322
[13:33:13.013] iteration 17051: total_loss: 0.566186, loss_sup: 0.161063, loss_mps: 0.144811, loss_cps: 0.260312
[13:33:13.159] iteration 17052: total_loss: 0.838931, loss_sup: 0.124483, loss_mps: 0.231516, loss_cps: 0.482932
[13:33:13.305] iteration 17053: total_loss: 0.402033, loss_sup: 0.030065, loss_mps: 0.136006, loss_cps: 0.235962
[13:33:13.451] iteration 17054: total_loss: 0.381012, loss_sup: 0.028332, loss_mps: 0.127784, loss_cps: 0.224896
[13:33:13.597] iteration 17055: total_loss: 1.229703, loss_sup: 0.240033, loss_mps: 0.322671, loss_cps: 0.667000
[13:33:13.744] iteration 17056: total_loss: 0.541821, loss_sup: 0.090885, loss_mps: 0.156696, loss_cps: 0.294239
[13:33:13.890] iteration 17057: total_loss: 1.145209, loss_sup: 0.204296, loss_mps: 0.302453, loss_cps: 0.638460
[13:33:14.036] iteration 17058: total_loss: 0.401882, loss_sup: 0.030108, loss_mps: 0.131537, loss_cps: 0.240237
[13:33:14.182] iteration 17059: total_loss: 0.393709, loss_sup: 0.072075, loss_mps: 0.110219, loss_cps: 0.211416
[13:33:14.328] iteration 17060: total_loss: 0.414303, loss_sup: 0.023168, loss_mps: 0.142274, loss_cps: 0.248861
[13:33:14.474] iteration 17061: total_loss: 0.543182, loss_sup: 0.081046, loss_mps: 0.158640, loss_cps: 0.303496
[13:33:14.620] iteration 17062: total_loss: 0.549306, loss_sup: 0.108249, loss_mps: 0.152601, loss_cps: 0.288456
[13:33:14.766] iteration 17063: total_loss: 0.480821, loss_sup: 0.078891, loss_mps: 0.135568, loss_cps: 0.266362
[13:33:14.911] iteration 17064: total_loss: 0.417275, loss_sup: 0.039231, loss_mps: 0.131404, loss_cps: 0.246640
[13:33:15.057] iteration 17065: total_loss: 0.672707, loss_sup: 0.077519, loss_mps: 0.192543, loss_cps: 0.402645
[13:33:15.203] iteration 17066: total_loss: 0.571191, loss_sup: 0.210220, loss_mps: 0.121020, loss_cps: 0.239951
[13:33:15.349] iteration 17067: total_loss: 0.224565, loss_sup: 0.052338, loss_mps: 0.068174, loss_cps: 0.104053
[13:33:15.495] iteration 17068: total_loss: 0.688746, loss_sup: 0.127815, loss_mps: 0.183684, loss_cps: 0.377247
[13:33:15.642] iteration 17069: total_loss: 0.790011, loss_sup: 0.286058, loss_mps: 0.161818, loss_cps: 0.342135
[13:33:15.787] iteration 17070: total_loss: 0.405155, loss_sup: 0.065056, loss_mps: 0.119317, loss_cps: 0.220781
[13:33:15.933] iteration 17071: total_loss: 0.395627, loss_sup: 0.038817, loss_mps: 0.120156, loss_cps: 0.236654
[13:33:16.080] iteration 17072: total_loss: 0.302682, loss_sup: 0.054747, loss_mps: 0.090244, loss_cps: 0.157691
[13:33:16.227] iteration 17073: total_loss: 0.776828, loss_sup: 0.066207, loss_mps: 0.224346, loss_cps: 0.486275
[13:33:16.375] iteration 17074: total_loss: 0.485361, loss_sup: 0.068378, loss_mps: 0.148043, loss_cps: 0.268941
[13:33:16.521] iteration 17075: total_loss: 0.906613, loss_sup: 0.190623, loss_mps: 0.226804, loss_cps: 0.489187
[13:33:16.668] iteration 17076: total_loss: 0.697360, loss_sup: 0.220649, loss_mps: 0.164090, loss_cps: 0.312621
[13:33:16.814] iteration 17077: total_loss: 0.565240, loss_sup: 0.085966, loss_mps: 0.156603, loss_cps: 0.322670
[13:33:16.962] iteration 17078: total_loss: 0.275263, loss_sup: 0.038165, loss_mps: 0.086345, loss_cps: 0.150753
[13:33:17.108] iteration 17079: total_loss: 0.405105, loss_sup: 0.043530, loss_mps: 0.127306, loss_cps: 0.234270
[13:33:17.255] iteration 17080: total_loss: 0.383456, loss_sup: 0.067137, loss_mps: 0.116744, loss_cps: 0.199575
[13:33:17.401] iteration 17081: total_loss: 0.338289, loss_sup: 0.017461, loss_mps: 0.118558, loss_cps: 0.202271
[13:33:17.547] iteration 17082: total_loss: 0.457580, loss_sup: 0.038598, loss_mps: 0.159019, loss_cps: 0.259962
[13:33:17.694] iteration 17083: total_loss: 0.735508, loss_sup: 0.151122, loss_mps: 0.187158, loss_cps: 0.397228
[13:33:17.841] iteration 17084: total_loss: 0.752690, loss_sup: 0.125636, loss_mps: 0.201576, loss_cps: 0.425478
[13:33:17.987] iteration 17085: total_loss: 0.399866, loss_sup: 0.014294, loss_mps: 0.133865, loss_cps: 0.251707
[13:33:18.133] iteration 17086: total_loss: 0.424083, loss_sup: 0.038928, loss_mps: 0.140110, loss_cps: 0.245044
[13:33:18.279] iteration 17087: total_loss: 0.695126, loss_sup: 0.165424, loss_mps: 0.183079, loss_cps: 0.346622
[13:33:18.425] iteration 17088: total_loss: 0.442982, loss_sup: 0.020132, loss_mps: 0.144942, loss_cps: 0.277908
[13:33:18.571] iteration 17089: total_loss: 0.374803, loss_sup: 0.051907, loss_mps: 0.117753, loss_cps: 0.205144
[13:33:18.717] iteration 17090: total_loss: 0.349659, loss_sup: 0.061269, loss_mps: 0.103611, loss_cps: 0.184779
[13:33:18.864] iteration 17091: total_loss: 0.528226, loss_sup: 0.031454, loss_mps: 0.169932, loss_cps: 0.326839
[13:33:19.011] iteration 17092: total_loss: 0.499628, loss_sup: 0.118124, loss_mps: 0.139479, loss_cps: 0.242025
[13:33:19.158] iteration 17093: total_loss: 0.667940, loss_sup: 0.038409, loss_mps: 0.201845, loss_cps: 0.427686
[13:33:19.306] iteration 17094: total_loss: 0.402140, loss_sup: 0.041431, loss_mps: 0.125715, loss_cps: 0.234994
[13:33:19.452] iteration 17095: total_loss: 0.407968, loss_sup: 0.052327, loss_mps: 0.125925, loss_cps: 0.229716
[13:33:19.599] iteration 17096: total_loss: 0.279125, loss_sup: 0.033140, loss_mps: 0.087445, loss_cps: 0.158540
[13:33:19.747] iteration 17097: total_loss: 0.378129, loss_sup: 0.024757, loss_mps: 0.117039, loss_cps: 0.236333
[13:33:19.892] iteration 17098: total_loss: 0.458028, loss_sup: 0.004943, loss_mps: 0.145953, loss_cps: 0.307132
[13:33:20.040] iteration 17099: total_loss: 0.747372, loss_sup: 0.278895, loss_mps: 0.152253, loss_cps: 0.316224
[13:33:20.186] iteration 17100: total_loss: 0.555544, loss_sup: 0.159301, loss_mps: 0.133072, loss_cps: 0.263170
[13:33:20.187] Evaluation Started ==>
[13:33:31.494] ==> valid iteration 17100: unet metrics: {'dc': 0.6262589513726006, 'jc': 0.510076984761401, 'pre': 0.7792472626498971, 'hd': 5.6312067299277855}, ynet metrics: {'dc': 0.5786044741908991, 'jc': 0.4648867655486092, 'pre': 0.7910666976480201, 'hd': 5.563798044102424}.
[13:33:31.495] Evaluation Finished!⏹️
[13:33:31.644] iteration 17101: total_loss: 0.497547, loss_sup: 0.264431, loss_mps: 0.080122, loss_cps: 0.152994
[13:33:31.792] iteration 17102: total_loss: 0.474454, loss_sup: 0.032907, loss_mps: 0.146350, loss_cps: 0.295197
[13:33:31.938] iteration 17103: total_loss: 0.511927, loss_sup: 0.063907, loss_mps: 0.155071, loss_cps: 0.292949
[13:33:32.084] iteration 17104: total_loss: 0.627163, loss_sup: 0.204301, loss_mps: 0.139477, loss_cps: 0.283385
[13:33:32.229] iteration 17105: total_loss: 0.506522, loss_sup: 0.021502, loss_mps: 0.160683, loss_cps: 0.324337
[13:33:32.374] iteration 17106: total_loss: 0.417780, loss_sup: 0.041501, loss_mps: 0.127914, loss_cps: 0.248365
[13:33:32.520] iteration 17107: total_loss: 0.302600, loss_sup: 0.030378, loss_mps: 0.096558, loss_cps: 0.175664
[13:33:32.666] iteration 17108: total_loss: 0.706143, loss_sup: 0.201385, loss_mps: 0.164450, loss_cps: 0.340308
[13:33:32.811] iteration 17109: total_loss: 0.485078, loss_sup: 0.031665, loss_mps: 0.143556, loss_cps: 0.309857
[13:33:32.957] iteration 17110: total_loss: 0.573068, loss_sup: 0.101988, loss_mps: 0.159593, loss_cps: 0.311487
[13:33:33.102] iteration 17111: total_loss: 0.462434, loss_sup: 0.054711, loss_mps: 0.141879, loss_cps: 0.265844
[13:33:33.248] iteration 17112: total_loss: 0.419590, loss_sup: 0.022132, loss_mps: 0.142456, loss_cps: 0.255003
[13:33:33.394] iteration 17113: total_loss: 0.351964, loss_sup: 0.032034, loss_mps: 0.109663, loss_cps: 0.210268
[13:33:33.542] iteration 17114: total_loss: 0.727487, loss_sup: 0.040094, loss_mps: 0.216520, loss_cps: 0.470873
[13:33:33.688] iteration 17115: total_loss: 0.444413, loss_sup: 0.129848, loss_mps: 0.107413, loss_cps: 0.207153
[13:33:33.834] iteration 17116: total_loss: 0.570740, loss_sup: 0.155097, loss_mps: 0.136999, loss_cps: 0.278644
[13:33:33.980] iteration 17117: total_loss: 0.722288, loss_sup: 0.065090, loss_mps: 0.209616, loss_cps: 0.447583
[13:33:34.126] iteration 17118: total_loss: 0.329799, loss_sup: 0.108981, loss_mps: 0.080286, loss_cps: 0.140532
[13:33:34.272] iteration 17119: total_loss: 0.467780, loss_sup: 0.088189, loss_mps: 0.127597, loss_cps: 0.251995
[13:33:34.418] iteration 17120: total_loss: 0.533817, loss_sup: 0.070083, loss_mps: 0.149259, loss_cps: 0.314475
[13:33:34.564] iteration 17121: total_loss: 1.306570, loss_sup: 0.058906, loss_mps: 0.371302, loss_cps: 0.876361
[13:33:34.710] iteration 17122: total_loss: 0.299428, loss_sup: 0.019456, loss_mps: 0.101601, loss_cps: 0.178372
[13:33:34.857] iteration 17123: total_loss: 0.642825, loss_sup: 0.065535, loss_mps: 0.184787, loss_cps: 0.392503
[13:33:35.002] iteration 17124: total_loss: 0.375685, loss_sup: 0.114127, loss_mps: 0.095778, loss_cps: 0.165780
[13:33:35.148] iteration 17125: total_loss: 0.569087, loss_sup: 0.142506, loss_mps: 0.138287, loss_cps: 0.288294
[13:33:35.293] iteration 17126: total_loss: 0.410463, loss_sup: 0.025564, loss_mps: 0.133831, loss_cps: 0.251068
[13:33:35.440] iteration 17127: total_loss: 0.744907, loss_sup: 0.153278, loss_mps: 0.196683, loss_cps: 0.394947
[13:33:35.585] iteration 17128: total_loss: 0.346508, loss_sup: 0.055367, loss_mps: 0.105996, loss_cps: 0.185145
[13:33:35.732] iteration 17129: total_loss: 0.250126, loss_sup: 0.017798, loss_mps: 0.080079, loss_cps: 0.152248
[13:33:35.878] iteration 17130: total_loss: 0.371378, loss_sup: 0.023168, loss_mps: 0.135264, loss_cps: 0.212946
[13:33:36.025] iteration 17131: total_loss: 0.481367, loss_sup: 0.100805, loss_mps: 0.139564, loss_cps: 0.240998
[13:33:36.171] iteration 17132: total_loss: 0.577232, loss_sup: 0.020622, loss_mps: 0.180512, loss_cps: 0.376097
[13:33:36.317] iteration 17133: total_loss: 0.906334, loss_sup: 0.245641, loss_mps: 0.207548, loss_cps: 0.453144
[13:33:36.462] iteration 17134: total_loss: 0.652345, loss_sup: 0.089047, loss_mps: 0.184713, loss_cps: 0.378585
[13:33:36.608] iteration 17135: total_loss: 0.464678, loss_sup: 0.091962, loss_mps: 0.135239, loss_cps: 0.237476
[13:33:36.754] iteration 17136: total_loss: 0.419769, loss_sup: 0.077859, loss_mps: 0.120385, loss_cps: 0.221525
[13:33:36.900] iteration 17137: total_loss: 0.416540, loss_sup: 0.046285, loss_mps: 0.127182, loss_cps: 0.243073
[13:33:36.962] iteration 17138: total_loss: 0.549199, loss_sup: 0.140703, loss_mps: 0.148831, loss_cps: 0.259665
[13:33:38.176] iteration 17139: total_loss: 0.501370, loss_sup: 0.022202, loss_mps: 0.159930, loss_cps: 0.319238
[13:33:38.324] iteration 17140: total_loss: 0.660815, loss_sup: 0.209531, loss_mps: 0.152460, loss_cps: 0.298825
[13:33:38.471] iteration 17141: total_loss: 0.620249, loss_sup: 0.108072, loss_mps: 0.172697, loss_cps: 0.339480
[13:33:38.617] iteration 17142: total_loss: 0.536021, loss_sup: 0.071022, loss_mps: 0.160857, loss_cps: 0.304142
[13:33:38.763] iteration 17143: total_loss: 0.350372, loss_sup: 0.049660, loss_mps: 0.105853, loss_cps: 0.194859
[13:33:38.911] iteration 17144: total_loss: 0.492389, loss_sup: 0.014338, loss_mps: 0.157424, loss_cps: 0.320627
[13:33:39.058] iteration 17145: total_loss: 0.425654, loss_sup: 0.078377, loss_mps: 0.116942, loss_cps: 0.230334
[13:33:39.206] iteration 17146: total_loss: 0.522597, loss_sup: 0.017449, loss_mps: 0.168376, loss_cps: 0.336773
[13:33:39.353] iteration 17147: total_loss: 0.519137, loss_sup: 0.125730, loss_mps: 0.138333, loss_cps: 0.255075
[13:33:39.499] iteration 17148: total_loss: 0.321461, loss_sup: 0.010690, loss_mps: 0.113894, loss_cps: 0.196877
[13:33:39.646] iteration 17149: total_loss: 0.835620, loss_sup: 0.220386, loss_mps: 0.193859, loss_cps: 0.421374
[13:33:39.794] iteration 17150: total_loss: 0.438433, loss_sup: 0.077706, loss_mps: 0.128977, loss_cps: 0.231750
[13:33:39.940] iteration 17151: total_loss: 0.435306, loss_sup: 0.104719, loss_mps: 0.112437, loss_cps: 0.218151
[13:33:40.086] iteration 17152: total_loss: 0.602691, loss_sup: 0.179594, loss_mps: 0.146880, loss_cps: 0.276217
[13:33:40.234] iteration 17153: total_loss: 0.581625, loss_sup: 0.128574, loss_mps: 0.151249, loss_cps: 0.301802
[13:33:40.380] iteration 17154: total_loss: 0.327647, loss_sup: 0.015276, loss_mps: 0.107573, loss_cps: 0.204798
[13:33:40.527] iteration 17155: total_loss: 0.728278, loss_sup: 0.149740, loss_mps: 0.187784, loss_cps: 0.390754
[13:33:40.672] iteration 17156: total_loss: 0.615925, loss_sup: 0.075166, loss_mps: 0.172294, loss_cps: 0.368465
[13:33:40.821] iteration 17157: total_loss: 0.661788, loss_sup: 0.016578, loss_mps: 0.212721, loss_cps: 0.432489
[13:33:40.966] iteration 17158: total_loss: 0.497424, loss_sup: 0.147885, loss_mps: 0.123675, loss_cps: 0.225864
[13:33:41.113] iteration 17159: total_loss: 0.787500, loss_sup: 0.090141, loss_mps: 0.230206, loss_cps: 0.467153
[13:33:41.258] iteration 17160: total_loss: 0.572676, loss_sup: 0.033445, loss_mps: 0.176478, loss_cps: 0.362752
[13:33:41.404] iteration 17161: total_loss: 0.410666, loss_sup: 0.057580, loss_mps: 0.128431, loss_cps: 0.224655
[13:33:41.551] iteration 17162: total_loss: 0.429713, loss_sup: 0.106330, loss_mps: 0.119710, loss_cps: 0.203673
[13:33:41.697] iteration 17163: total_loss: 0.429960, loss_sup: 0.057063, loss_mps: 0.126625, loss_cps: 0.246272
[13:33:41.842] iteration 17164: total_loss: 0.514992, loss_sup: 0.034847, loss_mps: 0.152948, loss_cps: 0.327197
[13:33:41.989] iteration 17165: total_loss: 0.388988, loss_sup: 0.013638, loss_mps: 0.125009, loss_cps: 0.250341
[13:33:42.135] iteration 17166: total_loss: 0.568297, loss_sup: 0.097406, loss_mps: 0.159564, loss_cps: 0.311327
[13:33:42.280] iteration 17167: total_loss: 0.644577, loss_sup: 0.065774, loss_mps: 0.181809, loss_cps: 0.396993
[13:33:42.426] iteration 17168: total_loss: 0.887904, loss_sup: 0.083908, loss_mps: 0.262607, loss_cps: 0.541389
[13:33:42.573] iteration 17169: total_loss: 0.452280, loss_sup: 0.072517, loss_mps: 0.131550, loss_cps: 0.248213
[13:33:42.720] iteration 17170: total_loss: 0.571869, loss_sup: 0.245879, loss_mps: 0.120148, loss_cps: 0.205842
[13:33:42.866] iteration 17171: total_loss: 0.374150, loss_sup: 0.041581, loss_mps: 0.114850, loss_cps: 0.217720
[13:33:43.014] iteration 17172: total_loss: 0.804036, loss_sup: 0.174995, loss_mps: 0.194127, loss_cps: 0.434913
[13:33:43.161] iteration 17173: total_loss: 0.552091, loss_sup: 0.291442, loss_mps: 0.094188, loss_cps: 0.166462
[13:33:43.309] iteration 17174: total_loss: 0.422681, loss_sup: 0.049781, loss_mps: 0.127644, loss_cps: 0.245256
[13:33:43.457] iteration 17175: total_loss: 0.237329, loss_sup: 0.040894, loss_mps: 0.073865, loss_cps: 0.122569
[13:33:43.602] iteration 17176: total_loss: 0.641674, loss_sup: 0.141960, loss_mps: 0.160283, loss_cps: 0.339431
[13:33:43.749] iteration 17177: total_loss: 0.413354, loss_sup: 0.022220, loss_mps: 0.130062, loss_cps: 0.261072
[13:33:43.895] iteration 17178: total_loss: 0.396091, loss_sup: 0.022205, loss_mps: 0.120557, loss_cps: 0.253329
[13:33:44.042] iteration 17179: total_loss: 0.675925, loss_sup: 0.077905, loss_mps: 0.188018, loss_cps: 0.410002
[13:33:44.188] iteration 17180: total_loss: 0.318469, loss_sup: 0.061607, loss_mps: 0.094408, loss_cps: 0.162454
[13:33:44.334] iteration 17181: total_loss: 0.510712, loss_sup: 0.047315, loss_mps: 0.158896, loss_cps: 0.304501
[13:33:44.483] iteration 17182: total_loss: 0.445692, loss_sup: 0.019356, loss_mps: 0.140024, loss_cps: 0.286312
[13:33:44.629] iteration 17183: total_loss: 0.527514, loss_sup: 0.032993, loss_mps: 0.156679, loss_cps: 0.337842
[13:33:44.776] iteration 17184: total_loss: 0.464321, loss_sup: 0.161410, loss_mps: 0.112025, loss_cps: 0.190886
[13:33:44.922] iteration 17185: total_loss: 0.606989, loss_sup: 0.078992, loss_mps: 0.181985, loss_cps: 0.346012
[13:33:45.069] iteration 17186: total_loss: 1.048650, loss_sup: 0.092440, loss_mps: 0.279243, loss_cps: 0.676968
[13:33:45.215] iteration 17187: total_loss: 0.725967, loss_sup: 0.026254, loss_mps: 0.219551, loss_cps: 0.480162
[13:33:45.361] iteration 17188: total_loss: 0.869285, loss_sup: 0.088734, loss_mps: 0.243811, loss_cps: 0.536739
[13:33:45.508] iteration 17189: total_loss: 0.395599, loss_sup: 0.007602, loss_mps: 0.133342, loss_cps: 0.254655
[13:33:45.656] iteration 17190: total_loss: 0.340300, loss_sup: 0.035961, loss_mps: 0.107248, loss_cps: 0.197091
[13:33:45.803] iteration 17191: total_loss: 0.700933, loss_sup: 0.021395, loss_mps: 0.219826, loss_cps: 0.459712
[13:33:45.949] iteration 17192: total_loss: 0.581527, loss_sup: 0.029895, loss_mps: 0.178207, loss_cps: 0.373425
[13:33:46.096] iteration 17193: total_loss: 1.063353, loss_sup: 0.049842, loss_mps: 0.306981, loss_cps: 0.706531
[13:33:46.243] iteration 17194: total_loss: 0.600152, loss_sup: 0.047062, loss_mps: 0.183585, loss_cps: 0.369505
[13:33:46.389] iteration 17195: total_loss: 0.505738, loss_sup: 0.131586, loss_mps: 0.130748, loss_cps: 0.243404
[13:33:46.536] iteration 17196: total_loss: 0.377700, loss_sup: 0.013181, loss_mps: 0.128439, loss_cps: 0.236080
[13:33:46.683] iteration 17197: total_loss: 0.339858, loss_sup: 0.062471, loss_mps: 0.095957, loss_cps: 0.181429
[13:33:46.831] iteration 17198: total_loss: 0.629456, loss_sup: 0.119292, loss_mps: 0.172267, loss_cps: 0.337897
[13:33:46.978] iteration 17199: total_loss: 0.491987, loss_sup: 0.124256, loss_mps: 0.131096, loss_cps: 0.236635
[13:33:47.125] iteration 17200: total_loss: 0.366419, loss_sup: 0.023450, loss_mps: 0.126255, loss_cps: 0.216714
[13:33:47.125] Evaluation Started ==>
[13:33:58.454] ==> valid iteration 17200: unet metrics: {'dc': 0.6241201199746181, 'jc': 0.5080845673672921, 'pre': 0.7842404557916028, 'hd': 5.534848503387416}, ynet metrics: {'dc': 0.6072541682358529, 'jc': 0.48715425448604677, 'pre': 0.7670569572591925, 'hd': 5.851356233130562}.
[13:33:58.456] Evaluation Finished!⏹️
[13:33:58.609] iteration 17201: total_loss: 0.477336, loss_sup: 0.028649, loss_mps: 0.138545, loss_cps: 0.310142
[13:33:58.756] iteration 17202: total_loss: 0.668242, loss_sup: 0.070659, loss_mps: 0.193372, loss_cps: 0.404210
[13:33:58.902] iteration 17203: total_loss: 0.853631, loss_sup: 0.314040, loss_mps: 0.183707, loss_cps: 0.355884
[13:33:59.047] iteration 17204: total_loss: 0.457233, loss_sup: 0.029819, loss_mps: 0.142190, loss_cps: 0.285224
[13:33:59.192] iteration 17205: total_loss: 0.291017, loss_sup: 0.021017, loss_mps: 0.094355, loss_cps: 0.175645
[13:33:59.339] iteration 17206: total_loss: 0.541277, loss_sup: 0.151748, loss_mps: 0.149996, loss_cps: 0.239533
[13:33:59.487] iteration 17207: total_loss: 0.284513, loss_sup: 0.023737, loss_mps: 0.093870, loss_cps: 0.166906
[13:33:59.635] iteration 17208: total_loss: 0.413403, loss_sup: 0.013787, loss_mps: 0.139420, loss_cps: 0.260195
[13:33:59.780] iteration 17209: total_loss: 0.394343, loss_sup: 0.039075, loss_mps: 0.110349, loss_cps: 0.244918
[13:33:59.926] iteration 17210: total_loss: 0.740785, loss_sup: 0.185696, loss_mps: 0.181840, loss_cps: 0.373249
[13:34:00.072] iteration 17211: total_loss: 0.330747, loss_sup: 0.025903, loss_mps: 0.109656, loss_cps: 0.195187
[13:34:00.217] iteration 17212: total_loss: 0.308295, loss_sup: 0.021514, loss_mps: 0.105661, loss_cps: 0.181120
[13:34:00.363] iteration 17213: total_loss: 0.517544, loss_sup: 0.039177, loss_mps: 0.152333, loss_cps: 0.326034
[13:34:00.510] iteration 17214: total_loss: 0.738029, loss_sup: 0.130295, loss_mps: 0.200044, loss_cps: 0.407689
[13:34:00.655] iteration 17215: total_loss: 0.572672, loss_sup: 0.144553, loss_mps: 0.152055, loss_cps: 0.276065
[13:34:00.801] iteration 17216: total_loss: 0.643137, loss_sup: 0.215853, loss_mps: 0.145871, loss_cps: 0.281413
[13:34:00.946] iteration 17217: total_loss: 0.489751, loss_sup: 0.140047, loss_mps: 0.125318, loss_cps: 0.224386
[13:34:01.094] iteration 17218: total_loss: 0.921226, loss_sup: 0.378411, loss_mps: 0.176952, loss_cps: 0.365863
[13:34:01.241] iteration 17219: total_loss: 1.596744, loss_sup: 0.426028, loss_mps: 0.334766, loss_cps: 0.835950
[13:34:01.386] iteration 17220: total_loss: 0.894047, loss_sup: 0.108383, loss_mps: 0.254536, loss_cps: 0.531127
[13:34:01.532] iteration 17221: total_loss: 0.554736, loss_sup: 0.086830, loss_mps: 0.158180, loss_cps: 0.309726
[13:34:01.677] iteration 17222: total_loss: 1.145310, loss_sup: 0.312667, loss_mps: 0.275407, loss_cps: 0.557237
[13:34:01.823] iteration 17223: total_loss: 0.994224, loss_sup: 0.100668, loss_mps: 0.276633, loss_cps: 0.616923
[13:34:01.969] iteration 17224: total_loss: 0.543674, loss_sup: 0.046711, loss_mps: 0.178534, loss_cps: 0.318429
[13:34:02.115] iteration 17225: total_loss: 0.316982, loss_sup: 0.010934, loss_mps: 0.117402, loss_cps: 0.188646
[13:34:02.261] iteration 17226: total_loss: 0.495846, loss_sup: 0.108442, loss_mps: 0.138253, loss_cps: 0.249151
[13:34:02.406] iteration 17227: total_loss: 0.568854, loss_sup: 0.072884, loss_mps: 0.169960, loss_cps: 0.326010
[13:34:02.552] iteration 17228: total_loss: 0.441960, loss_sup: 0.070949, loss_mps: 0.129484, loss_cps: 0.241527
[13:34:02.698] iteration 17229: total_loss: 0.884351, loss_sup: 0.074564, loss_mps: 0.261931, loss_cps: 0.547856
[13:34:02.846] iteration 17230: total_loss: 0.882754, loss_sup: 0.092231, loss_mps: 0.252469, loss_cps: 0.538054
[13:34:02.992] iteration 17231: total_loss: 0.649156, loss_sup: 0.070450, loss_mps: 0.194782, loss_cps: 0.383924
[13:34:03.137] iteration 17232: total_loss: 1.175442, loss_sup: 0.416179, loss_mps: 0.243441, loss_cps: 0.515823
[13:34:03.283] iteration 17233: total_loss: 0.561391, loss_sup: 0.118270, loss_mps: 0.158395, loss_cps: 0.284726
[13:34:03.431] iteration 17234: total_loss: 0.784052, loss_sup: 0.050711, loss_mps: 0.235262, loss_cps: 0.498080
[13:34:03.576] iteration 17235: total_loss: 0.441048, loss_sup: 0.049926, loss_mps: 0.139649, loss_cps: 0.251473
[13:34:03.722] iteration 17236: total_loss: 0.490121, loss_sup: 0.074257, loss_mps: 0.145755, loss_cps: 0.270110
[13:34:03.869] iteration 17237: total_loss: 0.417028, loss_sup: 0.077757, loss_mps: 0.128372, loss_cps: 0.210898
[13:34:04.015] iteration 17238: total_loss: 0.939189, loss_sup: 0.045465, loss_mps: 0.298715, loss_cps: 0.595009
[13:34:04.162] iteration 17239: total_loss: 0.490336, loss_sup: 0.062010, loss_mps: 0.156597, loss_cps: 0.271729
[13:34:04.308] iteration 17240: total_loss: 0.748270, loss_sup: 0.167967, loss_mps: 0.206821, loss_cps: 0.373482
[13:34:04.454] iteration 17241: total_loss: 0.342614, loss_sup: 0.047188, loss_mps: 0.108337, loss_cps: 0.187089
[13:34:04.600] iteration 17242: total_loss: 0.952292, loss_sup: 0.219357, loss_mps: 0.237154, loss_cps: 0.495781
[13:34:04.746] iteration 17243: total_loss: 1.180189, loss_sup: 0.112843, loss_mps: 0.329144, loss_cps: 0.738202
[13:34:04.892] iteration 17244: total_loss: 0.311573, loss_sup: 0.032818, loss_mps: 0.110959, loss_cps: 0.167796
[13:34:05.038] iteration 17245: total_loss: 0.663635, loss_sup: 0.175621, loss_mps: 0.166006, loss_cps: 0.322009
[13:34:05.185] iteration 17246: total_loss: 0.485535, loss_sup: 0.100426, loss_mps: 0.137843, loss_cps: 0.247266
[13:34:05.331] iteration 17247: total_loss: 0.400642, loss_sup: 0.043838, loss_mps: 0.124115, loss_cps: 0.232689
[13:34:05.478] iteration 17248: total_loss: 0.778889, loss_sup: 0.147885, loss_mps: 0.210208, loss_cps: 0.420796
[13:34:05.624] iteration 17249: total_loss: 0.550822, loss_sup: 0.095454, loss_mps: 0.154505, loss_cps: 0.300862
[13:34:05.770] iteration 17250: total_loss: 0.404005, loss_sup: 0.084893, loss_mps: 0.116983, loss_cps: 0.202129
[13:34:05.916] iteration 17251: total_loss: 0.651164, loss_sup: 0.159068, loss_mps: 0.173482, loss_cps: 0.318614
[13:34:06.062] iteration 17252: total_loss: 0.671975, loss_sup: 0.097751, loss_mps: 0.184081, loss_cps: 0.390143
[13:34:06.209] iteration 17253: total_loss: 0.446059, loss_sup: 0.038458, loss_mps: 0.134739, loss_cps: 0.272862
[13:34:06.355] iteration 17254: total_loss: 0.672652, loss_sup: 0.132595, loss_mps: 0.175788, loss_cps: 0.364270
[13:34:06.503] iteration 17255: total_loss: 0.524973, loss_sup: 0.052886, loss_mps: 0.169960, loss_cps: 0.302127
[13:34:06.649] iteration 17256: total_loss: 0.925154, loss_sup: 0.084142, loss_mps: 0.264199, loss_cps: 0.576814
[13:34:06.795] iteration 17257: total_loss: 0.538502, loss_sup: 0.097622, loss_mps: 0.153215, loss_cps: 0.287664
[13:34:06.942] iteration 17258: total_loss: 0.604191, loss_sup: 0.082721, loss_mps: 0.173394, loss_cps: 0.348076
[13:34:07.089] iteration 17259: total_loss: 0.889138, loss_sup: 0.086090, loss_mps: 0.258377, loss_cps: 0.544670
[13:34:07.234] iteration 17260: total_loss: 0.656661, loss_sup: 0.328216, loss_mps: 0.119413, loss_cps: 0.209032
[13:34:07.381] iteration 17261: total_loss: 0.389767, loss_sup: 0.033237, loss_mps: 0.126253, loss_cps: 0.230277
[13:34:07.527] iteration 17262: total_loss: 0.335641, loss_sup: 0.014827, loss_mps: 0.120834, loss_cps: 0.199979
[13:34:07.674] iteration 17263: total_loss: 0.745076, loss_sup: 0.117960, loss_mps: 0.208398, loss_cps: 0.418718
[13:34:07.821] iteration 17264: total_loss: 0.793926, loss_sup: 0.084560, loss_mps: 0.221962, loss_cps: 0.487404
[13:34:07.968] iteration 17265: total_loss: 1.035446, loss_sup: 0.056962, loss_mps: 0.324121, loss_cps: 0.654363
[13:34:08.114] iteration 17266: total_loss: 0.904299, loss_sup: 0.167111, loss_mps: 0.253893, loss_cps: 0.483295
[13:34:08.263] iteration 17267: total_loss: 0.329653, loss_sup: 0.046962, loss_mps: 0.112197, loss_cps: 0.170494
[13:34:08.410] iteration 17268: total_loss: 0.888465, loss_sup: 0.106539, loss_mps: 0.255331, loss_cps: 0.526595
[13:34:08.555] iteration 17269: total_loss: 0.488536, loss_sup: 0.097697, loss_mps: 0.133912, loss_cps: 0.256926
[13:34:08.701] iteration 17270: total_loss: 0.326688, loss_sup: 0.056275, loss_mps: 0.099142, loss_cps: 0.171271
[13:34:08.849] iteration 17271: total_loss: 0.482544, loss_sup: 0.025086, loss_mps: 0.161578, loss_cps: 0.295880
[13:34:08.996] iteration 17272: total_loss: 0.544308, loss_sup: 0.131704, loss_mps: 0.141278, loss_cps: 0.271325
[13:34:09.143] iteration 17273: total_loss: 0.611667, loss_sup: 0.008871, loss_mps: 0.192904, loss_cps: 0.409892
[13:34:09.294] iteration 17274: total_loss: 0.509717, loss_sup: 0.031313, loss_mps: 0.163066, loss_cps: 0.315338
[13:34:09.442] iteration 17275: total_loss: 0.644837, loss_sup: 0.091500, loss_mps: 0.196408, loss_cps: 0.356929
[13:34:09.592] iteration 17276: total_loss: 0.407398, loss_sup: 0.030427, loss_mps: 0.130414, loss_cps: 0.246558
[13:34:09.738] iteration 17277: total_loss: 0.367018, loss_sup: 0.064329, loss_mps: 0.106831, loss_cps: 0.195858
[13:34:09.884] iteration 17278: total_loss: 0.454477, loss_sup: 0.015977, loss_mps: 0.149302, loss_cps: 0.289198
[13:34:10.030] iteration 17279: total_loss: 0.390805, loss_sup: 0.100245, loss_mps: 0.104967, loss_cps: 0.185594
[13:34:10.175] iteration 17280: total_loss: 0.694260, loss_sup: 0.158288, loss_mps: 0.180280, loss_cps: 0.355692
[13:34:10.322] iteration 17281: total_loss: 0.421473, loss_sup: 0.089689, loss_mps: 0.117448, loss_cps: 0.214336
[13:34:10.468] iteration 17282: total_loss: 0.426744, loss_sup: 0.062013, loss_mps: 0.127205, loss_cps: 0.237527
[13:34:10.614] iteration 17283: total_loss: 0.281246, loss_sup: 0.043975, loss_mps: 0.086508, loss_cps: 0.150764
[13:34:10.760] iteration 17284: total_loss: 0.426418, loss_sup: 0.072322, loss_mps: 0.132028, loss_cps: 0.222067
[13:34:10.907] iteration 17285: total_loss: 0.357101, loss_sup: 0.050451, loss_mps: 0.116205, loss_cps: 0.190445
[13:34:11.052] iteration 17286: total_loss: 0.548895, loss_sup: 0.041151, loss_mps: 0.165860, loss_cps: 0.341884
[13:34:11.199] iteration 17287: total_loss: 0.306547, loss_sup: 0.029463, loss_mps: 0.098487, loss_cps: 0.178597
[13:34:11.345] iteration 17288: total_loss: 0.271969, loss_sup: 0.033044, loss_mps: 0.085616, loss_cps: 0.153309
[13:34:11.491] iteration 17289: total_loss: 0.324938, loss_sup: 0.015827, loss_mps: 0.111397, loss_cps: 0.197714
[13:34:11.637] iteration 17290: total_loss: 0.592107, loss_sup: 0.070820, loss_mps: 0.167068, loss_cps: 0.354220
[13:34:11.784] iteration 17291: total_loss: 0.438799, loss_sup: 0.001739, loss_mps: 0.141679, loss_cps: 0.295381
[13:34:11.931] iteration 17292: total_loss: 0.540142, loss_sup: 0.185868, loss_mps: 0.114795, loss_cps: 0.239478
[13:34:12.078] iteration 17293: total_loss: 0.416730, loss_sup: 0.138924, loss_mps: 0.099082, loss_cps: 0.178725
[13:34:12.226] iteration 17294: total_loss: 0.673742, loss_sup: 0.201691, loss_mps: 0.156813, loss_cps: 0.315239
[13:34:12.375] iteration 17295: total_loss: 0.662478, loss_sup: 0.318616, loss_mps: 0.123553, loss_cps: 0.220308
[13:34:12.523] iteration 17296: total_loss: 0.674947, loss_sup: 0.247758, loss_mps: 0.148062, loss_cps: 0.279127
[13:34:12.669] iteration 17297: total_loss: 0.444898, loss_sup: 0.043831, loss_mps: 0.139320, loss_cps: 0.261747
[13:34:12.815] iteration 17298: total_loss: 0.570939, loss_sup: 0.204584, loss_mps: 0.128646, loss_cps: 0.237709
[13:34:12.962] iteration 17299: total_loss: 0.381139, loss_sup: 0.014513, loss_mps: 0.120101, loss_cps: 0.246525
[13:34:13.108] iteration 17300: total_loss: 0.494093, loss_sup: 0.087660, loss_mps: 0.142402, loss_cps: 0.264031
[13:34:13.109] Evaluation Started ==>
[13:34:24.441] ==> valid iteration 17300: unet metrics: {'dc': 0.6289863131427731, 'jc': 0.5130396384385991, 'pre': 0.7867893713517794, 'hd': 5.443775535444237}, ynet metrics: {'dc': 0.565738519259665, 'jc': 0.4523007629736803, 'pre': 0.8008028124772855, 'hd': 5.488273470267576}.
[13:34:24.443] Evaluation Finished!⏹️
[13:34:24.595] iteration 17301: total_loss: 0.527919, loss_sup: 0.100091, loss_mps: 0.150440, loss_cps: 0.277389
[13:34:24.743] iteration 17302: total_loss: 0.480954, loss_sup: 0.125848, loss_mps: 0.117280, loss_cps: 0.237826
[13:34:24.888] iteration 17303: total_loss: 0.451244, loss_sup: 0.025274, loss_mps: 0.147300, loss_cps: 0.278671
[13:34:25.033] iteration 17304: total_loss: 0.309875, loss_sup: 0.007028, loss_mps: 0.108746, loss_cps: 0.194101
[13:34:25.180] iteration 17305: total_loss: 0.260098, loss_sup: 0.073793, loss_mps: 0.068914, loss_cps: 0.117391
[13:34:25.329] iteration 17306: total_loss: 0.516572, loss_sup: 0.105908, loss_mps: 0.144801, loss_cps: 0.265863
[13:34:25.475] iteration 17307: total_loss: 0.294366, loss_sup: 0.014192, loss_mps: 0.107438, loss_cps: 0.172736
[13:34:25.623] iteration 17308: total_loss: 0.500310, loss_sup: 0.064525, loss_mps: 0.148957, loss_cps: 0.286828
[13:34:25.771] iteration 17309: total_loss: 0.345403, loss_sup: 0.027470, loss_mps: 0.112841, loss_cps: 0.205091
[13:34:25.917] iteration 17310: total_loss: 0.512680, loss_sup: 0.131236, loss_mps: 0.134324, loss_cps: 0.247120
[13:34:26.063] iteration 17311: total_loss: 0.497125, loss_sup: 0.040515, loss_mps: 0.148562, loss_cps: 0.308048
[13:34:26.208] iteration 17312: total_loss: 0.572242, loss_sup: 0.075947, loss_mps: 0.159487, loss_cps: 0.336809
[13:34:26.356] iteration 17313: total_loss: 0.484205, loss_sup: 0.079847, loss_mps: 0.135768, loss_cps: 0.268590
[13:34:26.503] iteration 17314: total_loss: 0.659732, loss_sup: 0.056046, loss_mps: 0.197524, loss_cps: 0.406163
[13:34:26.650] iteration 17315: total_loss: 1.053254, loss_sup: 0.073028, loss_mps: 0.303901, loss_cps: 0.676325
[13:34:26.798] iteration 17316: total_loss: 0.434330, loss_sup: 0.043159, loss_mps: 0.143086, loss_cps: 0.248084
[13:34:26.945] iteration 17317: total_loss: 0.731017, loss_sup: 0.310202, loss_mps: 0.148283, loss_cps: 0.272533
[13:34:27.090] iteration 17318: total_loss: 0.259487, loss_sup: 0.049860, loss_mps: 0.079383, loss_cps: 0.130244
[13:34:27.236] iteration 17319: total_loss: 0.402519, loss_sup: 0.056148, loss_mps: 0.120048, loss_cps: 0.226323
[13:34:27.381] iteration 17320: total_loss: 0.295832, loss_sup: 0.035775, loss_mps: 0.089916, loss_cps: 0.170142
[13:34:27.526] iteration 17321: total_loss: 0.625488, loss_sup: 0.076204, loss_mps: 0.182808, loss_cps: 0.366475
[13:34:27.672] iteration 17322: total_loss: 0.902204, loss_sup: 0.170261, loss_mps: 0.233658, loss_cps: 0.498285
[13:34:27.817] iteration 17323: total_loss: 0.340007, loss_sup: 0.074958, loss_mps: 0.093663, loss_cps: 0.171387
[13:34:27.963] iteration 17324: total_loss: 0.318039, loss_sup: 0.068336, loss_mps: 0.091548, loss_cps: 0.158155
[13:34:28.110] iteration 17325: total_loss: 0.444297, loss_sup: 0.046498, loss_mps: 0.137944, loss_cps: 0.259856
[13:34:28.256] iteration 17326: total_loss: 0.696738, loss_sup: 0.171321, loss_mps: 0.169917, loss_cps: 0.355500
[13:34:28.402] iteration 17327: total_loss: 0.526610, loss_sup: 0.062616, loss_mps: 0.164913, loss_cps: 0.299082
[13:34:28.549] iteration 17328: total_loss: 0.518752, loss_sup: 0.114834, loss_mps: 0.142041, loss_cps: 0.261877
[13:34:28.695] iteration 17329: total_loss: 0.464666, loss_sup: 0.163144, loss_mps: 0.118710, loss_cps: 0.182812
[13:34:28.840] iteration 17330: total_loss: 0.256528, loss_sup: 0.002852, loss_mps: 0.090955, loss_cps: 0.162721
[13:34:28.987] iteration 17331: total_loss: 0.714381, loss_sup: 0.144508, loss_mps: 0.184737, loss_cps: 0.385135
[13:34:29.135] iteration 17332: total_loss: 0.559075, loss_sup: 0.135888, loss_mps: 0.145899, loss_cps: 0.277289
[13:34:29.281] iteration 17333: total_loss: 0.269076, loss_sup: 0.021265, loss_mps: 0.093303, loss_cps: 0.154508
[13:34:29.428] iteration 17334: total_loss: 0.492050, loss_sup: 0.116006, loss_mps: 0.128014, loss_cps: 0.248029
[13:34:29.581] iteration 17335: total_loss: 0.440303, loss_sup: 0.039550, loss_mps: 0.139931, loss_cps: 0.260822
[13:34:29.728] iteration 17336: total_loss: 0.378176, loss_sup: 0.056746, loss_mps: 0.107724, loss_cps: 0.213707
[13:34:29.877] iteration 17337: total_loss: 0.471301, loss_sup: 0.096946, loss_mps: 0.127519, loss_cps: 0.246836
[13:34:30.024] iteration 17338: total_loss: 0.625157, loss_sup: 0.212247, loss_mps: 0.143063, loss_cps: 0.269847
[13:34:30.170] iteration 17339: total_loss: 0.343714, loss_sup: 0.010214, loss_mps: 0.118042, loss_cps: 0.215457
[13:34:30.318] iteration 17340: total_loss: 0.221408, loss_sup: 0.003413, loss_mps: 0.079108, loss_cps: 0.138887
[13:34:30.466] iteration 17341: total_loss: 0.589560, loss_sup: 0.116454, loss_mps: 0.155180, loss_cps: 0.317925
[13:34:30.615] iteration 17342: total_loss: 0.305204, loss_sup: 0.024179, loss_mps: 0.103001, loss_cps: 0.178024
[13:34:30.760] iteration 17343: total_loss: 0.279192, loss_sup: 0.030891, loss_mps: 0.094968, loss_cps: 0.153333
[13:34:30.906] iteration 17344: total_loss: 0.476248, loss_sup: 0.101981, loss_mps: 0.126313, loss_cps: 0.247955
[13:34:31.052] iteration 17345: total_loss: 0.413162, loss_sup: 0.053586, loss_mps: 0.126906, loss_cps: 0.232670
[13:34:31.198] iteration 17346: total_loss: 0.680867, loss_sup: 0.092020, loss_mps: 0.186553, loss_cps: 0.402294
[13:34:31.347] iteration 17347: total_loss: 0.406712, loss_sup: 0.029676, loss_mps: 0.123635, loss_cps: 0.253401
[13:34:31.494] iteration 17348: total_loss: 0.621323, loss_sup: 0.003473, loss_mps: 0.194466, loss_cps: 0.423384
[13:34:31.640] iteration 17349: total_loss: 0.586279, loss_sup: 0.114929, loss_mps: 0.148649, loss_cps: 0.322701
[13:34:31.786] iteration 17350: total_loss: 0.559939, loss_sup: 0.071767, loss_mps: 0.162079, loss_cps: 0.326093
[13:34:31.934] iteration 17351: total_loss: 0.608756, loss_sup: 0.130002, loss_mps: 0.156767, loss_cps: 0.321987
[13:34:32.080] iteration 17352: total_loss: 0.433950, loss_sup: 0.016378, loss_mps: 0.133077, loss_cps: 0.284494
[13:34:32.225] iteration 17353: total_loss: 0.582782, loss_sup: 0.036258, loss_mps: 0.175524, loss_cps: 0.371000
[13:34:32.371] iteration 17354: total_loss: 0.480079, loss_sup: 0.152021, loss_mps: 0.114064, loss_cps: 0.213994
[13:34:32.518] iteration 17355: total_loss: 0.477504, loss_sup: 0.027702, loss_mps: 0.145992, loss_cps: 0.303810
[13:34:32.665] iteration 17356: total_loss: 0.634754, loss_sup: 0.113139, loss_mps: 0.170824, loss_cps: 0.350791
[13:34:32.812] iteration 17357: total_loss: 0.305769, loss_sup: 0.029198, loss_mps: 0.100364, loss_cps: 0.176206
[13:34:32.959] iteration 17358: total_loss: 0.527907, loss_sup: 0.078605, loss_mps: 0.154104, loss_cps: 0.295199
[13:34:33.108] iteration 17359: total_loss: 0.399532, loss_sup: 0.014189, loss_mps: 0.128983, loss_cps: 0.256360
[13:34:33.254] iteration 17360: total_loss: 0.409939, loss_sup: 0.193293, loss_mps: 0.080203, loss_cps: 0.136443
[13:34:33.400] iteration 17361: total_loss: 0.431483, loss_sup: 0.063038, loss_mps: 0.130441, loss_cps: 0.238004
[13:34:33.546] iteration 17362: total_loss: 0.383061, loss_sup: 0.140760, loss_mps: 0.094963, loss_cps: 0.147337
[13:34:33.692] iteration 17363: total_loss: 0.619518, loss_sup: 0.203233, loss_mps: 0.141984, loss_cps: 0.274301
[13:34:33.838] iteration 17364: total_loss: 0.406720, loss_sup: 0.091003, loss_mps: 0.112200, loss_cps: 0.203517
[13:34:33.984] iteration 17365: total_loss: 0.622785, loss_sup: 0.061373, loss_mps: 0.180883, loss_cps: 0.380529
[13:34:34.130] iteration 17366: total_loss: 0.481688, loss_sup: 0.217536, loss_mps: 0.095044, loss_cps: 0.169107
[13:34:34.276] iteration 17367: total_loss: 0.547080, loss_sup: 0.043643, loss_mps: 0.170213, loss_cps: 0.333224
[13:34:34.423] iteration 17368: total_loss: 0.213388, loss_sup: 0.003907, loss_mps: 0.079903, loss_cps: 0.129578
[13:34:34.569] iteration 17369: total_loss: 0.369585, loss_sup: 0.026276, loss_mps: 0.119919, loss_cps: 0.223390
[13:34:34.715] iteration 17370: total_loss: 0.322222, loss_sup: 0.011762, loss_mps: 0.113710, loss_cps: 0.196749
[13:34:34.863] iteration 17371: total_loss: 0.248763, loss_sup: 0.015877, loss_mps: 0.085936, loss_cps: 0.146950
[13:34:35.009] iteration 17372: total_loss: 0.453139, loss_sup: 0.126658, loss_mps: 0.109370, loss_cps: 0.217111
[13:34:35.155] iteration 17373: total_loss: 0.565457, loss_sup: 0.256454, loss_mps: 0.108541, loss_cps: 0.200462
[13:34:35.301] iteration 17374: total_loss: 0.348926, loss_sup: 0.029837, loss_mps: 0.108933, loss_cps: 0.210157
[13:34:35.451] iteration 17375: total_loss: 0.478391, loss_sup: 0.013653, loss_mps: 0.156139, loss_cps: 0.308598
[13:34:35.597] iteration 17376: total_loss: 0.878569, loss_sup: 0.121216, loss_mps: 0.244579, loss_cps: 0.512774
[13:34:35.742] iteration 17377: total_loss: 0.894429, loss_sup: 0.241866, loss_mps: 0.216250, loss_cps: 0.436313
[13:34:35.888] iteration 17378: total_loss: 0.559291, loss_sup: 0.071001, loss_mps: 0.158061, loss_cps: 0.330229
[13:34:36.034] iteration 17379: total_loss: 0.562286, loss_sup: 0.040668, loss_mps: 0.169683, loss_cps: 0.351935
[13:34:36.180] iteration 17380: total_loss: 0.445581, loss_sup: 0.078667, loss_mps: 0.121224, loss_cps: 0.245690
[13:34:36.326] iteration 17381: total_loss: 0.479785, loss_sup: 0.088341, loss_mps: 0.139631, loss_cps: 0.251813
[13:34:36.473] iteration 17382: total_loss: 0.995589, loss_sup: 0.417553, loss_mps: 0.201584, loss_cps: 0.376452
[13:34:36.619] iteration 17383: total_loss: 0.451897, loss_sup: 0.075328, loss_mps: 0.127841, loss_cps: 0.248728
[13:34:36.765] iteration 17384: total_loss: 0.788196, loss_sup: 0.192560, loss_mps: 0.194683, loss_cps: 0.400953
[13:34:36.910] iteration 17385: total_loss: 0.286841, loss_sup: 0.004162, loss_mps: 0.103356, loss_cps: 0.179322
[13:34:37.057] iteration 17386: total_loss: 0.353485, loss_sup: 0.032587, loss_mps: 0.114152, loss_cps: 0.206746
[13:34:37.204] iteration 17387: total_loss: 0.454318, loss_sup: 0.052592, loss_mps: 0.138292, loss_cps: 0.263434
[13:34:37.350] iteration 17388: total_loss: 0.435142, loss_sup: 0.091890, loss_mps: 0.118726, loss_cps: 0.224526
[13:34:37.497] iteration 17389: total_loss: 0.564995, loss_sup: 0.186235, loss_mps: 0.137237, loss_cps: 0.241523
[13:34:37.644] iteration 17390: total_loss: 0.408937, loss_sup: 0.020628, loss_mps: 0.133143, loss_cps: 0.255165
[13:34:37.790] iteration 17391: total_loss: 0.352090, loss_sup: 0.013582, loss_mps: 0.113557, loss_cps: 0.224951
[13:34:37.936] iteration 17392: total_loss: 0.438142, loss_sup: 0.029572, loss_mps: 0.136633, loss_cps: 0.271937
[13:34:38.082] iteration 17393: total_loss: 0.355790, loss_sup: 0.017693, loss_mps: 0.120283, loss_cps: 0.217814
[13:34:38.228] iteration 17394: total_loss: 0.435065, loss_sup: 0.047344, loss_mps: 0.134137, loss_cps: 0.253585
[13:34:38.374] iteration 17395: total_loss: 0.391267, loss_sup: 0.071318, loss_mps: 0.114415, loss_cps: 0.205534
[13:34:38.521] iteration 17396: total_loss: 0.404897, loss_sup: 0.060724, loss_mps: 0.120247, loss_cps: 0.223925
[13:34:38.666] iteration 17397: total_loss: 0.290144, loss_sup: 0.035546, loss_mps: 0.089371, loss_cps: 0.165227
[13:34:38.812] iteration 17398: total_loss: 0.491947, loss_sup: 0.173971, loss_mps: 0.107529, loss_cps: 0.210447
[13:34:38.959] iteration 17399: total_loss: 0.391373, loss_sup: 0.017012, loss_mps: 0.127856, loss_cps: 0.246505
[13:34:39.104] iteration 17400: total_loss: 0.426109, loss_sup: 0.117536, loss_mps: 0.113468, loss_cps: 0.195105
[13:34:39.105] Evaluation Started ==>
[13:34:50.467] ==> valid iteration 17400: unet metrics: {'dc': 0.6438298510512879, 'jc': 0.5294043411176822, 'pre': 0.7690769805438147, 'hd': 5.468959836496395}, ynet metrics: {'dc': 0.5677425274933305, 'jc': 0.45558814433104927, 'pre': 0.7885683290762652, 'hd': 5.432716723000703}.
[13:34:50.469] Evaluation Finished!⏹️
[13:34:50.622] iteration 17401: total_loss: 1.059006, loss_sup: 0.278659, loss_mps: 0.275696, loss_cps: 0.504652
[13:34:50.774] iteration 17402: total_loss: 0.289813, loss_sup: 0.065718, loss_mps: 0.088208, loss_cps: 0.135886
[13:34:50.920] iteration 17403: total_loss: 0.372122, loss_sup: 0.091391, loss_mps: 0.109845, loss_cps: 0.170887
[13:34:51.066] iteration 17404: total_loss: 0.296217, loss_sup: 0.017624, loss_mps: 0.096453, loss_cps: 0.182141
[13:34:51.212] iteration 17405: total_loss: 0.562109, loss_sup: 0.131609, loss_mps: 0.146145, loss_cps: 0.284355
[13:34:51.358] iteration 17406: total_loss: 0.649240, loss_sup: 0.078078, loss_mps: 0.192132, loss_cps: 0.379029
[13:34:51.503] iteration 17407: total_loss: 0.499599, loss_sup: 0.095680, loss_mps: 0.135505, loss_cps: 0.268415
[13:34:51.649] iteration 17408: total_loss: 0.521771, loss_sup: 0.126501, loss_mps: 0.143202, loss_cps: 0.252068
[13:34:51.795] iteration 17409: total_loss: 0.409960, loss_sup: 0.096035, loss_mps: 0.115140, loss_cps: 0.198784
[13:34:51.941] iteration 17410: total_loss: 0.365087, loss_sup: 0.067551, loss_mps: 0.105306, loss_cps: 0.192230
[13:34:52.087] iteration 17411: total_loss: 0.724233, loss_sup: 0.074656, loss_mps: 0.206808, loss_cps: 0.442769
[13:34:52.234] iteration 17412: total_loss: 0.511559, loss_sup: 0.132166, loss_mps: 0.134047, loss_cps: 0.245346
[13:34:52.380] iteration 17413: total_loss: 0.450061, loss_sup: 0.108040, loss_mps: 0.116310, loss_cps: 0.225710
[13:34:52.528] iteration 17414: total_loss: 0.384600, loss_sup: 0.041684, loss_mps: 0.118092, loss_cps: 0.224824
[13:34:52.676] iteration 17415: total_loss: 0.453714, loss_sup: 0.019598, loss_mps: 0.139882, loss_cps: 0.294234
[13:34:52.821] iteration 17416: total_loss: 0.419337, loss_sup: 0.039940, loss_mps: 0.128795, loss_cps: 0.250602
[13:34:52.967] iteration 17417: total_loss: 0.344956, loss_sup: 0.055866, loss_mps: 0.104795, loss_cps: 0.184296
[13:34:53.115] iteration 17418: total_loss: 0.619021, loss_sup: 0.251919, loss_mps: 0.128684, loss_cps: 0.238418
[13:34:53.261] iteration 17419: total_loss: 0.299789, loss_sup: 0.043113, loss_mps: 0.094024, loss_cps: 0.162652
[13:34:53.408] iteration 17420: total_loss: 0.462272, loss_sup: 0.044962, loss_mps: 0.140732, loss_cps: 0.276578
[13:34:53.555] iteration 17421: total_loss: 0.394672, loss_sup: 0.058432, loss_mps: 0.122881, loss_cps: 0.213359
[13:34:53.702] iteration 17422: total_loss: 0.629216, loss_sup: 0.077256, loss_mps: 0.171870, loss_cps: 0.380091
[13:34:53.848] iteration 17423: total_loss: 0.411102, loss_sup: 0.038092, loss_mps: 0.126704, loss_cps: 0.246307
[13:34:53.995] iteration 17424: total_loss: 0.590049, loss_sup: 0.102834, loss_mps: 0.166896, loss_cps: 0.320319
[13:34:54.140] iteration 17425: total_loss: 0.390154, loss_sup: 0.058087, loss_mps: 0.121908, loss_cps: 0.210158
[13:34:54.287] iteration 17426: total_loss: 0.442902, loss_sup: 0.042694, loss_mps: 0.135845, loss_cps: 0.264362
[13:34:54.433] iteration 17427: total_loss: 0.312845, loss_sup: 0.029218, loss_mps: 0.099155, loss_cps: 0.184472
[13:34:54.580] iteration 17428: total_loss: 0.705632, loss_sup: 0.186133, loss_mps: 0.165410, loss_cps: 0.354089
[13:34:54.726] iteration 17429: total_loss: 0.708394, loss_sup: 0.093465, loss_mps: 0.189846, loss_cps: 0.425083
[13:34:54.874] iteration 17430: total_loss: 0.373788, loss_sup: 0.046630, loss_mps: 0.109930, loss_cps: 0.217229
[13:34:55.020] iteration 17431: total_loss: 0.942487, loss_sup: 0.341082, loss_mps: 0.190935, loss_cps: 0.410470
[13:34:55.168] iteration 17432: total_loss: 1.030722, loss_sup: 0.044613, loss_mps: 0.291482, loss_cps: 0.694628
[13:34:55.314] iteration 17433: total_loss: 0.484613, loss_sup: 0.068671, loss_mps: 0.139368, loss_cps: 0.276574
[13:34:55.460] iteration 17434: total_loss: 0.558105, loss_sup: 0.177810, loss_mps: 0.129266, loss_cps: 0.251030
[13:34:55.607] iteration 17435: total_loss: 0.381873, loss_sup: 0.078388, loss_mps: 0.118413, loss_cps: 0.185072
[13:34:55.753] iteration 17436: total_loss: 0.370580, loss_sup: 0.020834, loss_mps: 0.122585, loss_cps: 0.227162
[13:34:55.898] iteration 17437: total_loss: 0.339338, loss_sup: 0.099059, loss_mps: 0.084979, loss_cps: 0.155300
[13:34:56.045] iteration 17438: total_loss: 0.325440, loss_sup: 0.010436, loss_mps: 0.111967, loss_cps: 0.203037
[13:34:56.191] iteration 17439: total_loss: 0.354218, loss_sup: 0.100499, loss_mps: 0.089191, loss_cps: 0.164528
[13:34:56.337] iteration 17440: total_loss: 0.440175, loss_sup: 0.048460, loss_mps: 0.130056, loss_cps: 0.261659
[13:34:56.483] iteration 17441: total_loss: 0.513849, loss_sup: 0.017269, loss_mps: 0.159539, loss_cps: 0.337041
[13:34:56.629] iteration 17442: total_loss: 0.622390, loss_sup: 0.140651, loss_mps: 0.167341, loss_cps: 0.314398
[13:34:56.776] iteration 17443: total_loss: 0.370521, loss_sup: 0.035890, loss_mps: 0.117658, loss_cps: 0.216973
[13:34:56.922] iteration 17444: total_loss: 0.552273, loss_sup: 0.204021, loss_mps: 0.123605, loss_cps: 0.224647
[13:34:57.070] iteration 17445: total_loss: 0.653537, loss_sup: 0.272751, loss_mps: 0.121684, loss_cps: 0.259102
[13:34:57.217] iteration 17446: total_loss: 0.410051, loss_sup: 0.012413, loss_mps: 0.133929, loss_cps: 0.263708
[13:34:57.364] iteration 17447: total_loss: 0.486932, loss_sup: 0.065056, loss_mps: 0.138782, loss_cps: 0.283094
[13:34:57.511] iteration 17448: total_loss: 0.507659, loss_sup: 0.062632, loss_mps: 0.154036, loss_cps: 0.290991
[13:34:57.657] iteration 17449: total_loss: 0.537650, loss_sup: 0.122953, loss_mps: 0.138942, loss_cps: 0.275755
[13:34:57.804] iteration 17450: total_loss: 0.954940, loss_sup: 0.159134, loss_mps: 0.258388, loss_cps: 0.537419
[13:34:57.951] iteration 17451: total_loss: 0.451729, loss_sup: 0.054936, loss_mps: 0.141813, loss_cps: 0.254981
[13:34:58.100] iteration 17452: total_loss: 0.643194, loss_sup: 0.166417, loss_mps: 0.161412, loss_cps: 0.315364
[13:34:58.247] iteration 17453: total_loss: 0.347755, loss_sup: 0.004939, loss_mps: 0.118547, loss_cps: 0.224270
[13:34:58.393] iteration 17454: total_loss: 0.480478, loss_sup: 0.048106, loss_mps: 0.146782, loss_cps: 0.285591
[13:34:58.541] iteration 17455: total_loss: 0.552579, loss_sup: 0.113872, loss_mps: 0.149098, loss_cps: 0.289609
[13:34:58.689] iteration 17456: total_loss: 0.466491, loss_sup: 0.020126, loss_mps: 0.146289, loss_cps: 0.300076
[13:34:58.836] iteration 17457: total_loss: 0.459209, loss_sup: 0.024598, loss_mps: 0.144044, loss_cps: 0.290567
[13:34:58.983] iteration 17458: total_loss: 0.705359, loss_sup: 0.157699, loss_mps: 0.175103, loss_cps: 0.372557
[13:34:59.130] iteration 17459: total_loss: 0.556656, loss_sup: 0.115703, loss_mps: 0.145155, loss_cps: 0.295799
[13:34:59.276] iteration 17460: total_loss: 0.497494, loss_sup: 0.099436, loss_mps: 0.134181, loss_cps: 0.263877
[13:34:59.425] iteration 17461: total_loss: 0.611177, loss_sup: 0.098687, loss_mps: 0.162199, loss_cps: 0.350291
[13:34:59.573] iteration 17462: total_loss: 0.692515, loss_sup: 0.076003, loss_mps: 0.202787, loss_cps: 0.413725
[13:34:59.720] iteration 17463: total_loss: 0.345909, loss_sup: 0.031344, loss_mps: 0.106341, loss_cps: 0.208225
[13:34:59.867] iteration 17464: total_loss: 0.556373, loss_sup: 0.182405, loss_mps: 0.140693, loss_cps: 0.233275
[13:35:00.014] iteration 17465: total_loss: 0.705892, loss_sup: 0.186872, loss_mps: 0.169997, loss_cps: 0.349023
[13:35:00.161] iteration 17466: total_loss: 0.644461, loss_sup: 0.081533, loss_mps: 0.183851, loss_cps: 0.379077
[13:35:00.308] iteration 17467: total_loss: 0.575472, loss_sup: 0.050766, loss_mps: 0.175309, loss_cps: 0.349397
[13:35:00.456] iteration 17468: total_loss: 0.337948, loss_sup: 0.084985, loss_mps: 0.087631, loss_cps: 0.165332
[13:35:00.602] iteration 17469: total_loss: 0.484132, loss_sup: 0.146658, loss_mps: 0.111372, loss_cps: 0.226102
[13:35:00.749] iteration 17470: total_loss: 0.571095, loss_sup: 0.121688, loss_mps: 0.145127, loss_cps: 0.304280
[13:35:00.895] iteration 17471: total_loss: 0.553829, loss_sup: 0.068140, loss_mps: 0.163834, loss_cps: 0.321855
[13:35:01.045] iteration 17472: total_loss: 0.850549, loss_sup: 0.279667, loss_mps: 0.180465, loss_cps: 0.390417
[13:35:01.192] iteration 17473: total_loss: 0.513213, loss_sup: 0.158378, loss_mps: 0.119496, loss_cps: 0.235339
[13:35:01.339] iteration 17474: total_loss: 0.929479, loss_sup: 0.159897, loss_mps: 0.232411, loss_cps: 0.537172
[13:35:01.486] iteration 17475: total_loss: 0.603932, loss_sup: 0.066773, loss_mps: 0.177712, loss_cps: 0.359446
[13:35:01.635] iteration 17476: total_loss: 0.560557, loss_sup: 0.124229, loss_mps: 0.144176, loss_cps: 0.292151
[13:35:01.782] iteration 17477: total_loss: 0.834558, loss_sup: 0.052730, loss_mps: 0.252190, loss_cps: 0.529639
[13:35:01.928] iteration 17478: total_loss: 0.502015, loss_sup: 0.023266, loss_mps: 0.157839, loss_cps: 0.320910
[13:35:02.075] iteration 17479: total_loss: 0.567604, loss_sup: 0.171872, loss_mps: 0.139914, loss_cps: 0.255819
[13:35:02.222] iteration 17480: total_loss: 0.547020, loss_sup: 0.034373, loss_mps: 0.167446, loss_cps: 0.345201
[13:35:02.374] iteration 17481: total_loss: 0.347289, loss_sup: 0.019638, loss_mps: 0.116220, loss_cps: 0.211431
[13:35:02.521] iteration 17482: total_loss: 0.527256, loss_sup: 0.063878, loss_mps: 0.160456, loss_cps: 0.302922
[13:35:02.668] iteration 17483: total_loss: 0.622015, loss_sup: 0.036770, loss_mps: 0.194926, loss_cps: 0.390319
[13:35:02.815] iteration 17484: total_loss: 0.553969, loss_sup: 0.113615, loss_mps: 0.138075, loss_cps: 0.302280
[13:35:02.962] iteration 17485: total_loss: 0.637504, loss_sup: 0.167884, loss_mps: 0.160955, loss_cps: 0.308665
[13:35:03.108] iteration 17486: total_loss: 0.821238, loss_sup: 0.085397, loss_mps: 0.232749, loss_cps: 0.503092
[13:35:03.254] iteration 17487: total_loss: 0.415521, loss_sup: 0.084233, loss_mps: 0.119427, loss_cps: 0.211860
[13:35:03.403] iteration 17488: total_loss: 0.456182, loss_sup: 0.127301, loss_mps: 0.121031, loss_cps: 0.207849
[13:35:03.550] iteration 17489: total_loss: 0.516156, loss_sup: 0.075934, loss_mps: 0.143348, loss_cps: 0.296874
[13:35:03.696] iteration 17490: total_loss: 0.679779, loss_sup: 0.047942, loss_mps: 0.206259, loss_cps: 0.425578
[13:35:03.844] iteration 17491: total_loss: 0.635022, loss_sup: 0.004727, loss_mps: 0.208072, loss_cps: 0.422223
[13:35:03.991] iteration 17492: total_loss: 0.299187, loss_sup: 0.006950, loss_mps: 0.107790, loss_cps: 0.184447
[13:35:04.142] iteration 17493: total_loss: 0.475379, loss_sup: 0.115478, loss_mps: 0.125322, loss_cps: 0.234579
[13:35:04.289] iteration 17494: total_loss: 0.326510, loss_sup: 0.026347, loss_mps: 0.102998, loss_cps: 0.197166
[13:35:04.435] iteration 17495: total_loss: 0.312656, loss_sup: 0.004636, loss_mps: 0.114365, loss_cps: 0.193655
[13:35:04.582] iteration 17496: total_loss: 0.543218, loss_sup: 0.061052, loss_mps: 0.168604, loss_cps: 0.313561
[13:35:04.729] iteration 17497: total_loss: 0.612343, loss_sup: 0.201274, loss_mps: 0.145493, loss_cps: 0.265576
[13:35:04.875] iteration 17498: total_loss: 0.353737, loss_sup: 0.146553, loss_mps: 0.075915, loss_cps: 0.131268
[13:35:05.023] iteration 17499: total_loss: 0.330938, loss_sup: 0.037502, loss_mps: 0.105725, loss_cps: 0.187710
[13:35:05.172] iteration 17500: total_loss: 0.607205, loss_sup: 0.106462, loss_mps: 0.173592, loss_cps: 0.327152
[13:35:05.172] Evaluation Started ==>
[13:35:16.506] ==> valid iteration 17500: unet metrics: {'dc': 0.6553357687185773, 'jc': 0.5402599351105112, 'pre': 0.8005595016713186, 'hd': 5.39523629328617}, ynet metrics: {'dc': 0.5774663581608321, 'jc': 0.4625523834539972, 'pre': 0.7797467117916935, 'hd': 5.559317520082369}.
[13:35:16.508] Evaluation Finished!⏹️
[13:35:16.661] iteration 17501: total_loss: 0.431127, loss_sup: 0.043726, loss_mps: 0.134559, loss_cps: 0.252843
[13:35:16.808] iteration 17502: total_loss: 0.402923, loss_sup: 0.066728, loss_mps: 0.123655, loss_cps: 0.212540
[13:35:16.955] iteration 17503: total_loss: 0.517206, loss_sup: 0.166289, loss_mps: 0.124509, loss_cps: 0.226408
[13:35:17.101] iteration 17504: total_loss: 0.368839, loss_sup: 0.013644, loss_mps: 0.121280, loss_cps: 0.233915
[13:35:17.247] iteration 17505: total_loss: 0.453090, loss_sup: 0.064861, loss_mps: 0.128943, loss_cps: 0.259286
[13:35:17.392] iteration 17506: total_loss: 0.539370, loss_sup: 0.035537, loss_mps: 0.172475, loss_cps: 0.331358
[13:35:17.538] iteration 17507: total_loss: 0.414101, loss_sup: 0.027068, loss_mps: 0.137384, loss_cps: 0.249649
[13:35:17.684] iteration 17508: total_loss: 0.290867, loss_sup: 0.016739, loss_mps: 0.100240, loss_cps: 0.173888
[13:35:17.834] iteration 17509: total_loss: 1.023660, loss_sup: 0.145379, loss_mps: 0.261906, loss_cps: 0.616375
[13:35:17.980] iteration 17510: total_loss: 0.510751, loss_sup: 0.081601, loss_mps: 0.138705, loss_cps: 0.290445
[13:35:18.126] iteration 17511: total_loss: 0.571900, loss_sup: 0.238305, loss_mps: 0.115239, loss_cps: 0.218356
[13:35:18.271] iteration 17512: total_loss: 0.456487, loss_sup: 0.079073, loss_mps: 0.133426, loss_cps: 0.243988
[13:35:18.416] iteration 17513: total_loss: 0.410566, loss_sup: 0.064124, loss_mps: 0.117245, loss_cps: 0.229196
[13:35:18.562] iteration 17514: total_loss: 0.329766, loss_sup: 0.063522, loss_mps: 0.096671, loss_cps: 0.169573
[13:35:18.709] iteration 17515: total_loss: 0.603893, loss_sup: 0.038901, loss_mps: 0.174816, loss_cps: 0.390177
[13:35:18.854] iteration 17516: total_loss: 0.745632, loss_sup: 0.089507, loss_mps: 0.194380, loss_cps: 0.461746
[13:35:19.002] iteration 17517: total_loss: 0.292314, loss_sup: 0.070121, loss_mps: 0.084204, loss_cps: 0.137989
[13:35:19.148] iteration 17518: total_loss: 0.294746, loss_sup: 0.029312, loss_mps: 0.093600, loss_cps: 0.171833
[13:35:19.293] iteration 17519: total_loss: 0.326649, loss_sup: 0.037173, loss_mps: 0.102673, loss_cps: 0.186802
[13:35:19.440] iteration 17520: total_loss: 0.391956, loss_sup: 0.081549, loss_mps: 0.111739, loss_cps: 0.198668
[13:35:19.587] iteration 17521: total_loss: 0.421804, loss_sup: 0.099724, loss_mps: 0.119393, loss_cps: 0.202687
[13:35:19.733] iteration 17522: total_loss: 0.708489, loss_sup: 0.014263, loss_mps: 0.223548, loss_cps: 0.470678
[13:35:19.879] iteration 17523: total_loss: 0.671852, loss_sup: 0.081459, loss_mps: 0.195835, loss_cps: 0.394559
[13:35:20.026] iteration 17524: total_loss: 0.381487, loss_sup: 0.091253, loss_mps: 0.103141, loss_cps: 0.187093
[13:35:20.172] iteration 17525: total_loss: 0.441519, loss_sup: 0.103556, loss_mps: 0.116407, loss_cps: 0.221556
[13:35:20.318] iteration 17526: total_loss: 0.588088, loss_sup: 0.198317, loss_mps: 0.137768, loss_cps: 0.252003
[13:35:20.464] iteration 17527: total_loss: 0.901186, loss_sup: 0.231127, loss_mps: 0.221867, loss_cps: 0.448192
[13:35:20.609] iteration 17528: total_loss: 0.484268, loss_sup: 0.129174, loss_mps: 0.127224, loss_cps: 0.227870
[13:35:20.755] iteration 17529: total_loss: 0.661418, loss_sup: 0.071359, loss_mps: 0.194399, loss_cps: 0.395660
[13:35:20.901] iteration 17530: total_loss: 0.301780, loss_sup: 0.015470, loss_mps: 0.100012, loss_cps: 0.186297
[13:35:21.047] iteration 17531: total_loss: 0.352601, loss_sup: 0.090876, loss_mps: 0.093200, loss_cps: 0.168524
[13:35:21.192] iteration 17532: total_loss: 0.239367, loss_sup: 0.007075, loss_mps: 0.084778, loss_cps: 0.147513
[13:35:21.339] iteration 17533: total_loss: 0.635590, loss_sup: 0.055660, loss_mps: 0.188797, loss_cps: 0.391133
[13:35:21.485] iteration 17534: total_loss: 1.275234, loss_sup: 0.179580, loss_mps: 0.325631, loss_cps: 0.770024
[13:35:21.631] iteration 17535: total_loss: 0.293782, loss_sup: 0.011082, loss_mps: 0.101714, loss_cps: 0.180986
[13:35:21.777] iteration 17536: total_loss: 0.480424, loss_sup: 0.152754, loss_mps: 0.110728, loss_cps: 0.216942
[13:35:21.923] iteration 17537: total_loss: 0.473560, loss_sup: 0.087323, loss_mps: 0.131825, loss_cps: 0.254412
[13:35:22.072] iteration 17538: total_loss: 0.694348, loss_sup: 0.083209, loss_mps: 0.192118, loss_cps: 0.419021
[13:35:22.220] iteration 17539: total_loss: 0.444289, loss_sup: 0.050786, loss_mps: 0.131161, loss_cps: 0.262342
[13:35:22.365] iteration 17540: total_loss: 0.382534, loss_sup: 0.015243, loss_mps: 0.123467, loss_cps: 0.243825
[13:35:22.511] iteration 17541: total_loss: 0.453756, loss_sup: 0.166674, loss_mps: 0.104728, loss_cps: 0.182354
[13:35:22.657] iteration 17542: total_loss: 0.553159, loss_sup: 0.139299, loss_mps: 0.136934, loss_cps: 0.276926
[13:35:22.803] iteration 17543: total_loss: 1.054137, loss_sup: 0.193280, loss_mps: 0.253471, loss_cps: 0.607386
[13:35:22.949] iteration 17544: total_loss: 0.526176, loss_sup: 0.084712, loss_mps: 0.144711, loss_cps: 0.296754
[13:35:23.094] iteration 17545: total_loss: 0.352903, loss_sup: 0.013254, loss_mps: 0.114614, loss_cps: 0.225034
[13:35:23.240] iteration 17546: total_loss: 0.533090, loss_sup: 0.029770, loss_mps: 0.176410, loss_cps: 0.326910
[13:35:23.386] iteration 17547: total_loss: 0.729033, loss_sup: 0.100926, loss_mps: 0.194091, loss_cps: 0.434016
[13:35:23.533] iteration 17548: total_loss: 0.386990, loss_sup: 0.081198, loss_mps: 0.107803, loss_cps: 0.197989
[13:35:23.679] iteration 17549: total_loss: 0.630473, loss_sup: 0.118853, loss_mps: 0.170445, loss_cps: 0.341175
[13:35:23.826] iteration 17550: total_loss: 0.711228, loss_sup: 0.335727, loss_mps: 0.121194, loss_cps: 0.254307
[13:35:23.971] iteration 17551: total_loss: 0.693724, loss_sup: 0.174647, loss_mps: 0.173295, loss_cps: 0.345782
[13:35:24.117] iteration 17552: total_loss: 0.692142, loss_sup: 0.076546, loss_mps: 0.212247, loss_cps: 0.403349
[13:35:24.263] iteration 17553: total_loss: 0.447976, loss_sup: 0.084956, loss_mps: 0.128860, loss_cps: 0.234160
[13:35:24.408] iteration 17554: total_loss: 0.803715, loss_sup: 0.152205, loss_mps: 0.208898, loss_cps: 0.442612
[13:35:24.554] iteration 17555: total_loss: 0.676627, loss_sup: 0.043802, loss_mps: 0.205359, loss_cps: 0.427466
[13:35:24.618] iteration 17556: total_loss: 0.676200, loss_sup: 0.111755, loss_mps: 0.198575, loss_cps: 0.365871
[13:35:25.828] iteration 17557: total_loss: 0.792477, loss_sup: 0.127717, loss_mps: 0.220901, loss_cps: 0.443859
[13:35:25.977] iteration 17558: total_loss: 0.423235, loss_sup: 0.031883, loss_mps: 0.128967, loss_cps: 0.262384
[13:35:26.123] iteration 17559: total_loss: 0.984874, loss_sup: 0.135029, loss_mps: 0.279060, loss_cps: 0.570785
[13:35:26.269] iteration 17560: total_loss: 1.106117, loss_sup: 0.188054, loss_mps: 0.285499, loss_cps: 0.632564
[13:35:26.415] iteration 17561: total_loss: 0.415316, loss_sup: 0.051442, loss_mps: 0.137352, loss_cps: 0.226523
[13:35:26.561] iteration 17562: total_loss: 0.409313, loss_sup: 0.015210, loss_mps: 0.145456, loss_cps: 0.248647
[13:35:26.706] iteration 17563: total_loss: 0.815808, loss_sup: 0.247782, loss_mps: 0.197564, loss_cps: 0.370463
[13:35:26.853] iteration 17564: total_loss: 0.724770, loss_sup: 0.064797, loss_mps: 0.212727, loss_cps: 0.447247
[13:35:27.000] iteration 17565: total_loss: 0.878495, loss_sup: 0.360129, loss_mps: 0.177429, loss_cps: 0.340936
[13:35:27.146] iteration 17566: total_loss: 0.728161, loss_sup: 0.025831, loss_mps: 0.224184, loss_cps: 0.478146
[13:35:27.291] iteration 17567: total_loss: 0.920310, loss_sup: 0.124487, loss_mps: 0.270903, loss_cps: 0.524920
[13:35:27.437] iteration 17568: total_loss: 0.282023, loss_sup: 0.018356, loss_mps: 0.099860, loss_cps: 0.163807
[13:35:27.583] iteration 17569: total_loss: 0.558003, loss_sup: 0.084843, loss_mps: 0.167710, loss_cps: 0.305450
[13:35:27.729] iteration 17570: total_loss: 0.555807, loss_sup: 0.041536, loss_mps: 0.169407, loss_cps: 0.344864
[13:35:27.874] iteration 17571: total_loss: 0.425427, loss_sup: 0.035291, loss_mps: 0.130418, loss_cps: 0.259718
[13:35:28.021] iteration 17572: total_loss: 0.343319, loss_sup: 0.027386, loss_mps: 0.109951, loss_cps: 0.205983
[13:35:28.167] iteration 17573: total_loss: 0.488785, loss_sup: 0.064227, loss_mps: 0.151609, loss_cps: 0.272948
[13:35:28.313] iteration 17574: total_loss: 0.726144, loss_sup: 0.068837, loss_mps: 0.209627, loss_cps: 0.447680
[13:35:28.459] iteration 17575: total_loss: 0.543511, loss_sup: 0.012403, loss_mps: 0.178156, loss_cps: 0.352952
[13:35:28.605] iteration 17576: total_loss: 0.756972, loss_sup: 0.186281, loss_mps: 0.183021, loss_cps: 0.387669
[13:35:28.750] iteration 17577: total_loss: 0.467719, loss_sup: 0.082557, loss_mps: 0.138232, loss_cps: 0.246929
[13:35:28.896] iteration 17578: total_loss: 0.338578, loss_sup: 0.015763, loss_mps: 0.114720, loss_cps: 0.208095
[13:35:29.043] iteration 17579: total_loss: 0.687143, loss_sup: 0.116637, loss_mps: 0.192633, loss_cps: 0.377873
[13:35:29.191] iteration 17580: total_loss: 0.448308, loss_sup: 0.067812, loss_mps: 0.134422, loss_cps: 0.246074
[13:35:29.339] iteration 17581: total_loss: 0.519272, loss_sup: 0.094010, loss_mps: 0.143697, loss_cps: 0.281565
[13:35:29.486] iteration 17582: total_loss: 0.408907, loss_sup: 0.126673, loss_mps: 0.101568, loss_cps: 0.180666
[13:35:29.632] iteration 17583: total_loss: 0.563000, loss_sup: 0.066697, loss_mps: 0.167066, loss_cps: 0.329236
[13:35:29.780] iteration 17584: total_loss: 0.771782, loss_sup: 0.110672, loss_mps: 0.205010, loss_cps: 0.456099
[13:35:29.926] iteration 17585: total_loss: 0.420627, loss_sup: 0.069565, loss_mps: 0.121366, loss_cps: 0.229696
[13:35:30.072] iteration 17586: total_loss: 0.794131, loss_sup: 0.091846, loss_mps: 0.222779, loss_cps: 0.479507
[13:35:30.220] iteration 17587: total_loss: 0.314743, loss_sup: 0.028076, loss_mps: 0.105677, loss_cps: 0.180990
[13:35:30.366] iteration 17588: total_loss: 0.632411, loss_sup: 0.076997, loss_mps: 0.181415, loss_cps: 0.373999
[13:35:30.513] iteration 17589: total_loss: 0.506110, loss_sup: 0.086096, loss_mps: 0.138363, loss_cps: 0.281650
[13:35:30.658] iteration 17590: total_loss: 0.540841, loss_sup: 0.127964, loss_mps: 0.137012, loss_cps: 0.275865
[13:35:30.805] iteration 17591: total_loss: 0.330163, loss_sup: 0.017249, loss_mps: 0.109228, loss_cps: 0.203686
[13:35:30.952] iteration 17592: total_loss: 0.596067, loss_sup: 0.086583, loss_mps: 0.163905, loss_cps: 0.345579
[13:35:31.099] iteration 17593: total_loss: 0.433491, loss_sup: 0.033594, loss_mps: 0.136497, loss_cps: 0.263401
[13:35:31.246] iteration 17594: total_loss: 0.295367, loss_sup: 0.036885, loss_mps: 0.096605, loss_cps: 0.161877
[13:35:31.392] iteration 17595: total_loss: 0.308041, loss_sup: 0.034145, loss_mps: 0.097635, loss_cps: 0.176261
[13:35:31.539] iteration 17596: total_loss: 0.632162, loss_sup: 0.057864, loss_mps: 0.200411, loss_cps: 0.373887
[13:35:31.686] iteration 17597: total_loss: 0.646554, loss_sup: 0.251713, loss_mps: 0.134252, loss_cps: 0.260589
[13:35:31.836] iteration 17598: total_loss: 0.481211, loss_sup: 0.148094, loss_mps: 0.117620, loss_cps: 0.215497
[13:35:31.982] iteration 17599: total_loss: 0.467873, loss_sup: 0.018419, loss_mps: 0.148531, loss_cps: 0.300924
[13:35:32.128] iteration 17600: total_loss: 0.364528, loss_sup: 0.090676, loss_mps: 0.099163, loss_cps: 0.174688
[13:35:32.128] Evaluation Started ==>
[13:35:43.490] ==> valid iteration 17600: unet metrics: {'dc': 0.6521836329184321, 'jc': 0.5359666961619138, 'pre': 0.7918809277812552, 'hd': 5.484884053732779}, ynet metrics: {'dc': 0.573931124749358, 'jc': 0.45936773248842083, 'pre': 0.7767242405205317, 'hd': 5.686676665788728}.
[13:35:43.492] Evaluation Finished!⏹️
[13:35:43.642] iteration 17601: total_loss: 0.684038, loss_sup: 0.019173, loss_mps: 0.205349, loss_cps: 0.459516
[13:35:43.789] iteration 17602: total_loss: 0.394160, loss_sup: 0.066809, loss_mps: 0.116022, loss_cps: 0.211329
[13:35:43.935] iteration 17603: total_loss: 0.692572, loss_sup: 0.123214, loss_mps: 0.192056, loss_cps: 0.377301
[13:35:44.080] iteration 17604: total_loss: 0.656631, loss_sup: 0.192116, loss_mps: 0.153793, loss_cps: 0.310722
[13:35:44.225] iteration 17605: total_loss: 0.785766, loss_sup: 0.065222, loss_mps: 0.241499, loss_cps: 0.479045
[13:35:44.370] iteration 17606: total_loss: 0.393336, loss_sup: 0.114101, loss_mps: 0.101571, loss_cps: 0.177664
[13:35:44.515] iteration 17607: total_loss: 0.379117, loss_sup: 0.018279, loss_mps: 0.130277, loss_cps: 0.230561
[13:35:44.663] iteration 17608: total_loss: 0.499851, loss_sup: 0.165105, loss_mps: 0.121120, loss_cps: 0.213627
[13:35:44.808] iteration 17609: total_loss: 0.483225, loss_sup: 0.081628, loss_mps: 0.138823, loss_cps: 0.262775
[13:35:44.954] iteration 17610: total_loss: 0.901057, loss_sup: 0.095452, loss_mps: 0.244725, loss_cps: 0.560880
[13:35:45.100] iteration 17611: total_loss: 0.298146, loss_sup: 0.028407, loss_mps: 0.100861, loss_cps: 0.168879
[13:35:45.248] iteration 17612: total_loss: 0.407840, loss_sup: 0.055728, loss_mps: 0.124638, loss_cps: 0.227473
[13:35:45.393] iteration 17613: total_loss: 0.439917, loss_sup: 0.088692, loss_mps: 0.130864, loss_cps: 0.220361
[13:35:45.538] iteration 17614: total_loss: 0.295424, loss_sup: 0.019190, loss_mps: 0.101787, loss_cps: 0.174447
[13:35:45.683] iteration 17615: total_loss: 0.290859, loss_sup: 0.013598, loss_mps: 0.107290, loss_cps: 0.169971
[13:35:45.828] iteration 17616: total_loss: 0.406829, loss_sup: 0.097695, loss_mps: 0.118860, loss_cps: 0.190274
[13:35:45.973] iteration 17617: total_loss: 0.538938, loss_sup: 0.044176, loss_mps: 0.162348, loss_cps: 0.332414
[13:35:46.118] iteration 17618: total_loss: 0.387399, loss_sup: 0.058763, loss_mps: 0.116177, loss_cps: 0.212459
[13:35:46.263] iteration 17619: total_loss: 0.345022, loss_sup: 0.050602, loss_mps: 0.105745, loss_cps: 0.188674
[13:35:46.410] iteration 17620: total_loss: 0.418340, loss_sup: 0.146369, loss_mps: 0.101998, loss_cps: 0.169973
[13:35:46.555] iteration 17621: total_loss: 0.279748, loss_sup: 0.040991, loss_mps: 0.087213, loss_cps: 0.151545
[13:35:46.702] iteration 17622: total_loss: 0.925746, loss_sup: 0.234959, loss_mps: 0.209511, loss_cps: 0.481277
[13:35:46.848] iteration 17623: total_loss: 0.297858, loss_sup: 0.037378, loss_mps: 0.096284, loss_cps: 0.164196
[13:35:46.994] iteration 17624: total_loss: 0.488251, loss_sup: 0.040368, loss_mps: 0.145408, loss_cps: 0.302475
[13:35:47.139] iteration 17625: total_loss: 0.528283, loss_sup: 0.108665, loss_mps: 0.154476, loss_cps: 0.265142
[13:35:47.285] iteration 17626: total_loss: 0.298421, loss_sup: 0.036489, loss_mps: 0.094220, loss_cps: 0.167712
[13:35:47.431] iteration 17627: total_loss: 0.633159, loss_sup: 0.039414, loss_mps: 0.187917, loss_cps: 0.405828
[13:35:47.577] iteration 17628: total_loss: 0.291771, loss_sup: 0.038466, loss_mps: 0.092472, loss_cps: 0.160834
[13:35:47.723] iteration 17629: total_loss: 0.499698, loss_sup: 0.056775, loss_mps: 0.152992, loss_cps: 0.289931
[13:35:47.869] iteration 17630: total_loss: 0.322158, loss_sup: 0.144275, loss_mps: 0.069008, loss_cps: 0.108875
[13:35:48.014] iteration 17631: total_loss: 0.845511, loss_sup: 0.058805, loss_mps: 0.246150, loss_cps: 0.540556
[13:35:48.161] iteration 17632: total_loss: 0.298099, loss_sup: 0.029928, loss_mps: 0.099409, loss_cps: 0.168761
[13:35:48.306] iteration 17633: total_loss: 0.464155, loss_sup: 0.014127, loss_mps: 0.145506, loss_cps: 0.304521
[13:35:48.452] iteration 17634: total_loss: 0.465607, loss_sup: 0.027491, loss_mps: 0.147552, loss_cps: 0.290565
[13:35:48.597] iteration 17635: total_loss: 0.502590, loss_sup: 0.040575, loss_mps: 0.147142, loss_cps: 0.314873
[13:35:48.743] iteration 17636: total_loss: 0.579981, loss_sup: 0.152184, loss_mps: 0.142953, loss_cps: 0.284845
[13:35:48.889] iteration 17637: total_loss: 0.449060, loss_sup: 0.047273, loss_mps: 0.132605, loss_cps: 0.269182
[13:35:49.035] iteration 17638: total_loss: 0.640600, loss_sup: 0.100444, loss_mps: 0.171760, loss_cps: 0.368396
[13:35:49.180] iteration 17639: total_loss: 0.458926, loss_sup: 0.047574, loss_mps: 0.130642, loss_cps: 0.280710
[13:35:49.326] iteration 17640: total_loss: 0.436024, loss_sup: 0.015964, loss_mps: 0.144563, loss_cps: 0.275497
[13:35:49.472] iteration 17641: total_loss: 0.380183, loss_sup: 0.073255, loss_mps: 0.105907, loss_cps: 0.201021
[13:35:49.618] iteration 17642: total_loss: 0.453664, loss_sup: 0.106253, loss_mps: 0.120827, loss_cps: 0.226585
[13:35:49.765] iteration 17643: total_loss: 0.565146, loss_sup: 0.079220, loss_mps: 0.152572, loss_cps: 0.333354
[13:35:49.911] iteration 17644: total_loss: 0.546320, loss_sup: 0.114020, loss_mps: 0.138558, loss_cps: 0.293742
[13:35:50.056] iteration 17645: total_loss: 0.587520, loss_sup: 0.084140, loss_mps: 0.164784, loss_cps: 0.338595
[13:35:50.203] iteration 17646: total_loss: 0.681833, loss_sup: 0.101349, loss_mps: 0.181453, loss_cps: 0.399031
[13:35:50.348] iteration 17647: total_loss: 0.808914, loss_sup: 0.256030, loss_mps: 0.184601, loss_cps: 0.368282
[13:35:50.494] iteration 17648: total_loss: 0.288913, loss_sup: 0.040407, loss_mps: 0.088723, loss_cps: 0.159783
[13:35:50.640] iteration 17649: total_loss: 0.485564, loss_sup: 0.085092, loss_mps: 0.139959, loss_cps: 0.260514
[13:35:50.786] iteration 17650: total_loss: 0.524188, loss_sup: 0.101966, loss_mps: 0.144364, loss_cps: 0.277858
[13:35:50.933] iteration 17651: total_loss: 0.253004, loss_sup: 0.022738, loss_mps: 0.086165, loss_cps: 0.144101
[13:35:51.079] iteration 17652: total_loss: 0.318729, loss_sup: 0.059622, loss_mps: 0.093684, loss_cps: 0.165423
[13:35:51.225] iteration 17653: total_loss: 0.675920, loss_sup: 0.027714, loss_mps: 0.205708, loss_cps: 0.442499
[13:35:51.371] iteration 17654: total_loss: 0.186188, loss_sup: 0.010498, loss_mps: 0.066965, loss_cps: 0.108725
[13:35:51.516] iteration 17655: total_loss: 0.281164, loss_sup: 0.062722, loss_mps: 0.079748, loss_cps: 0.138695
[13:35:51.662] iteration 17656: total_loss: 0.534189, loss_sup: 0.140406, loss_mps: 0.133250, loss_cps: 0.260533
[13:35:51.808] iteration 17657: total_loss: 0.498417, loss_sup: 0.071058, loss_mps: 0.141797, loss_cps: 0.285562
[13:35:51.954] iteration 17658: total_loss: 0.608612, loss_sup: 0.229734, loss_mps: 0.139741, loss_cps: 0.239137
[13:35:52.100] iteration 17659: total_loss: 0.449243, loss_sup: 0.068707, loss_mps: 0.133397, loss_cps: 0.247138
[13:35:52.246] iteration 17660: total_loss: 0.599355, loss_sup: 0.055654, loss_mps: 0.176550, loss_cps: 0.367151
[13:35:52.392] iteration 17661: total_loss: 1.026992, loss_sup: 0.052159, loss_mps: 0.304851, loss_cps: 0.669981
[13:35:52.538] iteration 17662: total_loss: 0.466250, loss_sup: 0.102933, loss_mps: 0.132545, loss_cps: 0.230772
[13:35:52.684] iteration 17663: total_loss: 0.731988, loss_sup: 0.133422, loss_mps: 0.194076, loss_cps: 0.404490
[13:35:52.830] iteration 17664: total_loss: 0.428690, loss_sup: 0.034171, loss_mps: 0.127615, loss_cps: 0.266904
[13:35:52.976] iteration 17665: total_loss: 0.377239, loss_sup: 0.095720, loss_mps: 0.099970, loss_cps: 0.181548
[13:35:53.121] iteration 17666: total_loss: 0.506083, loss_sup: 0.230921, loss_mps: 0.090531, loss_cps: 0.184632
[13:35:53.268] iteration 17667: total_loss: 0.676919, loss_sup: 0.149377, loss_mps: 0.172127, loss_cps: 0.355416
[13:35:53.414] iteration 17668: total_loss: 0.452219, loss_sup: 0.055116, loss_mps: 0.129020, loss_cps: 0.268083
[13:35:53.560] iteration 17669: total_loss: 0.505076, loss_sup: 0.078555, loss_mps: 0.142330, loss_cps: 0.284191
[13:35:53.705] iteration 17670: total_loss: 0.357590, loss_sup: 0.034588, loss_mps: 0.114746, loss_cps: 0.208256
[13:35:53.851] iteration 17671: total_loss: 0.561966, loss_sup: 0.140712, loss_mps: 0.138154, loss_cps: 0.283100
[13:35:53.997] iteration 17672: total_loss: 1.046922, loss_sup: 0.480290, loss_mps: 0.182767, loss_cps: 0.383864
[13:35:54.142] iteration 17673: total_loss: 0.458818, loss_sup: 0.051750, loss_mps: 0.143789, loss_cps: 0.263279
[13:35:54.288] iteration 17674: total_loss: 0.621433, loss_sup: 0.173459, loss_mps: 0.157976, loss_cps: 0.289998
[13:35:54.434] iteration 17675: total_loss: 0.656624, loss_sup: 0.031256, loss_mps: 0.186649, loss_cps: 0.438719
[13:35:54.580] iteration 17676: total_loss: 0.636201, loss_sup: 0.072858, loss_mps: 0.186788, loss_cps: 0.376554
[13:35:54.725] iteration 17677: total_loss: 0.528944, loss_sup: 0.133278, loss_mps: 0.151182, loss_cps: 0.244484
[13:35:54.871] iteration 17678: total_loss: 0.418648, loss_sup: 0.061776, loss_mps: 0.127371, loss_cps: 0.229501
[13:35:55.017] iteration 17679: total_loss: 0.409309, loss_sup: 0.031597, loss_mps: 0.134446, loss_cps: 0.243267
[13:35:55.162] iteration 17680: total_loss: 0.436028, loss_sup: 0.161080, loss_mps: 0.104839, loss_cps: 0.170109
[13:35:55.309] iteration 17681: total_loss: 0.417928, loss_sup: 0.163024, loss_mps: 0.097192, loss_cps: 0.157713
[13:35:55.455] iteration 17682: total_loss: 0.585528, loss_sup: 0.043806, loss_mps: 0.179664, loss_cps: 0.362059
[13:35:55.601] iteration 17683: total_loss: 0.396113, loss_sup: 0.052207, loss_mps: 0.123983, loss_cps: 0.219923
[13:35:55.747] iteration 17684: total_loss: 0.711708, loss_sup: 0.030210, loss_mps: 0.220470, loss_cps: 0.461028
[13:35:55.893] iteration 17685: total_loss: 0.390767, loss_sup: 0.038699, loss_mps: 0.124196, loss_cps: 0.227872
[13:35:56.040] iteration 17686: total_loss: 0.430176, loss_sup: 0.074734, loss_mps: 0.129072, loss_cps: 0.226370
[13:35:56.186] iteration 17687: total_loss: 0.519867, loss_sup: 0.101561, loss_mps: 0.141523, loss_cps: 0.276784
[13:35:56.334] iteration 17688: total_loss: 0.428253, loss_sup: 0.070626, loss_mps: 0.120377, loss_cps: 0.237250
[13:35:56.480] iteration 17689: total_loss: 0.331137, loss_sup: 0.014663, loss_mps: 0.119543, loss_cps: 0.196931
[13:35:56.626] iteration 17690: total_loss: 0.502796, loss_sup: 0.059875, loss_mps: 0.152198, loss_cps: 0.290722
[13:35:56.772] iteration 17691: total_loss: 0.601560, loss_sup: 0.042171, loss_mps: 0.184396, loss_cps: 0.374993
[13:35:56.917] iteration 17692: total_loss: 0.342047, loss_sup: 0.015362, loss_mps: 0.114869, loss_cps: 0.211816
[13:35:57.064] iteration 17693: total_loss: 0.530099, loss_sup: 0.009874, loss_mps: 0.162181, loss_cps: 0.358044
[13:35:57.210] iteration 17694: total_loss: 0.223759, loss_sup: 0.012159, loss_mps: 0.080235, loss_cps: 0.131365
[13:35:57.358] iteration 17695: total_loss: 0.361048, loss_sup: 0.066592, loss_mps: 0.106210, loss_cps: 0.188246
[13:35:57.504] iteration 17696: total_loss: 0.561771, loss_sup: 0.032415, loss_mps: 0.171272, loss_cps: 0.358084
[13:35:57.650] iteration 17697: total_loss: 0.431522, loss_sup: 0.030947, loss_mps: 0.132620, loss_cps: 0.267954
[13:35:57.797] iteration 17698: total_loss: 0.457970, loss_sup: 0.065606, loss_mps: 0.133104, loss_cps: 0.259260
[13:35:57.946] iteration 17699: total_loss: 0.392922, loss_sup: 0.118082, loss_mps: 0.098059, loss_cps: 0.176781
[13:35:58.092] iteration 17700: total_loss: 0.455747, loss_sup: 0.110631, loss_mps: 0.122539, loss_cps: 0.222577
[13:35:58.092] Evaluation Started ==>
[13:36:09.434] ==> valid iteration 17700: unet metrics: {'dc': 0.657916158682892, 'jc': 0.5409656427025336, 'pre': 0.7742172659909398, 'hd': 5.616524143828223}, ynet metrics: {'dc': 0.628938126948428, 'jc': 0.512742888047142, 'pre': 0.7802480248463612, 'hd': 5.643422252984277}.
[13:36:09.436] Evaluation Finished!⏹️
[13:36:09.586] iteration 17701: total_loss: 0.311029, loss_sup: 0.038482, loss_mps: 0.097362, loss_cps: 0.175185
[13:36:09.733] iteration 17702: total_loss: 0.500506, loss_sup: 0.022672, loss_mps: 0.163095, loss_cps: 0.314740
[13:36:09.879] iteration 17703: total_loss: 0.333253, loss_sup: 0.097273, loss_mps: 0.086549, loss_cps: 0.149431
[13:36:10.024] iteration 17704: total_loss: 0.272381, loss_sup: 0.009653, loss_mps: 0.094923, loss_cps: 0.167805
[13:36:10.170] iteration 17705: total_loss: 0.296085, loss_sup: 0.028327, loss_mps: 0.096498, loss_cps: 0.171260
[13:36:10.316] iteration 17706: total_loss: 0.396989, loss_sup: 0.047544, loss_mps: 0.115658, loss_cps: 0.233787
[13:36:10.462] iteration 17707: total_loss: 0.483381, loss_sup: 0.096170, loss_mps: 0.129579, loss_cps: 0.257631
[13:36:10.608] iteration 17708: total_loss: 0.351833, loss_sup: 0.042169, loss_mps: 0.120951, loss_cps: 0.188713
[13:36:10.755] iteration 17709: total_loss: 0.526171, loss_sup: 0.189892, loss_mps: 0.118082, loss_cps: 0.218198
[13:36:10.901] iteration 17710: total_loss: 0.444454, loss_sup: 0.011189, loss_mps: 0.138742, loss_cps: 0.294523
[13:36:11.047] iteration 17711: total_loss: 0.508805, loss_sup: 0.031316, loss_mps: 0.154694, loss_cps: 0.322794
[13:36:11.193] iteration 17712: total_loss: 0.372410, loss_sup: 0.047079, loss_mps: 0.111305, loss_cps: 0.214025
[13:36:11.339] iteration 17713: total_loss: 0.544391, loss_sup: 0.100121, loss_mps: 0.155335, loss_cps: 0.288934
[13:36:11.484] iteration 17714: total_loss: 0.474585, loss_sup: 0.017462, loss_mps: 0.150620, loss_cps: 0.306503
[13:36:11.630] iteration 17715: total_loss: 0.533652, loss_sup: 0.027085, loss_mps: 0.166919, loss_cps: 0.339648
[13:36:11.776] iteration 17716: total_loss: 0.657109, loss_sup: 0.050041, loss_mps: 0.183992, loss_cps: 0.423077
[13:36:11.922] iteration 17717: total_loss: 0.983646, loss_sup: 0.263908, loss_mps: 0.222231, loss_cps: 0.497507
[13:36:12.068] iteration 17718: total_loss: 0.623710, loss_sup: 0.255472, loss_mps: 0.126742, loss_cps: 0.241497
[13:36:12.213] iteration 17719: total_loss: 0.402207, loss_sup: 0.101279, loss_mps: 0.104766, loss_cps: 0.196162
[13:36:12.359] iteration 17720: total_loss: 0.523741, loss_sup: 0.012285, loss_mps: 0.165896, loss_cps: 0.345560
[13:36:12.506] iteration 17721: total_loss: 0.543289, loss_sup: 0.092265, loss_mps: 0.152627, loss_cps: 0.298398
[13:36:12.652] iteration 17722: total_loss: 0.664689, loss_sup: 0.273302, loss_mps: 0.138128, loss_cps: 0.253259
[13:36:12.797] iteration 17723: total_loss: 0.640753, loss_sup: 0.051421, loss_mps: 0.189492, loss_cps: 0.399840
[13:36:12.943] iteration 17724: total_loss: 0.654833, loss_sup: 0.173347, loss_mps: 0.152787, loss_cps: 0.328699
[13:36:13.089] iteration 17725: total_loss: 1.269558, loss_sup: 0.150130, loss_mps: 0.342253, loss_cps: 0.777174
[13:36:13.235] iteration 17726: total_loss: 0.329563, loss_sup: 0.062539, loss_mps: 0.099817, loss_cps: 0.167207
[13:36:13.382] iteration 17727: total_loss: 0.381284, loss_sup: 0.010275, loss_mps: 0.126054, loss_cps: 0.244954
[13:36:13.528] iteration 17728: total_loss: 0.324357, loss_sup: 0.021328, loss_mps: 0.107313, loss_cps: 0.195717
[13:36:13.674] iteration 17729: total_loss: 0.463094, loss_sup: 0.146954, loss_mps: 0.108794, loss_cps: 0.207346
[13:36:13.820] iteration 17730: total_loss: 0.258320, loss_sup: 0.009329, loss_mps: 0.089395, loss_cps: 0.159595
[13:36:13.966] iteration 17731: total_loss: 0.318181, loss_sup: 0.003513, loss_mps: 0.112929, loss_cps: 0.201739
[13:36:14.111] iteration 17732: total_loss: 0.454734, loss_sup: 0.048973, loss_mps: 0.137642, loss_cps: 0.268119
[13:36:14.258] iteration 17733: total_loss: 0.831364, loss_sup: 0.330491, loss_mps: 0.156316, loss_cps: 0.344557
[13:36:14.405] iteration 17734: total_loss: 0.497650, loss_sup: 0.017629, loss_mps: 0.168234, loss_cps: 0.311787
[13:36:14.552] iteration 17735: total_loss: 1.000274, loss_sup: 0.164625, loss_mps: 0.264206, loss_cps: 0.571443
[13:36:14.698] iteration 17736: total_loss: 0.376592, loss_sup: 0.049828, loss_mps: 0.111696, loss_cps: 0.215068
[13:36:14.848] iteration 17737: total_loss: 0.672289, loss_sup: 0.102732, loss_mps: 0.190640, loss_cps: 0.378916
[13:36:14.995] iteration 17738: total_loss: 0.647870, loss_sup: 0.180332, loss_mps: 0.164554, loss_cps: 0.302983
[13:36:15.141] iteration 17739: total_loss: 0.371956, loss_sup: 0.077068, loss_mps: 0.106638, loss_cps: 0.188250
[13:36:15.288] iteration 17740: total_loss: 0.616859, loss_sup: 0.073319, loss_mps: 0.184891, loss_cps: 0.358649
[13:36:15.433] iteration 17741: total_loss: 0.820153, loss_sup: 0.130813, loss_mps: 0.227035, loss_cps: 0.462305
[13:36:15.579] iteration 17742: total_loss: 0.655406, loss_sup: 0.108400, loss_mps: 0.177735, loss_cps: 0.369271
[13:36:15.726] iteration 17743: total_loss: 0.579848, loss_sup: 0.124270, loss_mps: 0.169712, loss_cps: 0.285866
[13:36:15.873] iteration 17744: total_loss: 0.229054, loss_sup: 0.024857, loss_mps: 0.079214, loss_cps: 0.124982
[13:36:16.019] iteration 17745: total_loss: 0.327654, loss_sup: 0.066532, loss_mps: 0.099969, loss_cps: 0.161153
[13:36:16.167] iteration 17746: total_loss: 0.263838, loss_sup: 0.024391, loss_mps: 0.086413, loss_cps: 0.153034
[13:36:16.314] iteration 17747: total_loss: 0.585472, loss_sup: 0.105468, loss_mps: 0.164818, loss_cps: 0.315186
[13:36:16.461] iteration 17748: total_loss: 0.906448, loss_sup: 0.136529, loss_mps: 0.237346, loss_cps: 0.532573
[13:36:16.608] iteration 17749: total_loss: 0.824807, loss_sup: 0.138322, loss_mps: 0.223461, loss_cps: 0.463024
[13:36:16.754] iteration 17750: total_loss: 0.657054, loss_sup: 0.087564, loss_mps: 0.193714, loss_cps: 0.375776
[13:36:16.901] iteration 17751: total_loss: 0.276563, loss_sup: 0.017331, loss_mps: 0.095277, loss_cps: 0.163954
[13:36:17.048] iteration 17752: total_loss: 0.894970, loss_sup: 0.086435, loss_mps: 0.259839, loss_cps: 0.548696
[13:36:17.195] iteration 17753: total_loss: 0.612238, loss_sup: 0.037806, loss_mps: 0.182239, loss_cps: 0.392193
[13:36:17.342] iteration 17754: total_loss: 0.571942, loss_sup: 0.082390, loss_mps: 0.167336, loss_cps: 0.322216
[13:36:17.488] iteration 17755: total_loss: 0.371827, loss_sup: 0.014912, loss_mps: 0.123784, loss_cps: 0.233131
[13:36:17.634] iteration 17756: total_loss: 0.429799, loss_sup: 0.016777, loss_mps: 0.138353, loss_cps: 0.274669
[13:36:17.781] iteration 17757: total_loss: 0.641225, loss_sup: 0.096471, loss_mps: 0.172518, loss_cps: 0.372237
[13:36:17.927] iteration 17758: total_loss: 0.377144, loss_sup: 0.049467, loss_mps: 0.113643, loss_cps: 0.214034
[13:36:18.073] iteration 17759: total_loss: 0.609500, loss_sup: 0.073525, loss_mps: 0.170501, loss_cps: 0.365474
[13:36:18.220] iteration 17760: total_loss: 0.514681, loss_sup: 0.055423, loss_mps: 0.151206, loss_cps: 0.308051
[13:36:18.366] iteration 17761: total_loss: 0.536418, loss_sup: 0.071735, loss_mps: 0.156871, loss_cps: 0.307812
[13:36:18.512] iteration 17762: total_loss: 0.432504, loss_sup: 0.054539, loss_mps: 0.137414, loss_cps: 0.240551
[13:36:18.658] iteration 17763: total_loss: 0.492021, loss_sup: 0.021620, loss_mps: 0.146683, loss_cps: 0.323717
[13:36:18.805] iteration 17764: total_loss: 0.394535, loss_sup: 0.092372, loss_mps: 0.098159, loss_cps: 0.204004
[13:36:18.951] iteration 17765: total_loss: 0.685914, loss_sup: 0.173758, loss_mps: 0.175505, loss_cps: 0.336650
[13:36:19.097] iteration 17766: total_loss: 0.551789, loss_sup: 0.092783, loss_mps: 0.152928, loss_cps: 0.306078
[13:36:19.244] iteration 17767: total_loss: 0.952004, loss_sup: 0.122726, loss_mps: 0.262060, loss_cps: 0.567218
[13:36:19.390] iteration 17768: total_loss: 0.547110, loss_sup: 0.105923, loss_mps: 0.143218, loss_cps: 0.297969
[13:36:19.536] iteration 17769: total_loss: 0.337500, loss_sup: 0.076966, loss_mps: 0.089152, loss_cps: 0.171381
[13:36:19.682] iteration 17770: total_loss: 0.468707, loss_sup: 0.062308, loss_mps: 0.138077, loss_cps: 0.268322
[13:36:19.829] iteration 17771: total_loss: 0.321496, loss_sup: 0.025833, loss_mps: 0.107960, loss_cps: 0.187703
[13:36:19.975] iteration 17772: total_loss: 0.455450, loss_sup: 0.040903, loss_mps: 0.146098, loss_cps: 0.268449
[13:36:20.122] iteration 17773: total_loss: 0.772567, loss_sup: 0.091068, loss_mps: 0.221435, loss_cps: 0.460064
[13:36:20.269] iteration 17774: total_loss: 0.813057, loss_sup: 0.184539, loss_mps: 0.199456, loss_cps: 0.429063
[13:36:20.415] iteration 17775: total_loss: 0.602171, loss_sup: 0.078805, loss_mps: 0.173420, loss_cps: 0.349946
[13:36:20.561] iteration 17776: total_loss: 0.661408, loss_sup: 0.168755, loss_mps: 0.164045, loss_cps: 0.328608
[13:36:20.708] iteration 17777: total_loss: 0.693846, loss_sup: 0.095967, loss_mps: 0.187732, loss_cps: 0.410147
[13:36:20.854] iteration 17778: total_loss: 0.478288, loss_sup: 0.052697, loss_mps: 0.157297, loss_cps: 0.268294
[13:36:21.004] iteration 17779: total_loss: 0.495671, loss_sup: 0.127934, loss_mps: 0.129517, loss_cps: 0.238220
[13:36:21.150] iteration 17780: total_loss: 0.754685, loss_sup: 0.052824, loss_mps: 0.229120, loss_cps: 0.472740
[13:36:21.296] iteration 17781: total_loss: 0.565235, loss_sup: 0.087598, loss_mps: 0.154219, loss_cps: 0.323417
[13:36:21.443] iteration 17782: total_loss: 0.593422, loss_sup: 0.078234, loss_mps: 0.169745, loss_cps: 0.345443
[13:36:21.589] iteration 17783: total_loss: 0.567350, loss_sup: 0.069772, loss_mps: 0.171624, loss_cps: 0.325954
[13:36:21.735] iteration 17784: total_loss: 0.761206, loss_sup: 0.172883, loss_mps: 0.195712, loss_cps: 0.392611
[13:36:21.882] iteration 17785: total_loss: 0.516531, loss_sup: 0.089738, loss_mps: 0.149843, loss_cps: 0.276950
[13:36:22.029] iteration 17786: total_loss: 0.382865, loss_sup: 0.050257, loss_mps: 0.120877, loss_cps: 0.211731
[13:36:22.176] iteration 17787: total_loss: 0.564655, loss_sup: 0.074174, loss_mps: 0.175077, loss_cps: 0.315404
[13:36:22.322] iteration 17788: total_loss: 0.440356, loss_sup: 0.007315, loss_mps: 0.147327, loss_cps: 0.285715
[13:36:22.468] iteration 17789: total_loss: 0.576463, loss_sup: 0.100231, loss_mps: 0.158892, loss_cps: 0.317341
[13:36:22.614] iteration 17790: total_loss: 0.317496, loss_sup: 0.041728, loss_mps: 0.103056, loss_cps: 0.172711
[13:36:22.761] iteration 17791: total_loss: 0.638090, loss_sup: 0.097027, loss_mps: 0.179017, loss_cps: 0.362046
[13:36:22.909] iteration 17792: total_loss: 0.369811, loss_sup: 0.047684, loss_mps: 0.117726, loss_cps: 0.204401
[13:36:23.055] iteration 17793: total_loss: 0.801713, loss_sup: 0.108038, loss_mps: 0.221084, loss_cps: 0.472590
[13:36:23.201] iteration 17794: total_loss: 0.560642, loss_sup: 0.094620, loss_mps: 0.152340, loss_cps: 0.313682
[13:36:23.348] iteration 17795: total_loss: 0.441048, loss_sup: 0.069163, loss_mps: 0.130838, loss_cps: 0.241047
[13:36:23.494] iteration 17796: total_loss: 0.742225, loss_sup: 0.119184, loss_mps: 0.200695, loss_cps: 0.422346
[13:36:23.641] iteration 17797: total_loss: 0.545147, loss_sup: 0.067980, loss_mps: 0.158440, loss_cps: 0.318727
[13:36:23.788] iteration 17798: total_loss: 0.626545, loss_sup: 0.118522, loss_mps: 0.169776, loss_cps: 0.338247
[13:36:23.934] iteration 17799: total_loss: 0.869423, loss_sup: 0.377068, loss_mps: 0.165487, loss_cps: 0.326868
[13:36:24.081] iteration 17800: total_loss: 0.648924, loss_sup: 0.124365, loss_mps: 0.188859, loss_cps: 0.335700
[13:36:24.081] Evaluation Started ==>
[13:36:35.449] ==> valid iteration 17800: unet metrics: {'dc': 0.6158295788512199, 'jc': 0.49791626744531625, 'pre': 0.8066165384633945, 'hd': 5.413210296438502}, ynet metrics: {'dc': 0.6168652546459734, 'jc': 0.4976440648098655, 'pre': 0.8012446558102622, 'hd': 5.568879297236019}.
[13:36:35.451] Evaluation Finished!⏹️
[13:36:35.603] iteration 17801: total_loss: 1.372571, loss_sup: 0.203358, loss_mps: 0.368858, loss_cps: 0.800356
[13:36:35.750] iteration 17802: total_loss: 0.578247, loss_sup: 0.151365, loss_mps: 0.153342, loss_cps: 0.273540
[13:36:35.895] iteration 17803: total_loss: 0.555101, loss_sup: 0.093772, loss_mps: 0.159006, loss_cps: 0.302323
[13:36:36.040] iteration 17804: total_loss: 0.482788, loss_sup: 0.053781, loss_mps: 0.155765, loss_cps: 0.273242
[13:36:36.186] iteration 17805: total_loss: 0.468985, loss_sup: 0.059021, loss_mps: 0.142606, loss_cps: 0.267358
[13:36:36.331] iteration 17806: total_loss: 0.417879, loss_sup: 0.066715, loss_mps: 0.127839, loss_cps: 0.223326
[13:36:36.477] iteration 17807: total_loss: 0.808675, loss_sup: 0.084440, loss_mps: 0.238119, loss_cps: 0.486115
[13:36:36.622] iteration 17808: total_loss: 0.893405, loss_sup: 0.166484, loss_mps: 0.234291, loss_cps: 0.492629
[13:36:36.767] iteration 17809: total_loss: 0.470293, loss_sup: 0.080514, loss_mps: 0.137844, loss_cps: 0.251935
[13:36:36.914] iteration 17810: total_loss: 0.411590, loss_sup: 0.098590, loss_mps: 0.117263, loss_cps: 0.195737
[13:36:37.059] iteration 17811: total_loss: 0.661949, loss_sup: 0.024117, loss_mps: 0.206839, loss_cps: 0.430993
[13:36:37.205] iteration 17812: total_loss: 0.338797, loss_sup: 0.061293, loss_mps: 0.105632, loss_cps: 0.171871
[13:36:37.350] iteration 17813: total_loss: 0.392096, loss_sup: 0.030587, loss_mps: 0.134681, loss_cps: 0.226828
[13:36:37.495] iteration 17814: total_loss: 0.463929, loss_sup: 0.121596, loss_mps: 0.127897, loss_cps: 0.214436
[13:36:37.640] iteration 17815: total_loss: 0.685143, loss_sup: 0.210598, loss_mps: 0.160268, loss_cps: 0.314277
[13:36:37.785] iteration 17816: total_loss: 0.464289, loss_sup: 0.031571, loss_mps: 0.152163, loss_cps: 0.280554
[13:36:37.931] iteration 17817: total_loss: 0.528096, loss_sup: 0.057440, loss_mps: 0.159795, loss_cps: 0.310861
[13:36:38.078] iteration 17818: total_loss: 0.526014, loss_sup: 0.162605, loss_mps: 0.125532, loss_cps: 0.237877
[13:36:38.223] iteration 17819: total_loss: 0.748174, loss_sup: 0.115809, loss_mps: 0.199357, loss_cps: 0.433007
[13:36:38.369] iteration 17820: total_loss: 0.511328, loss_sup: 0.056468, loss_mps: 0.156719, loss_cps: 0.298140
[13:36:38.517] iteration 17821: total_loss: 0.552668, loss_sup: 0.040896, loss_mps: 0.171996, loss_cps: 0.339776
[13:36:38.665] iteration 17822: total_loss: 0.435564, loss_sup: 0.037627, loss_mps: 0.138563, loss_cps: 0.259374
[13:36:38.811] iteration 17823: total_loss: 0.468725, loss_sup: 0.098613, loss_mps: 0.131983, loss_cps: 0.238129
[13:36:38.957] iteration 17824: total_loss: 0.480156, loss_sup: 0.065135, loss_mps: 0.148169, loss_cps: 0.266852
[13:36:39.102] iteration 17825: total_loss: 0.562839, loss_sup: 0.043000, loss_mps: 0.169265, loss_cps: 0.350574
[13:36:39.250] iteration 17826: total_loss: 0.505407, loss_sup: 0.034113, loss_mps: 0.159778, loss_cps: 0.311517
[13:36:39.396] iteration 17827: total_loss: 0.329854, loss_sup: 0.019482, loss_mps: 0.112358, loss_cps: 0.198014
[13:36:39.541] iteration 17828: total_loss: 0.550307, loss_sup: 0.006848, loss_mps: 0.176219, loss_cps: 0.367241
[13:36:39.688] iteration 17829: total_loss: 0.781088, loss_sup: 0.285016, loss_mps: 0.169779, loss_cps: 0.326293
[13:36:39.838] iteration 17830: total_loss: 0.498654, loss_sup: 0.163038, loss_mps: 0.124376, loss_cps: 0.211240
[13:36:39.983] iteration 17831: total_loss: 0.516985, loss_sup: 0.057486, loss_mps: 0.160800, loss_cps: 0.298699
[13:36:40.129] iteration 17832: total_loss: 0.365502, loss_sup: 0.038220, loss_mps: 0.115686, loss_cps: 0.211596
[13:36:40.275] iteration 17833: total_loss: 0.431276, loss_sup: 0.033484, loss_mps: 0.136379, loss_cps: 0.261413
[13:36:40.425] iteration 17834: total_loss: 0.515156, loss_sup: 0.058017, loss_mps: 0.161101, loss_cps: 0.296038
[13:36:40.572] iteration 17835: total_loss: 0.664863, loss_sup: 0.124704, loss_mps: 0.184926, loss_cps: 0.355233
[13:36:40.718] iteration 17836: total_loss: 0.433663, loss_sup: 0.123457, loss_mps: 0.118656, loss_cps: 0.191550
[13:36:40.864] iteration 17837: total_loss: 0.939107, loss_sup: 0.104167, loss_mps: 0.261833, loss_cps: 0.573108
[13:36:41.012] iteration 17838: total_loss: 0.437531, loss_sup: 0.042551, loss_mps: 0.133836, loss_cps: 0.261145
[13:36:41.160] iteration 17839: total_loss: 0.412266, loss_sup: 0.014483, loss_mps: 0.138643, loss_cps: 0.259140
[13:36:41.305] iteration 17840: total_loss: 0.313526, loss_sup: 0.044136, loss_mps: 0.101895, loss_cps: 0.167495
[13:36:41.453] iteration 17841: total_loss: 0.553068, loss_sup: 0.238308, loss_mps: 0.112177, loss_cps: 0.202582
[13:36:41.600] iteration 17842: total_loss: 0.400129, loss_sup: 0.108322, loss_mps: 0.104621, loss_cps: 0.187186
[13:36:41.746] iteration 17843: total_loss: 0.438795, loss_sup: 0.049675, loss_mps: 0.133620, loss_cps: 0.255500
[13:36:41.892] iteration 17844: total_loss: 0.583251, loss_sup: 0.058898, loss_mps: 0.164732, loss_cps: 0.359621
[13:36:42.038] iteration 17845: total_loss: 0.638732, loss_sup: 0.057307, loss_mps: 0.187397, loss_cps: 0.394028
[13:36:42.183] iteration 17846: total_loss: 0.421109, loss_sup: 0.015555, loss_mps: 0.138786, loss_cps: 0.266767
[13:36:42.329] iteration 17847: total_loss: 0.542766, loss_sup: 0.142806, loss_mps: 0.139377, loss_cps: 0.260583
[13:36:42.475] iteration 17848: total_loss: 0.427309, loss_sup: 0.036964, loss_mps: 0.135791, loss_cps: 0.254554
[13:36:42.621] iteration 17849: total_loss: 0.384011, loss_sup: 0.029151, loss_mps: 0.115071, loss_cps: 0.239789
[13:36:42.766] iteration 17850: total_loss: 0.331631, loss_sup: 0.066176, loss_mps: 0.098446, loss_cps: 0.167008
[13:36:42.914] iteration 17851: total_loss: 0.574720, loss_sup: 0.030533, loss_mps: 0.186678, loss_cps: 0.357509
[13:36:43.059] iteration 17852: total_loss: 0.809911, loss_sup: 0.177893, loss_mps: 0.201716, loss_cps: 0.430302
[13:36:43.205] iteration 17853: total_loss: 0.764492, loss_sup: 0.082944, loss_mps: 0.213484, loss_cps: 0.468065
[13:36:43.351] iteration 17854: total_loss: 0.257449, loss_sup: 0.009617, loss_mps: 0.089706, loss_cps: 0.158127
[13:36:43.498] iteration 17855: total_loss: 0.542873, loss_sup: 0.058964, loss_mps: 0.161997, loss_cps: 0.321912
[13:36:43.644] iteration 17856: total_loss: 0.867804, loss_sup: 0.070062, loss_mps: 0.243641, loss_cps: 0.554101
[13:36:43.790] iteration 17857: total_loss: 0.659002, loss_sup: 0.160123, loss_mps: 0.168445, loss_cps: 0.330434
[13:36:43.936] iteration 17858: total_loss: 0.382931, loss_sup: 0.008506, loss_mps: 0.133189, loss_cps: 0.241236
[13:36:44.082] iteration 17859: total_loss: 0.730072, loss_sup: 0.228199, loss_mps: 0.162392, loss_cps: 0.339481
[13:36:44.229] iteration 17860: total_loss: 0.436541, loss_sup: 0.024800, loss_mps: 0.138872, loss_cps: 0.272869
[13:36:44.375] iteration 17861: total_loss: 0.435452, loss_sup: 0.027971, loss_mps: 0.139250, loss_cps: 0.268231
[13:36:44.522] iteration 17862: total_loss: 0.489915, loss_sup: 0.057625, loss_mps: 0.145545, loss_cps: 0.286745
[13:36:44.668] iteration 17863: total_loss: 0.580606, loss_sup: 0.057416, loss_mps: 0.168039, loss_cps: 0.355151
[13:36:44.814] iteration 17864: total_loss: 0.355906, loss_sup: 0.011423, loss_mps: 0.113945, loss_cps: 0.230538
[13:36:44.959] iteration 17865: total_loss: 0.599289, loss_sup: 0.259605, loss_mps: 0.116251, loss_cps: 0.223434
[13:36:45.105] iteration 17866: total_loss: 0.380276, loss_sup: 0.106854, loss_mps: 0.096190, loss_cps: 0.177231
[13:36:45.252] iteration 17867: total_loss: 0.705570, loss_sup: 0.178169, loss_mps: 0.171759, loss_cps: 0.355642
[13:36:45.398] iteration 17868: total_loss: 0.237485, loss_sup: 0.007740, loss_mps: 0.085934, loss_cps: 0.143810
[13:36:45.544] iteration 17869: total_loss: 0.361846, loss_sup: 0.046217, loss_mps: 0.109329, loss_cps: 0.206300
[13:36:45.691] iteration 17870: total_loss: 0.450623, loss_sup: 0.068257, loss_mps: 0.130788, loss_cps: 0.251578
[13:36:45.837] iteration 17871: total_loss: 0.419579, loss_sup: 0.117632, loss_mps: 0.105472, loss_cps: 0.196475
[13:36:45.983] iteration 17872: total_loss: 0.381302, loss_sup: 0.062277, loss_mps: 0.116056, loss_cps: 0.202970
[13:36:46.129] iteration 17873: total_loss: 0.332025, loss_sup: 0.019058, loss_mps: 0.113504, loss_cps: 0.199463
[13:36:46.275] iteration 17874: total_loss: 0.530852, loss_sup: 0.057229, loss_mps: 0.162100, loss_cps: 0.311523
[13:36:46.422] iteration 17875: total_loss: 0.331862, loss_sup: 0.055297, loss_mps: 0.100226, loss_cps: 0.176340
[13:36:46.569] iteration 17876: total_loss: 0.447345, loss_sup: 0.039772, loss_mps: 0.142429, loss_cps: 0.265144
[13:36:46.715] iteration 17877: total_loss: 0.328245, loss_sup: 0.032457, loss_mps: 0.109914, loss_cps: 0.185875
[13:36:46.861] iteration 17878: total_loss: 0.413332, loss_sup: 0.019177, loss_mps: 0.133936, loss_cps: 0.260219
[13:36:47.007] iteration 17879: total_loss: 0.432450, loss_sup: 0.083966, loss_mps: 0.127223, loss_cps: 0.221261
[13:36:47.153] iteration 17880: total_loss: 0.786758, loss_sup: 0.294372, loss_mps: 0.166349, loss_cps: 0.326037
[13:36:47.299] iteration 17881: total_loss: 0.511405, loss_sup: 0.062638, loss_mps: 0.157429, loss_cps: 0.291339
[13:36:47.444] iteration 17882: total_loss: 0.812618, loss_sup: 0.067854, loss_mps: 0.234256, loss_cps: 0.510508
[13:36:47.591] iteration 17883: total_loss: 0.511427, loss_sup: 0.103184, loss_mps: 0.139636, loss_cps: 0.268606
[13:36:47.740] iteration 17884: total_loss: 0.679625, loss_sup: 0.102351, loss_mps: 0.191430, loss_cps: 0.385844
[13:36:47.885] iteration 17885: total_loss: 0.794646, loss_sup: 0.087652, loss_mps: 0.222957, loss_cps: 0.484037
[13:36:48.031] iteration 17886: total_loss: 0.495217, loss_sup: 0.050109, loss_mps: 0.156260, loss_cps: 0.288849
[13:36:48.178] iteration 17887: total_loss: 0.623433, loss_sup: 0.223050, loss_mps: 0.133425, loss_cps: 0.266958
[13:36:48.325] iteration 17888: total_loss: 0.843166, loss_sup: 0.217769, loss_mps: 0.197740, loss_cps: 0.427657
[13:36:48.471] iteration 17889: total_loss: 0.899253, loss_sup: 0.056214, loss_mps: 0.253296, loss_cps: 0.589743
[13:36:48.618] iteration 17890: total_loss: 0.447823, loss_sup: 0.086509, loss_mps: 0.122651, loss_cps: 0.238664
[13:36:48.764] iteration 17891: total_loss: 0.558569, loss_sup: 0.053803, loss_mps: 0.167221, loss_cps: 0.337545
[13:36:48.910] iteration 17892: total_loss: 0.537824, loss_sup: 0.123709, loss_mps: 0.146801, loss_cps: 0.267314
[13:36:49.056] iteration 17893: total_loss: 0.432149, loss_sup: 0.080338, loss_mps: 0.118965, loss_cps: 0.232846
[13:36:49.201] iteration 17894: total_loss: 0.897428, loss_sup: 0.235016, loss_mps: 0.210589, loss_cps: 0.451823
[13:36:49.347] iteration 17895: total_loss: 0.391951, loss_sup: 0.070124, loss_mps: 0.110630, loss_cps: 0.211197
[13:36:49.493] iteration 17896: total_loss: 0.677555, loss_sup: 0.064949, loss_mps: 0.210105, loss_cps: 0.402501
[13:36:49.639] iteration 17897: total_loss: 0.257131, loss_sup: 0.014427, loss_mps: 0.088201, loss_cps: 0.154503
[13:36:49.785] iteration 17898: total_loss: 0.591968, loss_sup: 0.120903, loss_mps: 0.164390, loss_cps: 0.306675
[13:36:49.931] iteration 17899: total_loss: 1.034613, loss_sup: 0.171744, loss_mps: 0.279168, loss_cps: 0.583701
[13:36:50.076] iteration 17900: total_loss: 0.612592, loss_sup: 0.058202, loss_mps: 0.195400, loss_cps: 0.358989
[13:36:50.077] Evaluation Started ==>
[13:37:01.424] ==> valid iteration 17900: unet metrics: {'dc': 0.6274749021386137, 'jc': 0.511729357244124, 'pre': 0.7851615138299517, 'hd': 5.5290256524718}, ynet metrics: {'dc': 0.5856996498549456, 'jc': 0.47048520085780954, 'pre': 0.7836410556425697, 'hd': 5.632697575127973}.
[13:37:01.426] Evaluation Finished!⏹️
[13:37:01.576] iteration 17901: total_loss: 0.229226, loss_sup: 0.024882, loss_mps: 0.079705, loss_cps: 0.124639
[13:37:01.728] iteration 17902: total_loss: 0.579541, loss_sup: 0.096049, loss_mps: 0.164534, loss_cps: 0.318959
[13:37:01.875] iteration 17903: total_loss: 0.901942, loss_sup: 0.268252, loss_mps: 0.215613, loss_cps: 0.418077
[13:37:02.021] iteration 17904: total_loss: 0.481606, loss_sup: 0.031157, loss_mps: 0.153082, loss_cps: 0.297366
[13:37:02.168] iteration 17905: total_loss: 0.309072, loss_sup: 0.045844, loss_mps: 0.093777, loss_cps: 0.169451
[13:37:02.313] iteration 17906: total_loss: 0.314845, loss_sup: 0.059664, loss_mps: 0.099034, loss_cps: 0.156147
[13:37:02.460] iteration 17907: total_loss: 0.347547, loss_sup: 0.062126, loss_mps: 0.110667, loss_cps: 0.174753
[13:37:02.606] iteration 17908: total_loss: 0.663681, loss_sup: 0.109066, loss_mps: 0.189759, loss_cps: 0.364856
[13:37:02.752] iteration 17909: total_loss: 0.584049, loss_sup: 0.032611, loss_mps: 0.193831, loss_cps: 0.357608
[13:37:02.898] iteration 17910: total_loss: 0.449438, loss_sup: 0.044527, loss_mps: 0.145333, loss_cps: 0.259579
[13:37:03.045] iteration 17911: total_loss: 0.830997, loss_sup: 0.066486, loss_mps: 0.244739, loss_cps: 0.519773
[13:37:03.191] iteration 17912: total_loss: 1.018255, loss_sup: 0.220251, loss_mps: 0.253032, loss_cps: 0.544972
[13:37:03.339] iteration 17913: total_loss: 0.633883, loss_sup: 0.226010, loss_mps: 0.135283, loss_cps: 0.272589
[13:37:03.484] iteration 17914: total_loss: 0.540036, loss_sup: 0.189440, loss_mps: 0.124112, loss_cps: 0.226484
[13:37:03.630] iteration 17915: total_loss: 0.552440, loss_sup: 0.074285, loss_mps: 0.158205, loss_cps: 0.319951
[13:37:03.776] iteration 17916: total_loss: 0.943914, loss_sup: 0.216145, loss_mps: 0.237721, loss_cps: 0.490047
[13:37:03.922] iteration 17917: total_loss: 0.586136, loss_sup: 0.071174, loss_mps: 0.177051, loss_cps: 0.337911
[13:37:04.068] iteration 17918: total_loss: 0.957305, loss_sup: 0.158076, loss_mps: 0.256843, loss_cps: 0.542386
[13:37:04.213] iteration 17919: total_loss: 0.583107, loss_sup: 0.127228, loss_mps: 0.160463, loss_cps: 0.295416
[13:37:04.359] iteration 17920: total_loss: 0.454753, loss_sup: 0.083442, loss_mps: 0.127100, loss_cps: 0.244211
[13:37:04.505] iteration 17921: total_loss: 0.723680, loss_sup: 0.107539, loss_mps: 0.198174, loss_cps: 0.417968
[13:37:04.651] iteration 17922: total_loss: 0.542927, loss_sup: 0.147732, loss_mps: 0.142182, loss_cps: 0.253012
[13:37:04.797] iteration 17923: total_loss: 0.362826, loss_sup: 0.044602, loss_mps: 0.115994, loss_cps: 0.202230
[13:37:04.943] iteration 17924: total_loss: 1.227560, loss_sup: 0.128039, loss_mps: 0.330025, loss_cps: 0.769497
[13:37:05.088] iteration 17925: total_loss: 0.324464, loss_sup: 0.080239, loss_mps: 0.091077, loss_cps: 0.153148
[13:37:05.234] iteration 17926: total_loss: 1.611957, loss_sup: 0.287161, loss_mps: 0.399618, loss_cps: 0.925177
[13:37:05.379] iteration 17927: total_loss: 0.552343, loss_sup: 0.198027, loss_mps: 0.133715, loss_cps: 0.220601
[13:37:05.525] iteration 17928: total_loss: 0.314085, loss_sup: 0.063775, loss_mps: 0.101619, loss_cps: 0.148692
[13:37:05.671] iteration 17929: total_loss: 0.510417, loss_sup: 0.078148, loss_mps: 0.158579, loss_cps: 0.273690
[13:37:05.817] iteration 17930: total_loss: 0.481773, loss_sup: 0.061163, loss_mps: 0.151330, loss_cps: 0.269280
[13:37:05.964] iteration 17931: total_loss: 0.479474, loss_sup: 0.050237, loss_mps: 0.163881, loss_cps: 0.265356
[13:37:06.110] iteration 17932: total_loss: 0.589142, loss_sup: 0.060705, loss_mps: 0.176165, loss_cps: 0.352273
[13:37:06.256] iteration 17933: total_loss: 0.820658, loss_sup: 0.237751, loss_mps: 0.203461, loss_cps: 0.379446
[13:37:06.403] iteration 17934: total_loss: 0.350983, loss_sup: 0.034288, loss_mps: 0.120869, loss_cps: 0.195827
[13:37:06.549] iteration 17935: total_loss: 0.877974, loss_sup: 0.021303, loss_mps: 0.274730, loss_cps: 0.581941
[13:37:06.696] iteration 17936: total_loss: 0.506320, loss_sup: 0.047449, loss_mps: 0.160153, loss_cps: 0.298718
[13:37:06.842] iteration 17937: total_loss: 0.780955, loss_sup: 0.093660, loss_mps: 0.216643, loss_cps: 0.470651
[13:37:06.988] iteration 17938: total_loss: 0.541381, loss_sup: 0.189330, loss_mps: 0.126222, loss_cps: 0.225829
[13:37:07.135] iteration 17939: total_loss: 0.461576, loss_sup: 0.078490, loss_mps: 0.140807, loss_cps: 0.242279
[13:37:07.281] iteration 17940: total_loss: 0.450329, loss_sup: 0.052669, loss_mps: 0.139532, loss_cps: 0.258129
[13:37:07.427] iteration 17941: total_loss: 0.463749, loss_sup: 0.116952, loss_mps: 0.128437, loss_cps: 0.218359
[13:37:07.574] iteration 17942: total_loss: 0.625161, loss_sup: 0.109243, loss_mps: 0.170616, loss_cps: 0.345301
[13:37:07.721] iteration 17943: total_loss: 0.400823, loss_sup: 0.043068, loss_mps: 0.128552, loss_cps: 0.229203
[13:37:07.867] iteration 17944: total_loss: 0.338506, loss_sup: 0.034581, loss_mps: 0.112225, loss_cps: 0.191700
[13:37:08.013] iteration 17945: total_loss: 0.427963, loss_sup: 0.027794, loss_mps: 0.137556, loss_cps: 0.262613
[13:37:08.159] iteration 17946: total_loss: 1.136536, loss_sup: 0.223632, loss_mps: 0.300552, loss_cps: 0.612352
[13:37:08.306] iteration 17947: total_loss: 0.848119, loss_sup: 0.032338, loss_mps: 0.236818, loss_cps: 0.578963
[13:37:08.452] iteration 17948: total_loss: 0.773159, loss_sup: 0.239879, loss_mps: 0.182738, loss_cps: 0.350541
[13:37:08.598] iteration 17949: total_loss: 0.686359, loss_sup: 0.111371, loss_mps: 0.203860, loss_cps: 0.371128
[13:37:08.744] iteration 17950: total_loss: 0.668737, loss_sup: 0.033946, loss_mps: 0.203364, loss_cps: 0.431427
[13:37:08.891] iteration 17951: total_loss: 0.865076, loss_sup: 0.189076, loss_mps: 0.217488, loss_cps: 0.458512
[13:37:09.037] iteration 17952: total_loss: 0.774053, loss_sup: 0.400462, loss_mps: 0.133077, loss_cps: 0.240514
[13:37:09.183] iteration 17953: total_loss: 0.584033, loss_sup: 0.061962, loss_mps: 0.167879, loss_cps: 0.354192
[13:37:09.329] iteration 17954: total_loss: 0.455088, loss_sup: 0.091732, loss_mps: 0.123092, loss_cps: 0.240264
[13:37:09.477] iteration 17955: total_loss: 0.530153, loss_sup: 0.020412, loss_mps: 0.168842, loss_cps: 0.340898
[13:37:09.624] iteration 17956: total_loss: 0.342502, loss_sup: 0.063891, loss_mps: 0.106500, loss_cps: 0.172110
[13:37:09.771] iteration 17957: total_loss: 0.421889, loss_sup: 0.154661, loss_mps: 0.101888, loss_cps: 0.165341
[13:37:09.916] iteration 17958: total_loss: 0.498168, loss_sup: 0.007775, loss_mps: 0.164146, loss_cps: 0.326247
[13:37:10.063] iteration 17959: total_loss: 0.572925, loss_sup: 0.101479, loss_mps: 0.163068, loss_cps: 0.308378
[13:37:10.210] iteration 17960: total_loss: 0.436953, loss_sup: 0.089630, loss_mps: 0.118805, loss_cps: 0.228518
[13:37:10.356] iteration 17961: total_loss: 0.367943, loss_sup: 0.031410, loss_mps: 0.111717, loss_cps: 0.224815
[13:37:10.503] iteration 17962: total_loss: 1.194431, loss_sup: 0.331263, loss_mps: 0.288398, loss_cps: 0.574769
[13:37:10.649] iteration 17963: total_loss: 1.063233, loss_sup: 0.357284, loss_mps: 0.219315, loss_cps: 0.486634
[13:37:10.795] iteration 17964: total_loss: 0.519100, loss_sup: 0.079023, loss_mps: 0.153814, loss_cps: 0.286263
[13:37:10.941] iteration 17965: total_loss: 0.698583, loss_sup: 0.109321, loss_mps: 0.190399, loss_cps: 0.398863
[13:37:11.088] iteration 17966: total_loss: 0.527331, loss_sup: 0.182728, loss_mps: 0.118564, loss_cps: 0.226038
[13:37:11.234] iteration 17967: total_loss: 0.776470, loss_sup: 0.200286, loss_mps: 0.185532, loss_cps: 0.390653
[13:37:11.380] iteration 17968: total_loss: 0.612083, loss_sup: 0.055949, loss_mps: 0.180960, loss_cps: 0.375173
[13:37:11.530] iteration 17969: total_loss: 0.332555, loss_sup: 0.026282, loss_mps: 0.120019, loss_cps: 0.186255
[13:37:11.676] iteration 17970: total_loss: 0.542046, loss_sup: 0.062009, loss_mps: 0.160748, loss_cps: 0.319290
[13:37:11.822] iteration 17971: total_loss: 0.467857, loss_sup: 0.050665, loss_mps: 0.137286, loss_cps: 0.279906
[13:37:11.969] iteration 17972: total_loss: 0.578853, loss_sup: 0.055380, loss_mps: 0.175253, loss_cps: 0.348220
[13:37:12.115] iteration 17973: total_loss: 0.868477, loss_sup: 0.091691, loss_mps: 0.250084, loss_cps: 0.526702
[13:37:12.182] iteration 17974: total_loss: 0.472297, loss_sup: 0.018440, loss_mps: 0.165610, loss_cps: 0.288247
[13:37:13.403] iteration 17975: total_loss: 1.188690, loss_sup: 0.188310, loss_mps: 0.306152, loss_cps: 0.694228
[13:37:13.551] iteration 17976: total_loss: 0.508771, loss_sup: 0.046665, loss_mps: 0.166477, loss_cps: 0.295630
[13:37:13.697] iteration 17977: total_loss: 0.575057, loss_sup: 0.047438, loss_mps: 0.184826, loss_cps: 0.342793
[13:37:13.844] iteration 17978: total_loss: 0.854493, loss_sup: 0.100448, loss_mps: 0.243278, loss_cps: 0.510768
[13:37:13.992] iteration 17979: total_loss: 0.457787, loss_sup: 0.038835, loss_mps: 0.144527, loss_cps: 0.274425
[13:37:14.138] iteration 17980: total_loss: 0.538533, loss_sup: 0.101263, loss_mps: 0.149727, loss_cps: 0.287544
[13:37:14.284] iteration 17981: total_loss: 0.517684, loss_sup: 0.083204, loss_mps: 0.154094, loss_cps: 0.280386
[13:37:14.430] iteration 17982: total_loss: 0.493148, loss_sup: 0.066211, loss_mps: 0.142132, loss_cps: 0.284805
[13:37:14.578] iteration 17983: total_loss: 0.478645, loss_sup: 0.122079, loss_mps: 0.133806, loss_cps: 0.222761
[13:37:14.725] iteration 17984: total_loss: 0.800490, loss_sup: 0.188981, loss_mps: 0.202544, loss_cps: 0.408965
[13:37:14.871] iteration 17985: total_loss: 0.490992, loss_sup: 0.139926, loss_mps: 0.123432, loss_cps: 0.227634
[13:37:15.017] iteration 17986: total_loss: 0.605133, loss_sup: 0.105561, loss_mps: 0.166396, loss_cps: 0.333176
[13:37:15.164] iteration 17987: total_loss: 0.609971, loss_sup: 0.166968, loss_mps: 0.158542, loss_cps: 0.284462
[13:37:15.310] iteration 17988: total_loss: 0.802012, loss_sup: 0.212967, loss_mps: 0.195241, loss_cps: 0.393804
[13:37:15.455] iteration 17989: total_loss: 0.691449, loss_sup: 0.041580, loss_mps: 0.200726, loss_cps: 0.449142
[13:37:15.607] iteration 17990: total_loss: 0.571979, loss_sup: 0.111122, loss_mps: 0.161756, loss_cps: 0.299100
[13:37:15.753] iteration 17991: total_loss: 0.755652, loss_sup: 0.065486, loss_mps: 0.241618, loss_cps: 0.448548
[13:37:15.899] iteration 17992: total_loss: 0.387509, loss_sup: 0.021638, loss_mps: 0.129387, loss_cps: 0.236483
[13:37:16.046] iteration 17993: total_loss: 1.018124, loss_sup: 0.077156, loss_mps: 0.294478, loss_cps: 0.646490
[13:37:16.192] iteration 17994: total_loss: 0.460415, loss_sup: 0.060281, loss_mps: 0.137314, loss_cps: 0.262820
[13:37:16.341] iteration 17995: total_loss: 0.385596, loss_sup: 0.079420, loss_mps: 0.112810, loss_cps: 0.193367
[13:37:16.487] iteration 17996: total_loss: 0.524889, loss_sup: 0.082651, loss_mps: 0.148611, loss_cps: 0.293627
[13:37:16.633] iteration 17997: total_loss: 0.680632, loss_sup: 0.175501, loss_mps: 0.178596, loss_cps: 0.326534
[13:37:16.779] iteration 17998: total_loss: 0.448887, loss_sup: 0.057923, loss_mps: 0.140536, loss_cps: 0.250428
[13:37:16.925] iteration 17999: total_loss: 0.857571, loss_sup: 0.127636, loss_mps: 0.237050, loss_cps: 0.492885
[13:37:17.071] iteration 18000: total_loss: 0.507595, loss_sup: 0.064372, loss_mps: 0.169407, loss_cps: 0.273815
[13:37:17.071] Evaluation Started ==>
[13:37:28.438] ==> valid iteration 18000: unet metrics: {'dc': 0.6388353846398928, 'jc': 0.5202502561660443, 'pre': 0.7792133180314176, 'hd': 5.619915664330988}, ynet metrics: {'dc': 0.5726998983943328, 'jc': 0.4599575460351195, 'pre': 0.7586958387270412, 'hd': 5.6440241484874445}.
[13:37:28.440] Evaluation Finished!⏹️
[13:37:28.591] iteration 18001: total_loss: 0.965499, loss_sup: 0.200445, loss_mps: 0.246349, loss_cps: 0.518705
[13:37:28.738] iteration 18002: total_loss: 0.782884, loss_sup: 0.015355, loss_mps: 0.243607, loss_cps: 0.523922
[13:37:28.883] iteration 18003: total_loss: 0.464858, loss_sup: 0.037906, loss_mps: 0.153191, loss_cps: 0.273761
[13:37:29.028] iteration 18004: total_loss: 0.512961, loss_sup: 0.076114, loss_mps: 0.160350, loss_cps: 0.276497
[13:37:29.174] iteration 18005: total_loss: 0.711803, loss_sup: 0.211821, loss_mps: 0.170406, loss_cps: 0.329577
[13:37:29.319] iteration 18006: total_loss: 0.471118, loss_sup: 0.043358, loss_mps: 0.150088, loss_cps: 0.277672
[13:37:29.464] iteration 18007: total_loss: 0.629085, loss_sup: 0.240006, loss_mps: 0.141722, loss_cps: 0.247356
[13:37:29.610] iteration 18008: total_loss: 0.806099, loss_sup: 0.033754, loss_mps: 0.250239, loss_cps: 0.522106
[13:37:29.758] iteration 18009: total_loss: 0.338104, loss_sup: 0.043509, loss_mps: 0.108157, loss_cps: 0.186438
[13:37:29.905] iteration 18010: total_loss: 0.424387, loss_sup: 0.019445, loss_mps: 0.141995, loss_cps: 0.262947
[13:37:30.053] iteration 18011: total_loss: 0.787389, loss_sup: 0.071794, loss_mps: 0.220921, loss_cps: 0.494675
[13:37:30.199] iteration 18012: total_loss: 0.681014, loss_sup: 0.064754, loss_mps: 0.206522, loss_cps: 0.409738
[13:37:30.346] iteration 18013: total_loss: 0.490874, loss_sup: 0.089342, loss_mps: 0.140430, loss_cps: 0.261102
[13:37:30.491] iteration 18014: total_loss: 0.739498, loss_sup: 0.080440, loss_mps: 0.221517, loss_cps: 0.437540
[13:37:30.638] iteration 18015: total_loss: 0.712267, loss_sup: 0.072555, loss_mps: 0.206983, loss_cps: 0.432730
[13:37:30.783] iteration 18016: total_loss: 0.567472, loss_sup: 0.089634, loss_mps: 0.160506, loss_cps: 0.317333
[13:37:30.929] iteration 18017: total_loss: 0.629299, loss_sup: 0.078733, loss_mps: 0.195946, loss_cps: 0.354620
[13:37:31.074] iteration 18018: total_loss: 0.679828, loss_sup: 0.063034, loss_mps: 0.196179, loss_cps: 0.420615
[13:37:31.220] iteration 18019: total_loss: 0.728925, loss_sup: 0.290558, loss_mps: 0.155466, loss_cps: 0.282900
[13:37:31.367] iteration 18020: total_loss: 0.717649, loss_sup: 0.282673, loss_mps: 0.150629, loss_cps: 0.284347
[13:37:31.514] iteration 18021: total_loss: 0.451113, loss_sup: 0.097719, loss_mps: 0.131265, loss_cps: 0.222129
[13:37:31.660] iteration 18022: total_loss: 0.561763, loss_sup: 0.172008, loss_mps: 0.129998, loss_cps: 0.259757
[13:37:31.807] iteration 18023: total_loss: 0.365548, loss_sup: 0.010795, loss_mps: 0.126991, loss_cps: 0.227762
[13:37:31.954] iteration 18024: total_loss: 0.234962, loss_sup: 0.006696, loss_mps: 0.089851, loss_cps: 0.138415
[13:37:32.100] iteration 18025: total_loss: 0.547284, loss_sup: 0.158777, loss_mps: 0.139733, loss_cps: 0.248773
[13:37:32.246] iteration 18026: total_loss: 0.414109, loss_sup: 0.044989, loss_mps: 0.135344, loss_cps: 0.233776
[13:37:32.392] iteration 18027: total_loss: 0.754867, loss_sup: 0.015288, loss_mps: 0.223469, loss_cps: 0.516111
[13:37:32.538] iteration 18028: total_loss: 0.696585, loss_sup: 0.149883, loss_mps: 0.183333, loss_cps: 0.363369
[13:37:32.684] iteration 18029: total_loss: 0.436894, loss_sup: 0.019274, loss_mps: 0.151403, loss_cps: 0.266217
[13:37:32.829] iteration 18030: total_loss: 0.798148, loss_sup: 0.156354, loss_mps: 0.218171, loss_cps: 0.423623
[13:37:32.977] iteration 18031: total_loss: 0.458376, loss_sup: 0.030172, loss_mps: 0.148095, loss_cps: 0.280109
[13:37:33.123] iteration 18032: total_loss: 0.637894, loss_sup: 0.261329, loss_mps: 0.140485, loss_cps: 0.236080
[13:37:33.269] iteration 18033: total_loss: 0.407088, loss_sup: 0.011155, loss_mps: 0.143472, loss_cps: 0.252462
[13:37:33.416] iteration 18034: total_loss: 0.676147, loss_sup: 0.159557, loss_mps: 0.181240, loss_cps: 0.335350
[13:37:33.563] iteration 18035: total_loss: 0.785110, loss_sup: 0.207641, loss_mps: 0.197002, loss_cps: 0.380468
[13:37:33.711] iteration 18036: total_loss: 0.508637, loss_sup: 0.150604, loss_mps: 0.133048, loss_cps: 0.224985
[13:37:33.857] iteration 18037: total_loss: 0.263946, loss_sup: 0.026761, loss_mps: 0.088479, loss_cps: 0.148705
[13:37:34.003] iteration 18038: total_loss: 0.425369, loss_sup: 0.122386, loss_mps: 0.109340, loss_cps: 0.193643
[13:37:34.150] iteration 18039: total_loss: 0.946427, loss_sup: 0.318533, loss_mps: 0.204713, loss_cps: 0.423181
[13:37:34.298] iteration 18040: total_loss: 0.501108, loss_sup: 0.044758, loss_mps: 0.166326, loss_cps: 0.290024
[13:37:34.445] iteration 18041: total_loss: 0.502381, loss_sup: 0.006560, loss_mps: 0.158401, loss_cps: 0.337420
[13:37:34.593] iteration 18042: total_loss: 0.558927, loss_sup: 0.030323, loss_mps: 0.181107, loss_cps: 0.347498
[13:37:34.742] iteration 18043: total_loss: 0.639938, loss_sup: 0.165872, loss_mps: 0.164998, loss_cps: 0.309068
[13:37:34.888] iteration 18044: total_loss: 0.821307, loss_sup: 0.092054, loss_mps: 0.238792, loss_cps: 0.490461
[13:37:35.034] iteration 18045: total_loss: 0.302617, loss_sup: 0.081877, loss_mps: 0.078902, loss_cps: 0.141838
[13:37:35.181] iteration 18046: total_loss: 0.422821, loss_sup: 0.006098, loss_mps: 0.146081, loss_cps: 0.270643
[13:37:35.327] iteration 18047: total_loss: 0.421579, loss_sup: 0.138807, loss_mps: 0.107995, loss_cps: 0.174776
[13:37:35.475] iteration 18048: total_loss: 0.517055, loss_sup: 0.081445, loss_mps: 0.145929, loss_cps: 0.289681
[13:37:35.622] iteration 18049: total_loss: 0.448595, loss_sup: 0.026868, loss_mps: 0.148049, loss_cps: 0.273678
[13:37:35.768] iteration 18050: total_loss: 0.368447, loss_sup: 0.053262, loss_mps: 0.111191, loss_cps: 0.203994
[13:37:35.915] iteration 18051: total_loss: 0.666244, loss_sup: 0.082195, loss_mps: 0.195083, loss_cps: 0.388966
[13:37:36.067] iteration 18052: total_loss: 0.262182, loss_sup: 0.004682, loss_mps: 0.099672, loss_cps: 0.157828
[13:37:36.214] iteration 18053: total_loss: 0.292077, loss_sup: 0.032373, loss_mps: 0.094736, loss_cps: 0.164968
[13:37:36.360] iteration 18054: total_loss: 0.313505, loss_sup: 0.100515, loss_mps: 0.080643, loss_cps: 0.132346
[13:37:36.506] iteration 18055: total_loss: 0.461558, loss_sup: 0.060696, loss_mps: 0.141611, loss_cps: 0.259252
[13:37:36.652] iteration 18056: total_loss: 0.532901, loss_sup: 0.028863, loss_mps: 0.168159, loss_cps: 0.335880
[13:37:36.800] iteration 18057: total_loss: 0.400324, loss_sup: 0.040357, loss_mps: 0.124646, loss_cps: 0.235321
[13:37:36.946] iteration 18058: total_loss: 0.471025, loss_sup: 0.106990, loss_mps: 0.121948, loss_cps: 0.242087
[13:37:37.093] iteration 18059: total_loss: 0.562904, loss_sup: 0.152837, loss_mps: 0.130796, loss_cps: 0.279270
[13:37:37.239] iteration 18060: total_loss: 0.317222, loss_sup: 0.026778, loss_mps: 0.103768, loss_cps: 0.186676
[13:37:37.386] iteration 18061: total_loss: 0.492570, loss_sup: 0.139786, loss_mps: 0.127782, loss_cps: 0.225003
[13:37:37.533] iteration 18062: total_loss: 0.453811, loss_sup: 0.117048, loss_mps: 0.118453, loss_cps: 0.218309
[13:37:37.680] iteration 18063: total_loss: 0.522169, loss_sup: 0.108578, loss_mps: 0.146845, loss_cps: 0.266746
[13:37:37.826] iteration 18064: total_loss: 0.416661, loss_sup: 0.146036, loss_mps: 0.100050, loss_cps: 0.170576
[13:37:37.974] iteration 18065: total_loss: 0.446207, loss_sup: 0.100852, loss_mps: 0.124751, loss_cps: 0.220605
[13:37:38.120] iteration 18066: total_loss: 0.512431, loss_sup: 0.049298, loss_mps: 0.159090, loss_cps: 0.304044
[13:37:38.269] iteration 18067: total_loss: 0.444148, loss_sup: 0.097990, loss_mps: 0.121483, loss_cps: 0.224675
[13:37:38.416] iteration 18068: total_loss: 0.518089, loss_sup: 0.057032, loss_mps: 0.151298, loss_cps: 0.309759
[13:37:38.563] iteration 18069: total_loss: 0.431400, loss_sup: 0.046628, loss_mps: 0.132394, loss_cps: 0.252378
[13:37:38.709] iteration 18070: total_loss: 0.775614, loss_sup: 0.201175, loss_mps: 0.186108, loss_cps: 0.388330
[13:37:38.855] iteration 18071: total_loss: 0.321963, loss_sup: 0.021638, loss_mps: 0.106990, loss_cps: 0.193336
[13:37:39.001] iteration 18072: total_loss: 0.690691, loss_sup: 0.075095, loss_mps: 0.211434, loss_cps: 0.404162
[13:37:39.147] iteration 18073: total_loss: 0.458459, loss_sup: 0.076959, loss_mps: 0.131126, loss_cps: 0.250374
[13:37:39.295] iteration 18074: total_loss: 0.631598, loss_sup: 0.090013, loss_mps: 0.184304, loss_cps: 0.357281
[13:37:39.441] iteration 18075: total_loss: 0.605615, loss_sup: 0.176240, loss_mps: 0.145443, loss_cps: 0.283932
[13:37:39.588] iteration 18076: total_loss: 0.290590, loss_sup: 0.039115, loss_mps: 0.094440, loss_cps: 0.157036
[13:37:39.735] iteration 18077: total_loss: 0.942694, loss_sup: 0.464607, loss_mps: 0.165898, loss_cps: 0.312189
[13:37:39.881] iteration 18078: total_loss: 0.630571, loss_sup: 0.204995, loss_mps: 0.159780, loss_cps: 0.265797
[13:37:40.027] iteration 18079: total_loss: 0.337043, loss_sup: 0.087467, loss_mps: 0.090949, loss_cps: 0.158627
[13:37:40.172] iteration 18080: total_loss: 0.550341, loss_sup: 0.154982, loss_mps: 0.139956, loss_cps: 0.255402
[13:37:40.320] iteration 18081: total_loss: 0.545308, loss_sup: 0.067285, loss_mps: 0.163223, loss_cps: 0.314800
[13:37:40.466] iteration 18082: total_loss: 0.357400, loss_sup: 0.023480, loss_mps: 0.118425, loss_cps: 0.215494
[13:37:40.612] iteration 18083: total_loss: 0.304638, loss_sup: 0.025698, loss_mps: 0.102200, loss_cps: 0.176741
[13:37:40.759] iteration 18084: total_loss: 0.360269, loss_sup: 0.025519, loss_mps: 0.121535, loss_cps: 0.213215
[13:37:40.904] iteration 18085: total_loss: 0.534906, loss_sup: 0.183175, loss_mps: 0.122145, loss_cps: 0.229585
[13:37:41.050] iteration 18086: total_loss: 0.292294, loss_sup: 0.011554, loss_mps: 0.102521, loss_cps: 0.178218
[13:37:41.196] iteration 18087: total_loss: 0.586705, loss_sup: 0.028865, loss_mps: 0.175323, loss_cps: 0.382516
[13:37:41.343] iteration 18088: total_loss: 0.295543, loss_sup: 0.018442, loss_mps: 0.101907, loss_cps: 0.175194
[13:37:41.489] iteration 18089: total_loss: 0.433637, loss_sup: 0.089548, loss_mps: 0.123996, loss_cps: 0.220093
[13:37:41.635] iteration 18090: total_loss: 0.323601, loss_sup: 0.013323, loss_mps: 0.110704, loss_cps: 0.199573
[13:37:41.781] iteration 18091: total_loss: 0.371438, loss_sup: 0.055264, loss_mps: 0.116268, loss_cps: 0.199906
[13:37:41.927] iteration 18092: total_loss: 0.599033, loss_sup: 0.030319, loss_mps: 0.187254, loss_cps: 0.381459
[13:37:42.074] iteration 18093: total_loss: 0.334843, loss_sup: 0.014422, loss_mps: 0.117140, loss_cps: 0.203281
[13:37:42.220] iteration 18094: total_loss: 0.587716, loss_sup: 0.070669, loss_mps: 0.168754, loss_cps: 0.348293
[13:37:42.366] iteration 18095: total_loss: 0.757906, loss_sup: 0.299159, loss_mps: 0.153601, loss_cps: 0.305146
[13:37:42.512] iteration 18096: total_loss: 0.448996, loss_sup: 0.081660, loss_mps: 0.119397, loss_cps: 0.247939
[13:37:42.659] iteration 18097: total_loss: 0.384204, loss_sup: 0.017515, loss_mps: 0.122069, loss_cps: 0.244620
[13:37:42.805] iteration 18098: total_loss: 0.363293, loss_sup: 0.026637, loss_mps: 0.120428, loss_cps: 0.216227
[13:37:42.951] iteration 18099: total_loss: 0.435553, loss_sup: 0.115762, loss_mps: 0.107669, loss_cps: 0.212123
[13:37:43.097] iteration 18100: total_loss: 0.305004, loss_sup: 0.032599, loss_mps: 0.094444, loss_cps: 0.177962
[13:37:43.097] Evaluation Started ==>
[13:37:54.424] ==> valid iteration 18100: unet metrics: {'dc': 0.6583564794179596, 'jc': 0.5402806673401023, 'pre': 0.8090609449845184, 'hd': 5.489387046423412}, ynet metrics: {'dc': 0.5937923233009952, 'jc': 0.47808836588855474, 'pre': 0.8038981939127583, 'hd': 5.530134523852596}.
[13:37:54.425] Evaluation Finished!⏹️
[13:37:54.574] iteration 18101: total_loss: 0.312522, loss_sup: 0.025290, loss_mps: 0.101909, loss_cps: 0.185324
[13:37:54.721] iteration 18102: total_loss: 0.425998, loss_sup: 0.091740, loss_mps: 0.114821, loss_cps: 0.219436
[13:37:54.867] iteration 18103: total_loss: 0.366841, loss_sup: 0.072127, loss_mps: 0.107988, loss_cps: 0.186726
[13:37:55.012] iteration 18104: total_loss: 0.520317, loss_sup: 0.036623, loss_mps: 0.160640, loss_cps: 0.323053
[13:37:55.158] iteration 18105: total_loss: 0.625794, loss_sup: 0.075111, loss_mps: 0.179728, loss_cps: 0.370954
[13:37:55.303] iteration 18106: total_loss: 0.658975, loss_sup: 0.097607, loss_mps: 0.186655, loss_cps: 0.374713
[13:37:55.449] iteration 18107: total_loss: 0.727641, loss_sup: 0.110426, loss_mps: 0.192257, loss_cps: 0.424958
[13:37:55.594] iteration 18108: total_loss: 0.338187, loss_sup: 0.033535, loss_mps: 0.107013, loss_cps: 0.197639
[13:37:55.739] iteration 18109: total_loss: 0.292293, loss_sup: 0.069455, loss_mps: 0.080859, loss_cps: 0.141980
[13:37:55.884] iteration 18110: total_loss: 0.563825, loss_sup: 0.150296, loss_mps: 0.140862, loss_cps: 0.272666
[13:37:56.030] iteration 18111: total_loss: 0.336067, loss_sup: 0.065831, loss_mps: 0.096595, loss_cps: 0.173641
[13:37:56.175] iteration 18112: total_loss: 0.638307, loss_sup: 0.121567, loss_mps: 0.166102, loss_cps: 0.350637
[13:37:56.319] iteration 18113: total_loss: 0.512518, loss_sup: 0.077399, loss_mps: 0.141729, loss_cps: 0.293391
[13:37:56.465] iteration 18114: total_loss: 0.366799, loss_sup: 0.095854, loss_mps: 0.096944, loss_cps: 0.174001
[13:37:56.610] iteration 18115: total_loss: 0.278896, loss_sup: 0.021717, loss_mps: 0.088832, loss_cps: 0.168348
[13:37:56.755] iteration 18116: total_loss: 0.447154, loss_sup: 0.052618, loss_mps: 0.139221, loss_cps: 0.255315
[13:37:56.900] iteration 18117: total_loss: 0.708701, loss_sup: 0.106469, loss_mps: 0.191443, loss_cps: 0.410789
[13:37:57.045] iteration 18118: total_loss: 0.879143, loss_sup: 0.192015, loss_mps: 0.211656, loss_cps: 0.475472
[13:37:57.190] iteration 18119: total_loss: 0.210354, loss_sup: 0.006629, loss_mps: 0.076980, loss_cps: 0.126745
[13:37:57.336] iteration 18120: total_loss: 0.629453, loss_sup: 0.096174, loss_mps: 0.176484, loss_cps: 0.356794
[13:37:57.480] iteration 18121: total_loss: 0.656259, loss_sup: 0.059548, loss_mps: 0.186307, loss_cps: 0.410404
[13:37:57.628] iteration 18122: total_loss: 0.350804, loss_sup: 0.038173, loss_mps: 0.107468, loss_cps: 0.205163
[13:37:57.774] iteration 18123: total_loss: 0.266059, loss_sup: 0.017430, loss_mps: 0.089570, loss_cps: 0.159060
[13:37:57.920] iteration 18124: total_loss: 0.808022, loss_sup: 0.245051, loss_mps: 0.181462, loss_cps: 0.381509
[13:37:58.065] iteration 18125: total_loss: 0.326481, loss_sup: 0.038142, loss_mps: 0.098109, loss_cps: 0.190229
[13:37:58.210] iteration 18126: total_loss: 0.399183, loss_sup: 0.081398, loss_mps: 0.108818, loss_cps: 0.208967
[13:37:58.356] iteration 18127: total_loss: 0.753181, loss_sup: 0.282722, loss_mps: 0.149161, loss_cps: 0.321299
[13:37:58.501] iteration 18128: total_loss: 1.230894, loss_sup: 0.038083, loss_mps: 0.354578, loss_cps: 0.838234
[13:37:58.648] iteration 18129: total_loss: 1.088347, loss_sup: 0.104737, loss_mps: 0.285406, loss_cps: 0.698204
[13:37:58.794] iteration 18130: total_loss: 1.143467, loss_sup: 0.307112, loss_mps: 0.267382, loss_cps: 0.568974
[13:37:58.939] iteration 18131: total_loss: 0.822439, loss_sup: 0.037301, loss_mps: 0.252499, loss_cps: 0.532639
[13:37:59.084] iteration 18132: total_loss: 0.486271, loss_sup: 0.059855, loss_mps: 0.143365, loss_cps: 0.283052
[13:37:59.230] iteration 18133: total_loss: 0.584354, loss_sup: 0.064353, loss_mps: 0.174964, loss_cps: 0.345036
[13:37:59.376] iteration 18134: total_loss: 0.673721, loss_sup: 0.041378, loss_mps: 0.194123, loss_cps: 0.438220
[13:37:59.522] iteration 18135: total_loss: 0.957633, loss_sup: 0.253918, loss_mps: 0.229544, loss_cps: 0.474171
[13:37:59.668] iteration 18136: total_loss: 0.613042, loss_sup: 0.189197, loss_mps: 0.139302, loss_cps: 0.284543
[13:37:59.814] iteration 18137: total_loss: 1.011337, loss_sup: 0.274039, loss_mps: 0.232947, loss_cps: 0.504351
[13:37:59.960] iteration 18138: total_loss: 0.422573, loss_sup: 0.037854, loss_mps: 0.132985, loss_cps: 0.251734
[13:38:00.108] iteration 18139: total_loss: 0.551192, loss_sup: 0.048053, loss_mps: 0.165490, loss_cps: 0.337649
[13:38:00.256] iteration 18140: total_loss: 0.568965, loss_sup: 0.048662, loss_mps: 0.176093, loss_cps: 0.344209
[13:38:00.402] iteration 18141: total_loss: 0.522745, loss_sup: 0.017066, loss_mps: 0.176621, loss_cps: 0.329058
[13:38:00.549] iteration 18142: total_loss: 0.782573, loss_sup: 0.089837, loss_mps: 0.248339, loss_cps: 0.444398
[13:38:00.695] iteration 18143: total_loss: 0.623897, loss_sup: 0.182250, loss_mps: 0.155912, loss_cps: 0.285735
[13:38:00.842] iteration 18144: total_loss: 0.393971, loss_sup: 0.053201, loss_mps: 0.132456, loss_cps: 0.208314
[13:38:00.990] iteration 18145: total_loss: 0.389126, loss_sup: 0.076551, loss_mps: 0.119925, loss_cps: 0.192650
[13:38:01.135] iteration 18146: total_loss: 0.576985, loss_sup: 0.025328, loss_mps: 0.182963, loss_cps: 0.368694
[13:38:01.282] iteration 18147: total_loss: 0.477005, loss_sup: 0.068355, loss_mps: 0.150061, loss_cps: 0.258589
[13:38:01.430] iteration 18148: total_loss: 0.264089, loss_sup: 0.015788, loss_mps: 0.097013, loss_cps: 0.151289
[13:38:01.576] iteration 18149: total_loss: 0.427896, loss_sup: 0.012851, loss_mps: 0.150748, loss_cps: 0.264297
[13:38:01.722] iteration 18150: total_loss: 0.684437, loss_sup: 0.076819, loss_mps: 0.195323, loss_cps: 0.412295
[13:38:01.869] iteration 18151: total_loss: 0.386504, loss_sup: 0.089302, loss_mps: 0.109827, loss_cps: 0.187375
[13:38:02.015] iteration 18152: total_loss: 0.485244, loss_sup: 0.039574, loss_mps: 0.154215, loss_cps: 0.291456
[13:38:02.161] iteration 18153: total_loss: 0.372853, loss_sup: 0.032313, loss_mps: 0.126742, loss_cps: 0.213798
[13:38:02.307] iteration 18154: total_loss: 0.797776, loss_sup: 0.072133, loss_mps: 0.233199, loss_cps: 0.492444
[13:38:02.452] iteration 18155: total_loss: 0.391081, loss_sup: 0.028735, loss_mps: 0.127259, loss_cps: 0.235087
[13:38:02.600] iteration 18156: total_loss: 0.564200, loss_sup: 0.190098, loss_mps: 0.135472, loss_cps: 0.238629
[13:38:02.748] iteration 18157: total_loss: 0.400070, loss_sup: 0.013574, loss_mps: 0.136023, loss_cps: 0.250473
[13:38:02.895] iteration 18158: total_loss: 0.587846, loss_sup: 0.066700, loss_mps: 0.173848, loss_cps: 0.347299
[13:38:03.040] iteration 18159: total_loss: 0.279288, loss_sup: 0.043054, loss_mps: 0.089957, loss_cps: 0.146277
[13:38:03.186] iteration 18160: total_loss: 0.505671, loss_sup: 0.031007, loss_mps: 0.162489, loss_cps: 0.312175
[13:38:03.334] iteration 18161: total_loss: 0.622550, loss_sup: 0.100497, loss_mps: 0.172370, loss_cps: 0.349682
[13:38:03.480] iteration 18162: total_loss: 0.727695, loss_sup: 0.139638, loss_mps: 0.194668, loss_cps: 0.393390
[13:38:03.626] iteration 18163: total_loss: 0.236989, loss_sup: 0.014260, loss_mps: 0.081807, loss_cps: 0.140922
[13:38:03.772] iteration 18164: total_loss: 0.967014, loss_sup: 0.081900, loss_mps: 0.280181, loss_cps: 0.604933
[13:38:03.917] iteration 18165: total_loss: 0.558258, loss_sup: 0.082130, loss_mps: 0.156992, loss_cps: 0.319137
[13:38:04.063] iteration 18166: total_loss: 0.544971, loss_sup: 0.095854, loss_mps: 0.147669, loss_cps: 0.301448
[13:38:04.213] iteration 18167: total_loss: 0.709844, loss_sup: 0.108141, loss_mps: 0.187917, loss_cps: 0.413786
[13:38:04.359] iteration 18168: total_loss: 0.715104, loss_sup: 0.074167, loss_mps: 0.206607, loss_cps: 0.434329
[13:38:04.505] iteration 18169: total_loss: 0.477063, loss_sup: 0.086405, loss_mps: 0.129483, loss_cps: 0.261174
[13:38:04.651] iteration 18170: total_loss: 0.356355, loss_sup: 0.061213, loss_mps: 0.106897, loss_cps: 0.188246
[13:38:04.797] iteration 18171: total_loss: 0.509288, loss_sup: 0.104272, loss_mps: 0.140333, loss_cps: 0.264682
[13:38:04.943] iteration 18172: total_loss: 0.417150, loss_sup: 0.054900, loss_mps: 0.117312, loss_cps: 0.244938
[13:38:05.089] iteration 18173: total_loss: 0.565770, loss_sup: 0.038913, loss_mps: 0.178653, loss_cps: 0.348204
[13:38:05.236] iteration 18174: total_loss: 0.423758, loss_sup: 0.062623, loss_mps: 0.128639, loss_cps: 0.232495
[13:38:05.383] iteration 18175: total_loss: 1.279760, loss_sup: 0.096971, loss_mps: 0.363912, loss_cps: 0.818877
[13:38:05.530] iteration 18176: total_loss: 0.336021, loss_sup: 0.038726, loss_mps: 0.109910, loss_cps: 0.187386
[13:38:05.676] iteration 18177: total_loss: 0.636364, loss_sup: 0.083887, loss_mps: 0.194969, loss_cps: 0.357509
[13:38:05.822] iteration 18178: total_loss: 0.702937, loss_sup: 0.242883, loss_mps: 0.154228, loss_cps: 0.305826
[13:38:05.968] iteration 18179: total_loss: 0.541227, loss_sup: 0.057982, loss_mps: 0.159045, loss_cps: 0.324200
[13:38:06.116] iteration 18180: total_loss: 0.583476, loss_sup: 0.045813, loss_mps: 0.173751, loss_cps: 0.363913
[13:38:06.266] iteration 18181: total_loss: 0.407655, loss_sup: 0.054684, loss_mps: 0.125042, loss_cps: 0.227929
[13:38:06.412] iteration 18182: total_loss: 0.401151, loss_sup: 0.043991, loss_mps: 0.132246, loss_cps: 0.224913
[13:38:06.559] iteration 18183: total_loss: 0.608569, loss_sup: 0.042744, loss_mps: 0.187529, loss_cps: 0.378296
[13:38:06.705] iteration 18184: total_loss: 0.314695, loss_sup: 0.054707, loss_mps: 0.089989, loss_cps: 0.169999
[13:38:06.851] iteration 18185: total_loss: 0.271149, loss_sup: 0.066865, loss_mps: 0.078417, loss_cps: 0.125867
[13:38:06.996] iteration 18186: total_loss: 0.696364, loss_sup: 0.271848, loss_mps: 0.129428, loss_cps: 0.295088
[13:38:07.146] iteration 18187: total_loss: 0.384863, loss_sup: 0.029135, loss_mps: 0.128909, loss_cps: 0.226819
[13:38:07.295] iteration 18188: total_loss: 0.487169, loss_sup: 0.102484, loss_mps: 0.135731, loss_cps: 0.248954
[13:38:07.441] iteration 18189: total_loss: 0.268319, loss_sup: 0.015509, loss_mps: 0.091314, loss_cps: 0.161496
[13:38:07.587] iteration 18190: total_loss: 0.485120, loss_sup: 0.071370, loss_mps: 0.143843, loss_cps: 0.269907
[13:38:07.733] iteration 18191: total_loss: 0.538693, loss_sup: 0.157002, loss_mps: 0.140414, loss_cps: 0.241278
[13:38:07.879] iteration 18192: total_loss: 0.304369, loss_sup: 0.062356, loss_mps: 0.092308, loss_cps: 0.149705
[13:38:08.025] iteration 18193: total_loss: 0.547228, loss_sup: 0.050341, loss_mps: 0.166032, loss_cps: 0.330855
[13:38:08.172] iteration 18194: total_loss: 0.496928, loss_sup: 0.083395, loss_mps: 0.142427, loss_cps: 0.271106
[13:38:08.318] iteration 18195: total_loss: 0.933605, loss_sup: 0.047028, loss_mps: 0.276243, loss_cps: 0.610334
[13:38:08.464] iteration 18196: total_loss: 0.534011, loss_sup: 0.044220, loss_mps: 0.172649, loss_cps: 0.317142
[13:38:08.610] iteration 18197: total_loss: 0.581838, loss_sup: 0.166059, loss_mps: 0.148917, loss_cps: 0.266863
[13:38:08.756] iteration 18198: total_loss: 0.630506, loss_sup: 0.013534, loss_mps: 0.203744, loss_cps: 0.413228
[13:38:08.902] iteration 18199: total_loss: 0.785499, loss_sup: 0.059811, loss_mps: 0.238182, loss_cps: 0.487507
[13:38:09.047] iteration 18200: total_loss: 0.597558, loss_sup: 0.129278, loss_mps: 0.157927, loss_cps: 0.310353
[13:38:09.048] Evaluation Started ==>
[13:38:20.349] ==> valid iteration 18200: unet metrics: {'dc': 0.6473815228489094, 'jc': 0.5314773001294038, 'pre': 0.791241710865053, 'hd': 5.4711980723096065}, ynet metrics: {'dc': 0.5728973630437425, 'jc': 0.4606877738758327, 'pre': 0.7764194103922241, 'hd': 5.596616904834868}.
[13:38:20.351] Evaluation Finished!⏹️
[13:38:20.501] iteration 18201: total_loss: 0.469534, loss_sup: 0.061104, loss_mps: 0.138650, loss_cps: 0.269780
[13:38:20.647] iteration 18202: total_loss: 0.508422, loss_sup: 0.025960, loss_mps: 0.161398, loss_cps: 0.321065
[13:38:20.793] iteration 18203: total_loss: 0.604172, loss_sup: 0.090414, loss_mps: 0.165713, loss_cps: 0.348045
[13:38:20.938] iteration 18204: total_loss: 0.410122, loss_sup: 0.100807, loss_mps: 0.111748, loss_cps: 0.197568
[13:38:21.083] iteration 18205: total_loss: 0.814082, loss_sup: 0.099041, loss_mps: 0.222484, loss_cps: 0.492557
[13:38:21.228] iteration 18206: total_loss: 0.453090, loss_sup: 0.044883, loss_mps: 0.139737, loss_cps: 0.268470
[13:38:21.373] iteration 18207: total_loss: 0.695669, loss_sup: 0.131188, loss_mps: 0.190470, loss_cps: 0.374011
[13:38:21.518] iteration 18208: total_loss: 0.658398, loss_sup: 0.039899, loss_mps: 0.195936, loss_cps: 0.422562
[13:38:21.663] iteration 18209: total_loss: 0.478877, loss_sup: 0.103940, loss_mps: 0.131487, loss_cps: 0.243450
[13:38:21.809] iteration 18210: total_loss: 0.425415, loss_sup: 0.034011, loss_mps: 0.136739, loss_cps: 0.254665
[13:38:21.954] iteration 18211: total_loss: 0.496004, loss_sup: 0.027478, loss_mps: 0.160339, loss_cps: 0.308187
[13:38:22.099] iteration 18212: total_loss: 0.377719, loss_sup: 0.011613, loss_mps: 0.131725, loss_cps: 0.234381
[13:38:22.245] iteration 18213: total_loss: 0.673773, loss_sup: 0.096390, loss_mps: 0.188725, loss_cps: 0.388659
[13:38:22.392] iteration 18214: total_loss: 0.505536, loss_sup: 0.041745, loss_mps: 0.156914, loss_cps: 0.306877
[13:38:22.537] iteration 18215: total_loss: 0.636782, loss_sup: 0.142580, loss_mps: 0.161596, loss_cps: 0.332606
[13:38:22.683] iteration 18216: total_loss: 0.422333, loss_sup: 0.085694, loss_mps: 0.119965, loss_cps: 0.216674
[13:38:22.828] iteration 18217: total_loss: 0.541522, loss_sup: 0.189856, loss_mps: 0.123597, loss_cps: 0.228069
[13:38:22.973] iteration 18218: total_loss: 0.559488, loss_sup: 0.025660, loss_mps: 0.180417, loss_cps: 0.353412
[13:38:23.118] iteration 18219: total_loss: 0.595000, loss_sup: 0.057257, loss_mps: 0.178230, loss_cps: 0.359513
[13:38:23.263] iteration 18220: total_loss: 0.430909, loss_sup: 0.078456, loss_mps: 0.128887, loss_cps: 0.223566
[13:38:23.410] iteration 18221: total_loss: 0.693234, loss_sup: 0.175731, loss_mps: 0.167064, loss_cps: 0.350439
[13:38:23.554] iteration 18222: total_loss: 0.585368, loss_sup: 0.054601, loss_mps: 0.174429, loss_cps: 0.356338
[13:38:23.699] iteration 18223: total_loss: 0.630290, loss_sup: 0.078445, loss_mps: 0.194841, loss_cps: 0.357004
[13:38:23.845] iteration 18224: total_loss: 0.271363, loss_sup: 0.004637, loss_mps: 0.096241, loss_cps: 0.170485
[13:38:23.992] iteration 18225: total_loss: 0.696642, loss_sup: 0.074371, loss_mps: 0.209680, loss_cps: 0.412591
[13:38:24.137] iteration 18226: total_loss: 0.482567, loss_sup: 0.047694, loss_mps: 0.146295, loss_cps: 0.288578
[13:38:24.284] iteration 18227: total_loss: 0.386455, loss_sup: 0.130276, loss_mps: 0.095629, loss_cps: 0.160550
[13:38:24.432] iteration 18228: total_loss: 0.498913, loss_sup: 0.122490, loss_mps: 0.130484, loss_cps: 0.245939
[13:38:24.578] iteration 18229: total_loss: 0.506986, loss_sup: 0.165646, loss_mps: 0.120867, loss_cps: 0.220474
[13:38:24.726] iteration 18230: total_loss: 1.057264, loss_sup: 0.521731, loss_mps: 0.174599, loss_cps: 0.360934
[13:38:24.872] iteration 18231: total_loss: 0.382610, loss_sup: 0.063917, loss_mps: 0.114928, loss_cps: 0.203765
[13:38:25.021] iteration 18232: total_loss: 0.397732, loss_sup: 0.042842, loss_mps: 0.129043, loss_cps: 0.225847
[13:38:25.167] iteration 18233: total_loss: 1.529843, loss_sup: 0.114273, loss_mps: 0.420465, loss_cps: 0.995104
[13:38:25.313] iteration 18234: total_loss: 0.890984, loss_sup: 0.337754, loss_mps: 0.185062, loss_cps: 0.368167
[13:38:25.459] iteration 18235: total_loss: 0.363236, loss_sup: 0.006740, loss_mps: 0.123224, loss_cps: 0.233273
[13:38:25.605] iteration 18236: total_loss: 0.436353, loss_sup: 0.087251, loss_mps: 0.120841, loss_cps: 0.228261
[13:38:25.751] iteration 18237: total_loss: 0.304191, loss_sup: 0.013138, loss_mps: 0.108927, loss_cps: 0.182127
[13:38:25.901] iteration 18238: total_loss: 0.701299, loss_sup: 0.010946, loss_mps: 0.208821, loss_cps: 0.481532
[13:38:26.050] iteration 18239: total_loss: 0.342404, loss_sup: 0.039792, loss_mps: 0.115885, loss_cps: 0.186727
[13:38:26.196] iteration 18240: total_loss: 0.338713, loss_sup: 0.003101, loss_mps: 0.116560, loss_cps: 0.219052
[13:38:26.343] iteration 18241: total_loss: 0.427618, loss_sup: 0.061809, loss_mps: 0.127598, loss_cps: 0.238212
[13:38:26.489] iteration 18242: total_loss: 0.773439, loss_sup: 0.096659, loss_mps: 0.207210, loss_cps: 0.469569
[13:38:26.635] iteration 18243: total_loss: 0.488554, loss_sup: 0.112204, loss_mps: 0.126915, loss_cps: 0.249436
[13:38:26.781] iteration 18244: total_loss: 0.351621, loss_sup: 0.076036, loss_mps: 0.107976, loss_cps: 0.167608
[13:38:26.927] iteration 18245: total_loss: 0.539762, loss_sup: 0.049068, loss_mps: 0.163386, loss_cps: 0.327307
[13:38:27.073] iteration 18246: total_loss: 0.422927, loss_sup: 0.019907, loss_mps: 0.148644, loss_cps: 0.254375
[13:38:27.219] iteration 18247: total_loss: 0.609104, loss_sup: 0.198473, loss_mps: 0.150750, loss_cps: 0.259881
[13:38:27.366] iteration 18248: total_loss: 0.409016, loss_sup: 0.099809, loss_mps: 0.119685, loss_cps: 0.189522
[13:38:27.512] iteration 18249: total_loss: 0.434077, loss_sup: 0.118661, loss_mps: 0.112091, loss_cps: 0.203324
[13:38:27.658] iteration 18250: total_loss: 0.459541, loss_sup: 0.136261, loss_mps: 0.119224, loss_cps: 0.204056
[13:38:27.805] iteration 18251: total_loss: 0.589704, loss_sup: 0.024338, loss_mps: 0.189771, loss_cps: 0.375595
[13:38:27.952] iteration 18252: total_loss: 0.439410, loss_sup: 0.049812, loss_mps: 0.135004, loss_cps: 0.254594
[13:38:28.098] iteration 18253: total_loss: 0.534057, loss_sup: 0.049274, loss_mps: 0.165976, loss_cps: 0.318807
[13:38:28.245] iteration 18254: total_loss: 0.383645, loss_sup: 0.030347, loss_mps: 0.133242, loss_cps: 0.220056
[13:38:28.391] iteration 18255: total_loss: 0.695322, loss_sup: 0.119388, loss_mps: 0.196129, loss_cps: 0.379806
[13:38:28.538] iteration 18256: total_loss: 0.836209, loss_sup: 0.044102, loss_mps: 0.269090, loss_cps: 0.523017
[13:38:28.687] iteration 18257: total_loss: 0.446136, loss_sup: 0.037771, loss_mps: 0.145387, loss_cps: 0.262977
[13:38:28.836] iteration 18258: total_loss: 0.451617, loss_sup: 0.025269, loss_mps: 0.140706, loss_cps: 0.285642
[13:38:28.983] iteration 18259: total_loss: 0.864564, loss_sup: 0.095311, loss_mps: 0.253882, loss_cps: 0.515371
[13:38:29.130] iteration 18260: total_loss: 0.287929, loss_sup: 0.018689, loss_mps: 0.103932, loss_cps: 0.165308
[13:38:29.275] iteration 18261: total_loss: 0.628057, loss_sup: 0.200393, loss_mps: 0.149526, loss_cps: 0.278139
[13:38:29.421] iteration 18262: total_loss: 0.531323, loss_sup: 0.038170, loss_mps: 0.168896, loss_cps: 0.324257
[13:38:29.566] iteration 18263: total_loss: 0.764386, loss_sup: 0.156265, loss_mps: 0.196159, loss_cps: 0.411963
[13:38:29.712] iteration 18264: total_loss: 0.322191, loss_sup: 0.063467, loss_mps: 0.096270, loss_cps: 0.162454
[13:38:29.858] iteration 18265: total_loss: 0.700525, loss_sup: 0.063484, loss_mps: 0.208836, loss_cps: 0.428205
[13:38:30.005] iteration 18266: total_loss: 0.617055, loss_sup: 0.094877, loss_mps: 0.184675, loss_cps: 0.337503
[13:38:30.151] iteration 18267: total_loss: 0.310666, loss_sup: 0.010544, loss_mps: 0.108417, loss_cps: 0.191705
[13:38:30.300] iteration 18268: total_loss: 0.545858, loss_sup: 0.027292, loss_mps: 0.175766, loss_cps: 0.342801
[13:38:30.447] iteration 18269: total_loss: 0.809055, loss_sup: 0.328093, loss_mps: 0.154162, loss_cps: 0.326799
[13:38:30.592] iteration 18270: total_loss: 0.935186, loss_sup: 0.294917, loss_mps: 0.215582, loss_cps: 0.424688
[13:38:30.738] iteration 18271: total_loss: 0.511697, loss_sup: 0.113773, loss_mps: 0.136940, loss_cps: 0.260983
[13:38:30.884] iteration 18272: total_loss: 0.473224, loss_sup: 0.085239, loss_mps: 0.133614, loss_cps: 0.254370
[13:38:31.031] iteration 18273: total_loss: 0.789647, loss_sup: 0.152724, loss_mps: 0.199220, loss_cps: 0.437703
[13:38:31.177] iteration 18274: total_loss: 0.671998, loss_sup: 0.064629, loss_mps: 0.196909, loss_cps: 0.410460
[13:38:31.323] iteration 18275: total_loss: 0.394646, loss_sup: 0.050616, loss_mps: 0.126197, loss_cps: 0.217833
[13:38:31.468] iteration 18276: total_loss: 0.708858, loss_sup: 0.145834, loss_mps: 0.189479, loss_cps: 0.373545
[13:38:31.614] iteration 18277: total_loss: 0.349128, loss_sup: 0.034395, loss_mps: 0.115334, loss_cps: 0.199398
[13:38:31.760] iteration 18278: total_loss: 0.507906, loss_sup: 0.138656, loss_mps: 0.141394, loss_cps: 0.227857
[13:38:31.905] iteration 18279: total_loss: 0.372366, loss_sup: 0.031341, loss_mps: 0.121963, loss_cps: 0.219062
[13:38:32.055] iteration 18280: total_loss: 0.570320, loss_sup: 0.027042, loss_mps: 0.177590, loss_cps: 0.365688
[13:38:32.201] iteration 18281: total_loss: 0.938007, loss_sup: 0.014918, loss_mps: 0.294174, loss_cps: 0.628915
[13:38:32.347] iteration 18282: total_loss: 0.527273, loss_sup: 0.089375, loss_mps: 0.151444, loss_cps: 0.286454
[13:38:32.492] iteration 18283: total_loss: 0.654304, loss_sup: 0.127192, loss_mps: 0.176895, loss_cps: 0.350217
[13:38:32.638] iteration 18284: total_loss: 0.427960, loss_sup: 0.078913, loss_mps: 0.127157, loss_cps: 0.221890
[13:38:32.784] iteration 18285: total_loss: 0.604464, loss_sup: 0.056314, loss_mps: 0.178362, loss_cps: 0.369788
[13:38:32.930] iteration 18286: total_loss: 1.115696, loss_sup: 0.309708, loss_mps: 0.242952, loss_cps: 0.563035
[13:38:33.077] iteration 18287: total_loss: 0.831521, loss_sup: 0.023179, loss_mps: 0.250883, loss_cps: 0.557459
[13:38:33.224] iteration 18288: total_loss: 0.371056, loss_sup: 0.007446, loss_mps: 0.124319, loss_cps: 0.239291
[13:38:33.370] iteration 18289: total_loss: 0.360697, loss_sup: 0.036684, loss_mps: 0.121514, loss_cps: 0.202498
[13:38:33.517] iteration 18290: total_loss: 0.531303, loss_sup: 0.081565, loss_mps: 0.156371, loss_cps: 0.293367
[13:38:33.663] iteration 18291: total_loss: 0.372849, loss_sup: 0.063939, loss_mps: 0.104756, loss_cps: 0.204154
[13:38:33.809] iteration 18292: total_loss: 0.717404, loss_sup: 0.264584, loss_mps: 0.159371, loss_cps: 0.293450
[13:38:33.957] iteration 18293: total_loss: 0.459642, loss_sup: 0.016195, loss_mps: 0.153905, loss_cps: 0.289542
[13:38:34.104] iteration 18294: total_loss: 0.611246, loss_sup: 0.108187, loss_mps: 0.155287, loss_cps: 0.347773
[13:38:34.250] iteration 18295: total_loss: 0.450551, loss_sup: 0.096708, loss_mps: 0.124642, loss_cps: 0.229201
[13:38:34.396] iteration 18296: total_loss: 0.559637, loss_sup: 0.022539, loss_mps: 0.175578, loss_cps: 0.361520
[13:38:34.541] iteration 18297: total_loss: 0.695695, loss_sup: 0.212361, loss_mps: 0.168393, loss_cps: 0.314940
[13:38:34.687] iteration 18298: total_loss: 0.810459, loss_sup: 0.063837, loss_mps: 0.235174, loss_cps: 0.511448
[13:38:34.833] iteration 18299: total_loss: 0.564830, loss_sup: 0.039838, loss_mps: 0.167316, loss_cps: 0.357676
[13:38:34.979] iteration 18300: total_loss: 0.383738, loss_sup: 0.037444, loss_mps: 0.128982, loss_cps: 0.217312
[13:38:34.979] Evaluation Started ==>
[13:38:46.311] ==> valid iteration 18300: unet metrics: {'dc': 0.6468885993316441, 'jc': 0.5286704385113864, 'pre': 0.7763442560361263, 'hd': 5.6235350586564214}, ynet metrics: {'dc': 0.6111544364135127, 'jc': 0.49307160835772806, 'pre': 0.7911432108895067, 'hd': 5.710203109094069}.
[13:38:46.313] Evaluation Finished!⏹️
[13:38:46.465] iteration 18301: total_loss: 1.029887, loss_sup: 0.156071, loss_mps: 0.269435, loss_cps: 0.604381
[13:38:46.612] iteration 18302: total_loss: 0.413004, loss_sup: 0.049956, loss_mps: 0.134282, loss_cps: 0.228766
[13:38:46.758] iteration 18303: total_loss: 0.585436, loss_sup: 0.024404, loss_mps: 0.186280, loss_cps: 0.374752
[13:38:46.903] iteration 18304: total_loss: 0.325641, loss_sup: 0.039474, loss_mps: 0.104490, loss_cps: 0.181677
[13:38:47.048] iteration 18305: total_loss: 0.579255, loss_sup: 0.072927, loss_mps: 0.175495, loss_cps: 0.330834
[13:38:47.193] iteration 18306: total_loss: 0.541703, loss_sup: 0.183440, loss_mps: 0.123672, loss_cps: 0.234590
[13:38:47.339] iteration 18307: total_loss: 0.616317, loss_sup: 0.120091, loss_mps: 0.166972, loss_cps: 0.329254
[13:38:47.485] iteration 18308: total_loss: 0.633358, loss_sup: 0.007251, loss_mps: 0.195393, loss_cps: 0.430714
[13:38:47.631] iteration 18309: total_loss: 0.509936, loss_sup: 0.025444, loss_mps: 0.172132, loss_cps: 0.312360
[13:38:47.776] iteration 18310: total_loss: 0.402500, loss_sup: 0.016255, loss_mps: 0.134569, loss_cps: 0.251676
[13:38:47.921] iteration 18311: total_loss: 0.519116, loss_sup: 0.017274, loss_mps: 0.167962, loss_cps: 0.333880
[13:38:48.067] iteration 18312: total_loss: 0.770124, loss_sup: 0.126413, loss_mps: 0.212823, loss_cps: 0.430888
[13:38:48.217] iteration 18313: total_loss: 0.527625, loss_sup: 0.066152, loss_mps: 0.151640, loss_cps: 0.309834
[13:38:48.363] iteration 18314: total_loss: 0.320939, loss_sup: 0.007987, loss_mps: 0.113761, loss_cps: 0.199191
[13:38:48.508] iteration 18315: total_loss: 0.409842, loss_sup: 0.012683, loss_mps: 0.152542, loss_cps: 0.244618
[13:38:48.654] iteration 18316: total_loss: 0.276548, loss_sup: 0.011713, loss_mps: 0.095139, loss_cps: 0.169695
[13:38:48.800] iteration 18317: total_loss: 0.482161, loss_sup: 0.029814, loss_mps: 0.159243, loss_cps: 0.293104
[13:38:48.945] iteration 18318: total_loss: 0.635136, loss_sup: 0.047608, loss_mps: 0.197044, loss_cps: 0.390484
[13:38:49.090] iteration 18319: total_loss: 0.623005, loss_sup: 0.156376, loss_mps: 0.156061, loss_cps: 0.310568
[13:38:49.236] iteration 18320: total_loss: 0.397948, loss_sup: 0.065823, loss_mps: 0.123885, loss_cps: 0.208240
[13:38:49.382] iteration 18321: total_loss: 0.537597, loss_sup: 0.080796, loss_mps: 0.150632, loss_cps: 0.306169
[13:38:49.528] iteration 18322: total_loss: 0.508711, loss_sup: 0.033614, loss_mps: 0.155402, loss_cps: 0.319695
[13:38:49.674] iteration 18323: total_loss: 0.633552, loss_sup: 0.064813, loss_mps: 0.192243, loss_cps: 0.376496
[13:38:49.823] iteration 18324: total_loss: 0.441611, loss_sup: 0.026802, loss_mps: 0.147369, loss_cps: 0.267440
[13:38:49.968] iteration 18325: total_loss: 1.022362, loss_sup: 0.063404, loss_mps: 0.304518, loss_cps: 0.654440
[13:38:50.114] iteration 18326: total_loss: 0.590625, loss_sup: 0.126486, loss_mps: 0.149883, loss_cps: 0.314256
[13:38:50.261] iteration 18327: total_loss: 0.560900, loss_sup: 0.053138, loss_mps: 0.165071, loss_cps: 0.342690
[13:38:50.405] iteration 18328: total_loss: 0.289404, loss_sup: 0.041198, loss_mps: 0.092776, loss_cps: 0.155430
[13:38:50.551] iteration 18329: total_loss: 0.396957, loss_sup: 0.031587, loss_mps: 0.126651, loss_cps: 0.238720
[13:38:50.699] iteration 18330: total_loss: 0.586697, loss_sup: 0.052427, loss_mps: 0.171858, loss_cps: 0.362412
[13:38:50.844] iteration 18331: total_loss: 0.270787, loss_sup: 0.017720, loss_mps: 0.091207, loss_cps: 0.161861
[13:38:50.990] iteration 18332: total_loss: 0.374639, loss_sup: 0.039473, loss_mps: 0.119978, loss_cps: 0.215188
[13:38:51.136] iteration 18333: total_loss: 0.695141, loss_sup: 0.073040, loss_mps: 0.196911, loss_cps: 0.425191
[13:38:51.281] iteration 18334: total_loss: 0.902172, loss_sup: 0.201914, loss_mps: 0.244046, loss_cps: 0.456212
[13:38:51.427] iteration 18335: total_loss: 0.281892, loss_sup: 0.020086, loss_mps: 0.088943, loss_cps: 0.172864
[13:38:51.573] iteration 18336: total_loss: 0.880109, loss_sup: 0.107406, loss_mps: 0.243224, loss_cps: 0.529479
[13:38:51.719] iteration 18337: total_loss: 0.931395, loss_sup: 0.115537, loss_mps: 0.275286, loss_cps: 0.540572
[13:38:51.865] iteration 18338: total_loss: 0.703157, loss_sup: 0.147120, loss_mps: 0.182553, loss_cps: 0.373484
[13:38:52.012] iteration 18339: total_loss: 0.831040, loss_sup: 0.270191, loss_mps: 0.178646, loss_cps: 0.382204
[13:38:52.159] iteration 18340: total_loss: 0.451492, loss_sup: 0.017647, loss_mps: 0.142463, loss_cps: 0.291382
[13:38:52.307] iteration 18341: total_loss: 0.412192, loss_sup: 0.157161, loss_mps: 0.096092, loss_cps: 0.158938
[13:38:52.453] iteration 18342: total_loss: 0.468277, loss_sup: 0.019589, loss_mps: 0.155841, loss_cps: 0.292847
[13:38:52.598] iteration 18343: total_loss: 0.552249, loss_sup: 0.075581, loss_mps: 0.162506, loss_cps: 0.314162
[13:38:52.744] iteration 18344: total_loss: 1.170112, loss_sup: 0.603478, loss_mps: 0.203856, loss_cps: 0.362778
[13:38:52.890] iteration 18345: total_loss: 0.526649, loss_sup: 0.036146, loss_mps: 0.173702, loss_cps: 0.316801
[13:38:53.037] iteration 18346: total_loss: 0.885198, loss_sup: 0.276273, loss_mps: 0.202868, loss_cps: 0.406057
[13:38:53.184] iteration 18347: total_loss: 0.770408, loss_sup: 0.257839, loss_mps: 0.176706, loss_cps: 0.335862
[13:38:53.330] iteration 18348: total_loss: 0.749317, loss_sup: 0.131503, loss_mps: 0.202540, loss_cps: 0.415273
[13:38:53.477] iteration 18349: total_loss: 0.518604, loss_sup: 0.253393, loss_mps: 0.101176, loss_cps: 0.164036
[13:38:53.623] iteration 18350: total_loss: 0.465688, loss_sup: 0.009428, loss_mps: 0.157548, loss_cps: 0.298712
[13:38:53.771] iteration 18351: total_loss: 0.595267, loss_sup: 0.079892, loss_mps: 0.171354, loss_cps: 0.344021
[13:38:53.918] iteration 18352: total_loss: 0.365816, loss_sup: 0.053147, loss_mps: 0.113242, loss_cps: 0.199427
[13:38:54.064] iteration 18353: total_loss: 0.292204, loss_sup: 0.058648, loss_mps: 0.089955, loss_cps: 0.143601
[13:38:54.211] iteration 18354: total_loss: 0.852392, loss_sup: 0.087832, loss_mps: 0.239696, loss_cps: 0.524864
[13:38:54.358] iteration 18355: total_loss: 0.882437, loss_sup: 0.099713, loss_mps: 0.256943, loss_cps: 0.525781
[13:38:54.505] iteration 18356: total_loss: 0.626228, loss_sup: 0.059348, loss_mps: 0.190344, loss_cps: 0.376536
[13:38:54.651] iteration 18357: total_loss: 0.520167, loss_sup: 0.067969, loss_mps: 0.156250, loss_cps: 0.295948
[13:38:54.797] iteration 18358: total_loss: 0.362569, loss_sup: 0.034412, loss_mps: 0.113295, loss_cps: 0.214862
[13:38:54.944] iteration 18359: total_loss: 0.405163, loss_sup: 0.024743, loss_mps: 0.136655, loss_cps: 0.243765
[13:38:55.090] iteration 18360: total_loss: 0.309995, loss_sup: 0.032380, loss_mps: 0.104492, loss_cps: 0.173123
[13:38:55.236] iteration 18361: total_loss: 0.617988, loss_sup: 0.261632, loss_mps: 0.130725, loss_cps: 0.225631
[13:38:55.382] iteration 18362: total_loss: 0.271722, loss_sup: 0.006731, loss_mps: 0.099221, loss_cps: 0.165770
[13:38:55.528] iteration 18363: total_loss: 0.662012, loss_sup: 0.026466, loss_mps: 0.212758, loss_cps: 0.422788
[13:38:55.676] iteration 18364: total_loss: 0.491233, loss_sup: 0.118279, loss_mps: 0.129077, loss_cps: 0.243877
[13:38:55.824] iteration 18365: total_loss: 0.419478, loss_sup: 0.074443, loss_mps: 0.124364, loss_cps: 0.220672
[13:38:55.970] iteration 18366: total_loss: 0.584801, loss_sup: 0.068500, loss_mps: 0.175899, loss_cps: 0.340402
[13:38:56.117] iteration 18367: total_loss: 0.721701, loss_sup: 0.123181, loss_mps: 0.193172, loss_cps: 0.405348
[13:38:56.263] iteration 18368: total_loss: 0.532651, loss_sup: 0.104545, loss_mps: 0.146259, loss_cps: 0.281847
[13:38:56.411] iteration 18369: total_loss: 0.396339, loss_sup: 0.128011, loss_mps: 0.103359, loss_cps: 0.164968
[13:38:56.558] iteration 18370: total_loss: 0.346046, loss_sup: 0.017150, loss_mps: 0.115986, loss_cps: 0.212910
[13:38:56.705] iteration 18371: total_loss: 0.829457, loss_sup: 0.146709, loss_mps: 0.223580, loss_cps: 0.459168
[13:38:56.850] iteration 18372: total_loss: 0.474530, loss_sup: 0.155843, loss_mps: 0.119193, loss_cps: 0.199494
[13:38:56.996] iteration 18373: total_loss: 0.656580, loss_sup: 0.076975, loss_mps: 0.192997, loss_cps: 0.386608
[13:38:57.142] iteration 18374: total_loss: 0.594074, loss_sup: 0.131484, loss_mps: 0.163052, loss_cps: 0.299538
[13:38:57.288] iteration 18375: total_loss: 0.601457, loss_sup: 0.091333, loss_mps: 0.171015, loss_cps: 0.339110
[13:38:57.434] iteration 18376: total_loss: 0.293233, loss_sup: 0.010925, loss_mps: 0.108582, loss_cps: 0.173726
[13:38:57.580] iteration 18377: total_loss: 0.362157, loss_sup: 0.016022, loss_mps: 0.124441, loss_cps: 0.221694
[13:38:57.726] iteration 18378: total_loss: 0.418534, loss_sup: 0.054044, loss_mps: 0.128804, loss_cps: 0.235686
[13:38:57.872] iteration 18379: total_loss: 0.363561, loss_sup: 0.071937, loss_mps: 0.109010, loss_cps: 0.182614
[13:38:58.018] iteration 18380: total_loss: 0.322117, loss_sup: 0.021829, loss_mps: 0.108031, loss_cps: 0.192257
[13:38:58.164] iteration 18381: total_loss: 0.880689, loss_sup: 0.204092, loss_mps: 0.220718, loss_cps: 0.455879
[13:38:58.311] iteration 18382: total_loss: 0.735232, loss_sup: 0.063596, loss_mps: 0.219371, loss_cps: 0.452265
[13:38:58.457] iteration 18383: total_loss: 0.540113, loss_sup: 0.074473, loss_mps: 0.156963, loss_cps: 0.308677
[13:38:58.604] iteration 18384: total_loss: 0.438570, loss_sup: 0.007880, loss_mps: 0.148965, loss_cps: 0.281725
[13:38:58.750] iteration 18385: total_loss: 0.475213, loss_sup: 0.046993, loss_mps: 0.137229, loss_cps: 0.290990
[13:38:58.896] iteration 18386: total_loss: 0.559880, loss_sup: 0.078828, loss_mps: 0.156405, loss_cps: 0.324646
[13:38:59.042] iteration 18387: total_loss: 0.627966, loss_sup: 0.226324, loss_mps: 0.136857, loss_cps: 0.264784
[13:38:59.188] iteration 18388: total_loss: 0.605869, loss_sup: 0.117638, loss_mps: 0.157835, loss_cps: 0.330396
[13:38:59.334] iteration 18389: total_loss: 0.478963, loss_sup: 0.056139, loss_mps: 0.135142, loss_cps: 0.287682
[13:38:59.480] iteration 18390: total_loss: 0.701308, loss_sup: 0.109337, loss_mps: 0.191290, loss_cps: 0.400681
[13:38:59.625] iteration 18391: total_loss: 0.761918, loss_sup: 0.122825, loss_mps: 0.196359, loss_cps: 0.442734
[13:38:59.689] iteration 18392: total_loss: 0.324980, loss_sup: 0.001658, loss_mps: 0.109862, loss_cps: 0.213460
[13:39:00.904] iteration 18393: total_loss: 0.333083, loss_sup: 0.060579, loss_mps: 0.098193, loss_cps: 0.174310
[13:39:01.054] iteration 18394: total_loss: 0.442466, loss_sup: 0.020625, loss_mps: 0.145452, loss_cps: 0.276390
[13:39:01.199] iteration 18395: total_loss: 0.392557, loss_sup: 0.023996, loss_mps: 0.125461, loss_cps: 0.243100
[13:39:01.345] iteration 18396: total_loss: 0.444508, loss_sup: 0.038992, loss_mps: 0.140836, loss_cps: 0.264680
[13:39:01.491] iteration 18397: total_loss: 0.474195, loss_sup: 0.022524, loss_mps: 0.154236, loss_cps: 0.297435
[13:39:01.637] iteration 18398: total_loss: 0.492470, loss_sup: 0.136828, loss_mps: 0.127018, loss_cps: 0.228624
[13:39:01.783] iteration 18399: total_loss: 0.600022, loss_sup: 0.163878, loss_mps: 0.140184, loss_cps: 0.295960
[13:39:01.932] iteration 18400: total_loss: 0.382094, loss_sup: 0.079867, loss_mps: 0.109286, loss_cps: 0.192941
[13:39:01.932] Evaluation Started ==>
[13:39:13.313] ==> valid iteration 18400: unet metrics: {'dc': 0.6481725152316223, 'jc': 0.5321098831868946, 'pre': 0.784907326571331, 'hd': 5.506334989869635}, ynet metrics: {'dc': 0.5674575096489946, 'jc': 0.45374672706907176, 'pre': 0.7734120288001147, 'hd': 5.586889095504889}.
[13:39:13.315] Evaluation Finished!⏹️
[13:39:13.465] iteration 18401: total_loss: 0.708125, loss_sup: 0.031287, loss_mps: 0.216529, loss_cps: 0.460310
[13:39:13.612] iteration 18402: total_loss: 0.557067, loss_sup: 0.057797, loss_mps: 0.166804, loss_cps: 0.332466
[13:39:13.763] iteration 18403: total_loss: 0.359944, loss_sup: 0.040184, loss_mps: 0.120092, loss_cps: 0.199668
[13:39:13.911] iteration 18404: total_loss: 0.424399, loss_sup: 0.044902, loss_mps: 0.135826, loss_cps: 0.243671
[13:39:14.058] iteration 18405: total_loss: 0.840320, loss_sup: 0.046760, loss_mps: 0.240216, loss_cps: 0.553344
[13:39:14.204] iteration 18406: total_loss: 0.436454, loss_sup: 0.009882, loss_mps: 0.139305, loss_cps: 0.287267
[13:39:14.349] iteration 18407: total_loss: 0.581729, loss_sup: 0.041117, loss_mps: 0.174488, loss_cps: 0.366125
[13:39:14.496] iteration 18408: total_loss: 0.635251, loss_sup: 0.069093, loss_mps: 0.182862, loss_cps: 0.383296
[13:39:14.642] iteration 18409: total_loss: 0.412650, loss_sup: 0.024543, loss_mps: 0.122774, loss_cps: 0.265334
[13:39:14.788] iteration 18410: total_loss: 0.569016, loss_sup: 0.040440, loss_mps: 0.179509, loss_cps: 0.349067
[13:39:14.938] iteration 18411: total_loss: 0.473797, loss_sup: 0.018675, loss_mps: 0.157647, loss_cps: 0.297475
[13:39:15.086] iteration 18412: total_loss: 0.677716, loss_sup: 0.005052, loss_mps: 0.207135, loss_cps: 0.465529
[13:39:15.232] iteration 18413: total_loss: 0.765041, loss_sup: 0.129562, loss_mps: 0.201199, loss_cps: 0.434280
[13:39:15.379] iteration 18414: total_loss: 0.403926, loss_sup: 0.049560, loss_mps: 0.121833, loss_cps: 0.232532
[13:39:15.524] iteration 18415: total_loss: 0.558148, loss_sup: 0.069621, loss_mps: 0.168590, loss_cps: 0.319937
[13:39:15.670] iteration 18416: total_loss: 0.597121, loss_sup: 0.064852, loss_mps: 0.178513, loss_cps: 0.353756
[13:39:15.817] iteration 18417: total_loss: 0.645269, loss_sup: 0.065905, loss_mps: 0.189435, loss_cps: 0.389929
[13:39:15.964] iteration 18418: total_loss: 1.246426, loss_sup: 0.086345, loss_mps: 0.342865, loss_cps: 0.817216
[13:39:16.111] iteration 18419: total_loss: 0.395374, loss_sup: 0.023075, loss_mps: 0.131867, loss_cps: 0.240432
[13:39:16.257] iteration 18420: total_loss: 0.411930, loss_sup: 0.042424, loss_mps: 0.126073, loss_cps: 0.243432
[13:39:16.403] iteration 18421: total_loss: 0.892124, loss_sup: 0.124609, loss_mps: 0.246294, loss_cps: 0.521222
[13:39:16.555] iteration 18422: total_loss: 0.939690, loss_sup: 0.166206, loss_mps: 0.235429, loss_cps: 0.538054
[13:39:16.701] iteration 18423: total_loss: 0.558098, loss_sup: 0.112906, loss_mps: 0.151629, loss_cps: 0.293564
[13:39:16.847] iteration 18424: total_loss: 0.515426, loss_sup: 0.008824, loss_mps: 0.164136, loss_cps: 0.342466
[13:39:16.993] iteration 18425: total_loss: 2.016316, loss_sup: 0.068612, loss_mps: 0.580745, loss_cps: 1.366959
[13:39:17.139] iteration 18426: total_loss: 0.625327, loss_sup: 0.158843, loss_mps: 0.155561, loss_cps: 0.310923
[13:39:17.285] iteration 18427: total_loss: 1.299062, loss_sup: 0.255078, loss_mps: 0.325437, loss_cps: 0.718548
[13:39:17.431] iteration 18428: total_loss: 0.358221, loss_sup: 0.039691, loss_mps: 0.120279, loss_cps: 0.198251
[13:39:17.577] iteration 18429: total_loss: 0.294630, loss_sup: 0.032425, loss_mps: 0.099442, loss_cps: 0.162763
[13:39:17.722] iteration 18430: total_loss: 1.078088, loss_sup: 0.221105, loss_mps: 0.279630, loss_cps: 0.577353
[13:39:17.868] iteration 18431: total_loss: 0.372494, loss_sup: 0.030441, loss_mps: 0.127333, loss_cps: 0.214721
[13:39:18.014] iteration 18432: total_loss: 0.478863, loss_sup: 0.096762, loss_mps: 0.129407, loss_cps: 0.252694
[13:39:18.160] iteration 18433: total_loss: 0.693531, loss_sup: 0.206884, loss_mps: 0.178283, loss_cps: 0.308363
[13:39:18.307] iteration 18434: total_loss: 0.669058, loss_sup: 0.165301, loss_mps: 0.171871, loss_cps: 0.331886
[13:39:18.452] iteration 18435: total_loss: 0.991996, loss_sup: 0.116573, loss_mps: 0.273481, loss_cps: 0.601942
[13:39:18.599] iteration 18436: total_loss: 0.590721, loss_sup: 0.065688, loss_mps: 0.190492, loss_cps: 0.334541
[13:39:18.748] iteration 18437: total_loss: 0.616852, loss_sup: 0.133855, loss_mps: 0.163788, loss_cps: 0.319209
[13:39:18.893] iteration 18438: total_loss: 0.677528, loss_sup: 0.114568, loss_mps: 0.183810, loss_cps: 0.379150
[13:39:19.040] iteration 18439: total_loss: 0.761260, loss_sup: 0.096156, loss_mps: 0.213551, loss_cps: 0.451553
[13:39:19.187] iteration 18440: total_loss: 0.613031, loss_sup: 0.147344, loss_mps: 0.165960, loss_cps: 0.299726
[13:39:19.332] iteration 18441: total_loss: 0.468061, loss_sup: 0.097377, loss_mps: 0.140166, loss_cps: 0.230518
[13:39:19.478] iteration 18442: total_loss: 0.711537, loss_sup: 0.097118, loss_mps: 0.207643, loss_cps: 0.406776
[13:39:19.626] iteration 18443: total_loss: 0.414657, loss_sup: 0.039143, loss_mps: 0.128414, loss_cps: 0.247100
[13:39:19.772] iteration 18444: total_loss: 0.537232, loss_sup: 0.236703, loss_mps: 0.119215, loss_cps: 0.181314
[13:39:19.919] iteration 18445: total_loss: 0.667393, loss_sup: 0.169529, loss_mps: 0.163427, loss_cps: 0.334437
[13:39:20.065] iteration 18446: total_loss: 0.542454, loss_sup: 0.105057, loss_mps: 0.154257, loss_cps: 0.283140
[13:39:20.212] iteration 18447: total_loss: 0.576312, loss_sup: 0.059835, loss_mps: 0.172855, loss_cps: 0.343622
[13:39:20.360] iteration 18448: total_loss: 0.517502, loss_sup: 0.076610, loss_mps: 0.158574, loss_cps: 0.282319
[13:39:20.506] iteration 18449: total_loss: 0.577804, loss_sup: 0.105164, loss_mps: 0.162941, loss_cps: 0.309698
[13:39:20.653] iteration 18450: total_loss: 0.366226, loss_sup: 0.029806, loss_mps: 0.121862, loss_cps: 0.214558
[13:39:20.799] iteration 18451: total_loss: 0.799961, loss_sup: 0.220204, loss_mps: 0.186957, loss_cps: 0.392800
[13:39:20.948] iteration 18452: total_loss: 0.441577, loss_sup: 0.096483, loss_mps: 0.124765, loss_cps: 0.220329
[13:39:21.094] iteration 18453: total_loss: 0.548657, loss_sup: 0.085497, loss_mps: 0.156821, loss_cps: 0.306339
[13:39:21.240] iteration 18454: total_loss: 0.544640, loss_sup: 0.141716, loss_mps: 0.146387, loss_cps: 0.256537
[13:39:21.385] iteration 18455: total_loss: 0.345219, loss_sup: 0.043147, loss_mps: 0.113883, loss_cps: 0.188190
[13:39:21.532] iteration 18456: total_loss: 0.436226, loss_sup: 0.088403, loss_mps: 0.123273, loss_cps: 0.224551
[13:39:21.678] iteration 18457: total_loss: 0.292530, loss_sup: 0.006346, loss_mps: 0.105120, loss_cps: 0.181064
[13:39:21.824] iteration 18458: total_loss: 0.506559, loss_sup: 0.097833, loss_mps: 0.136276, loss_cps: 0.272451
[13:39:21.970] iteration 18459: total_loss: 0.500417, loss_sup: 0.072523, loss_mps: 0.151785, loss_cps: 0.276109
[13:39:22.117] iteration 18460: total_loss: 0.627562, loss_sup: 0.177460, loss_mps: 0.165547, loss_cps: 0.284555
[13:39:22.268] iteration 18461: total_loss: 0.622317, loss_sup: 0.130655, loss_mps: 0.167164, loss_cps: 0.324498
[13:39:22.415] iteration 18462: total_loss: 1.076155, loss_sup: 0.129679, loss_mps: 0.307968, loss_cps: 0.638509
[13:39:22.564] iteration 18463: total_loss: 0.559835, loss_sup: 0.088670, loss_mps: 0.166516, loss_cps: 0.304650
[13:39:22.711] iteration 18464: total_loss: 0.740591, loss_sup: 0.117786, loss_mps: 0.209274, loss_cps: 0.413531
[13:39:22.858] iteration 18465: total_loss: 0.742716, loss_sup: 0.021926, loss_mps: 0.233536, loss_cps: 0.487254
[13:39:23.004] iteration 18466: total_loss: 0.419316, loss_sup: 0.031562, loss_mps: 0.131042, loss_cps: 0.256712
[13:39:23.150] iteration 18467: total_loss: 0.250094, loss_sup: 0.018752, loss_mps: 0.091080, loss_cps: 0.140261
[13:39:23.295] iteration 18468: total_loss: 0.343848, loss_sup: 0.006928, loss_mps: 0.117134, loss_cps: 0.219786
[13:39:23.441] iteration 18469: total_loss: 0.584493, loss_sup: 0.053148, loss_mps: 0.193745, loss_cps: 0.337600
[13:39:23.589] iteration 18470: total_loss: 0.619164, loss_sup: 0.079558, loss_mps: 0.181375, loss_cps: 0.358231
[13:39:23.734] iteration 18471: total_loss: 0.350111, loss_sup: 0.028453, loss_mps: 0.111261, loss_cps: 0.210397
[13:39:23.881] iteration 18472: total_loss: 0.421762, loss_sup: 0.012856, loss_mps: 0.144572, loss_cps: 0.264334
[13:39:24.027] iteration 18473: total_loss: 0.566180, loss_sup: 0.044789, loss_mps: 0.170248, loss_cps: 0.351143
[13:39:24.173] iteration 18474: total_loss: 0.616862, loss_sup: 0.040181, loss_mps: 0.189584, loss_cps: 0.387097
[13:39:24.321] iteration 18475: total_loss: 0.709817, loss_sup: 0.103117, loss_mps: 0.195206, loss_cps: 0.411494
[13:39:24.467] iteration 18476: total_loss: 1.283363, loss_sup: 0.253620, loss_mps: 0.326296, loss_cps: 0.703447
[13:39:24.613] iteration 18477: total_loss: 0.347732, loss_sup: 0.055457, loss_mps: 0.111091, loss_cps: 0.181184
[13:39:24.759] iteration 18478: total_loss: 0.387599, loss_sup: 0.030272, loss_mps: 0.121688, loss_cps: 0.235639
[13:39:24.905] iteration 18479: total_loss: 0.640771, loss_sup: 0.062022, loss_mps: 0.180758, loss_cps: 0.397991
[13:39:25.051] iteration 18480: total_loss: 0.397248, loss_sup: 0.089556, loss_mps: 0.104729, loss_cps: 0.202964
[13:39:25.196] iteration 18481: total_loss: 0.283235, loss_sup: 0.017929, loss_mps: 0.097828, loss_cps: 0.167477
[13:39:25.344] iteration 18482: total_loss: 0.785707, loss_sup: 0.158752, loss_mps: 0.226696, loss_cps: 0.400259
[13:39:25.491] iteration 18483: total_loss: 0.761516, loss_sup: 0.110354, loss_mps: 0.206346, loss_cps: 0.444817
[13:39:25.637] iteration 18484: total_loss: 0.438147, loss_sup: 0.018676, loss_mps: 0.147934, loss_cps: 0.271538
[13:39:25.783] iteration 18485: total_loss: 0.504152, loss_sup: 0.041247, loss_mps: 0.155418, loss_cps: 0.307487
[13:39:25.929] iteration 18486: total_loss: 0.609222, loss_sup: 0.160660, loss_mps: 0.150469, loss_cps: 0.298093
[13:39:26.079] iteration 18487: total_loss: 0.784079, loss_sup: 0.276659, loss_mps: 0.175394, loss_cps: 0.332027
[13:39:26.227] iteration 18488: total_loss: 0.483166, loss_sup: 0.088981, loss_mps: 0.133028, loss_cps: 0.261157
[13:39:26.373] iteration 18489: total_loss: 0.786192, loss_sup: 0.317781, loss_mps: 0.166581, loss_cps: 0.301831
[13:39:26.520] iteration 18490: total_loss: 0.559059, loss_sup: 0.006780, loss_mps: 0.175687, loss_cps: 0.376593
[13:39:26.667] iteration 18491: total_loss: 0.423708, loss_sup: 0.011938, loss_mps: 0.138863, loss_cps: 0.272907
[13:39:26.813] iteration 18492: total_loss: 0.668787, loss_sup: 0.106741, loss_mps: 0.174592, loss_cps: 0.387453
[13:39:26.960] iteration 18493: total_loss: 0.562522, loss_sup: 0.106926, loss_mps: 0.156438, loss_cps: 0.299158
[13:39:27.106] iteration 18494: total_loss: 0.600646, loss_sup: 0.184307, loss_mps: 0.140037, loss_cps: 0.276302
[13:39:27.252] iteration 18495: total_loss: 0.518128, loss_sup: 0.035526, loss_mps: 0.165875, loss_cps: 0.316728
[13:39:27.398] iteration 18496: total_loss: 0.305544, loss_sup: 0.030755, loss_mps: 0.096779, loss_cps: 0.178010
[13:39:27.545] iteration 18497: total_loss: 0.380022, loss_sup: 0.026916, loss_mps: 0.123899, loss_cps: 0.229207
[13:39:27.691] iteration 18498: total_loss: 0.653026, loss_sup: 0.064501, loss_mps: 0.197221, loss_cps: 0.391304
[13:39:27.837] iteration 18499: total_loss: 0.439791, loss_sup: 0.054444, loss_mps: 0.131161, loss_cps: 0.254187
[13:39:27.983] iteration 18500: total_loss: 0.599959, loss_sup: 0.146623, loss_mps: 0.161365, loss_cps: 0.291970
[13:39:27.983] Evaluation Started ==>
[13:39:39.330] ==> valid iteration 18500: unet metrics: {'dc': 0.6375841518333156, 'jc': 0.5218348893693857, 'pre': 0.7507621500389496, 'hd': 5.5664399584558195}, ynet metrics: {'dc': 0.5910521828911276, 'jc': 0.47664022391125005, 'pre': 0.7630477634754137, 'hd': 5.817744046392953}.
[13:39:39.332] Evaluation Finished!⏹️
[13:39:39.482] iteration 18501: total_loss: 0.693228, loss_sup: 0.318529, loss_mps: 0.141317, loss_cps: 0.233382
[13:39:39.634] iteration 18502: total_loss: 0.535861, loss_sup: 0.086980, loss_mps: 0.158678, loss_cps: 0.290203
[13:39:39.780] iteration 18503: total_loss: 0.384796, loss_sup: 0.005642, loss_mps: 0.138164, loss_cps: 0.240990
[13:39:39.925] iteration 18504: total_loss: 0.468711, loss_sup: 0.067044, loss_mps: 0.132385, loss_cps: 0.269283
[13:39:40.070] iteration 18505: total_loss: 0.435462, loss_sup: 0.056200, loss_mps: 0.130117, loss_cps: 0.249145
[13:39:40.214] iteration 18506: total_loss: 0.367374, loss_sup: 0.053607, loss_mps: 0.112154, loss_cps: 0.201613
[13:39:40.359] iteration 18507: total_loss: 0.436313, loss_sup: 0.037096, loss_mps: 0.140154, loss_cps: 0.259063
[13:39:40.504] iteration 18508: total_loss: 0.419657, loss_sup: 0.076033, loss_mps: 0.118740, loss_cps: 0.224884
[13:39:40.649] iteration 18509: total_loss: 0.304126, loss_sup: 0.043765, loss_mps: 0.100327, loss_cps: 0.160034
[13:39:40.795] iteration 18510: total_loss: 0.294948, loss_sup: 0.016554, loss_mps: 0.107286, loss_cps: 0.171108
[13:39:40.940] iteration 18511: total_loss: 0.428078, loss_sup: 0.009358, loss_mps: 0.138494, loss_cps: 0.280226
[13:39:41.086] iteration 18512: total_loss: 0.696625, loss_sup: 0.105051, loss_mps: 0.198316, loss_cps: 0.393258
[13:39:41.231] iteration 18513: total_loss: 0.345179, loss_sup: 0.081099, loss_mps: 0.098083, loss_cps: 0.165997
[13:39:41.377] iteration 18514: total_loss: 0.440715, loss_sup: 0.017400, loss_mps: 0.139267, loss_cps: 0.284048
[13:39:41.522] iteration 18515: total_loss: 0.437789, loss_sup: 0.021505, loss_mps: 0.143206, loss_cps: 0.273078
[13:39:41.669] iteration 18516: total_loss: 0.443059, loss_sup: 0.006543, loss_mps: 0.145679, loss_cps: 0.290837
[13:39:41.815] iteration 18517: total_loss: 0.857791, loss_sup: 0.066044, loss_mps: 0.248688, loss_cps: 0.543059
[13:39:41.963] iteration 18518: total_loss: 0.685442, loss_sup: 0.054503, loss_mps: 0.206097, loss_cps: 0.424842
[13:39:42.108] iteration 18519: total_loss: 0.515362, loss_sup: 0.032046, loss_mps: 0.167466, loss_cps: 0.315850
[13:39:42.253] iteration 18520: total_loss: 0.701692, loss_sup: 0.080482, loss_mps: 0.193057, loss_cps: 0.428153
[13:39:42.400] iteration 18521: total_loss: 0.566649, loss_sup: 0.134445, loss_mps: 0.148159, loss_cps: 0.284044
[13:39:42.548] iteration 18522: total_loss: 0.758238, loss_sup: 0.078972, loss_mps: 0.218292, loss_cps: 0.460974
[13:39:42.694] iteration 18523: total_loss: 0.546903, loss_sup: 0.120926, loss_mps: 0.143008, loss_cps: 0.282968
[13:39:42.839] iteration 18524: total_loss: 0.567810, loss_sup: 0.045654, loss_mps: 0.173483, loss_cps: 0.348674
[13:39:42.985] iteration 18525: total_loss: 0.404212, loss_sup: 0.065786, loss_mps: 0.121041, loss_cps: 0.217385
[13:39:43.130] iteration 18526: total_loss: 0.655933, loss_sup: 0.082334, loss_mps: 0.174487, loss_cps: 0.399112
[13:39:43.275] iteration 18527: total_loss: 0.445728, loss_sup: 0.041585, loss_mps: 0.133249, loss_cps: 0.270894
[13:39:43.421] iteration 18528: total_loss: 0.482105, loss_sup: 0.044537, loss_mps: 0.147342, loss_cps: 0.290226
[13:39:43.567] iteration 18529: total_loss: 0.467334, loss_sup: 0.075826, loss_mps: 0.123612, loss_cps: 0.267896
[13:39:43.713] iteration 18530: total_loss: 0.374757, loss_sup: 0.028118, loss_mps: 0.119257, loss_cps: 0.227382
[13:39:43.859] iteration 18531: total_loss: 0.411088, loss_sup: 0.085129, loss_mps: 0.110420, loss_cps: 0.215539
[13:39:44.005] iteration 18532: total_loss: 0.598792, loss_sup: 0.062573, loss_mps: 0.173744, loss_cps: 0.362475
[13:39:44.152] iteration 18533: total_loss: 0.635861, loss_sup: 0.204445, loss_mps: 0.142186, loss_cps: 0.289230
[13:39:44.298] iteration 18534: total_loss: 0.546763, loss_sup: 0.175908, loss_mps: 0.131143, loss_cps: 0.239712
[13:39:44.446] iteration 18535: total_loss: 0.600253, loss_sup: 0.025827, loss_mps: 0.179697, loss_cps: 0.394729
[13:39:44.598] iteration 18536: total_loss: 0.867954, loss_sup: 0.062842, loss_mps: 0.245085, loss_cps: 0.560027
[13:39:44.744] iteration 18537: total_loss: 0.295988, loss_sup: 0.011865, loss_mps: 0.104344, loss_cps: 0.179779
[13:39:44.892] iteration 18538: total_loss: 0.383833, loss_sup: 0.088752, loss_mps: 0.110683, loss_cps: 0.184397
[13:39:45.038] iteration 18539: total_loss: 0.331583, loss_sup: 0.094841, loss_mps: 0.091012, loss_cps: 0.145731
[13:39:45.184] iteration 18540: total_loss: 0.665889, loss_sup: 0.202181, loss_mps: 0.156111, loss_cps: 0.307597
[13:39:45.330] iteration 18541: total_loss: 0.577888, loss_sup: 0.287623, loss_mps: 0.100726, loss_cps: 0.189539
[13:39:45.476] iteration 18542: total_loss: 0.391102, loss_sup: 0.066336, loss_mps: 0.112299, loss_cps: 0.212466
[13:39:45.623] iteration 18543: total_loss: 0.414582, loss_sup: 0.099010, loss_mps: 0.118698, loss_cps: 0.196874
[13:39:45.770] iteration 18544: total_loss: 0.260070, loss_sup: 0.023657, loss_mps: 0.088146, loss_cps: 0.148267
[13:39:45.918] iteration 18545: total_loss: 0.484321, loss_sup: 0.131402, loss_mps: 0.119569, loss_cps: 0.233350
[13:39:46.064] iteration 18546: total_loss: 0.753169, loss_sup: 0.198975, loss_mps: 0.187020, loss_cps: 0.367174
[13:39:46.212] iteration 18547: total_loss: 0.710720, loss_sup: 0.019172, loss_mps: 0.222945, loss_cps: 0.468603
[13:39:46.359] iteration 18548: total_loss: 0.293308, loss_sup: 0.017069, loss_mps: 0.101452, loss_cps: 0.174787
[13:39:46.505] iteration 18549: total_loss: 0.672324, loss_sup: 0.104962, loss_mps: 0.182596, loss_cps: 0.384766
[13:39:46.653] iteration 18550: total_loss: 0.300168, loss_sup: 0.053276, loss_mps: 0.095966, loss_cps: 0.150926
[13:39:46.799] iteration 18551: total_loss: 0.555889, loss_sup: 0.162445, loss_mps: 0.131813, loss_cps: 0.261631
[13:39:46.945] iteration 18552: total_loss: 0.609670, loss_sup: 0.191219, loss_mps: 0.145874, loss_cps: 0.272577
[13:39:47.094] iteration 18553: total_loss: 0.291385, loss_sup: 0.020500, loss_mps: 0.093355, loss_cps: 0.177530
[13:39:47.240] iteration 18554: total_loss: 0.781050, loss_sup: 0.062129, loss_mps: 0.233302, loss_cps: 0.485619
[13:39:47.386] iteration 18555: total_loss: 0.366771, loss_sup: 0.013886, loss_mps: 0.119597, loss_cps: 0.233288
[13:39:47.532] iteration 18556: total_loss: 0.608487, loss_sup: 0.173787, loss_mps: 0.143294, loss_cps: 0.291405
[13:39:47.678] iteration 18557: total_loss: 0.924932, loss_sup: 0.234091, loss_mps: 0.223519, loss_cps: 0.467322
[13:39:47.824] iteration 18558: total_loss: 0.639400, loss_sup: 0.088868, loss_mps: 0.177621, loss_cps: 0.372911
[13:39:47.972] iteration 18559: total_loss: 0.371302, loss_sup: 0.019958, loss_mps: 0.123697, loss_cps: 0.227647
[13:39:48.122] iteration 18560: total_loss: 0.695447, loss_sup: 0.090301, loss_mps: 0.203726, loss_cps: 0.401420
[13:39:48.267] iteration 18561: total_loss: 0.613404, loss_sup: 0.097721, loss_mps: 0.177729, loss_cps: 0.337954
[13:39:48.416] iteration 18562: total_loss: 0.487018, loss_sup: 0.077586, loss_mps: 0.141853, loss_cps: 0.267579
[13:39:48.566] iteration 18563: total_loss: 0.634980, loss_sup: 0.025207, loss_mps: 0.199718, loss_cps: 0.410054
[13:39:48.713] iteration 18564: total_loss: 0.349757, loss_sup: 0.046551, loss_mps: 0.113188, loss_cps: 0.190017
[13:39:48.859] iteration 18565: total_loss: 0.347363, loss_sup: 0.059861, loss_mps: 0.107305, loss_cps: 0.180196
[13:39:49.005] iteration 18566: total_loss: 1.146624, loss_sup: 0.205378, loss_mps: 0.305323, loss_cps: 0.635924
[13:39:49.155] iteration 18567: total_loss: 0.557796, loss_sup: 0.146103, loss_mps: 0.139548, loss_cps: 0.272144
[13:39:49.302] iteration 18568: total_loss: 0.558658, loss_sup: 0.088750, loss_mps: 0.162780, loss_cps: 0.307128
[13:39:49.447] iteration 18569: total_loss: 0.658898, loss_sup: 0.087008, loss_mps: 0.192393, loss_cps: 0.379496
[13:39:49.594] iteration 18570: total_loss: 0.652311, loss_sup: 0.102523, loss_mps: 0.182425, loss_cps: 0.367363
[13:39:49.740] iteration 18571: total_loss: 0.388031, loss_sup: 0.155047, loss_mps: 0.087485, loss_cps: 0.145499
[13:39:49.887] iteration 18572: total_loss: 0.352617, loss_sup: 0.083405, loss_mps: 0.096628, loss_cps: 0.172584
[13:39:50.037] iteration 18573: total_loss: 0.463426, loss_sup: 0.049991, loss_mps: 0.133214, loss_cps: 0.280221
[13:39:50.183] iteration 18574: total_loss: 0.344751, loss_sup: 0.013878, loss_mps: 0.122832, loss_cps: 0.208041
[13:39:50.330] iteration 18575: total_loss: 0.369264, loss_sup: 0.073670, loss_mps: 0.106729, loss_cps: 0.188865
[13:39:50.476] iteration 18576: total_loss: 0.309586, loss_sup: 0.055495, loss_mps: 0.095830, loss_cps: 0.158261
[13:39:50.622] iteration 18577: total_loss: 0.541302, loss_sup: 0.184818, loss_mps: 0.127300, loss_cps: 0.229184
[13:39:50.770] iteration 18578: total_loss: 0.546648, loss_sup: 0.090838, loss_mps: 0.156270, loss_cps: 0.299540
[13:39:50.917] iteration 18579: total_loss: 0.692350, loss_sup: 0.054507, loss_mps: 0.200904, loss_cps: 0.436939
[13:39:51.063] iteration 18580: total_loss: 0.638282, loss_sup: 0.078743, loss_mps: 0.196541, loss_cps: 0.362998
[13:39:51.211] iteration 18581: total_loss: 0.499679, loss_sup: 0.091530, loss_mps: 0.142019, loss_cps: 0.266130
[13:39:51.359] iteration 18582: total_loss: 0.456681, loss_sup: 0.081044, loss_mps: 0.128352, loss_cps: 0.247285
[13:39:51.505] iteration 18583: total_loss: 0.664347, loss_sup: 0.008787, loss_mps: 0.203001, loss_cps: 0.452558
[13:39:51.651] iteration 18584: total_loss: 0.665300, loss_sup: 0.170027, loss_mps: 0.166652, loss_cps: 0.328621
[13:39:51.797] iteration 18585: total_loss: 0.473611, loss_sup: 0.014902, loss_mps: 0.149253, loss_cps: 0.309456
[13:39:51.943] iteration 18586: total_loss: 0.607228, loss_sup: 0.118729, loss_mps: 0.157782, loss_cps: 0.330718
[13:39:52.089] iteration 18587: total_loss: 0.343960, loss_sup: 0.003342, loss_mps: 0.122963, loss_cps: 0.217655
[13:39:52.235] iteration 18588: total_loss: 0.524008, loss_sup: 0.091677, loss_mps: 0.136404, loss_cps: 0.295928
[13:39:52.381] iteration 18589: total_loss: 0.285080, loss_sup: 0.034024, loss_mps: 0.093369, loss_cps: 0.157687
[13:39:52.527] iteration 18590: total_loss: 0.539521, loss_sup: 0.098883, loss_mps: 0.148778, loss_cps: 0.291860
[13:39:52.673] iteration 18591: total_loss: 0.862983, loss_sup: 0.065607, loss_mps: 0.255471, loss_cps: 0.541905
[13:39:52.820] iteration 18592: total_loss: 0.684450, loss_sup: 0.031846, loss_mps: 0.221313, loss_cps: 0.431291
[13:39:52.970] iteration 18593: total_loss: 0.410831, loss_sup: 0.077522, loss_mps: 0.114047, loss_cps: 0.219262
[13:39:53.117] iteration 18594: total_loss: 0.314611, loss_sup: 0.039668, loss_mps: 0.100466, loss_cps: 0.174476
[13:39:53.263] iteration 18595: total_loss: 0.618938, loss_sup: 0.040414, loss_mps: 0.180764, loss_cps: 0.397760
[13:39:53.410] iteration 18596: total_loss: 0.531259, loss_sup: 0.137999, loss_mps: 0.138743, loss_cps: 0.254517
[13:39:53.559] iteration 18597: total_loss: 0.450856, loss_sup: 0.072279, loss_mps: 0.136767, loss_cps: 0.241811
[13:39:53.705] iteration 18598: total_loss: 0.400202, loss_sup: 0.058916, loss_mps: 0.116966, loss_cps: 0.224320
[13:39:53.851] iteration 18599: total_loss: 0.560360, loss_sup: 0.003871, loss_mps: 0.177131, loss_cps: 0.379358
[13:39:53.997] iteration 18600: total_loss: 0.400512, loss_sup: 0.051918, loss_mps: 0.120840, loss_cps: 0.227754
[13:39:53.998] Evaluation Started ==>
[13:40:05.393] ==> valid iteration 18600: unet metrics: {'dc': 0.6450746475523926, 'jc': 0.5256583508820494, 'pre': 0.7741062193220289, 'hd': 5.624935580002017}, ynet metrics: {'dc': 0.62109788484958, 'jc': 0.5052603470877672, 'pre': 0.7547230683907393, 'hd': 5.667834528142502}.
[13:40:05.395] Evaluation Finished!⏹️
[13:40:05.545] iteration 18601: total_loss: 0.569465, loss_sup: 0.080580, loss_mps: 0.165395, loss_cps: 0.323489
[13:40:05.692] iteration 18602: total_loss: 1.021187, loss_sup: 0.061175, loss_mps: 0.299736, loss_cps: 0.660277
[13:40:05.838] iteration 18603: total_loss: 0.366512, loss_sup: 0.019375, loss_mps: 0.118102, loss_cps: 0.229035
[13:40:05.983] iteration 18604: total_loss: 0.419575, loss_sup: 0.100249, loss_mps: 0.117168, loss_cps: 0.202157
[13:40:06.128] iteration 18605: total_loss: 0.786655, loss_sup: 0.303438, loss_mps: 0.170090, loss_cps: 0.313128
[13:40:06.274] iteration 18606: total_loss: 0.797228, loss_sup: 0.113068, loss_mps: 0.224273, loss_cps: 0.459886
[13:40:06.419] iteration 18607: total_loss: 0.979879, loss_sup: 0.189401, loss_mps: 0.247971, loss_cps: 0.542508
[13:40:06.564] iteration 18608: total_loss: 0.673405, loss_sup: 0.361099, loss_mps: 0.115002, loss_cps: 0.197304
[13:40:06.709] iteration 18609: total_loss: 0.543010, loss_sup: 0.062894, loss_mps: 0.162593, loss_cps: 0.317523
[13:40:06.854] iteration 18610: total_loss: 0.546916, loss_sup: 0.054684, loss_mps: 0.165018, loss_cps: 0.327214
[13:40:06.999] iteration 18611: total_loss: 0.398385, loss_sup: 0.063404, loss_mps: 0.127291, loss_cps: 0.207691
[13:40:07.144] iteration 18612: total_loss: 0.494722, loss_sup: 0.109106, loss_mps: 0.131136, loss_cps: 0.254480
[13:40:07.290] iteration 18613: total_loss: 0.546664, loss_sup: 0.031209, loss_mps: 0.158303, loss_cps: 0.357151
[13:40:07.435] iteration 18614: total_loss: 0.505896, loss_sup: 0.010733, loss_mps: 0.163839, loss_cps: 0.331325
[13:40:07.587] iteration 18615: total_loss: 0.595561, loss_sup: 0.102492, loss_mps: 0.176886, loss_cps: 0.316182
[13:40:07.733] iteration 18616: total_loss: 0.703588, loss_sup: 0.056647, loss_mps: 0.221659, loss_cps: 0.425282
[13:40:07.878] iteration 18617: total_loss: 0.442768, loss_sup: 0.024305, loss_mps: 0.149156, loss_cps: 0.269307
[13:40:08.024] iteration 18618: total_loss: 0.267448, loss_sup: 0.011931, loss_mps: 0.093769, loss_cps: 0.161748
[13:40:08.169] iteration 18619: total_loss: 0.478527, loss_sup: 0.066623, loss_mps: 0.146376, loss_cps: 0.265529
[13:40:08.314] iteration 18620: total_loss: 0.610143, loss_sup: 0.099647, loss_mps: 0.178937, loss_cps: 0.331559
[13:40:08.460] iteration 18621: total_loss: 0.493423, loss_sup: 0.106198, loss_mps: 0.131707, loss_cps: 0.255518
[13:40:08.605] iteration 18622: total_loss: 0.405056, loss_sup: 0.083353, loss_mps: 0.125435, loss_cps: 0.196269
[13:40:08.753] iteration 18623: total_loss: 0.630800, loss_sup: 0.086941, loss_mps: 0.183821, loss_cps: 0.360037
[13:40:08.900] iteration 18624: total_loss: 0.496861, loss_sup: 0.041725, loss_mps: 0.161447, loss_cps: 0.293689
[13:40:09.047] iteration 18625: total_loss: 0.409043, loss_sup: 0.196236, loss_mps: 0.082481, loss_cps: 0.130326
[13:40:09.194] iteration 18626: total_loss: 0.512349, loss_sup: 0.055487, loss_mps: 0.155679, loss_cps: 0.301182
[13:40:09.340] iteration 18627: total_loss: 0.373260, loss_sup: 0.019197, loss_mps: 0.123296, loss_cps: 0.230766
[13:40:09.486] iteration 18628: total_loss: 0.817735, loss_sup: 0.159785, loss_mps: 0.214318, loss_cps: 0.443632
[13:40:09.632] iteration 18629: total_loss: 0.416827, loss_sup: 0.039960, loss_mps: 0.132351, loss_cps: 0.244516
[13:40:09.779] iteration 18630: total_loss: 0.521401, loss_sup: 0.175227, loss_mps: 0.121557, loss_cps: 0.224617
[13:40:09.927] iteration 18631: total_loss: 0.429974, loss_sup: 0.020836, loss_mps: 0.140342, loss_cps: 0.268795
[13:40:10.074] iteration 18632: total_loss: 0.652326, loss_sup: 0.148503, loss_mps: 0.166957, loss_cps: 0.336866
[13:40:10.220] iteration 18633: total_loss: 0.420287, loss_sup: 0.023225, loss_mps: 0.137928, loss_cps: 0.259134
[13:40:10.367] iteration 18634: total_loss: 0.675757, loss_sup: 0.066507, loss_mps: 0.202900, loss_cps: 0.406350
[13:40:10.513] iteration 18635: total_loss: 0.453054, loss_sup: 0.046170, loss_mps: 0.153797, loss_cps: 0.253087
[13:40:10.658] iteration 18636: total_loss: 0.414057, loss_sup: 0.104635, loss_mps: 0.113021, loss_cps: 0.196402
[13:40:10.804] iteration 18637: total_loss: 0.968187, loss_sup: 0.082745, loss_mps: 0.290893, loss_cps: 0.594549
[13:40:10.950] iteration 18638: total_loss: 0.592704, loss_sup: 0.109458, loss_mps: 0.154591, loss_cps: 0.328655
[13:40:11.095] iteration 18639: total_loss: 0.315487, loss_sup: 0.068313, loss_mps: 0.095189, loss_cps: 0.151985
[13:40:11.240] iteration 18640: total_loss: 0.403438, loss_sup: 0.024011, loss_mps: 0.132082, loss_cps: 0.247344
[13:40:11.386] iteration 18641: total_loss: 0.524717, loss_sup: 0.121863, loss_mps: 0.145582, loss_cps: 0.257273
[13:40:11.532] iteration 18642: total_loss: 0.231761, loss_sup: 0.029447, loss_mps: 0.075977, loss_cps: 0.126337
[13:40:11.677] iteration 18643: total_loss: 0.290343, loss_sup: 0.015269, loss_mps: 0.100875, loss_cps: 0.174199
[13:40:11.824] iteration 18644: total_loss: 0.316048, loss_sup: 0.043897, loss_mps: 0.101819, loss_cps: 0.170332
[13:40:11.970] iteration 18645: total_loss: 1.053360, loss_sup: 0.209008, loss_mps: 0.259227, loss_cps: 0.585125
[13:40:12.115] iteration 18646: total_loss: 0.438164, loss_sup: 0.121773, loss_mps: 0.110993, loss_cps: 0.205398
[13:40:12.261] iteration 18647: total_loss: 0.429133, loss_sup: 0.086115, loss_mps: 0.127411, loss_cps: 0.215607
[13:40:12.407] iteration 18648: total_loss: 0.700524, loss_sup: 0.249006, loss_mps: 0.141742, loss_cps: 0.309776
[13:40:12.553] iteration 18649: total_loss: 0.321672, loss_sup: 0.033379, loss_mps: 0.100479, loss_cps: 0.187813
[13:40:12.699] iteration 18650: total_loss: 0.397044, loss_sup: 0.126346, loss_mps: 0.096033, loss_cps: 0.174666
[13:40:12.845] iteration 18651: total_loss: 0.726829, loss_sup: 0.377977, loss_mps: 0.120115, loss_cps: 0.228736
[13:40:12.991] iteration 18652: total_loss: 1.449682, loss_sup: 0.166295, loss_mps: 0.395118, loss_cps: 0.888269
[13:40:13.136] iteration 18653: total_loss: 0.641409, loss_sup: 0.115919, loss_mps: 0.169456, loss_cps: 0.356034
[13:40:13.282] iteration 18654: total_loss: 0.842605, loss_sup: 0.059321, loss_mps: 0.233055, loss_cps: 0.550229
[13:40:13.428] iteration 18655: total_loss: 0.424268, loss_sup: 0.105820, loss_mps: 0.118417, loss_cps: 0.200031
[13:40:13.573] iteration 18656: total_loss: 0.867899, loss_sup: 0.165547, loss_mps: 0.222372, loss_cps: 0.479981
[13:40:13.720] iteration 18657: total_loss: 0.799981, loss_sup: 0.113341, loss_mps: 0.214005, loss_cps: 0.472635
[13:40:13.866] iteration 18658: total_loss: 0.349822, loss_sup: 0.032685, loss_mps: 0.108815, loss_cps: 0.208322
[13:40:14.012] iteration 18659: total_loss: 0.878353, loss_sup: 0.242436, loss_mps: 0.210659, loss_cps: 0.425258
[13:40:14.159] iteration 18660: total_loss: 0.544193, loss_sup: 0.047625, loss_mps: 0.164306, loss_cps: 0.332261
[13:40:14.306] iteration 18661: total_loss: 0.473672, loss_sup: 0.014834, loss_mps: 0.152141, loss_cps: 0.306698
[13:40:14.452] iteration 18662: total_loss: 0.403482, loss_sup: 0.045816, loss_mps: 0.124727, loss_cps: 0.232940
[13:40:14.597] iteration 18663: total_loss: 0.730276, loss_sup: 0.079027, loss_mps: 0.211745, loss_cps: 0.439504
[13:40:14.745] iteration 18664: total_loss: 0.687989, loss_sup: 0.193234, loss_mps: 0.170641, loss_cps: 0.324114
[13:40:14.892] iteration 18665: total_loss: 0.413763, loss_sup: 0.016214, loss_mps: 0.146583, loss_cps: 0.250967
[13:40:15.038] iteration 18666: total_loss: 0.421494, loss_sup: 0.023342, loss_mps: 0.143809, loss_cps: 0.254343
[13:40:15.184] iteration 18667: total_loss: 0.349964, loss_sup: 0.029723, loss_mps: 0.111970, loss_cps: 0.208271
[13:40:15.330] iteration 18668: total_loss: 0.920361, loss_sup: 0.146243, loss_mps: 0.257737, loss_cps: 0.516381
[13:40:15.476] iteration 18669: total_loss: 0.428312, loss_sup: 0.069034, loss_mps: 0.130421, loss_cps: 0.228857
[13:40:15.622] iteration 18670: total_loss: 0.540091, loss_sup: 0.135511, loss_mps: 0.137793, loss_cps: 0.266787
[13:40:15.769] iteration 18671: total_loss: 0.510255, loss_sup: 0.009058, loss_mps: 0.173766, loss_cps: 0.327431
[13:40:15.915] iteration 18672: total_loss: 0.656408, loss_sup: 0.069639, loss_mps: 0.190711, loss_cps: 0.396058
[13:40:16.062] iteration 18673: total_loss: 0.639382, loss_sup: 0.174557, loss_mps: 0.162389, loss_cps: 0.302436
[13:40:16.208] iteration 18674: total_loss: 0.474952, loss_sup: 0.022826, loss_mps: 0.146171, loss_cps: 0.305955
[13:40:16.357] iteration 18675: total_loss: 0.490912, loss_sup: 0.058612, loss_mps: 0.146935, loss_cps: 0.285365
[13:40:16.503] iteration 18676: total_loss: 1.118631, loss_sup: 0.234900, loss_mps: 0.289227, loss_cps: 0.594504
[13:40:16.649] iteration 18677: total_loss: 0.372631, loss_sup: 0.009022, loss_mps: 0.133230, loss_cps: 0.230379
[13:40:16.795] iteration 18678: total_loss: 0.828925, loss_sup: 0.055570, loss_mps: 0.248047, loss_cps: 0.525309
[13:40:16.941] iteration 18679: total_loss: 0.382334, loss_sup: 0.029660, loss_mps: 0.126911, loss_cps: 0.225763
[13:40:17.088] iteration 18680: total_loss: 0.714383, loss_sup: 0.049758, loss_mps: 0.212923, loss_cps: 0.451701
[13:40:17.234] iteration 18681: total_loss: 0.459763, loss_sup: 0.027395, loss_mps: 0.143499, loss_cps: 0.288869
[13:40:17.381] iteration 18682: total_loss: 1.227465, loss_sup: 0.384691, loss_mps: 0.267143, loss_cps: 0.575631
[13:40:17.529] iteration 18683: total_loss: 0.366356, loss_sup: 0.040804, loss_mps: 0.111038, loss_cps: 0.214513
[13:40:17.675] iteration 18684: total_loss: 0.370695, loss_sup: 0.063868, loss_mps: 0.112832, loss_cps: 0.193995
[13:40:17.821] iteration 18685: total_loss: 0.843794, loss_sup: 0.165090, loss_mps: 0.215111, loss_cps: 0.463593
[13:40:17.968] iteration 18686: total_loss: 0.690022, loss_sup: 0.096231, loss_mps: 0.197228, loss_cps: 0.396563
[13:40:18.115] iteration 18687: total_loss: 0.593817, loss_sup: 0.063300, loss_mps: 0.178098, loss_cps: 0.352420
[13:40:18.261] iteration 18688: total_loss: 0.479750, loss_sup: 0.033382, loss_mps: 0.158798, loss_cps: 0.287570
[13:40:18.408] iteration 18689: total_loss: 0.791219, loss_sup: 0.052892, loss_mps: 0.235661, loss_cps: 0.502666
[13:40:18.553] iteration 18690: total_loss: 0.483470, loss_sup: 0.042515, loss_mps: 0.147323, loss_cps: 0.293633
[13:40:18.700] iteration 18691: total_loss: 0.735217, loss_sup: 0.169053, loss_mps: 0.190682, loss_cps: 0.375482
[13:40:18.846] iteration 18692: total_loss: 0.952872, loss_sup: 0.310730, loss_mps: 0.204789, loss_cps: 0.437353
[13:40:18.992] iteration 18693: total_loss: 0.726125, loss_sup: 0.226467, loss_mps: 0.174466, loss_cps: 0.325192
[13:40:19.138] iteration 18694: total_loss: 0.593782, loss_sup: 0.145918, loss_mps: 0.152601, loss_cps: 0.295263
[13:40:19.284] iteration 18695: total_loss: 0.826320, loss_sup: 0.106559, loss_mps: 0.230151, loss_cps: 0.489610
[13:40:19.430] iteration 18696: total_loss: 0.790230, loss_sup: 0.029685, loss_mps: 0.254373, loss_cps: 0.506171
[13:40:19.577] iteration 18697: total_loss: 0.694462, loss_sup: 0.035855, loss_mps: 0.211431, loss_cps: 0.447176
[13:40:19.723] iteration 18698: total_loss: 0.442271, loss_sup: 0.027285, loss_mps: 0.146922, loss_cps: 0.268064
[13:40:19.870] iteration 18699: total_loss: 0.617431, loss_sup: 0.115181, loss_mps: 0.170320, loss_cps: 0.331930
[13:40:20.016] iteration 18700: total_loss: 0.401616, loss_sup: 0.045567, loss_mps: 0.120832, loss_cps: 0.235216
[13:40:20.016] Evaluation Started ==>
[13:40:31.359] ==> valid iteration 18700: unet metrics: {'dc': 0.5996584711717904, 'jc': 0.48467486377194663, 'pre': 0.7867929811722996, 'hd': 5.513026883361638}, ynet metrics: {'dc': 0.599575372068885, 'jc': 0.48306615497396865, 'pre': 0.779935301785737, 'hd': 5.659826416167471}.
[13:40:31.361] Evaluation Finished!⏹️
[13:40:31.513] iteration 18701: total_loss: 0.786124, loss_sup: 0.054337, loss_mps: 0.230191, loss_cps: 0.501595
[13:40:31.660] iteration 18702: total_loss: 0.467783, loss_sup: 0.054797, loss_mps: 0.142300, loss_cps: 0.270686
[13:40:31.805] iteration 18703: total_loss: 0.714451, loss_sup: 0.152736, loss_mps: 0.209877, loss_cps: 0.351838
[13:40:31.951] iteration 18704: total_loss: 0.591366, loss_sup: 0.130890, loss_mps: 0.155913, loss_cps: 0.304563
[13:40:32.097] iteration 18705: total_loss: 0.618609, loss_sup: 0.198129, loss_mps: 0.156878, loss_cps: 0.263602
[13:40:32.245] iteration 18706: total_loss: 0.336027, loss_sup: 0.053882, loss_mps: 0.102285, loss_cps: 0.179861
[13:40:32.391] iteration 18707: total_loss: 0.581670, loss_sup: 0.083282, loss_mps: 0.166707, loss_cps: 0.331682
[13:40:32.536] iteration 18708: total_loss: 0.746625, loss_sup: 0.049370, loss_mps: 0.227581, loss_cps: 0.469675
[13:40:32.681] iteration 18709: total_loss: 0.550387, loss_sup: 0.057817, loss_mps: 0.165512, loss_cps: 0.327058
[13:40:32.826] iteration 18710: total_loss: 0.501963, loss_sup: 0.097026, loss_mps: 0.139805, loss_cps: 0.265132
[13:40:32.971] iteration 18711: total_loss: 0.600562, loss_sup: 0.039024, loss_mps: 0.192295, loss_cps: 0.369244
[13:40:33.117] iteration 18712: total_loss: 0.550582, loss_sup: 0.020093, loss_mps: 0.170182, loss_cps: 0.360306
[13:40:33.263] iteration 18713: total_loss: 0.574763, loss_sup: 0.140122, loss_mps: 0.142915, loss_cps: 0.291726
[13:40:33.408] iteration 18714: total_loss: 0.341408, loss_sup: 0.040790, loss_mps: 0.110087, loss_cps: 0.190531
[13:40:33.553] iteration 18715: total_loss: 0.966747, loss_sup: 0.094059, loss_mps: 0.268249, loss_cps: 0.604439
[13:40:33.698] iteration 18716: total_loss: 0.483648, loss_sup: 0.069520, loss_mps: 0.152248, loss_cps: 0.261881
[13:40:33.844] iteration 18717: total_loss: 0.578910, loss_sup: 0.038420, loss_mps: 0.184232, loss_cps: 0.356257
[13:40:33.990] iteration 18718: total_loss: 0.602432, loss_sup: 0.027176, loss_mps: 0.191370, loss_cps: 0.383886
[13:40:34.135] iteration 18719: total_loss: 0.355775, loss_sup: 0.043720, loss_mps: 0.115678, loss_cps: 0.196376
[13:40:34.280] iteration 18720: total_loss: 0.823943, loss_sup: 0.071072, loss_mps: 0.238479, loss_cps: 0.514392
[13:40:34.426] iteration 18721: total_loss: 0.346131, loss_sup: 0.019214, loss_mps: 0.114968, loss_cps: 0.211949
[13:40:34.571] iteration 18722: total_loss: 0.445476, loss_sup: 0.065553, loss_mps: 0.131916, loss_cps: 0.248006
[13:40:34.720] iteration 18723: total_loss: 0.543172, loss_sup: 0.068930, loss_mps: 0.165548, loss_cps: 0.308694
[13:40:34.865] iteration 18724: total_loss: 0.835476, loss_sup: 0.058622, loss_mps: 0.251772, loss_cps: 0.525082
[13:40:35.011] iteration 18725: total_loss: 0.444635, loss_sup: 0.024454, loss_mps: 0.147189, loss_cps: 0.272992
[13:40:35.156] iteration 18726: total_loss: 1.245585, loss_sup: 0.135839, loss_mps: 0.345407, loss_cps: 0.764338
[13:40:35.302] iteration 18727: total_loss: 0.832287, loss_sup: 0.128669, loss_mps: 0.234176, loss_cps: 0.469443
[13:40:35.449] iteration 18728: total_loss: 0.455455, loss_sup: 0.022531, loss_mps: 0.149005, loss_cps: 0.283918
[13:40:35.595] iteration 18729: total_loss: 0.445683, loss_sup: 0.078497, loss_mps: 0.135260, loss_cps: 0.231926
[13:40:35.740] iteration 18730: total_loss: 0.311167, loss_sup: 0.076238, loss_mps: 0.087418, loss_cps: 0.147510
[13:40:35.886] iteration 18731: total_loss: 0.610095, loss_sup: 0.119081, loss_mps: 0.166111, loss_cps: 0.324903
[13:40:36.032] iteration 18732: total_loss: 0.683408, loss_sup: 0.069380, loss_mps: 0.204177, loss_cps: 0.409851
[13:40:36.180] iteration 18733: total_loss: 0.582359, loss_sup: 0.136428, loss_mps: 0.158799, loss_cps: 0.287132
[13:40:36.325] iteration 18734: total_loss: 0.702268, loss_sup: 0.074288, loss_mps: 0.208569, loss_cps: 0.419411
[13:40:36.471] iteration 18735: total_loss: 0.472951, loss_sup: 0.056950, loss_mps: 0.151576, loss_cps: 0.264425
[13:40:36.617] iteration 18736: total_loss: 0.785808, loss_sup: 0.508551, loss_mps: 0.099523, loss_cps: 0.177734
[13:40:36.763] iteration 18737: total_loss: 0.611431, loss_sup: 0.032822, loss_mps: 0.185025, loss_cps: 0.393584
[13:40:36.909] iteration 18738: total_loss: 0.761156, loss_sup: 0.110907, loss_mps: 0.206977, loss_cps: 0.443273
[13:40:37.056] iteration 18739: total_loss: 0.490387, loss_sup: 0.119900, loss_mps: 0.131945, loss_cps: 0.238542
[13:40:37.202] iteration 18740: total_loss: 0.783992, loss_sup: 0.086610, loss_mps: 0.220161, loss_cps: 0.477220
[13:40:37.347] iteration 18741: total_loss: 0.476297, loss_sup: 0.030854, loss_mps: 0.154227, loss_cps: 0.291215
[13:40:37.493] iteration 18742: total_loss: 0.320100, loss_sup: 0.012894, loss_mps: 0.108140, loss_cps: 0.199066
[13:40:37.639] iteration 18743: total_loss: 0.966182, loss_sup: 0.495640, loss_mps: 0.158416, loss_cps: 0.312125
[13:40:37.785] iteration 18744: total_loss: 0.965401, loss_sup: 0.198735, loss_mps: 0.246072, loss_cps: 0.520595
[13:40:37.931] iteration 18745: total_loss: 1.111322, loss_sup: 0.063927, loss_mps: 0.323344, loss_cps: 0.724051
[13:40:38.077] iteration 18746: total_loss: 0.562152, loss_sup: 0.213989, loss_mps: 0.124859, loss_cps: 0.223304
[13:40:38.223] iteration 18747: total_loss: 0.534609, loss_sup: 0.036010, loss_mps: 0.176395, loss_cps: 0.322205
[13:40:38.369] iteration 18748: total_loss: 1.173120, loss_sup: 0.129430, loss_mps: 0.336244, loss_cps: 0.707446
[13:40:38.515] iteration 18749: total_loss: 0.692984, loss_sup: 0.194607, loss_mps: 0.185894, loss_cps: 0.312483
[13:40:38.661] iteration 18750: total_loss: 0.590835, loss_sup: 0.074390, loss_mps: 0.190306, loss_cps: 0.326139
[13:40:38.807] iteration 18751: total_loss: 0.619221, loss_sup: 0.036091, loss_mps: 0.194420, loss_cps: 0.388710
[13:40:38.953] iteration 18752: total_loss: 1.250242, loss_sup: 0.094701, loss_mps: 0.365111, loss_cps: 0.790429
[13:40:39.100] iteration 18753: total_loss: 0.697280, loss_sup: 0.040496, loss_mps: 0.213310, loss_cps: 0.443474
[13:40:39.246] iteration 18754: total_loss: 0.885896, loss_sup: 0.138517, loss_mps: 0.246334, loss_cps: 0.501045
[13:40:39.391] iteration 18755: total_loss: 0.384331, loss_sup: 0.016698, loss_mps: 0.137913, loss_cps: 0.229720
[13:40:39.537] iteration 18756: total_loss: 0.602515, loss_sup: 0.014018, loss_mps: 0.203358, loss_cps: 0.385139
[13:40:39.683] iteration 18757: total_loss: 0.668860, loss_sup: 0.059270, loss_mps: 0.222878, loss_cps: 0.386712
[13:40:39.829] iteration 18758: total_loss: 0.560508, loss_sup: 0.044198, loss_mps: 0.177783, loss_cps: 0.338527
[13:40:39.975] iteration 18759: total_loss: 0.998297, loss_sup: 0.453937, loss_mps: 0.185047, loss_cps: 0.359314
[13:40:40.124] iteration 18760: total_loss: 0.584101, loss_sup: 0.222869, loss_mps: 0.133393, loss_cps: 0.227839
[13:40:40.270] iteration 18761: total_loss: 0.620346, loss_sup: 0.007967, loss_mps: 0.209715, loss_cps: 0.402665
[13:40:40.417] iteration 18762: total_loss: 0.502683, loss_sup: 0.092711, loss_mps: 0.145085, loss_cps: 0.264887
[13:40:40.565] iteration 18763: total_loss: 0.598452, loss_sup: 0.118121, loss_mps: 0.172617, loss_cps: 0.307714
[13:40:40.711] iteration 18764: total_loss: 0.467464, loss_sup: 0.176306, loss_mps: 0.109011, loss_cps: 0.182146
[13:40:40.857] iteration 18765: total_loss: 0.639718, loss_sup: 0.188439, loss_mps: 0.163343, loss_cps: 0.287936
[13:40:41.004] iteration 18766: total_loss: 0.352502, loss_sup: 0.061476, loss_mps: 0.109281, loss_cps: 0.181746
[13:40:41.152] iteration 18767: total_loss: 0.746872, loss_sup: 0.106525, loss_mps: 0.215517, loss_cps: 0.424831
[13:40:41.299] iteration 18768: total_loss: 0.639198, loss_sup: 0.099553, loss_mps: 0.186268, loss_cps: 0.353377
[13:40:41.445] iteration 18769: total_loss: 0.543946, loss_sup: 0.060097, loss_mps: 0.160438, loss_cps: 0.323410
[13:40:41.590] iteration 18770: total_loss: 0.982536, loss_sup: 0.189842, loss_mps: 0.263297, loss_cps: 0.529398
[13:40:41.737] iteration 18771: total_loss: 0.299965, loss_sup: 0.021297, loss_mps: 0.110927, loss_cps: 0.167740
[13:40:41.883] iteration 18772: total_loss: 0.485131, loss_sup: 0.080342, loss_mps: 0.138526, loss_cps: 0.266263
[13:40:42.029] iteration 18773: total_loss: 0.524829, loss_sup: 0.133952, loss_mps: 0.136516, loss_cps: 0.254360
[13:40:42.176] iteration 18774: total_loss: 0.444977, loss_sup: 0.068094, loss_mps: 0.133782, loss_cps: 0.243101
[13:40:42.322] iteration 18775: total_loss: 0.385138, loss_sup: 0.026238, loss_mps: 0.124750, loss_cps: 0.234149
[13:40:42.470] iteration 18776: total_loss: 0.640611, loss_sup: 0.134160, loss_mps: 0.177120, loss_cps: 0.329331
[13:40:42.617] iteration 18777: total_loss: 0.669583, loss_sup: 0.147880, loss_mps: 0.181678, loss_cps: 0.340025
[13:40:42.763] iteration 18778: total_loss: 0.814888, loss_sup: 0.215284, loss_mps: 0.185236, loss_cps: 0.414368
[13:40:42.908] iteration 18779: total_loss: 0.536798, loss_sup: 0.080206, loss_mps: 0.156327, loss_cps: 0.300264
[13:40:43.055] iteration 18780: total_loss: 0.548335, loss_sup: 0.040527, loss_mps: 0.168986, loss_cps: 0.338822
[13:40:43.202] iteration 18781: total_loss: 0.335912, loss_sup: 0.012023, loss_mps: 0.115792, loss_cps: 0.208097
[13:40:43.348] iteration 18782: total_loss: 0.395510, loss_sup: 0.094217, loss_mps: 0.109878, loss_cps: 0.191416
[13:40:43.494] iteration 18783: total_loss: 1.358761, loss_sup: 0.053592, loss_mps: 0.388170, loss_cps: 0.916999
[13:40:43.641] iteration 18784: total_loss: 1.230973, loss_sup: 0.116601, loss_mps: 0.347003, loss_cps: 0.767369
[13:40:43.787] iteration 18785: total_loss: 0.568239, loss_sup: 0.147784, loss_mps: 0.146442, loss_cps: 0.274013
[13:40:43.933] iteration 18786: total_loss: 0.441802, loss_sup: 0.049695, loss_mps: 0.139720, loss_cps: 0.252387
[13:40:44.079] iteration 18787: total_loss: 0.938025, loss_sup: 0.130148, loss_mps: 0.261901, loss_cps: 0.545975
[13:40:44.226] iteration 18788: total_loss: 0.658317, loss_sup: 0.082654, loss_mps: 0.197732, loss_cps: 0.377931
[13:40:44.372] iteration 18789: total_loss: 0.709198, loss_sup: 0.034291, loss_mps: 0.217396, loss_cps: 0.457512
[13:40:44.518] iteration 18790: total_loss: 0.520650, loss_sup: 0.070586, loss_mps: 0.149837, loss_cps: 0.300227
[13:40:44.664] iteration 18791: total_loss: 0.568308, loss_sup: 0.029079, loss_mps: 0.174552, loss_cps: 0.364676
[13:40:44.810] iteration 18792: total_loss: 0.612007, loss_sup: 0.025122, loss_mps: 0.189095, loss_cps: 0.397790
[13:40:44.956] iteration 18793: total_loss: 0.298565, loss_sup: 0.043606, loss_mps: 0.098206, loss_cps: 0.156754
[13:40:45.103] iteration 18794: total_loss: 0.529814, loss_sup: 0.035047, loss_mps: 0.174143, loss_cps: 0.320625
[13:40:45.250] iteration 18795: total_loss: 0.627359, loss_sup: 0.070126, loss_mps: 0.193648, loss_cps: 0.363585
[13:40:45.398] iteration 18796: total_loss: 0.713839, loss_sup: 0.092702, loss_mps: 0.208167, loss_cps: 0.412970
[13:40:45.543] iteration 18797: total_loss: 0.594362, loss_sup: 0.113152, loss_mps: 0.163275, loss_cps: 0.317935
[13:40:45.689] iteration 18798: total_loss: 0.559963, loss_sup: 0.207027, loss_mps: 0.119047, loss_cps: 0.233888
[13:40:45.837] iteration 18799: total_loss: 1.350342, loss_sup: 0.329991, loss_mps: 0.307929, loss_cps: 0.712423
[13:40:45.983] iteration 18800: total_loss: 0.643362, loss_sup: 0.051251, loss_mps: 0.193780, loss_cps: 0.398331
[13:40:45.984] Evaluation Started ==>
[13:40:57.293] ==> valid iteration 18800: unet metrics: {'dc': 0.5951799730873042, 'jc': 0.48062158649892056, 'pre': 0.7644806113621073, 'hd': 5.532609016621786}, ynet metrics: {'dc': 0.6000095380856673, 'jc': 0.4827994348392572, 'pre': 0.8012669146960568, 'hd': 5.458415935819531}.
[13:40:57.296] Evaluation Finished!⏹️
[13:40:57.445] iteration 18801: total_loss: 0.793464, loss_sup: 0.043676, loss_mps: 0.231895, loss_cps: 0.517893
[13:40:57.595] iteration 18802: total_loss: 0.458731, loss_sup: 0.065347, loss_mps: 0.133110, loss_cps: 0.260273
[13:40:57.741] iteration 18803: total_loss: 0.565588, loss_sup: 0.085491, loss_mps: 0.161675, loss_cps: 0.318422
[13:40:57.886] iteration 18804: total_loss: 0.668380, loss_sup: 0.106414, loss_mps: 0.192327, loss_cps: 0.369639
[13:40:58.032] iteration 18805: total_loss: 0.798931, loss_sup: 0.227537, loss_mps: 0.202206, loss_cps: 0.369189
[13:40:58.177] iteration 18806: total_loss: 0.514866, loss_sup: 0.040633, loss_mps: 0.162493, loss_cps: 0.311739
[13:40:58.322] iteration 18807: total_loss: 0.575303, loss_sup: 0.189738, loss_mps: 0.138843, loss_cps: 0.246722
[13:40:58.467] iteration 18808: total_loss: 0.973224, loss_sup: 0.071326, loss_mps: 0.292051, loss_cps: 0.609847
[13:40:58.612] iteration 18809: total_loss: 0.722623, loss_sup: 0.229692, loss_mps: 0.183262, loss_cps: 0.309670
[13:40:58.673] iteration 18810: total_loss: 0.839074, loss_sup: 0.268723, loss_mps: 0.192799, loss_cps: 0.377552
[13:40:59.894] iteration 18811: total_loss: 0.547046, loss_sup: 0.109440, loss_mps: 0.157065, loss_cps: 0.280540
[13:41:00.042] iteration 18812: total_loss: 1.019315, loss_sup: 0.276772, loss_mps: 0.230244, loss_cps: 0.512300
[13:41:00.189] iteration 18813: total_loss: 0.802676, loss_sup: 0.185321, loss_mps: 0.208509, loss_cps: 0.408846
[13:41:00.335] iteration 18814: total_loss: 1.013078, loss_sup: 0.391470, loss_mps: 0.212774, loss_cps: 0.408834
[13:41:00.480] iteration 18815: total_loss: 0.599534, loss_sup: 0.064938, loss_mps: 0.188525, loss_cps: 0.346071
[13:41:00.625] iteration 18816: total_loss: 1.050070, loss_sup: 0.143808, loss_mps: 0.289121, loss_cps: 0.617141
[13:41:00.771] iteration 18817: total_loss: 0.445041, loss_sup: 0.118530, loss_mps: 0.124159, loss_cps: 0.202353
[13:41:00.924] iteration 18818: total_loss: 0.633078, loss_sup: 0.044515, loss_mps: 0.200196, loss_cps: 0.388367
[13:41:01.070] iteration 18819: total_loss: 0.918304, loss_sup: 0.195698, loss_mps: 0.226790, loss_cps: 0.495816
[13:41:01.216] iteration 18820: total_loss: 0.868435, loss_sup: 0.143696, loss_mps: 0.238244, loss_cps: 0.486495
[13:41:01.361] iteration 18821: total_loss: 0.358947, loss_sup: 0.053044, loss_mps: 0.117166, loss_cps: 0.188737
[13:41:01.507] iteration 18822: total_loss: 0.553006, loss_sup: 0.035884, loss_mps: 0.181227, loss_cps: 0.335895
[13:41:01.653] iteration 18823: total_loss: 0.565562, loss_sup: 0.053319, loss_mps: 0.184547, loss_cps: 0.327696
[13:41:01.800] iteration 18824: total_loss: 1.054883, loss_sup: 0.173955, loss_mps: 0.283452, loss_cps: 0.597476
[13:41:01.946] iteration 18825: total_loss: 0.524174, loss_sup: 0.092225, loss_mps: 0.151458, loss_cps: 0.280490
[13:41:02.091] iteration 18826: total_loss: 0.304399, loss_sup: 0.007199, loss_mps: 0.113191, loss_cps: 0.184009
[13:41:02.238] iteration 18827: total_loss: 0.468648, loss_sup: 0.054952, loss_mps: 0.153496, loss_cps: 0.260200
[13:41:02.385] iteration 18828: total_loss: 0.702221, loss_sup: 0.207558, loss_mps: 0.180288, loss_cps: 0.314375
[13:41:02.531] iteration 18829: total_loss: 0.776251, loss_sup: 0.097616, loss_mps: 0.223079, loss_cps: 0.455557
[13:41:02.678] iteration 18830: total_loss: 0.670848, loss_sup: 0.033719, loss_mps: 0.206393, loss_cps: 0.430736
[13:41:02.825] iteration 18831: total_loss: 0.714124, loss_sup: 0.012432, loss_mps: 0.220154, loss_cps: 0.481538
[13:41:02.971] iteration 18832: total_loss: 0.762350, loss_sup: 0.039027, loss_mps: 0.239378, loss_cps: 0.483945
[13:41:03.117] iteration 18833: total_loss: 0.966207, loss_sup: 0.278741, loss_mps: 0.226549, loss_cps: 0.460917
[13:41:03.265] iteration 18834: total_loss: 0.449724, loss_sup: 0.066422, loss_mps: 0.139002, loss_cps: 0.244299
[13:41:03.410] iteration 18835: total_loss: 0.358081, loss_sup: 0.071809, loss_mps: 0.112777, loss_cps: 0.173495
[13:41:03.556] iteration 18836: total_loss: 0.627576, loss_sup: 0.052676, loss_mps: 0.199014, loss_cps: 0.375886
[13:41:03.702] iteration 18837: total_loss: 0.989376, loss_sup: 0.434656, loss_mps: 0.195106, loss_cps: 0.359614
[13:41:03.851] iteration 18838: total_loss: 0.429300, loss_sup: 0.010321, loss_mps: 0.149556, loss_cps: 0.269423
[13:41:03.997] iteration 18839: total_loss: 0.593946, loss_sup: 0.031920, loss_mps: 0.199196, loss_cps: 0.362830
[13:41:04.142] iteration 18840: total_loss: 0.707020, loss_sup: 0.144794, loss_mps: 0.194713, loss_cps: 0.367514
[13:41:04.287] iteration 18841: total_loss: 0.651252, loss_sup: 0.142444, loss_mps: 0.183801, loss_cps: 0.325007
[13:41:04.433] iteration 18842: total_loss: 0.406104, loss_sup: 0.125434, loss_mps: 0.106982, loss_cps: 0.173689
[13:41:04.578] iteration 18843: total_loss: 0.468228, loss_sup: 0.028892, loss_mps: 0.153021, loss_cps: 0.286315
[13:41:04.725] iteration 18844: total_loss: 0.275012, loss_sup: 0.017569, loss_mps: 0.097858, loss_cps: 0.159585
[13:41:04.871] iteration 18845: total_loss: 0.682909, loss_sup: 0.056203, loss_mps: 0.196827, loss_cps: 0.429879
[13:41:05.019] iteration 18846: total_loss: 0.443400, loss_sup: 0.017499, loss_mps: 0.144895, loss_cps: 0.281005
[13:41:05.165] iteration 18847: total_loss: 0.570882, loss_sup: 0.033583, loss_mps: 0.180231, loss_cps: 0.357068
[13:41:05.312] iteration 18848: total_loss: 0.602742, loss_sup: 0.030645, loss_mps: 0.186841, loss_cps: 0.385255
[13:41:05.457] iteration 18849: total_loss: 0.466641, loss_sup: 0.048723, loss_mps: 0.141030, loss_cps: 0.276889
[13:41:05.604] iteration 18850: total_loss: 1.118607, loss_sup: 0.088189, loss_mps: 0.312569, loss_cps: 0.717849
[13:41:05.751] iteration 18851: total_loss: 0.625976, loss_sup: 0.157115, loss_mps: 0.162408, loss_cps: 0.306453
[13:41:05.898] iteration 18852: total_loss: 0.621284, loss_sup: 0.056149, loss_mps: 0.191187, loss_cps: 0.373947
[13:41:06.046] iteration 18853: total_loss: 0.300520, loss_sup: 0.035927, loss_mps: 0.099691, loss_cps: 0.164902
[13:41:06.192] iteration 18854: total_loss: 0.806096, loss_sup: 0.137843, loss_mps: 0.213361, loss_cps: 0.454892
[13:41:06.339] iteration 18855: total_loss: 0.876940, loss_sup: 0.073681, loss_mps: 0.246794, loss_cps: 0.556464
[13:41:06.485] iteration 18856: total_loss: 0.743878, loss_sup: 0.237534, loss_mps: 0.169309, loss_cps: 0.337035
[13:41:06.631] iteration 18857: total_loss: 0.520426, loss_sup: 0.108453, loss_mps: 0.152414, loss_cps: 0.259559
[13:41:06.777] iteration 18858: total_loss: 0.421661, loss_sup: 0.080290, loss_mps: 0.124153, loss_cps: 0.217217
[13:41:06.922] iteration 18859: total_loss: 0.976351, loss_sup: 0.156830, loss_mps: 0.266319, loss_cps: 0.553203
[13:41:07.069] iteration 18860: total_loss: 0.543036, loss_sup: 0.101962, loss_mps: 0.157190, loss_cps: 0.283884
[13:41:07.217] iteration 18861: total_loss: 0.759880, loss_sup: 0.108873, loss_mps: 0.214665, loss_cps: 0.436342
[13:41:07.363] iteration 18862: total_loss: 0.915320, loss_sup: 0.092798, loss_mps: 0.264847, loss_cps: 0.557675
[13:41:07.508] iteration 18863: total_loss: 0.338657, loss_sup: 0.009151, loss_mps: 0.116823, loss_cps: 0.212683
[13:41:07.655] iteration 18864: total_loss: 0.654275, loss_sup: 0.085355, loss_mps: 0.192702, loss_cps: 0.376218
[13:41:07.801] iteration 18865: total_loss: 0.525514, loss_sup: 0.056560, loss_mps: 0.161142, loss_cps: 0.307812
[13:41:07.947] iteration 18866: total_loss: 0.551925, loss_sup: 0.103954, loss_mps: 0.159753, loss_cps: 0.288218
[13:41:08.092] iteration 18867: total_loss: 0.479400, loss_sup: 0.071127, loss_mps: 0.153407, loss_cps: 0.254866
[13:41:08.237] iteration 18868: total_loss: 0.781688, loss_sup: 0.052975, loss_mps: 0.244886, loss_cps: 0.483826
[13:41:08.385] iteration 18869: total_loss: 0.589405, loss_sup: 0.138200, loss_mps: 0.156829, loss_cps: 0.294376
[13:41:08.532] iteration 18870: total_loss: 0.478467, loss_sup: 0.068370, loss_mps: 0.139324, loss_cps: 0.270774
[13:41:08.677] iteration 18871: total_loss: 0.803312, loss_sup: 0.011394, loss_mps: 0.243106, loss_cps: 0.548811
[13:41:08.824] iteration 18872: total_loss: 0.358060, loss_sup: 0.041392, loss_mps: 0.111696, loss_cps: 0.204972
[13:41:08.970] iteration 18873: total_loss: 0.526163, loss_sup: 0.125621, loss_mps: 0.137957, loss_cps: 0.262585
[13:41:09.115] iteration 18874: total_loss: 0.549335, loss_sup: 0.157177, loss_mps: 0.137513, loss_cps: 0.254646
[13:41:09.261] iteration 18875: total_loss: 0.738800, loss_sup: 0.161072, loss_mps: 0.200310, loss_cps: 0.377418
[13:41:09.407] iteration 18876: total_loss: 0.337638, loss_sup: 0.080704, loss_mps: 0.097129, loss_cps: 0.159805
[13:41:09.554] iteration 18877: total_loss: 0.428012, loss_sup: 0.130792, loss_mps: 0.110853, loss_cps: 0.186367
[13:41:09.699] iteration 18878: total_loss: 0.585621, loss_sup: 0.058299, loss_mps: 0.172435, loss_cps: 0.354888
[13:41:09.846] iteration 18879: total_loss: 0.499253, loss_sup: 0.076449, loss_mps: 0.146984, loss_cps: 0.275821
[13:41:09.993] iteration 18880: total_loss: 0.710567, loss_sup: 0.255939, loss_mps: 0.159800, loss_cps: 0.294828
[13:41:10.139] iteration 18881: total_loss: 0.634353, loss_sup: 0.103512, loss_mps: 0.183319, loss_cps: 0.347522
[13:41:10.286] iteration 18882: total_loss: 0.663146, loss_sup: 0.174103, loss_mps: 0.172075, loss_cps: 0.316969
[13:41:10.432] iteration 18883: total_loss: 0.555844, loss_sup: 0.120455, loss_mps: 0.147114, loss_cps: 0.288274
[13:41:10.578] iteration 18884: total_loss: 0.265335, loss_sup: 0.039350, loss_mps: 0.082680, loss_cps: 0.143305
[13:41:10.724] iteration 18885: total_loss: 0.616342, loss_sup: 0.074628, loss_mps: 0.179347, loss_cps: 0.362367
[13:41:10.871] iteration 18886: total_loss: 0.840088, loss_sup: 0.095489, loss_mps: 0.227833, loss_cps: 0.516765
[13:41:11.017] iteration 18887: total_loss: 0.435203, loss_sup: 0.041405, loss_mps: 0.136136, loss_cps: 0.257663
[13:41:11.164] iteration 18888: total_loss: 0.517594, loss_sup: 0.062204, loss_mps: 0.156021, loss_cps: 0.299370
[13:41:11.311] iteration 18889: total_loss: 0.310363, loss_sup: 0.066634, loss_mps: 0.091322, loss_cps: 0.152406
[13:41:11.457] iteration 18890: total_loss: 0.391519, loss_sup: 0.012564, loss_mps: 0.128283, loss_cps: 0.250672
[13:41:11.603] iteration 18891: total_loss: 0.507891, loss_sup: 0.119244, loss_mps: 0.137071, loss_cps: 0.251576
[13:41:11.749] iteration 18892: total_loss: 0.986177, loss_sup: 0.174639, loss_mps: 0.267199, loss_cps: 0.544339
[13:41:11.896] iteration 18893: total_loss: 0.782773, loss_sup: 0.033729, loss_mps: 0.245052, loss_cps: 0.503991
[13:41:12.043] iteration 18894: total_loss: 0.661875, loss_sup: 0.020136, loss_mps: 0.201523, loss_cps: 0.440216
[13:41:12.189] iteration 18895: total_loss: 0.723098, loss_sup: 0.108190, loss_mps: 0.189838, loss_cps: 0.425070
[13:41:12.335] iteration 18896: total_loss: 0.648433, loss_sup: 0.117587, loss_mps: 0.176216, loss_cps: 0.354629
[13:41:12.480] iteration 18897: total_loss: 0.560963, loss_sup: 0.165720, loss_mps: 0.143531, loss_cps: 0.251711
[13:41:12.626] iteration 18898: total_loss: 1.293735, loss_sup: 0.550578, loss_mps: 0.254733, loss_cps: 0.488424
[13:41:12.774] iteration 18899: total_loss: 0.337507, loss_sup: 0.084947, loss_mps: 0.091826, loss_cps: 0.160734
[13:41:12.920] iteration 18900: total_loss: 0.567631, loss_sup: 0.068232, loss_mps: 0.170376, loss_cps: 0.329023
[13:41:12.920] Evaluation Started ==>
[13:41:24.339] ==> valid iteration 18900: unet metrics: {'dc': 0.5977483749752099, 'jc': 0.48273805682138843, 'pre': 0.7999893879058428, 'hd': 5.557156460156272}, ynet metrics: {'dc': 0.5622133384718823, 'jc': 0.4479278963312528, 'pre': 0.7791657926841482, 'hd': 5.726148591060459}.
[13:41:24.341] Evaluation Finished!⏹️
[13:41:24.490] iteration 18901: total_loss: 0.387124, loss_sup: 0.024855, loss_mps: 0.127548, loss_cps: 0.234721
[13:41:24.638] iteration 18902: total_loss: 0.575032, loss_sup: 0.082885, loss_mps: 0.162308, loss_cps: 0.329839
[13:41:24.783] iteration 18903: total_loss: 0.925983, loss_sup: 0.131198, loss_mps: 0.253547, loss_cps: 0.541238
[13:41:24.931] iteration 18904: total_loss: 0.773140, loss_sup: 0.061207, loss_mps: 0.233708, loss_cps: 0.478225
[13:41:25.077] iteration 18905: total_loss: 0.878942, loss_sup: 0.167596, loss_mps: 0.230142, loss_cps: 0.481204
[13:41:25.224] iteration 18906: total_loss: 0.404977, loss_sup: 0.041980, loss_mps: 0.135350, loss_cps: 0.227648
[13:41:25.373] iteration 18907: total_loss: 0.492124, loss_sup: 0.017252, loss_mps: 0.167127, loss_cps: 0.307745
[13:41:25.519] iteration 18908: total_loss: 0.478657, loss_sup: 0.060938, loss_mps: 0.151686, loss_cps: 0.266033
[13:41:25.666] iteration 18909: total_loss: 0.593071, loss_sup: 0.051040, loss_mps: 0.187390, loss_cps: 0.354641
[13:41:25.812] iteration 18910: total_loss: 0.608628, loss_sup: 0.154325, loss_mps: 0.169275, loss_cps: 0.285028
[13:41:25.958] iteration 18911: total_loss: 0.416288, loss_sup: 0.062578, loss_mps: 0.123223, loss_cps: 0.230487
[13:41:26.107] iteration 18912: total_loss: 0.623540, loss_sup: 0.106294, loss_mps: 0.181995, loss_cps: 0.335252
[13:41:26.252] iteration 18913: total_loss: 0.597477, loss_sup: 0.010813, loss_mps: 0.203817, loss_cps: 0.382847
[13:41:26.398] iteration 18914: total_loss: 0.309434, loss_sup: 0.083609, loss_mps: 0.093154, loss_cps: 0.132671
[13:41:26.544] iteration 18915: total_loss: 0.423952, loss_sup: 0.122115, loss_mps: 0.119504, loss_cps: 0.182332
[13:41:26.689] iteration 18916: total_loss: 0.279987, loss_sup: 0.005466, loss_mps: 0.102040, loss_cps: 0.172481
[13:41:26.836] iteration 18917: total_loss: 0.593189, loss_sup: 0.108003, loss_mps: 0.163542, loss_cps: 0.321644
[13:41:26.981] iteration 18918: total_loss: 0.880540, loss_sup: 0.166612, loss_mps: 0.250917, loss_cps: 0.463011
[13:41:27.127] iteration 18919: total_loss: 0.803986, loss_sup: 0.083900, loss_mps: 0.228240, loss_cps: 0.491845
[13:41:27.273] iteration 18920: total_loss: 0.261525, loss_sup: 0.008636, loss_mps: 0.096246, loss_cps: 0.156643
[13:41:27.418] iteration 18921: total_loss: 0.477862, loss_sup: 0.023041, loss_mps: 0.162907, loss_cps: 0.291913
[13:41:27.563] iteration 18922: total_loss: 0.546470, loss_sup: 0.006410, loss_mps: 0.174692, loss_cps: 0.365368
[13:41:27.710] iteration 18923: total_loss: 0.368720, loss_sup: 0.023994, loss_mps: 0.120987, loss_cps: 0.223739
[13:41:27.855] iteration 18924: total_loss: 0.516235, loss_sup: 0.202146, loss_mps: 0.119883, loss_cps: 0.194205
[13:41:28.001] iteration 18925: total_loss: 0.329042, loss_sup: 0.013120, loss_mps: 0.113365, loss_cps: 0.202557
[13:41:28.146] iteration 18926: total_loss: 0.495732, loss_sup: 0.039210, loss_mps: 0.158659, loss_cps: 0.297862
[13:41:28.293] iteration 18927: total_loss: 0.505927, loss_sup: 0.006575, loss_mps: 0.164978, loss_cps: 0.334374
[13:41:28.439] iteration 18928: total_loss: 0.388707, loss_sup: 0.103328, loss_mps: 0.108084, loss_cps: 0.177295
[13:41:28.585] iteration 18929: total_loss: 0.667651, loss_sup: 0.173958, loss_mps: 0.166028, loss_cps: 0.327665
[13:41:28.734] iteration 18930: total_loss: 0.572383, loss_sup: 0.022929, loss_mps: 0.188339, loss_cps: 0.361116
[13:41:28.883] iteration 18931: total_loss: 0.585036, loss_sup: 0.195279, loss_mps: 0.140531, loss_cps: 0.249226
[13:41:29.028] iteration 18932: total_loss: 0.399951, loss_sup: 0.065903, loss_mps: 0.118983, loss_cps: 0.215065
[13:41:29.175] iteration 18933: total_loss: 0.449955, loss_sup: 0.096110, loss_mps: 0.129109, loss_cps: 0.224735
[13:41:29.322] iteration 18934: total_loss: 0.472108, loss_sup: 0.151168, loss_mps: 0.113944, loss_cps: 0.206996
[13:41:29.467] iteration 18935: total_loss: 0.596204, loss_sup: 0.058996, loss_mps: 0.186843, loss_cps: 0.350366
[13:41:29.612] iteration 18936: total_loss: 0.454322, loss_sup: 0.114590, loss_mps: 0.120604, loss_cps: 0.219128
[13:41:29.757] iteration 18937: total_loss: 0.389798, loss_sup: 0.038144, loss_mps: 0.121839, loss_cps: 0.229816
[13:41:29.903] iteration 18938: total_loss: 0.428305, loss_sup: 0.068524, loss_mps: 0.126983, loss_cps: 0.232797
[13:41:30.048] iteration 18939: total_loss: 0.712919, loss_sup: 0.131147, loss_mps: 0.193312, loss_cps: 0.388460
[13:41:30.194] iteration 18940: total_loss: 0.681009, loss_sup: 0.073775, loss_mps: 0.193410, loss_cps: 0.413824
[13:41:30.341] iteration 18941: total_loss: 0.506943, loss_sup: 0.056750, loss_mps: 0.153366, loss_cps: 0.296827
[13:41:30.487] iteration 18942: total_loss: 0.387530, loss_sup: 0.039532, loss_mps: 0.123344, loss_cps: 0.224653
[13:41:30.632] iteration 18943: total_loss: 0.397183, loss_sup: 0.054923, loss_mps: 0.115338, loss_cps: 0.226922
[13:41:30.778] iteration 18944: total_loss: 0.262179, loss_sup: 0.034184, loss_mps: 0.086040, loss_cps: 0.141955
[13:41:30.924] iteration 18945: total_loss: 0.323616, loss_sup: 0.028460, loss_mps: 0.106601, loss_cps: 0.188556
[13:41:31.069] iteration 18946: total_loss: 0.534028, loss_sup: 0.122020, loss_mps: 0.139334, loss_cps: 0.272675
[13:41:31.217] iteration 18947: total_loss: 0.330166, loss_sup: 0.076647, loss_mps: 0.090493, loss_cps: 0.163026
[13:41:31.365] iteration 18948: total_loss: 0.344735, loss_sup: 0.095936, loss_mps: 0.091089, loss_cps: 0.157709
[13:41:31.511] iteration 18949: total_loss: 0.835794, loss_sup: 0.127846, loss_mps: 0.237560, loss_cps: 0.470388
[13:41:31.656] iteration 18950: total_loss: 0.301770, loss_sup: 0.018108, loss_mps: 0.104346, loss_cps: 0.179317
[13:41:31.802] iteration 18951: total_loss: 0.423351, loss_sup: 0.042918, loss_mps: 0.132300, loss_cps: 0.248133
[13:41:31.948] iteration 18952: total_loss: 0.780776, loss_sup: 0.178636, loss_mps: 0.196245, loss_cps: 0.405895
[13:41:32.094] iteration 18953: total_loss: 0.492424, loss_sup: 0.060196, loss_mps: 0.147847, loss_cps: 0.284381
[13:41:32.240] iteration 18954: total_loss: 0.298428, loss_sup: 0.039423, loss_mps: 0.095545, loss_cps: 0.163460
[13:41:32.388] iteration 18955: total_loss: 0.601382, loss_sup: 0.063122, loss_mps: 0.176894, loss_cps: 0.361366
[13:41:32.538] iteration 18956: total_loss: 0.558705, loss_sup: 0.103535, loss_mps: 0.154772, loss_cps: 0.300399
[13:41:32.686] iteration 18957: total_loss: 0.486653, loss_sup: 0.062535, loss_mps: 0.144749, loss_cps: 0.279370
[13:41:32.832] iteration 18958: total_loss: 0.572612, loss_sup: 0.230215, loss_mps: 0.127182, loss_cps: 0.215215
[13:41:32.979] iteration 18959: total_loss: 0.405190, loss_sup: 0.075679, loss_mps: 0.123278, loss_cps: 0.206233
[13:41:33.126] iteration 18960: total_loss: 0.341752, loss_sup: 0.011086, loss_mps: 0.113421, loss_cps: 0.217245
[13:41:33.272] iteration 18961: total_loss: 0.608684, loss_sup: 0.202349, loss_mps: 0.142046, loss_cps: 0.264289
[13:41:33.418] iteration 18962: total_loss: 0.353725, loss_sup: 0.132794, loss_mps: 0.084181, loss_cps: 0.136751
[13:41:33.566] iteration 18963: total_loss: 0.658657, loss_sup: 0.213854, loss_mps: 0.156034, loss_cps: 0.288769
[13:41:33.711] iteration 18964: total_loss: 0.468188, loss_sup: 0.072942, loss_mps: 0.144154, loss_cps: 0.251091
[13:41:33.857] iteration 18965: total_loss: 0.362978, loss_sup: 0.049151, loss_mps: 0.110659, loss_cps: 0.203168
[13:41:34.004] iteration 18966: total_loss: 0.360524, loss_sup: 0.050662, loss_mps: 0.114951, loss_cps: 0.194912
[13:41:34.150] iteration 18967: total_loss: 0.306854, loss_sup: 0.025291, loss_mps: 0.101959, loss_cps: 0.179604
[13:41:34.296] iteration 18968: total_loss: 0.357550, loss_sup: 0.077303, loss_mps: 0.100127, loss_cps: 0.180120
[13:41:34.442] iteration 18969: total_loss: 0.471491, loss_sup: 0.012133, loss_mps: 0.155714, loss_cps: 0.303644
[13:41:34.588] iteration 18970: total_loss: 0.289358, loss_sup: 0.014850, loss_mps: 0.097253, loss_cps: 0.177255
[13:41:34.734] iteration 18971: total_loss: 0.239799, loss_sup: 0.034848, loss_mps: 0.075101, loss_cps: 0.129850
[13:41:34.880] iteration 18972: total_loss: 0.559150, loss_sup: 0.018154, loss_mps: 0.180020, loss_cps: 0.360976
[13:41:35.027] iteration 18973: total_loss: 0.964480, loss_sup: 0.023257, loss_mps: 0.289563, loss_cps: 0.651660
[13:41:35.173] iteration 18974: total_loss: 0.232460, loss_sup: 0.015807, loss_mps: 0.080385, loss_cps: 0.136268
[13:41:35.320] iteration 18975: total_loss: 0.485445, loss_sup: 0.094550, loss_mps: 0.133761, loss_cps: 0.257134
[13:41:35.467] iteration 18976: total_loss: 0.568956, loss_sup: 0.113596, loss_mps: 0.154222, loss_cps: 0.301138
[13:41:35.613] iteration 18977: total_loss: 0.558124, loss_sup: 0.035540, loss_mps: 0.168376, loss_cps: 0.354208
[13:41:35.760] iteration 18978: total_loss: 0.553586, loss_sup: 0.047987, loss_mps: 0.164991, loss_cps: 0.340608
[13:41:35.906] iteration 18979: total_loss: 0.848695, loss_sup: 0.123362, loss_mps: 0.227626, loss_cps: 0.497708
[13:41:36.053] iteration 18980: total_loss: 0.379646, loss_sup: 0.078760, loss_mps: 0.110239, loss_cps: 0.190647
[13:41:36.199] iteration 18981: total_loss: 0.627045, loss_sup: 0.191884, loss_mps: 0.150266, loss_cps: 0.284896
[13:41:36.345] iteration 18982: total_loss: 0.898967, loss_sup: 0.235535, loss_mps: 0.200797, loss_cps: 0.462634
[13:41:36.492] iteration 18983: total_loss: 0.300216, loss_sup: 0.011877, loss_mps: 0.100232, loss_cps: 0.188107
[13:41:36.638] iteration 18984: total_loss: 0.453738, loss_sup: 0.083209, loss_mps: 0.124223, loss_cps: 0.246306
[13:41:36.784] iteration 18985: total_loss: 0.616596, loss_sup: 0.036475, loss_mps: 0.185669, loss_cps: 0.394452
[13:41:36.930] iteration 18986: total_loss: 0.494653, loss_sup: 0.059572, loss_mps: 0.150857, loss_cps: 0.284224
[13:41:37.077] iteration 18987: total_loss: 0.279778, loss_sup: 0.013940, loss_mps: 0.096355, loss_cps: 0.169483
[13:41:37.223] iteration 18988: total_loss: 0.432752, loss_sup: 0.050767, loss_mps: 0.128169, loss_cps: 0.253816
[13:41:37.369] iteration 18989: total_loss: 0.456352, loss_sup: 0.084996, loss_mps: 0.130264, loss_cps: 0.241092
[13:41:37.516] iteration 18990: total_loss: 0.625556, loss_sup: 0.111962, loss_mps: 0.171179, loss_cps: 0.342415
[13:41:37.662] iteration 18991: total_loss: 0.624106, loss_sup: 0.070002, loss_mps: 0.175402, loss_cps: 0.378702
[13:41:37.809] iteration 18992: total_loss: 0.384336, loss_sup: 0.037617, loss_mps: 0.124430, loss_cps: 0.222289
[13:41:37.954] iteration 18993: total_loss: 0.635590, loss_sup: 0.230598, loss_mps: 0.134994, loss_cps: 0.269999
[13:41:38.101] iteration 18994: total_loss: 0.505227, loss_sup: 0.072964, loss_mps: 0.150154, loss_cps: 0.282109
[13:41:38.247] iteration 18995: total_loss: 0.226740, loss_sup: 0.008371, loss_mps: 0.083634, loss_cps: 0.134734
[13:41:38.392] iteration 18996: total_loss: 0.927200, loss_sup: 0.097409, loss_mps: 0.264286, loss_cps: 0.565504
[13:41:38.539] iteration 18997: total_loss: 0.354091, loss_sup: 0.091586, loss_mps: 0.101242, loss_cps: 0.161263
[13:41:38.685] iteration 18998: total_loss: 0.599248, loss_sup: 0.040971, loss_mps: 0.185129, loss_cps: 0.373148
[13:41:38.832] iteration 18999: total_loss: 0.763120, loss_sup: 0.236483, loss_mps: 0.183688, loss_cps: 0.342949
[13:41:38.978] iteration 19000: total_loss: 0.347607, loss_sup: 0.023087, loss_mps: 0.117340, loss_cps: 0.207179
[13:41:38.978] Evaluation Started ==>
[13:41:50.344] ==> valid iteration 19000: unet metrics: {'dc': 0.64257969626811, 'jc': 0.5255614126535219, 'pre': 0.7745703249210607, 'hd': 5.623248145353439}, ynet metrics: {'dc': 0.5500537739448595, 'jc': 0.4426827264389759, 'pre': 0.7759231735603344, 'hd': 5.519001330151079}.
[13:41:50.346] Evaluation Finished!⏹️
[13:41:50.497] iteration 19001: total_loss: 0.379926, loss_sup: 0.044386, loss_mps: 0.120993, loss_cps: 0.214547
[13:41:50.644] iteration 19002: total_loss: 0.455331, loss_sup: 0.061231, loss_mps: 0.130829, loss_cps: 0.263271
[13:41:50.789] iteration 19003: total_loss: 0.538718, loss_sup: 0.087050, loss_mps: 0.150428, loss_cps: 0.301239
[13:41:50.934] iteration 19004: total_loss: 0.339609, loss_sup: 0.039470, loss_mps: 0.118660, loss_cps: 0.181479
[13:41:51.080] iteration 19005: total_loss: 0.451643, loss_sup: 0.005472, loss_mps: 0.146531, loss_cps: 0.299640
[13:41:51.226] iteration 19006: total_loss: 0.547797, loss_sup: 0.038201, loss_mps: 0.166794, loss_cps: 0.342802
[13:41:51.373] iteration 19007: total_loss: 0.621586, loss_sup: 0.053358, loss_mps: 0.181672, loss_cps: 0.386556
[13:41:51.520] iteration 19008: total_loss: 0.765702, loss_sup: 0.163910, loss_mps: 0.197862, loss_cps: 0.403930
[13:41:51.667] iteration 19009: total_loss: 0.498390, loss_sup: 0.031014, loss_mps: 0.146127, loss_cps: 0.321248
[13:41:51.812] iteration 19010: total_loss: 0.472999, loss_sup: 0.068376, loss_mps: 0.133374, loss_cps: 0.271249
[13:41:51.958] iteration 19011: total_loss: 0.578755, loss_sup: 0.054398, loss_mps: 0.175616, loss_cps: 0.348741
[13:41:52.105] iteration 19012: total_loss: 0.556833, loss_sup: 0.133947, loss_mps: 0.145106, loss_cps: 0.277780
[13:41:52.251] iteration 19013: total_loss: 0.376371, loss_sup: 0.050731, loss_mps: 0.118003, loss_cps: 0.207638
[13:41:52.398] iteration 19014: total_loss: 0.381562, loss_sup: 0.058417, loss_mps: 0.115396, loss_cps: 0.207750
[13:41:52.544] iteration 19015: total_loss: 0.367985, loss_sup: 0.049405, loss_mps: 0.104795, loss_cps: 0.213785
[13:41:52.690] iteration 19016: total_loss: 0.439692, loss_sup: 0.061000, loss_mps: 0.126238, loss_cps: 0.252454
[13:41:52.835] iteration 19017: total_loss: 0.582525, loss_sup: 0.019133, loss_mps: 0.186939, loss_cps: 0.376453
[13:41:52.981] iteration 19018: total_loss: 0.548986, loss_sup: 0.092746, loss_mps: 0.147161, loss_cps: 0.309079
[13:41:53.126] iteration 19019: total_loss: 0.385949, loss_sup: 0.016108, loss_mps: 0.135670, loss_cps: 0.234171
[13:41:53.272] iteration 19020: total_loss: 0.665462, loss_sup: 0.019985, loss_mps: 0.209224, loss_cps: 0.436253
[13:41:53.417] iteration 19021: total_loss: 0.433290, loss_sup: 0.081595, loss_mps: 0.126230, loss_cps: 0.225464
[13:41:53.563] iteration 19022: total_loss: 0.496278, loss_sup: 0.046491, loss_mps: 0.151531, loss_cps: 0.298256
[13:41:53.709] iteration 19023: total_loss: 0.642134, loss_sup: 0.219179, loss_mps: 0.141181, loss_cps: 0.281775
[13:41:53.855] iteration 19024: total_loss: 0.434362, loss_sup: 0.018972, loss_mps: 0.140730, loss_cps: 0.274660
[13:41:54.000] iteration 19025: total_loss: 0.724480, loss_sup: 0.182478, loss_mps: 0.168887, loss_cps: 0.373116
[13:41:54.146] iteration 19026: total_loss: 0.896567, loss_sup: 0.036053, loss_mps: 0.263716, loss_cps: 0.596798
[13:41:54.291] iteration 19027: total_loss: 0.446767, loss_sup: 0.058219, loss_mps: 0.133510, loss_cps: 0.255038
[13:41:54.438] iteration 19028: total_loss: 0.428297, loss_sup: 0.064593, loss_mps: 0.122191, loss_cps: 0.241512
[13:41:54.584] iteration 19029: total_loss: 1.251789, loss_sup: 0.331449, loss_mps: 0.278894, loss_cps: 0.641447
[13:41:54.729] iteration 19030: total_loss: 0.352777, loss_sup: 0.042764, loss_mps: 0.109772, loss_cps: 0.200241
[13:41:54.875] iteration 19031: total_loss: 0.455391, loss_sup: 0.111346, loss_mps: 0.117406, loss_cps: 0.226639
[13:41:55.021] iteration 19032: total_loss: 0.650416, loss_sup: 0.005070, loss_mps: 0.207376, loss_cps: 0.437970
[13:41:55.166] iteration 19033: total_loss: 1.197179, loss_sup: 0.215406, loss_mps: 0.306130, loss_cps: 0.675643
[13:41:55.313] iteration 19034: total_loss: 1.021361, loss_sup: 0.181133, loss_mps: 0.273950, loss_cps: 0.566277
[13:41:55.459] iteration 19035: total_loss: 1.153613, loss_sup: 0.111969, loss_mps: 0.322075, loss_cps: 0.719569
[13:41:55.604] iteration 19036: total_loss: 0.523184, loss_sup: 0.036477, loss_mps: 0.168708, loss_cps: 0.317998
[13:41:55.751] iteration 19037: total_loss: 0.676123, loss_sup: 0.071468, loss_mps: 0.208744, loss_cps: 0.395911
[13:41:55.899] iteration 19038: total_loss: 0.430273, loss_sup: 0.024327, loss_mps: 0.134658, loss_cps: 0.271287
[13:41:56.044] iteration 19039: total_loss: 0.404122, loss_sup: 0.023781, loss_mps: 0.132333, loss_cps: 0.248008
[13:41:56.190] iteration 19040: total_loss: 1.257684, loss_sup: 0.665553, loss_mps: 0.197325, loss_cps: 0.394806
[13:41:56.336] iteration 19041: total_loss: 0.444385, loss_sup: 0.013895, loss_mps: 0.150021, loss_cps: 0.280468
[13:41:56.481] iteration 19042: total_loss: 0.812910, loss_sup: 0.159387, loss_mps: 0.212365, loss_cps: 0.441159
[13:41:56.627] iteration 19043: total_loss: 0.403072, loss_sup: 0.146879, loss_mps: 0.096608, loss_cps: 0.159585
[13:41:56.772] iteration 19044: total_loss: 0.577639, loss_sup: 0.078090, loss_mps: 0.173540, loss_cps: 0.326008
[13:41:56.918] iteration 19045: total_loss: 0.363207, loss_sup: 0.023877, loss_mps: 0.125408, loss_cps: 0.213922
[13:41:57.065] iteration 19046: total_loss: 0.557756, loss_sup: 0.016258, loss_mps: 0.183675, loss_cps: 0.357822
[13:41:57.211] iteration 19047: total_loss: 0.390783, loss_sup: 0.029177, loss_mps: 0.128838, loss_cps: 0.232768
[13:41:57.357] iteration 19048: total_loss: 0.733634, loss_sup: 0.103541, loss_mps: 0.204726, loss_cps: 0.425366
[13:41:57.502] iteration 19049: total_loss: 0.520677, loss_sup: 0.036611, loss_mps: 0.161507, loss_cps: 0.322560
[13:41:57.648] iteration 19050: total_loss: 0.533319, loss_sup: 0.010984, loss_mps: 0.177931, loss_cps: 0.344404
[13:41:57.794] iteration 19051: total_loss: 0.431974, loss_sup: 0.106145, loss_mps: 0.122635, loss_cps: 0.203194
[13:41:57.939] iteration 19052: total_loss: 0.363790, loss_sup: 0.015745, loss_mps: 0.129239, loss_cps: 0.218806
[13:41:58.085] iteration 19053: total_loss: 0.392140, loss_sup: 0.028337, loss_mps: 0.130732, loss_cps: 0.233071
[13:41:58.234] iteration 19054: total_loss: 0.676085, loss_sup: 0.350310, loss_mps: 0.116004, loss_cps: 0.209771
[13:41:58.381] iteration 19055: total_loss: 0.578384, loss_sup: 0.111555, loss_mps: 0.163719, loss_cps: 0.303109
[13:41:58.526] iteration 19056: total_loss: 0.587633, loss_sup: 0.118430, loss_mps: 0.166274, loss_cps: 0.302928
[13:41:58.672] iteration 19057: total_loss: 1.168309, loss_sup: 0.123342, loss_mps: 0.339245, loss_cps: 0.705722
[13:41:58.818] iteration 19058: total_loss: 0.598035, loss_sup: 0.224904, loss_mps: 0.141580, loss_cps: 0.231551
[13:41:58.966] iteration 19059: total_loss: 0.742556, loss_sup: 0.208671, loss_mps: 0.180142, loss_cps: 0.353742
[13:41:59.112] iteration 19060: total_loss: 0.468443, loss_sup: 0.031329, loss_mps: 0.157737, loss_cps: 0.279376
[13:41:59.258] iteration 19061: total_loss: 0.500917, loss_sup: 0.051339, loss_mps: 0.155687, loss_cps: 0.293892
[13:41:59.404] iteration 19062: total_loss: 0.577314, loss_sup: 0.060209, loss_mps: 0.180643, loss_cps: 0.336463
[13:41:59.551] iteration 19063: total_loss: 0.635422, loss_sup: 0.038936, loss_mps: 0.194444, loss_cps: 0.402041
[13:41:59.697] iteration 19064: total_loss: 0.403516, loss_sup: 0.028918, loss_mps: 0.126872, loss_cps: 0.247726
[13:41:59.842] iteration 19065: total_loss: 0.760686, loss_sup: 0.084856, loss_mps: 0.225186, loss_cps: 0.450643
[13:41:59.990] iteration 19066: total_loss: 0.428847, loss_sup: 0.058148, loss_mps: 0.132374, loss_cps: 0.238324
[13:42:00.136] iteration 19067: total_loss: 0.322940, loss_sup: 0.076002, loss_mps: 0.094637, loss_cps: 0.152301
[13:42:00.282] iteration 19068: total_loss: 0.512543, loss_sup: 0.116423, loss_mps: 0.140157, loss_cps: 0.255963
[13:42:00.427] iteration 19069: total_loss: 0.222782, loss_sup: 0.006262, loss_mps: 0.081688, loss_cps: 0.134832
[13:42:00.573] iteration 19070: total_loss: 0.315104, loss_sup: 0.016737, loss_mps: 0.111892, loss_cps: 0.186475
[13:42:00.719] iteration 19071: total_loss: 0.460529, loss_sup: 0.065923, loss_mps: 0.136051, loss_cps: 0.258555
[13:42:00.865] iteration 19072: total_loss: 0.715615, loss_sup: 0.210033, loss_mps: 0.178738, loss_cps: 0.326844
[13:42:01.011] iteration 19073: total_loss: 0.832020, loss_sup: 0.053953, loss_mps: 0.242721, loss_cps: 0.535346
[13:42:01.159] iteration 19074: total_loss: 0.352212, loss_sup: 0.087991, loss_mps: 0.102123, loss_cps: 0.162099
[13:42:01.305] iteration 19075: total_loss: 0.403500, loss_sup: 0.043189, loss_mps: 0.125379, loss_cps: 0.234932
[13:42:01.451] iteration 19076: total_loss: 0.748135, loss_sup: 0.201260, loss_mps: 0.178871, loss_cps: 0.368005
[13:42:01.597] iteration 19077: total_loss: 0.604701, loss_sup: 0.059807, loss_mps: 0.179018, loss_cps: 0.365876
[13:42:01.743] iteration 19078: total_loss: 0.989580, loss_sup: 0.211891, loss_mps: 0.240410, loss_cps: 0.537278
[13:42:01.889] iteration 19079: total_loss: 0.627023, loss_sup: 0.103575, loss_mps: 0.170526, loss_cps: 0.352922
[13:42:02.035] iteration 19080: total_loss: 0.426420, loss_sup: 0.118804, loss_mps: 0.113218, loss_cps: 0.194398
[13:42:02.182] iteration 19081: total_loss: 0.587687, loss_sup: 0.043165, loss_mps: 0.186471, loss_cps: 0.358050
[13:42:02.328] iteration 19082: total_loss: 0.354150, loss_sup: 0.041406, loss_mps: 0.115457, loss_cps: 0.197288
[13:42:02.474] iteration 19083: total_loss: 0.775989, loss_sup: 0.151587, loss_mps: 0.209884, loss_cps: 0.414517
[13:42:02.620] iteration 19084: total_loss: 0.710913, loss_sup: 0.189375, loss_mps: 0.176023, loss_cps: 0.345515
[13:42:02.766] iteration 19085: total_loss: 0.661498, loss_sup: 0.124817, loss_mps: 0.182737, loss_cps: 0.353944
[13:42:02.912] iteration 19086: total_loss: 0.725025, loss_sup: 0.011538, loss_mps: 0.227892, loss_cps: 0.485595
[13:42:03.058] iteration 19087: total_loss: 0.284629, loss_sup: 0.012347, loss_mps: 0.099733, loss_cps: 0.172549
[13:42:03.204] iteration 19088: total_loss: 0.279771, loss_sup: 0.044090, loss_mps: 0.087664, loss_cps: 0.148018
[13:42:03.350] iteration 19089: total_loss: 0.688259, loss_sup: 0.055712, loss_mps: 0.215983, loss_cps: 0.416563
[13:42:03.497] iteration 19090: total_loss: 0.630235, loss_sup: 0.194788, loss_mps: 0.152461, loss_cps: 0.282986
[13:42:03.643] iteration 19091: total_loss: 0.367346, loss_sup: 0.017784, loss_mps: 0.122525, loss_cps: 0.227037
[13:42:03.790] iteration 19092: total_loss: 0.784287, loss_sup: 0.091881, loss_mps: 0.238580, loss_cps: 0.453826
[13:42:03.936] iteration 19093: total_loss: 0.415591, loss_sup: 0.149559, loss_mps: 0.100737, loss_cps: 0.165296
[13:42:04.082] iteration 19094: total_loss: 0.423858, loss_sup: 0.089366, loss_mps: 0.116106, loss_cps: 0.218385
[13:42:04.228] iteration 19095: total_loss: 0.769177, loss_sup: 0.292738, loss_mps: 0.161760, loss_cps: 0.314679
[13:42:04.374] iteration 19096: total_loss: 0.452801, loss_sup: 0.031347, loss_mps: 0.143064, loss_cps: 0.278391
[13:42:04.521] iteration 19097: total_loss: 0.299123, loss_sup: 0.044148, loss_mps: 0.095693, loss_cps: 0.159282
[13:42:04.667] iteration 19098: total_loss: 0.654099, loss_sup: 0.042552, loss_mps: 0.197671, loss_cps: 0.413876
[13:42:04.815] iteration 19099: total_loss: 1.300231, loss_sup: 0.104168, loss_mps: 0.368560, loss_cps: 0.827503
[13:42:04.960] iteration 19100: total_loss: 0.589579, loss_sup: 0.176724, loss_mps: 0.139042, loss_cps: 0.273813
[13:42:04.961] Evaluation Started ==>
[13:42:16.297] ==> valid iteration 19100: unet metrics: {'dc': 0.6331262703018339, 'jc': 0.5181310939636723, 'pre': 0.7915652645041745, 'hd': 5.530572419931494}, ynet metrics: {'dc': 0.5481154995517382, 'jc': 0.44030433991324686, 'pre': 0.7792391582906362, 'hd': 5.551836806518329}.
[13:42:16.299] Evaluation Finished!⏹️
[13:42:16.448] iteration 19101: total_loss: 0.351715, loss_sup: 0.045314, loss_mps: 0.116474, loss_cps: 0.189926
[13:42:16.596] iteration 19102: total_loss: 0.505174, loss_sup: 0.040839, loss_mps: 0.164286, loss_cps: 0.300049
[13:42:16.741] iteration 19103: total_loss: 0.295752, loss_sup: 0.054860, loss_mps: 0.092149, loss_cps: 0.148743
[13:42:16.887] iteration 19104: total_loss: 0.358533, loss_sup: 0.023032, loss_mps: 0.113045, loss_cps: 0.222456
[13:42:17.032] iteration 19105: total_loss: 0.432717, loss_sup: 0.044631, loss_mps: 0.136266, loss_cps: 0.251821
[13:42:17.181] iteration 19106: total_loss: 0.397720, loss_sup: 0.016223, loss_mps: 0.129943, loss_cps: 0.251554
[13:42:17.327] iteration 19107: total_loss: 0.640736, loss_sup: 0.037780, loss_mps: 0.197967, loss_cps: 0.404989
[13:42:17.472] iteration 19108: total_loss: 0.683314, loss_sup: 0.141557, loss_mps: 0.178757, loss_cps: 0.363000
[13:42:17.617] iteration 19109: total_loss: 0.525826, loss_sup: 0.105010, loss_mps: 0.145832, loss_cps: 0.274984
[13:42:17.762] iteration 19110: total_loss: 0.374919, loss_sup: 0.014850, loss_mps: 0.126243, loss_cps: 0.233827
[13:42:17.907] iteration 19111: total_loss: 0.494083, loss_sup: 0.106569, loss_mps: 0.130485, loss_cps: 0.257028
[13:42:18.052] iteration 19112: total_loss: 0.406992, loss_sup: 0.034527, loss_mps: 0.125427, loss_cps: 0.247037
[13:42:18.197] iteration 19113: total_loss: 0.526524, loss_sup: 0.112427, loss_mps: 0.142136, loss_cps: 0.271961
[13:42:18.343] iteration 19114: total_loss: 0.602393, loss_sup: 0.111525, loss_mps: 0.166819, loss_cps: 0.324049
[13:42:18.488] iteration 19115: total_loss: 0.956541, loss_sup: 0.302442, loss_mps: 0.217568, loss_cps: 0.436531
[13:42:18.633] iteration 19116: total_loss: 0.434999, loss_sup: 0.039468, loss_mps: 0.138551, loss_cps: 0.256980
[13:42:18.777] iteration 19117: total_loss: 0.788794, loss_sup: 0.215483, loss_mps: 0.200708, loss_cps: 0.372603
[13:42:18.923] iteration 19118: total_loss: 0.716448, loss_sup: 0.160480, loss_mps: 0.189921, loss_cps: 0.366047
[13:42:19.068] iteration 19119: total_loss: 0.498540, loss_sup: 0.040471, loss_mps: 0.159361, loss_cps: 0.298708
[13:42:19.215] iteration 19120: total_loss: 0.620611, loss_sup: 0.026300, loss_mps: 0.193213, loss_cps: 0.401098
[13:42:19.360] iteration 19121: total_loss: 0.334502, loss_sup: 0.046583, loss_mps: 0.102204, loss_cps: 0.185715
[13:42:19.506] iteration 19122: total_loss: 0.873259, loss_sup: 0.172294, loss_mps: 0.226039, loss_cps: 0.474927
[13:42:19.652] iteration 19123: total_loss: 0.588143, loss_sup: 0.097743, loss_mps: 0.174698, loss_cps: 0.315702
[13:42:19.797] iteration 19124: total_loss: 0.401679, loss_sup: 0.065439, loss_mps: 0.130357, loss_cps: 0.205883
[13:42:19.943] iteration 19125: total_loss: 0.795745, loss_sup: 0.138852, loss_mps: 0.205365, loss_cps: 0.451528
[13:42:20.088] iteration 19126: total_loss: 1.183659, loss_sup: 0.190165, loss_mps: 0.318806, loss_cps: 0.674688
[13:42:20.233] iteration 19127: total_loss: 0.700704, loss_sup: 0.126991, loss_mps: 0.197154, loss_cps: 0.376559
[13:42:20.378] iteration 19128: total_loss: 1.051387, loss_sup: 0.075830, loss_mps: 0.304047, loss_cps: 0.671510
[13:42:20.525] iteration 19129: total_loss: 0.417360, loss_sup: 0.017532, loss_mps: 0.140653, loss_cps: 0.259175
[13:42:20.670] iteration 19130: total_loss: 0.392454, loss_sup: 0.027915, loss_mps: 0.129547, loss_cps: 0.234993
[13:42:20.815] iteration 19131: total_loss: 0.438557, loss_sup: 0.156094, loss_mps: 0.103535, loss_cps: 0.178928
[13:42:20.960] iteration 19132: total_loss: 0.500203, loss_sup: 0.103610, loss_mps: 0.145781, loss_cps: 0.250812
[13:42:21.106] iteration 19133: total_loss: 0.399333, loss_sup: 0.020392, loss_mps: 0.146922, loss_cps: 0.232019
[13:42:21.252] iteration 19134: total_loss: 0.421859, loss_sup: 0.067334, loss_mps: 0.122403, loss_cps: 0.232122
[13:42:21.398] iteration 19135: total_loss: 0.673471, loss_sup: 0.204106, loss_mps: 0.165348, loss_cps: 0.304017
[13:42:21.545] iteration 19136: total_loss: 0.560961, loss_sup: 0.100742, loss_mps: 0.159918, loss_cps: 0.300301
[13:42:21.690] iteration 19137: total_loss: 0.501448, loss_sup: 0.069492, loss_mps: 0.154821, loss_cps: 0.277134
[13:42:21.837] iteration 19138: total_loss: 0.462775, loss_sup: 0.065185, loss_mps: 0.138247, loss_cps: 0.259343
[13:42:21.983] iteration 19139: total_loss: 0.750629, loss_sup: 0.067321, loss_mps: 0.218907, loss_cps: 0.464401
[13:42:22.128] iteration 19140: total_loss: 0.400698, loss_sup: 0.102686, loss_mps: 0.107370, loss_cps: 0.190642
[13:42:22.274] iteration 19141: total_loss: 0.558266, loss_sup: 0.080824, loss_mps: 0.164065, loss_cps: 0.313377
[13:42:22.419] iteration 19142: total_loss: 0.755347, loss_sup: 0.053368, loss_mps: 0.225109, loss_cps: 0.476870
[13:42:22.566] iteration 19143: total_loss: 0.628919, loss_sup: 0.121681, loss_mps: 0.167509, loss_cps: 0.339729
[13:42:22.711] iteration 19144: total_loss: 0.275545, loss_sup: 0.017804, loss_mps: 0.095568, loss_cps: 0.162172
[13:42:22.857] iteration 19145: total_loss: 0.735293, loss_sup: 0.077833, loss_mps: 0.219470, loss_cps: 0.437990
[13:42:23.002] iteration 19146: total_loss: 0.527919, loss_sup: 0.047519, loss_mps: 0.166945, loss_cps: 0.313455
[13:42:23.148] iteration 19147: total_loss: 0.549367, loss_sup: 0.043399, loss_mps: 0.169782, loss_cps: 0.336187
[13:42:23.293] iteration 19148: total_loss: 0.443224, loss_sup: 0.024771, loss_mps: 0.145845, loss_cps: 0.272608
[13:42:23.439] iteration 19149: total_loss: 0.829631, loss_sup: 0.102701, loss_mps: 0.234560, loss_cps: 0.492370
[13:42:23.586] iteration 19150: total_loss: 0.395680, loss_sup: 0.058914, loss_mps: 0.120405, loss_cps: 0.216361
[13:42:23.732] iteration 19151: total_loss: 0.506367, loss_sup: 0.037502, loss_mps: 0.156418, loss_cps: 0.312447
[13:42:23.877] iteration 19152: total_loss: 0.517542, loss_sup: 0.115818, loss_mps: 0.142180, loss_cps: 0.259544
[13:42:24.023] iteration 19153: total_loss: 0.640705, loss_sup: 0.107091, loss_mps: 0.178504, loss_cps: 0.355110
[13:42:24.169] iteration 19154: total_loss: 0.503132, loss_sup: 0.097475, loss_mps: 0.140896, loss_cps: 0.264761
[13:42:24.315] iteration 19155: total_loss: 0.255881, loss_sup: 0.003813, loss_mps: 0.093008, loss_cps: 0.159060
[13:42:24.461] iteration 19156: total_loss: 0.555525, loss_sup: 0.097344, loss_mps: 0.152494, loss_cps: 0.305687
[13:42:24.608] iteration 19157: total_loss: 0.607724, loss_sup: 0.042209, loss_mps: 0.185697, loss_cps: 0.379818
[13:42:24.754] iteration 19158: total_loss: 0.928278, loss_sup: 0.072190, loss_mps: 0.273996, loss_cps: 0.582093
[13:42:24.899] iteration 19159: total_loss: 0.331865, loss_sup: 0.009059, loss_mps: 0.118391, loss_cps: 0.204415
[13:42:25.045] iteration 19160: total_loss: 0.562148, loss_sup: 0.053572, loss_mps: 0.172652, loss_cps: 0.335924
[13:42:25.191] iteration 19161: total_loss: 0.464804, loss_sup: 0.053454, loss_mps: 0.142122, loss_cps: 0.269227
[13:42:25.337] iteration 19162: total_loss: 0.744331, loss_sup: 0.114737, loss_mps: 0.206586, loss_cps: 0.423008
[13:42:25.482] iteration 19163: total_loss: 0.876541, loss_sup: 0.207648, loss_mps: 0.218079, loss_cps: 0.450815
[13:42:25.629] iteration 19164: total_loss: 0.703923, loss_sup: 0.121190, loss_mps: 0.192363, loss_cps: 0.390370
[13:42:25.774] iteration 19165: total_loss: 0.658016, loss_sup: 0.149904, loss_mps: 0.173752, loss_cps: 0.334360
[13:42:25.920] iteration 19166: total_loss: 0.358853, loss_sup: 0.029227, loss_mps: 0.116883, loss_cps: 0.212742
[13:42:26.066] iteration 19167: total_loss: 0.387593, loss_sup: 0.010899, loss_mps: 0.125374, loss_cps: 0.251320
[13:42:26.212] iteration 19168: total_loss: 0.512266, loss_sup: 0.089439, loss_mps: 0.143293, loss_cps: 0.279534
[13:42:26.357] iteration 19169: total_loss: 0.339922, loss_sup: 0.121535, loss_mps: 0.084207, loss_cps: 0.134180
[13:42:26.503] iteration 19170: total_loss: 0.486509, loss_sup: 0.014573, loss_mps: 0.162505, loss_cps: 0.309431
[13:42:26.648] iteration 19171: total_loss: 0.551172, loss_sup: 0.324828, loss_mps: 0.084588, loss_cps: 0.141756
[13:42:26.794] iteration 19172: total_loss: 0.446243, loss_sup: 0.031736, loss_mps: 0.146643, loss_cps: 0.267864
[13:42:26.940] iteration 19173: total_loss: 0.474917, loss_sup: 0.058281, loss_mps: 0.155938, loss_cps: 0.260699
[13:42:27.085] iteration 19174: total_loss: 0.657572, loss_sup: 0.191168, loss_mps: 0.162730, loss_cps: 0.303675
[13:42:27.231] iteration 19175: total_loss: 0.455374, loss_sup: 0.032615, loss_mps: 0.134743, loss_cps: 0.288016
[13:42:27.377] iteration 19176: total_loss: 0.659485, loss_sup: 0.123660, loss_mps: 0.168686, loss_cps: 0.367139
[13:42:27.523] iteration 19177: total_loss: 0.411744, loss_sup: 0.038099, loss_mps: 0.137288, loss_cps: 0.236357
[13:42:27.669] iteration 19178: total_loss: 0.556108, loss_sup: 0.102127, loss_mps: 0.151755, loss_cps: 0.302226
[13:42:27.815] iteration 19179: total_loss: 0.724071, loss_sup: 0.200086, loss_mps: 0.184199, loss_cps: 0.339785
[13:42:27.960] iteration 19180: total_loss: 0.382636, loss_sup: 0.034411, loss_mps: 0.124182, loss_cps: 0.224043
[13:42:28.106] iteration 19181: total_loss: 0.526023, loss_sup: 0.143537, loss_mps: 0.137365, loss_cps: 0.245120
[13:42:28.252] iteration 19182: total_loss: 0.514177, loss_sup: 0.223836, loss_mps: 0.109367, loss_cps: 0.180974
[13:42:28.397] iteration 19183: total_loss: 0.745069, loss_sup: 0.155847, loss_mps: 0.188627, loss_cps: 0.400594
[13:42:28.543] iteration 19184: total_loss: 0.491025, loss_sup: 0.028062, loss_mps: 0.152406, loss_cps: 0.310557
[13:42:28.689] iteration 19185: total_loss: 1.016846, loss_sup: 0.273536, loss_mps: 0.233024, loss_cps: 0.510286
[13:42:28.835] iteration 19186: total_loss: 0.428578, loss_sup: 0.120010, loss_mps: 0.110556, loss_cps: 0.198012
[13:42:28.980] iteration 19187: total_loss: 0.393758, loss_sup: 0.131964, loss_mps: 0.098157, loss_cps: 0.163638
[13:42:29.126] iteration 19188: total_loss: 0.614474, loss_sup: 0.134972, loss_mps: 0.159570, loss_cps: 0.319932
[13:42:29.272] iteration 19189: total_loss: 0.730143, loss_sup: 0.076721, loss_mps: 0.210555, loss_cps: 0.442867
[13:42:29.417] iteration 19190: total_loss: 0.424760, loss_sup: 0.071030, loss_mps: 0.127164, loss_cps: 0.226565
[13:42:29.563] iteration 19191: total_loss: 0.270245, loss_sup: 0.008095, loss_mps: 0.096648, loss_cps: 0.165501
[13:42:29.710] iteration 19192: total_loss: 0.420739, loss_sup: 0.065931, loss_mps: 0.126132, loss_cps: 0.228676
[13:42:29.856] iteration 19193: total_loss: 0.412966, loss_sup: 0.055840, loss_mps: 0.128296, loss_cps: 0.228830
[13:42:30.002] iteration 19194: total_loss: 0.464823, loss_sup: 0.074490, loss_mps: 0.133244, loss_cps: 0.257088
[13:42:30.149] iteration 19195: total_loss: 0.303466, loss_sup: 0.065027, loss_mps: 0.088794, loss_cps: 0.149645
[13:42:30.295] iteration 19196: total_loss: 0.595857, loss_sup: 0.205446, loss_mps: 0.133826, loss_cps: 0.256586
[13:42:30.441] iteration 19197: total_loss: 0.589302, loss_sup: 0.324590, loss_mps: 0.096784, loss_cps: 0.167928
[13:42:30.587] iteration 19198: total_loss: 0.432691, loss_sup: 0.051883, loss_mps: 0.134342, loss_cps: 0.246466
[13:42:30.732] iteration 19199: total_loss: 0.191088, loss_sup: 0.014042, loss_mps: 0.070184, loss_cps: 0.106861
[13:42:30.878] iteration 19200: total_loss: 0.491546, loss_sup: 0.013960, loss_mps: 0.168433, loss_cps: 0.309152
[13:42:30.878] Evaluation Started ==>
[13:42:42.222] ==> valid iteration 19200: unet metrics: {'dc': 0.6400312111362666, 'jc': 0.523507250858433, 'pre': 0.7953714849605695, 'hd': 5.443649847946812}, ynet metrics: {'dc': 0.5621014665713657, 'jc': 0.4518842367624102, 'pre': 0.8125335815301786, 'hd': 5.50600832242006}.
[13:42:42.224] Evaluation Finished!⏹️
[13:42:42.374] iteration 19201: total_loss: 0.408475, loss_sup: 0.024606, loss_mps: 0.132909, loss_cps: 0.250960
[13:42:42.521] iteration 19202: total_loss: 0.449965, loss_sup: 0.042992, loss_mps: 0.136471, loss_cps: 0.270502
[13:42:42.667] iteration 19203: total_loss: 0.246980, loss_sup: 0.020223, loss_mps: 0.086057, loss_cps: 0.140700
[13:42:42.811] iteration 19204: total_loss: 0.349360, loss_sup: 0.096639, loss_mps: 0.097383, loss_cps: 0.155338
[13:42:42.958] iteration 19205: total_loss: 0.231970, loss_sup: 0.014635, loss_mps: 0.084766, loss_cps: 0.132570
[13:42:43.103] iteration 19206: total_loss: 0.588924, loss_sup: 0.037217, loss_mps: 0.182039, loss_cps: 0.369668
[13:42:43.249] iteration 19207: total_loss: 0.272319, loss_sup: 0.021567, loss_mps: 0.093113, loss_cps: 0.157639
[13:42:43.394] iteration 19208: total_loss: 0.311896, loss_sup: 0.014061, loss_mps: 0.104649, loss_cps: 0.193187
[13:42:43.539] iteration 19209: total_loss: 0.555262, loss_sup: 0.123722, loss_mps: 0.149460, loss_cps: 0.282081
[13:42:43.684] iteration 19210: total_loss: 0.788767, loss_sup: 0.084322, loss_mps: 0.218778, loss_cps: 0.485667
[13:42:43.829] iteration 19211: total_loss: 0.717472, loss_sup: 0.280769, loss_mps: 0.152557, loss_cps: 0.284146
[13:42:43.975] iteration 19212: total_loss: 0.583054, loss_sup: 0.074801, loss_mps: 0.169900, loss_cps: 0.338353
[13:42:44.122] iteration 19213: total_loss: 0.457235, loss_sup: 0.067746, loss_mps: 0.136485, loss_cps: 0.253004
[13:42:44.267] iteration 19214: total_loss: 0.570975, loss_sup: 0.108913, loss_mps: 0.158978, loss_cps: 0.303084
[13:42:44.413] iteration 19215: total_loss: 0.684863, loss_sup: 0.036663, loss_mps: 0.215234, loss_cps: 0.432967
[13:42:44.558] iteration 19216: total_loss: 0.399383, loss_sup: 0.011531, loss_mps: 0.139958, loss_cps: 0.247894
[13:42:44.704] iteration 19217: total_loss: 0.273058, loss_sup: 0.012231, loss_mps: 0.096969, loss_cps: 0.163859
[13:42:44.849] iteration 19218: total_loss: 0.477640, loss_sup: 0.109638, loss_mps: 0.133602, loss_cps: 0.234400
[13:42:44.995] iteration 19219: total_loss: 0.416510, loss_sup: 0.060101, loss_mps: 0.116498, loss_cps: 0.239911
[13:42:45.140] iteration 19220: total_loss: 0.490187, loss_sup: 0.072290, loss_mps: 0.140991, loss_cps: 0.276907
[13:42:45.286] iteration 19221: total_loss: 0.443607, loss_sup: 0.063298, loss_mps: 0.132919, loss_cps: 0.247391
[13:42:45.431] iteration 19222: total_loss: 0.633253, loss_sup: 0.237231, loss_mps: 0.137436, loss_cps: 0.258586
[13:42:45.577] iteration 19223: total_loss: 0.476186, loss_sup: 0.059234, loss_mps: 0.136305, loss_cps: 0.280647
[13:42:45.722] iteration 19224: total_loss: 0.609902, loss_sup: 0.082975, loss_mps: 0.184016, loss_cps: 0.342910
[13:42:45.867] iteration 19225: total_loss: 0.470001, loss_sup: 0.064074, loss_mps: 0.137344, loss_cps: 0.268582
[13:42:46.012] iteration 19226: total_loss: 0.684333, loss_sup: 0.196207, loss_mps: 0.157105, loss_cps: 0.331022
[13:42:46.158] iteration 19227: total_loss: 0.649614, loss_sup: 0.066306, loss_mps: 0.184932, loss_cps: 0.398375
[13:42:46.222] iteration 19228: total_loss: 0.614473, loss_sup: 0.062173, loss_mps: 0.177823, loss_cps: 0.374478
[13:42:47.504] iteration 19229: total_loss: 0.434927, loss_sup: 0.030808, loss_mps: 0.136986, loss_cps: 0.267134
[13:42:47.652] iteration 19230: total_loss: 0.237242, loss_sup: 0.012082, loss_mps: 0.084672, loss_cps: 0.140487
[13:42:47.798] iteration 19231: total_loss: 0.319604, loss_sup: 0.032068, loss_mps: 0.108292, loss_cps: 0.179244
[13:42:47.945] iteration 19232: total_loss: 0.379677, loss_sup: 0.087480, loss_mps: 0.102275, loss_cps: 0.189922
[13:42:48.090] iteration 19233: total_loss: 0.354309, loss_sup: 0.042886, loss_mps: 0.108669, loss_cps: 0.202755
[13:42:48.237] iteration 19234: total_loss: 0.635520, loss_sup: 0.177855, loss_mps: 0.154556, loss_cps: 0.303109
[13:42:48.382] iteration 19235: total_loss: 0.393490, loss_sup: 0.075767, loss_mps: 0.119270, loss_cps: 0.198453
[13:42:48.527] iteration 19236: total_loss: 0.336398, loss_sup: 0.010989, loss_mps: 0.113141, loss_cps: 0.212268
[13:42:48.673] iteration 19237: total_loss: 0.451456, loss_sup: 0.076899, loss_mps: 0.125624, loss_cps: 0.248933
[13:42:48.818] iteration 19238: total_loss: 0.474599, loss_sup: 0.060822, loss_mps: 0.143016, loss_cps: 0.270761
[13:42:48.965] iteration 19239: total_loss: 0.655568, loss_sup: 0.186312, loss_mps: 0.170344, loss_cps: 0.298912
[13:42:49.110] iteration 19240: total_loss: 0.472282, loss_sup: 0.018223, loss_mps: 0.157950, loss_cps: 0.296109
[13:42:49.256] iteration 19241: total_loss: 0.563221, loss_sup: 0.005111, loss_mps: 0.184727, loss_cps: 0.373384
[13:42:49.402] iteration 19242: total_loss: 0.343583, loss_sup: 0.025866, loss_mps: 0.110260, loss_cps: 0.207457
[13:42:49.548] iteration 19243: total_loss: 0.474058, loss_sup: 0.048280, loss_mps: 0.141423, loss_cps: 0.284356
[13:42:49.694] iteration 19244: total_loss: 0.825903, loss_sup: 0.161033, loss_mps: 0.217786, loss_cps: 0.447084
[13:42:49.840] iteration 19245: total_loss: 1.028497, loss_sup: 0.081187, loss_mps: 0.284870, loss_cps: 0.662441
[13:42:49.986] iteration 19246: total_loss: 0.783694, loss_sup: 0.044187, loss_mps: 0.227208, loss_cps: 0.512299
[13:42:50.133] iteration 19247: total_loss: 0.350294, loss_sup: 0.058236, loss_mps: 0.105496, loss_cps: 0.186561
[13:42:50.279] iteration 19248: total_loss: 0.503123, loss_sup: 0.003846, loss_mps: 0.158616, loss_cps: 0.340661
[13:42:50.425] iteration 19249: total_loss: 0.448187, loss_sup: 0.088831, loss_mps: 0.121002, loss_cps: 0.238353
[13:42:50.572] iteration 19250: total_loss: 0.371638, loss_sup: 0.061466, loss_mps: 0.108274, loss_cps: 0.201898
[13:42:50.718] iteration 19251: total_loss: 0.662122, loss_sup: 0.042198, loss_mps: 0.199939, loss_cps: 0.419985
[13:42:50.865] iteration 19252: total_loss: 0.581040, loss_sup: 0.043556, loss_mps: 0.181575, loss_cps: 0.355910
[13:42:51.011] iteration 19253: total_loss: 1.326266, loss_sup: 0.249041, loss_mps: 0.339453, loss_cps: 0.737773
[13:42:51.157] iteration 19254: total_loss: 0.829164, loss_sup: 0.108120, loss_mps: 0.219580, loss_cps: 0.501464
[13:42:51.309] iteration 19255: total_loss: 0.410323, loss_sup: 0.020430, loss_mps: 0.129413, loss_cps: 0.260480
[13:42:51.455] iteration 19256: total_loss: 0.494040, loss_sup: 0.019245, loss_mps: 0.156270, loss_cps: 0.318524
[13:42:51.601] iteration 19257: total_loss: 0.580939, loss_sup: 0.138871, loss_mps: 0.145686, loss_cps: 0.296381
[13:42:51.747] iteration 19258: total_loss: 0.879854, loss_sup: 0.069667, loss_mps: 0.255722, loss_cps: 0.554465
[13:42:51.894] iteration 19259: total_loss: 0.712013, loss_sup: 0.104804, loss_mps: 0.202564, loss_cps: 0.404644
[13:42:52.041] iteration 19260: total_loss: 0.415613, loss_sup: 0.055592, loss_mps: 0.126082, loss_cps: 0.233938
[13:42:52.188] iteration 19261: total_loss: 0.512520, loss_sup: 0.127882, loss_mps: 0.137096, loss_cps: 0.247542
[13:42:52.336] iteration 19262: total_loss: 0.727378, loss_sup: 0.120229, loss_mps: 0.181052, loss_cps: 0.426096
[13:42:52.485] iteration 19263: total_loss: 0.443464, loss_sup: 0.136218, loss_mps: 0.105427, loss_cps: 0.201819
[13:42:52.632] iteration 19264: total_loss: 0.456675, loss_sup: 0.056201, loss_mps: 0.142293, loss_cps: 0.258181
[13:42:52.778] iteration 19265: total_loss: 0.233094, loss_sup: 0.013015, loss_mps: 0.081955, loss_cps: 0.138124
[13:42:52.924] iteration 19266: total_loss: 0.528926, loss_sup: 0.060844, loss_mps: 0.160210, loss_cps: 0.307872
[13:42:53.069] iteration 19267: total_loss: 0.363418, loss_sup: 0.022601, loss_mps: 0.121648, loss_cps: 0.219169
[13:42:53.216] iteration 19268: total_loss: 0.570666, loss_sup: 0.067283, loss_mps: 0.166942, loss_cps: 0.336441
[13:42:53.363] iteration 19269: total_loss: 0.409812, loss_sup: 0.075877, loss_mps: 0.119965, loss_cps: 0.213969
[13:42:53.509] iteration 19270: total_loss: 0.431082, loss_sup: 0.014908, loss_mps: 0.144187, loss_cps: 0.271987
[13:42:53.655] iteration 19271: total_loss: 0.645687, loss_sup: 0.164341, loss_mps: 0.165579, loss_cps: 0.315767
[13:42:53.801] iteration 19272: total_loss: 0.326329, loss_sup: 0.051610, loss_mps: 0.097100, loss_cps: 0.177619
[13:42:53.947] iteration 19273: total_loss: 0.492030, loss_sup: 0.062665, loss_mps: 0.151045, loss_cps: 0.278321
[13:42:54.094] iteration 19274: total_loss: 0.591231, loss_sup: 0.299699, loss_mps: 0.111817, loss_cps: 0.179714
[13:42:54.240] iteration 19275: total_loss: 0.332323, loss_sup: 0.029876, loss_mps: 0.111679, loss_cps: 0.190768
[13:42:54.387] iteration 19276: total_loss: 0.900765, loss_sup: 0.399472, loss_mps: 0.178987, loss_cps: 0.322307
[13:42:54.535] iteration 19277: total_loss: 0.533196, loss_sup: 0.064545, loss_mps: 0.158619, loss_cps: 0.310032
[13:42:54.681] iteration 19278: total_loss: 0.577827, loss_sup: 0.082991, loss_mps: 0.167325, loss_cps: 0.327511
[13:42:54.827] iteration 19279: total_loss: 0.602234, loss_sup: 0.039069, loss_mps: 0.186802, loss_cps: 0.376364
[13:42:54.973] iteration 19280: total_loss: 0.484881, loss_sup: 0.079466, loss_mps: 0.142425, loss_cps: 0.262989
[13:42:55.120] iteration 19281: total_loss: 0.322489, loss_sup: 0.041132, loss_mps: 0.105296, loss_cps: 0.176061
[13:42:55.267] iteration 19282: total_loss: 0.354969, loss_sup: 0.023672, loss_mps: 0.112102, loss_cps: 0.219195
[13:42:55.413] iteration 19283: total_loss: 0.250325, loss_sup: 0.008011, loss_mps: 0.092662, loss_cps: 0.149651
[13:42:55.561] iteration 19284: total_loss: 0.638934, loss_sup: 0.184968, loss_mps: 0.169681, loss_cps: 0.284285
[13:42:55.707] iteration 19285: total_loss: 0.893388, loss_sup: 0.063973, loss_mps: 0.261247, loss_cps: 0.568168
[13:42:55.853] iteration 19286: total_loss: 0.533579, loss_sup: 0.132896, loss_mps: 0.135630, loss_cps: 0.265052
[13:42:55.999] iteration 19287: total_loss: 0.443315, loss_sup: 0.020858, loss_mps: 0.140845, loss_cps: 0.281611
[13:42:56.145] iteration 19288: total_loss: 0.327931, loss_sup: 0.049180, loss_mps: 0.104687, loss_cps: 0.174064
[13:42:56.293] iteration 19289: total_loss: 0.456929, loss_sup: 0.013928, loss_mps: 0.150797, loss_cps: 0.292204
[13:42:56.440] iteration 19290: total_loss: 0.932612, loss_sup: 0.110246, loss_mps: 0.257871, loss_cps: 0.564495
[13:42:56.587] iteration 19291: total_loss: 1.235526, loss_sup: 0.365208, loss_mps: 0.281053, loss_cps: 0.589266
[13:42:56.734] iteration 19292: total_loss: 0.546062, loss_sup: 0.042774, loss_mps: 0.164175, loss_cps: 0.339114
[13:42:56.881] iteration 19293: total_loss: 0.566899, loss_sup: 0.115943, loss_mps: 0.149415, loss_cps: 0.301541
[13:42:57.027] iteration 19294: total_loss: 0.399530, loss_sup: 0.039219, loss_mps: 0.130168, loss_cps: 0.230143
[13:42:57.173] iteration 19295: total_loss: 0.342818, loss_sup: 0.012033, loss_mps: 0.117347, loss_cps: 0.213439
[13:42:57.320] iteration 19296: total_loss: 0.919424, loss_sup: 0.047133, loss_mps: 0.267485, loss_cps: 0.604806
[13:42:57.465] iteration 19297: total_loss: 0.329729, loss_sup: 0.032395, loss_mps: 0.111220, loss_cps: 0.186114
[13:42:57.611] iteration 19298: total_loss: 0.811267, loss_sup: 0.098468, loss_mps: 0.227036, loss_cps: 0.485763
[13:42:57.758] iteration 19299: total_loss: 0.491002, loss_sup: 0.095408, loss_mps: 0.132018, loss_cps: 0.263577
[13:42:57.903] iteration 19300: total_loss: 0.626891, loss_sup: 0.148688, loss_mps: 0.164409, loss_cps: 0.313794
[13:42:57.904] Evaluation Started ==>
[13:43:09.296] ==> valid iteration 19300: unet metrics: {'dc': 0.632315065611435, 'jc': 0.5182748889173077, 'pre': 0.7850107930698429, 'hd': 5.392986880664972}, ynet metrics: {'dc': 0.5627877869880247, 'jc': 0.45727198429565974, 'pre': 0.7900372729195743, 'hd': 5.55291095128723}.
[13:43:09.298] Evaluation Finished!⏹️
[13:43:09.448] iteration 19301: total_loss: 0.364557, loss_sup: 0.042787, loss_mps: 0.114831, loss_cps: 0.206938
[13:43:09.595] iteration 19302: total_loss: 0.459871, loss_sup: 0.102561, loss_mps: 0.120571, loss_cps: 0.236740
[13:43:09.740] iteration 19303: total_loss: 0.526064, loss_sup: 0.093363, loss_mps: 0.145545, loss_cps: 0.287156
[13:43:09.885] iteration 19304: total_loss: 1.174336, loss_sup: 0.345134, loss_mps: 0.280033, loss_cps: 0.549169
[13:43:10.032] iteration 19305: total_loss: 0.594602, loss_sup: 0.331224, loss_mps: 0.103445, loss_cps: 0.159933
[13:43:10.177] iteration 19306: total_loss: 0.792057, loss_sup: 0.063676, loss_mps: 0.239458, loss_cps: 0.488923
[13:43:10.324] iteration 19307: total_loss: 0.410394, loss_sup: 0.010097, loss_mps: 0.136585, loss_cps: 0.263712
[13:43:10.471] iteration 19308: total_loss: 0.373238, loss_sup: 0.036461, loss_mps: 0.123217, loss_cps: 0.213560
[13:43:10.617] iteration 19309: total_loss: 0.510398, loss_sup: 0.065893, loss_mps: 0.153021, loss_cps: 0.291484
[13:43:10.762] iteration 19310: total_loss: 0.599624, loss_sup: 0.070175, loss_mps: 0.174857, loss_cps: 0.354592
[13:43:10.909] iteration 19311: total_loss: 0.381038, loss_sup: 0.084436, loss_mps: 0.107398, loss_cps: 0.189205
[13:43:11.056] iteration 19312: total_loss: 0.661222, loss_sup: 0.061447, loss_mps: 0.213291, loss_cps: 0.386485
[13:43:11.201] iteration 19313: total_loss: 0.853578, loss_sup: 0.363660, loss_mps: 0.169800, loss_cps: 0.320118
[13:43:11.348] iteration 19314: total_loss: 0.490820, loss_sup: 0.034105, loss_mps: 0.149203, loss_cps: 0.307512
[13:43:11.497] iteration 19315: total_loss: 0.815905, loss_sup: 0.197399, loss_mps: 0.210636, loss_cps: 0.407870
[13:43:11.643] iteration 19316: total_loss: 0.459521, loss_sup: 0.122969, loss_mps: 0.125294, loss_cps: 0.211257
[13:43:11.789] iteration 19317: total_loss: 0.315522, loss_sup: 0.030798, loss_mps: 0.109280, loss_cps: 0.175444
[13:43:11.934] iteration 19318: total_loss: 0.361893, loss_sup: 0.035530, loss_mps: 0.114574, loss_cps: 0.211789
[13:43:12.082] iteration 19319: total_loss: 0.568351, loss_sup: 0.097486, loss_mps: 0.166589, loss_cps: 0.304275
[13:43:12.228] iteration 19320: total_loss: 0.343713, loss_sup: 0.010558, loss_mps: 0.120739, loss_cps: 0.212416
[13:43:12.374] iteration 19321: total_loss: 0.373086, loss_sup: 0.065354, loss_mps: 0.107906, loss_cps: 0.199826
[13:43:12.519] iteration 19322: total_loss: 0.359377, loss_sup: 0.026723, loss_mps: 0.117762, loss_cps: 0.214891
[13:43:12.664] iteration 19323: total_loss: 0.603630, loss_sup: 0.075864, loss_mps: 0.178964, loss_cps: 0.348802
[13:43:12.810] iteration 19324: total_loss: 0.486364, loss_sup: 0.011334, loss_mps: 0.153324, loss_cps: 0.321706
[13:43:12.955] iteration 19325: total_loss: 0.888511, loss_sup: 0.080549, loss_mps: 0.243840, loss_cps: 0.564122
[13:43:13.101] iteration 19326: total_loss: 0.701752, loss_sup: 0.117396, loss_mps: 0.205441, loss_cps: 0.378915
[13:43:13.247] iteration 19327: total_loss: 0.500585, loss_sup: 0.012341, loss_mps: 0.167908, loss_cps: 0.320336
[13:43:13.392] iteration 19328: total_loss: 0.385101, loss_sup: 0.028638, loss_mps: 0.119767, loss_cps: 0.236696
[13:43:13.538] iteration 19329: total_loss: 0.952683, loss_sup: 0.243538, loss_mps: 0.230217, loss_cps: 0.478928
[13:43:13.685] iteration 19330: total_loss: 0.822583, loss_sup: 0.166128, loss_mps: 0.215654, loss_cps: 0.440801
[13:43:13.830] iteration 19331: total_loss: 0.897940, loss_sup: 0.178567, loss_mps: 0.226905, loss_cps: 0.492467
[13:43:13.975] iteration 19332: total_loss: 0.468487, loss_sup: 0.027359, loss_mps: 0.153973, loss_cps: 0.287155
[13:43:14.121] iteration 19333: total_loss: 0.632127, loss_sup: 0.118090, loss_mps: 0.180281, loss_cps: 0.333756
[13:43:14.268] iteration 19334: total_loss: 0.659198, loss_sup: 0.165468, loss_mps: 0.169175, loss_cps: 0.324555
[13:43:14.414] iteration 19335: total_loss: 0.922632, loss_sup: 0.207528, loss_mps: 0.226449, loss_cps: 0.488655
[13:43:14.559] iteration 19336: total_loss: 0.472926, loss_sup: 0.148328, loss_mps: 0.120415, loss_cps: 0.204184
[13:43:14.705] iteration 19337: total_loss: 1.014584, loss_sup: 0.013771, loss_mps: 0.294358, loss_cps: 0.706455
[13:43:14.851] iteration 19338: total_loss: 0.692527, loss_sup: 0.169908, loss_mps: 0.176605, loss_cps: 0.346013
[13:43:14.996] iteration 19339: total_loss: 0.381900, loss_sup: 0.062014, loss_mps: 0.109946, loss_cps: 0.209940
[13:43:15.144] iteration 19340: total_loss: 0.996173, loss_sup: 0.205974, loss_mps: 0.254537, loss_cps: 0.535662
[13:43:15.290] iteration 19341: total_loss: 0.471675, loss_sup: 0.060802, loss_mps: 0.137601, loss_cps: 0.273271
[13:43:15.436] iteration 19342: total_loss: 0.572878, loss_sup: 0.020955, loss_mps: 0.185541, loss_cps: 0.366382
[13:43:15.581] iteration 19343: total_loss: 0.377807, loss_sup: 0.047347, loss_mps: 0.119790, loss_cps: 0.210669
[13:43:15.727] iteration 19344: total_loss: 0.613401, loss_sup: 0.061164, loss_mps: 0.188085, loss_cps: 0.364151
[13:43:15.873] iteration 19345: total_loss: 0.807018, loss_sup: 0.098759, loss_mps: 0.235177, loss_cps: 0.473082
[13:43:16.019] iteration 19346: total_loss: 0.660815, loss_sup: 0.109604, loss_mps: 0.179741, loss_cps: 0.371470
[13:43:16.165] iteration 19347: total_loss: 0.536580, loss_sup: 0.128961, loss_mps: 0.147326, loss_cps: 0.260293
[13:43:16.310] iteration 19348: total_loss: 1.499953, loss_sup: 0.088197, loss_mps: 0.416922, loss_cps: 0.994834
[13:43:16.458] iteration 19349: total_loss: 0.589629, loss_sup: 0.155434, loss_mps: 0.143787, loss_cps: 0.290408
[13:43:16.605] iteration 19350: total_loss: 0.844592, loss_sup: 0.095770, loss_mps: 0.239493, loss_cps: 0.509329
[13:43:16.751] iteration 19351: total_loss: 0.493353, loss_sup: 0.012873, loss_mps: 0.172440, loss_cps: 0.308039
[13:43:16.897] iteration 19352: total_loss: 0.463960, loss_sup: 0.080071, loss_mps: 0.136926, loss_cps: 0.246963
[13:43:17.048] iteration 19353: total_loss: 0.497634, loss_sup: 0.029394, loss_mps: 0.155361, loss_cps: 0.312879
[13:43:17.194] iteration 19354: total_loss: 0.761844, loss_sup: 0.080854, loss_mps: 0.217610, loss_cps: 0.463379
[13:43:17.341] iteration 19355: total_loss: 0.918741, loss_sup: 0.114273, loss_mps: 0.270225, loss_cps: 0.534243
[13:43:17.487] iteration 19356: total_loss: 0.578175, loss_sup: 0.075415, loss_mps: 0.168366, loss_cps: 0.334394
[13:43:17.633] iteration 19357: total_loss: 1.003853, loss_sup: 0.197482, loss_mps: 0.263329, loss_cps: 0.543042
[13:43:17.779] iteration 19358: total_loss: 0.536354, loss_sup: 0.118226, loss_mps: 0.146474, loss_cps: 0.271654
[13:43:17.925] iteration 19359: total_loss: 0.637428, loss_sup: 0.125341, loss_mps: 0.172849, loss_cps: 0.339239
[13:43:18.071] iteration 19360: total_loss: 0.647859, loss_sup: 0.082916, loss_mps: 0.186513, loss_cps: 0.378430
[13:43:18.219] iteration 19361: total_loss: 0.558911, loss_sup: 0.085847, loss_mps: 0.170515, loss_cps: 0.302550
[13:43:18.365] iteration 19362: total_loss: 0.542612, loss_sup: 0.070264, loss_mps: 0.159071, loss_cps: 0.313277
[13:43:18.515] iteration 19363: total_loss: 0.460292, loss_sup: 0.119223, loss_mps: 0.126281, loss_cps: 0.214788
[13:43:18.662] iteration 19364: total_loss: 0.643372, loss_sup: 0.201114, loss_mps: 0.157452, loss_cps: 0.284805
[13:43:18.809] iteration 19365: total_loss: 0.453867, loss_sup: 0.052694, loss_mps: 0.150402, loss_cps: 0.250771
[13:43:18.955] iteration 19366: total_loss: 0.760111, loss_sup: 0.076971, loss_mps: 0.229565, loss_cps: 0.453575
[13:43:19.100] iteration 19367: total_loss: 0.379354, loss_sup: 0.008682, loss_mps: 0.134593, loss_cps: 0.236080
[13:43:19.248] iteration 19368: total_loss: 0.338345, loss_sup: 0.051143, loss_mps: 0.105424, loss_cps: 0.181778
[13:43:19.396] iteration 19369: total_loss: 1.350760, loss_sup: 0.249649, loss_mps: 0.337534, loss_cps: 0.763577
[13:43:19.542] iteration 19370: total_loss: 0.852464, loss_sup: 0.193508, loss_mps: 0.213034, loss_cps: 0.445922
[13:43:19.688] iteration 19371: total_loss: 0.665869, loss_sup: 0.050573, loss_mps: 0.204164, loss_cps: 0.411132
[13:43:19.835] iteration 19372: total_loss: 0.563055, loss_sup: 0.013357, loss_mps: 0.180298, loss_cps: 0.369400
[13:43:19.981] iteration 19373: total_loss: 0.830856, loss_sup: 0.110210, loss_mps: 0.237321, loss_cps: 0.483324
[13:43:20.128] iteration 19374: total_loss: 0.433519, loss_sup: 0.083216, loss_mps: 0.115840, loss_cps: 0.234463
[13:43:20.274] iteration 19375: total_loss: 0.459100, loss_sup: 0.055297, loss_mps: 0.141716, loss_cps: 0.262087
[13:43:20.424] iteration 19376: total_loss: 1.067558, loss_sup: 0.233909, loss_mps: 0.270900, loss_cps: 0.562749
[13:43:20.571] iteration 19377: total_loss: 0.384977, loss_sup: 0.009151, loss_mps: 0.138350, loss_cps: 0.237476
[13:43:20.718] iteration 19378: total_loss: 0.688893, loss_sup: 0.028798, loss_mps: 0.206646, loss_cps: 0.453449
[13:43:20.864] iteration 19379: total_loss: 0.382315, loss_sup: 0.058243, loss_mps: 0.117611, loss_cps: 0.206460
[13:43:21.011] iteration 19380: total_loss: 0.802084, loss_sup: 0.075478, loss_mps: 0.241785, loss_cps: 0.484821
[13:43:21.157] iteration 19381: total_loss: 0.477659, loss_sup: 0.068781, loss_mps: 0.144704, loss_cps: 0.264175
[13:43:21.303] iteration 19382: total_loss: 0.647402, loss_sup: 0.124462, loss_mps: 0.170294, loss_cps: 0.352646
[13:43:21.449] iteration 19383: total_loss: 0.718952, loss_sup: 0.085768, loss_mps: 0.200900, loss_cps: 0.432283
[13:43:21.595] iteration 19384: total_loss: 0.508171, loss_sup: 0.047193, loss_mps: 0.160315, loss_cps: 0.300662
[13:43:21.741] iteration 19385: total_loss: 0.379079, loss_sup: 0.062274, loss_mps: 0.109440, loss_cps: 0.207365
[13:43:21.887] iteration 19386: total_loss: 0.410633, loss_sup: 0.049623, loss_mps: 0.133439, loss_cps: 0.227571
[13:43:22.033] iteration 19387: total_loss: 0.452122, loss_sup: 0.027186, loss_mps: 0.140619, loss_cps: 0.284318
[13:43:22.179] iteration 19388: total_loss: 1.007289, loss_sup: 0.098196, loss_mps: 0.286602, loss_cps: 0.622491
[13:43:22.325] iteration 19389: total_loss: 0.364901, loss_sup: 0.078852, loss_mps: 0.103976, loss_cps: 0.182073
[13:43:22.473] iteration 19390: total_loss: 0.945442, loss_sup: 0.103592, loss_mps: 0.275156, loss_cps: 0.566693
[13:43:22.619] iteration 19391: total_loss: 0.719321, loss_sup: 0.201193, loss_mps: 0.170000, loss_cps: 0.348129
[13:43:22.765] iteration 19392: total_loss: 0.686392, loss_sup: 0.059587, loss_mps: 0.204565, loss_cps: 0.422240
[13:43:22.911] iteration 19393: total_loss: 0.954516, loss_sup: 0.178807, loss_mps: 0.246166, loss_cps: 0.529543
[13:43:23.057] iteration 19394: total_loss: 0.284180, loss_sup: 0.014306, loss_mps: 0.097105, loss_cps: 0.172769
[13:43:23.204] iteration 19395: total_loss: 0.556672, loss_sup: 0.094405, loss_mps: 0.151059, loss_cps: 0.311209
[13:43:23.350] iteration 19396: total_loss: 0.548718, loss_sup: 0.077529, loss_mps: 0.152455, loss_cps: 0.318734
[13:43:23.496] iteration 19397: total_loss: 0.883686, loss_sup: 0.346491, loss_mps: 0.175700, loss_cps: 0.361495
[13:43:23.642] iteration 19398: total_loss: 1.044225, loss_sup: 0.113825, loss_mps: 0.278683, loss_cps: 0.651717
[13:43:23.788] iteration 19399: total_loss: 0.512068, loss_sup: 0.051399, loss_mps: 0.150869, loss_cps: 0.309800
[13:43:23.933] iteration 19400: total_loss: 0.482447, loss_sup: 0.096347, loss_mps: 0.132065, loss_cps: 0.254034
[13:43:23.933] Evaluation Started ==>
[13:43:35.247] ==> valid iteration 19400: unet metrics: {'dc': 0.6056704367280776, 'jc': 0.4922037921612332, 'pre': 0.8043642634793339, 'hd': 5.355884937027074}, ynet metrics: {'dc': 0.5611135332049775, 'jc': 0.4483320903828327, 'pre': 0.8247426455227594, 'hd': 5.419511777150206}.
[13:43:35.249] Evaluation Finished!⏹️
[13:43:35.398] iteration 19401: total_loss: 0.768452, loss_sup: 0.122382, loss_mps: 0.217141, loss_cps: 0.428929
[13:43:35.548] iteration 19402: total_loss: 0.517118, loss_sup: 0.009234, loss_mps: 0.167520, loss_cps: 0.340364
[13:43:35.694] iteration 19403: total_loss: 0.739127, loss_sup: 0.217060, loss_mps: 0.180863, loss_cps: 0.341203
[13:43:35.840] iteration 19404: total_loss: 0.869895, loss_sup: 0.106087, loss_mps: 0.238926, loss_cps: 0.524881
[13:43:35.987] iteration 19405: total_loss: 0.493822, loss_sup: 0.113365, loss_mps: 0.134153, loss_cps: 0.246304
[13:43:36.132] iteration 19406: total_loss: 0.421858, loss_sup: 0.032999, loss_mps: 0.135852, loss_cps: 0.253006
[13:43:36.277] iteration 19407: total_loss: 0.683521, loss_sup: 0.068329, loss_mps: 0.186505, loss_cps: 0.428687
[13:43:36.424] iteration 19408: total_loss: 1.061626, loss_sup: 0.170188, loss_mps: 0.289223, loss_cps: 0.602215
[13:43:36.570] iteration 19409: total_loss: 0.430159, loss_sup: 0.086592, loss_mps: 0.117256, loss_cps: 0.226311
[13:43:36.721] iteration 19410: total_loss: 0.758107, loss_sup: 0.172918, loss_mps: 0.180787, loss_cps: 0.404402
[13:43:36.870] iteration 19411: total_loss: 0.797750, loss_sup: 0.094739, loss_mps: 0.232275, loss_cps: 0.470736
[13:43:37.016] iteration 19412: total_loss: 0.437553, loss_sup: 0.055296, loss_mps: 0.136751, loss_cps: 0.245507
[13:43:37.164] iteration 19413: total_loss: 0.491618, loss_sup: 0.050724, loss_mps: 0.159066, loss_cps: 0.281828
[13:43:37.310] iteration 19414: total_loss: 0.857148, loss_sup: 0.232507, loss_mps: 0.215140, loss_cps: 0.409501
[13:43:37.455] iteration 19415: total_loss: 0.377738, loss_sup: 0.089441, loss_mps: 0.104234, loss_cps: 0.184064
[13:43:37.601] iteration 19416: total_loss: 0.344257, loss_sup: 0.087341, loss_mps: 0.093998, loss_cps: 0.162918
[13:43:37.747] iteration 19417: total_loss: 0.458572, loss_sup: 0.092345, loss_mps: 0.133498, loss_cps: 0.232730
[13:43:37.893] iteration 19418: total_loss: 0.789804, loss_sup: 0.153750, loss_mps: 0.216260, loss_cps: 0.419795
[13:43:38.038] iteration 19419: total_loss: 0.512956, loss_sup: 0.049565, loss_mps: 0.158198, loss_cps: 0.305194
[13:43:38.185] iteration 19420: total_loss: 0.738562, loss_sup: 0.045029, loss_mps: 0.234134, loss_cps: 0.459398
[13:43:38.333] iteration 19421: total_loss: 0.734598, loss_sup: 0.034641, loss_mps: 0.231102, loss_cps: 0.468855
[13:43:38.480] iteration 19422: total_loss: 0.528912, loss_sup: 0.080301, loss_mps: 0.160305, loss_cps: 0.288306
[13:43:38.626] iteration 19423: total_loss: 0.373322, loss_sup: 0.064610, loss_mps: 0.113117, loss_cps: 0.195594
[13:43:38.772] iteration 19424: total_loss: 0.531016, loss_sup: 0.025766, loss_mps: 0.169595, loss_cps: 0.335655
[13:43:38.920] iteration 19425: total_loss: 0.811917, loss_sup: 0.146744, loss_mps: 0.220172, loss_cps: 0.445001
[13:43:39.068] iteration 19426: total_loss: 0.602822, loss_sup: 0.047408, loss_mps: 0.184858, loss_cps: 0.370556
[13:43:39.215] iteration 19427: total_loss: 1.163825, loss_sup: 0.198850, loss_mps: 0.303355, loss_cps: 0.661619
[13:43:39.361] iteration 19428: total_loss: 0.435966, loss_sup: 0.026830, loss_mps: 0.143802, loss_cps: 0.265334
[13:43:39.513] iteration 19429: total_loss: 0.490149, loss_sup: 0.020990, loss_mps: 0.155546, loss_cps: 0.313613
[13:43:39.659] iteration 19430: total_loss: 0.551072, loss_sup: 0.082130, loss_mps: 0.155554, loss_cps: 0.313389
[13:43:39.806] iteration 19431: total_loss: 0.492969, loss_sup: 0.058332, loss_mps: 0.143289, loss_cps: 0.291348
[13:43:39.952] iteration 19432: total_loss: 0.683799, loss_sup: 0.085421, loss_mps: 0.200377, loss_cps: 0.398001
[13:43:40.100] iteration 19433: total_loss: 0.368740, loss_sup: 0.113209, loss_mps: 0.099030, loss_cps: 0.156500
[13:43:40.248] iteration 19434: total_loss: 0.770997, loss_sup: 0.072741, loss_mps: 0.240655, loss_cps: 0.457601
[13:43:40.395] iteration 19435: total_loss: 0.423303, loss_sup: 0.021353, loss_mps: 0.141936, loss_cps: 0.260014
[13:43:40.542] iteration 19436: total_loss: 0.645285, loss_sup: 0.051263, loss_mps: 0.202497, loss_cps: 0.391524
[13:43:40.688] iteration 19437: total_loss: 0.828150, loss_sup: 0.053192, loss_mps: 0.234619, loss_cps: 0.540339
[13:43:40.834] iteration 19438: total_loss: 0.317749, loss_sup: 0.018749, loss_mps: 0.115265, loss_cps: 0.183736
[13:43:40.981] iteration 19439: total_loss: 0.571041, loss_sup: 0.065083, loss_mps: 0.180150, loss_cps: 0.325808
[13:43:41.127] iteration 19440: total_loss: 0.534033, loss_sup: 0.054879, loss_mps: 0.160161, loss_cps: 0.318994
[13:43:41.273] iteration 19441: total_loss: 1.091944, loss_sup: 0.270529, loss_mps: 0.258661, loss_cps: 0.562755
[13:43:41.419] iteration 19442: total_loss: 0.495909, loss_sup: 0.128142, loss_mps: 0.137354, loss_cps: 0.230412
[13:43:41.565] iteration 19443: total_loss: 0.342887, loss_sup: 0.026560, loss_mps: 0.114926, loss_cps: 0.201400
[13:43:41.711] iteration 19444: total_loss: 0.525197, loss_sup: 0.065259, loss_mps: 0.158674, loss_cps: 0.301265
[13:43:41.857] iteration 19445: total_loss: 0.587760, loss_sup: 0.024873, loss_mps: 0.182110, loss_cps: 0.380777
[13:43:42.004] iteration 19446: total_loss: 0.558306, loss_sup: 0.087558, loss_mps: 0.152978, loss_cps: 0.317770
[13:43:42.150] iteration 19447: total_loss: 0.569547, loss_sup: 0.050904, loss_mps: 0.179481, loss_cps: 0.339162
[13:43:42.296] iteration 19448: total_loss: 0.453245, loss_sup: 0.024952, loss_mps: 0.138701, loss_cps: 0.289591
[13:43:42.443] iteration 19449: total_loss: 0.387763, loss_sup: 0.037036, loss_mps: 0.122235, loss_cps: 0.228492
[13:43:42.590] iteration 19450: total_loss: 0.863317, loss_sup: 0.118376, loss_mps: 0.244059, loss_cps: 0.500882
[13:43:42.736] iteration 19451: total_loss: 0.689544, loss_sup: 0.185836, loss_mps: 0.175897, loss_cps: 0.327811
[13:43:42.882] iteration 19452: total_loss: 0.531518, loss_sup: 0.186753, loss_mps: 0.115829, loss_cps: 0.228936
[13:43:43.028] iteration 19453: total_loss: 0.449974, loss_sup: 0.082944, loss_mps: 0.135749, loss_cps: 0.231281
[13:43:43.174] iteration 19454: total_loss: 0.708891, loss_sup: 0.208754, loss_mps: 0.161786, loss_cps: 0.338352
[13:43:43.321] iteration 19455: total_loss: 0.529761, loss_sup: 0.154558, loss_mps: 0.132323, loss_cps: 0.242880
[13:43:43.467] iteration 19456: total_loss: 0.433003, loss_sup: 0.019560, loss_mps: 0.139377, loss_cps: 0.274067
[13:43:43.613] iteration 19457: total_loss: 0.492437, loss_sup: 0.042216, loss_mps: 0.149941, loss_cps: 0.300280
[13:43:43.764] iteration 19458: total_loss: 0.427610, loss_sup: 0.006591, loss_mps: 0.142322, loss_cps: 0.278697
[13:43:43.912] iteration 19459: total_loss: 0.525662, loss_sup: 0.033220, loss_mps: 0.162835, loss_cps: 0.329606
[13:43:44.060] iteration 19460: total_loss: 0.301195, loss_sup: 0.011908, loss_mps: 0.106319, loss_cps: 0.182968
[13:43:44.206] iteration 19461: total_loss: 0.326333, loss_sup: 0.063456, loss_mps: 0.099246, loss_cps: 0.163631
[13:43:44.359] iteration 19462: total_loss: 0.356840, loss_sup: 0.031248, loss_mps: 0.121549, loss_cps: 0.204042
[13:43:44.505] iteration 19463: total_loss: 1.205606, loss_sup: 0.215386, loss_mps: 0.314671, loss_cps: 0.675550
[13:43:44.651] iteration 19464: total_loss: 0.657673, loss_sup: 0.046271, loss_mps: 0.198318, loss_cps: 0.413084
[13:43:44.797] iteration 19465: total_loss: 0.394967, loss_sup: 0.014593, loss_mps: 0.139070, loss_cps: 0.241304
[13:43:44.944] iteration 19466: total_loss: 0.523171, loss_sup: 0.014806, loss_mps: 0.175106, loss_cps: 0.333259
[13:43:45.090] iteration 19467: total_loss: 0.654450, loss_sup: 0.103179, loss_mps: 0.177221, loss_cps: 0.374050
[13:43:45.238] iteration 19468: total_loss: 0.538563, loss_sup: 0.067337, loss_mps: 0.161699, loss_cps: 0.309527
[13:43:45.386] iteration 19469: total_loss: 0.954246, loss_sup: 0.321070, loss_mps: 0.212733, loss_cps: 0.420443
[13:43:45.532] iteration 19470: total_loss: 0.453894, loss_sup: 0.072183, loss_mps: 0.133013, loss_cps: 0.248698
[13:43:45.679] iteration 19471: total_loss: 0.586201, loss_sup: 0.056118, loss_mps: 0.174006, loss_cps: 0.356077
[13:43:45.826] iteration 19472: total_loss: 0.499347, loss_sup: 0.051935, loss_mps: 0.139067, loss_cps: 0.308344
[13:43:45.976] iteration 19473: total_loss: 0.456541, loss_sup: 0.129757, loss_mps: 0.111925, loss_cps: 0.214858
[13:43:46.122] iteration 19474: total_loss: 0.429135, loss_sup: 0.032575, loss_mps: 0.145604, loss_cps: 0.250955
[13:43:46.268] iteration 19475: total_loss: 0.425154, loss_sup: 0.039492, loss_mps: 0.145433, loss_cps: 0.240229
[13:43:46.415] iteration 19476: total_loss: 0.322928, loss_sup: 0.020661, loss_mps: 0.107731, loss_cps: 0.194536
[13:43:46.563] iteration 19477: total_loss: 0.544332, loss_sup: 0.049443, loss_mps: 0.164296, loss_cps: 0.330593
[13:43:46.710] iteration 19478: total_loss: 1.224771, loss_sup: 0.182376, loss_mps: 0.330563, loss_cps: 0.711831
[13:43:46.856] iteration 19479: total_loss: 0.594901, loss_sup: 0.025881, loss_mps: 0.199531, loss_cps: 0.369488
[13:43:47.003] iteration 19480: total_loss: 0.402287, loss_sup: 0.103639, loss_mps: 0.114649, loss_cps: 0.183998
[13:43:47.149] iteration 19481: total_loss: 0.341247, loss_sup: 0.051715, loss_mps: 0.104921, loss_cps: 0.184611
[13:43:47.297] iteration 19482: total_loss: 0.506605, loss_sup: 0.044560, loss_mps: 0.157855, loss_cps: 0.304190
[13:43:47.444] iteration 19483: total_loss: 0.594123, loss_sup: 0.114645, loss_mps: 0.170456, loss_cps: 0.309022
[13:43:47.590] iteration 19484: total_loss: 0.474755, loss_sup: 0.047142, loss_mps: 0.156035, loss_cps: 0.271577
[13:43:47.736] iteration 19485: total_loss: 0.332662, loss_sup: 0.018295, loss_mps: 0.114611, loss_cps: 0.199756
[13:43:47.883] iteration 19486: total_loss: 0.644009, loss_sup: 0.065039, loss_mps: 0.178668, loss_cps: 0.400303
[13:43:48.030] iteration 19487: total_loss: 0.757066, loss_sup: 0.105361, loss_mps: 0.211456, loss_cps: 0.440249
[13:43:48.181] iteration 19488: total_loss: 0.559046, loss_sup: 0.006102, loss_mps: 0.181065, loss_cps: 0.371879
[13:43:48.328] iteration 19489: total_loss: 0.722994, loss_sup: 0.016478, loss_mps: 0.233509, loss_cps: 0.473007
[13:43:48.476] iteration 19490: total_loss: 0.340016, loss_sup: 0.011820, loss_mps: 0.113336, loss_cps: 0.214860
[13:43:48.623] iteration 19491: total_loss: 0.342136, loss_sup: 0.071838, loss_mps: 0.101798, loss_cps: 0.168500
[13:43:48.769] iteration 19492: total_loss: 0.498763, loss_sup: 0.184946, loss_mps: 0.117102, loss_cps: 0.196715
[13:43:48.915] iteration 19493: total_loss: 0.737292, loss_sup: 0.091938, loss_mps: 0.200215, loss_cps: 0.445138
[13:43:49.062] iteration 19494: total_loss: 0.638052, loss_sup: 0.175263, loss_mps: 0.169654, loss_cps: 0.293136
[13:43:49.209] iteration 19495: total_loss: 0.433909, loss_sup: 0.058690, loss_mps: 0.130651, loss_cps: 0.244567
[13:43:49.356] iteration 19496: total_loss: 0.665990, loss_sup: 0.135903, loss_mps: 0.181001, loss_cps: 0.349086
[13:43:49.503] iteration 19497: total_loss: 0.354897, loss_sup: 0.055828, loss_mps: 0.111516, loss_cps: 0.187552
[13:43:49.650] iteration 19498: total_loss: 0.374235, loss_sup: 0.054936, loss_mps: 0.113830, loss_cps: 0.205468
[13:43:49.796] iteration 19499: total_loss: 0.659565, loss_sup: 0.157428, loss_mps: 0.171055, loss_cps: 0.331082
[13:43:49.943] iteration 19500: total_loss: 0.329936, loss_sup: 0.026604, loss_mps: 0.101843, loss_cps: 0.201490
[13:43:49.943] Evaluation Started ==>
[13:44:01.277] ==> valid iteration 19500: unet metrics: {'dc': 0.631787000608319, 'jc': 0.5144886333772826, 'pre': 0.8021439714002313, 'hd': 5.398074042608296}, ynet metrics: {'dc': 0.5548631144542728, 'jc': 0.4428499156148843, 'pre': 0.7948637914175308, 'hd': 5.611677302540213}.
[13:44:01.279] Evaluation Finished!⏹️
[13:44:01.430] iteration 19501: total_loss: 0.653977, loss_sup: 0.153525, loss_mps: 0.169718, loss_cps: 0.330734
[13:44:01.577] iteration 19502: total_loss: 0.496124, loss_sup: 0.042134, loss_mps: 0.152557, loss_cps: 0.301432
[13:44:01.723] iteration 19503: total_loss: 0.654131, loss_sup: 0.093532, loss_mps: 0.187052, loss_cps: 0.373547
[13:44:01.868] iteration 19504: total_loss: 0.713959, loss_sup: 0.177128, loss_mps: 0.184685, loss_cps: 0.352145
[13:44:02.014] iteration 19505: total_loss: 0.445003, loss_sup: 0.026759, loss_mps: 0.138524, loss_cps: 0.279720
[13:44:02.160] iteration 19506: total_loss: 0.330806, loss_sup: 0.018756, loss_mps: 0.122699, loss_cps: 0.189351
[13:44:02.306] iteration 19507: total_loss: 0.503418, loss_sup: 0.139985, loss_mps: 0.124278, loss_cps: 0.239155
[13:44:02.451] iteration 19508: total_loss: 0.517822, loss_sup: 0.128565, loss_mps: 0.130172, loss_cps: 0.259086
[13:44:02.597] iteration 19509: total_loss: 0.688150, loss_sup: 0.078842, loss_mps: 0.211359, loss_cps: 0.397949
[13:44:02.743] iteration 19510: total_loss: 0.338471, loss_sup: 0.039428, loss_mps: 0.108636, loss_cps: 0.190406
[13:44:02.888] iteration 19511: total_loss: 0.364403, loss_sup: 0.021335, loss_mps: 0.121876, loss_cps: 0.221192
[13:44:03.035] iteration 19512: total_loss: 0.362956, loss_sup: 0.050177, loss_mps: 0.116130, loss_cps: 0.196648
[13:44:03.182] iteration 19513: total_loss: 0.715648, loss_sup: 0.089984, loss_mps: 0.192811, loss_cps: 0.432853
[13:44:03.328] iteration 19514: total_loss: 0.582942, loss_sup: 0.150938, loss_mps: 0.151960, loss_cps: 0.280044
[13:44:03.473] iteration 19515: total_loss: 0.571494, loss_sup: 0.096083, loss_mps: 0.149618, loss_cps: 0.325793
[13:44:03.622] iteration 19516: total_loss: 0.407782, loss_sup: 0.014467, loss_mps: 0.135689, loss_cps: 0.257626
[13:44:03.768] iteration 19517: total_loss: 0.628248, loss_sup: 0.135726, loss_mps: 0.168135, loss_cps: 0.324387
[13:44:03.913] iteration 19518: total_loss: 0.307025, loss_sup: 0.006894, loss_mps: 0.112962, loss_cps: 0.187169
[13:44:04.059] iteration 19519: total_loss: 0.434130, loss_sup: 0.069522, loss_mps: 0.129392, loss_cps: 0.235216
[13:44:04.205] iteration 19520: total_loss: 0.271869, loss_sup: 0.005287, loss_mps: 0.093673, loss_cps: 0.172909
[13:44:04.355] iteration 19521: total_loss: 1.200905, loss_sup: 0.101913, loss_mps: 0.328876, loss_cps: 0.770116
[13:44:04.502] iteration 19522: total_loss: 0.368868, loss_sup: 0.066844, loss_mps: 0.105673, loss_cps: 0.196350
[13:44:04.647] iteration 19523: total_loss: 0.498316, loss_sup: 0.049380, loss_mps: 0.159794, loss_cps: 0.289142
[13:44:04.794] iteration 19524: total_loss: 0.537338, loss_sup: 0.101436, loss_mps: 0.143258, loss_cps: 0.292644
[13:44:04.939] iteration 19525: total_loss: 0.435491, loss_sup: 0.144828, loss_mps: 0.104166, loss_cps: 0.186497
[13:44:05.085] iteration 19526: total_loss: 0.680374, loss_sup: 0.059621, loss_mps: 0.189025, loss_cps: 0.431728
[13:44:05.230] iteration 19527: total_loss: 0.419977, loss_sup: 0.105058, loss_mps: 0.106737, loss_cps: 0.208181
[13:44:05.377] iteration 19528: total_loss: 0.531572, loss_sup: 0.030191, loss_mps: 0.163361, loss_cps: 0.338021
[13:44:05.523] iteration 19529: total_loss: 0.642775, loss_sup: 0.060737, loss_mps: 0.190929, loss_cps: 0.391109
[13:44:05.669] iteration 19530: total_loss: 0.458368, loss_sup: 0.063968, loss_mps: 0.132425, loss_cps: 0.261975
[13:44:05.815] iteration 19531: total_loss: 0.409763, loss_sup: 0.094068, loss_mps: 0.110981, loss_cps: 0.204714
[13:44:05.964] iteration 19532: total_loss: 0.328877, loss_sup: 0.020380, loss_mps: 0.103844, loss_cps: 0.204653
[13:44:06.111] iteration 19533: total_loss: 0.416952, loss_sup: 0.038904, loss_mps: 0.122257, loss_cps: 0.255791
[13:44:06.258] iteration 19534: total_loss: 0.513063, loss_sup: 0.007994, loss_mps: 0.162856, loss_cps: 0.342213
[13:44:06.404] iteration 19535: total_loss: 0.576002, loss_sup: 0.070559, loss_mps: 0.167473, loss_cps: 0.337970
[13:44:06.550] iteration 19536: total_loss: 0.475725, loss_sup: 0.150237, loss_mps: 0.111045, loss_cps: 0.214443
[13:44:06.696] iteration 19537: total_loss: 0.629801, loss_sup: 0.122041, loss_mps: 0.169949, loss_cps: 0.337811
[13:44:06.845] iteration 19538: total_loss: 0.801282, loss_sup: 0.152019, loss_mps: 0.199673, loss_cps: 0.449590
[13:44:06.992] iteration 19539: total_loss: 0.572945, loss_sup: 0.223415, loss_mps: 0.121323, loss_cps: 0.228207
[13:44:07.138] iteration 19540: total_loss: 0.595004, loss_sup: 0.023519, loss_mps: 0.185823, loss_cps: 0.385663
[13:44:07.285] iteration 19541: total_loss: 0.611803, loss_sup: 0.087258, loss_mps: 0.163919, loss_cps: 0.360626
[13:44:07.437] iteration 19542: total_loss: 0.454911, loss_sup: 0.142860, loss_mps: 0.111417, loss_cps: 0.200633
[13:44:07.583] iteration 19543: total_loss: 0.678648, loss_sup: 0.109305, loss_mps: 0.179057, loss_cps: 0.390285
[13:44:07.729] iteration 19544: total_loss: 0.679471, loss_sup: 0.233905, loss_mps: 0.155132, loss_cps: 0.290434
[13:44:07.878] iteration 19545: total_loss: 0.590777, loss_sup: 0.066447, loss_mps: 0.176969, loss_cps: 0.347361
[13:44:08.024] iteration 19546: total_loss: 0.501482, loss_sup: 0.107890, loss_mps: 0.139846, loss_cps: 0.253745
[13:44:08.170] iteration 19547: total_loss: 0.520123, loss_sup: 0.036792, loss_mps: 0.171259, loss_cps: 0.312072
[13:44:08.317] iteration 19548: total_loss: 0.441982, loss_sup: 0.015687, loss_mps: 0.146896, loss_cps: 0.279399
[13:44:08.464] iteration 19549: total_loss: 0.686133, loss_sup: 0.049610, loss_mps: 0.202111, loss_cps: 0.434413
[13:44:08.610] iteration 19550: total_loss: 0.739038, loss_sup: 0.133603, loss_mps: 0.202072, loss_cps: 0.403364
[13:44:08.756] iteration 19551: total_loss: 0.519878, loss_sup: 0.110373, loss_mps: 0.131475, loss_cps: 0.278029
[13:44:08.903] iteration 19552: total_loss: 0.930491, loss_sup: 0.094603, loss_mps: 0.268320, loss_cps: 0.567569
[13:44:09.049] iteration 19553: total_loss: 0.332235, loss_sup: 0.016227, loss_mps: 0.118899, loss_cps: 0.197110
[13:44:09.197] iteration 19554: total_loss: 0.793462, loss_sup: 0.132183, loss_mps: 0.217450, loss_cps: 0.443829
[13:44:09.343] iteration 19555: total_loss: 0.775428, loss_sup: 0.136732, loss_mps: 0.211923, loss_cps: 0.426773
[13:44:09.491] iteration 19556: total_loss: 0.346238, loss_sup: 0.066787, loss_mps: 0.111345, loss_cps: 0.168106
[13:44:09.637] iteration 19557: total_loss: 0.712180, loss_sup: 0.061240, loss_mps: 0.231566, loss_cps: 0.419374
[13:44:09.788] iteration 19558: total_loss: 0.330439, loss_sup: 0.009935, loss_mps: 0.113002, loss_cps: 0.207501
[13:44:09.934] iteration 19559: total_loss: 0.578534, loss_sup: 0.082846, loss_mps: 0.167436, loss_cps: 0.328252
[13:44:10.080] iteration 19560: total_loss: 0.592716, loss_sup: 0.066500, loss_mps: 0.177951, loss_cps: 0.348265
[13:44:10.228] iteration 19561: total_loss: 0.889004, loss_sup: 0.135055, loss_mps: 0.243724, loss_cps: 0.510224
[13:44:10.375] iteration 19562: total_loss: 0.447480, loss_sup: 0.065874, loss_mps: 0.138490, loss_cps: 0.243116
[13:44:10.522] iteration 19563: total_loss: 0.315566, loss_sup: 0.019719, loss_mps: 0.111236, loss_cps: 0.184611
[13:44:10.668] iteration 19564: total_loss: 0.518615, loss_sup: 0.066881, loss_mps: 0.155674, loss_cps: 0.296060
[13:44:10.816] iteration 19565: total_loss: 0.502556, loss_sup: 0.007222, loss_mps: 0.167025, loss_cps: 0.328310
[13:44:10.962] iteration 19566: total_loss: 0.932673, loss_sup: 0.011241, loss_mps: 0.297821, loss_cps: 0.623612
[13:44:11.109] iteration 19567: total_loss: 0.370813, loss_sup: 0.097586, loss_mps: 0.097959, loss_cps: 0.175268
[13:44:11.256] iteration 19568: total_loss: 0.687198, loss_sup: 0.078577, loss_mps: 0.192641, loss_cps: 0.415980
[13:44:11.403] iteration 19569: total_loss: 0.651818, loss_sup: 0.094196, loss_mps: 0.182196, loss_cps: 0.375427
[13:44:11.550] iteration 19570: total_loss: 0.710160, loss_sup: 0.067669, loss_mps: 0.203267, loss_cps: 0.439224
[13:44:11.697] iteration 19571: total_loss: 0.455146, loss_sup: 0.034742, loss_mps: 0.150528, loss_cps: 0.269876
[13:44:11.843] iteration 19572: total_loss: 0.576400, loss_sup: 0.088040, loss_mps: 0.159523, loss_cps: 0.328837
[13:44:11.989] iteration 19573: total_loss: 0.574990, loss_sup: 0.156484, loss_mps: 0.138656, loss_cps: 0.279850
[13:44:12.136] iteration 19574: total_loss: 0.860560, loss_sup: 0.283361, loss_mps: 0.181168, loss_cps: 0.396031
[13:44:12.282] iteration 19575: total_loss: 0.628296, loss_sup: 0.122008, loss_mps: 0.173400, loss_cps: 0.332888
[13:44:12.429] iteration 19576: total_loss: 0.481951, loss_sup: 0.014033, loss_mps: 0.151398, loss_cps: 0.316520
[13:44:12.575] iteration 19577: total_loss: 0.603763, loss_sup: 0.079073, loss_mps: 0.182726, loss_cps: 0.341964
[13:44:12.721] iteration 19578: total_loss: 0.537516, loss_sup: 0.196135, loss_mps: 0.123062, loss_cps: 0.218319
[13:44:12.867] iteration 19579: total_loss: 0.395530, loss_sup: 0.064137, loss_mps: 0.119839, loss_cps: 0.211553
[13:44:13.013] iteration 19580: total_loss: 0.404346, loss_sup: 0.050727, loss_mps: 0.130264, loss_cps: 0.223356
[13:44:13.159] iteration 19581: total_loss: 0.497267, loss_sup: 0.141359, loss_mps: 0.123785, loss_cps: 0.232122
[13:44:13.306] iteration 19582: total_loss: 0.514426, loss_sup: 0.103920, loss_mps: 0.140113, loss_cps: 0.270394
[13:44:13.451] iteration 19583: total_loss: 0.707068, loss_sup: 0.141129, loss_mps: 0.184321, loss_cps: 0.381618
[13:44:13.599] iteration 19584: total_loss: 0.837333, loss_sup: 0.172799, loss_mps: 0.221069, loss_cps: 0.443466
[13:44:13.745] iteration 19585: total_loss: 0.365519, loss_sup: 0.022398, loss_mps: 0.124753, loss_cps: 0.218368
[13:44:13.891] iteration 19586: total_loss: 0.499520, loss_sup: 0.075181, loss_mps: 0.148283, loss_cps: 0.276056
[13:44:14.037] iteration 19587: total_loss: 0.332679, loss_sup: 0.059468, loss_mps: 0.102651, loss_cps: 0.170559
[13:44:14.183] iteration 19588: total_loss: 0.744548, loss_sup: 0.107875, loss_mps: 0.219938, loss_cps: 0.416735
[13:44:14.330] iteration 19589: total_loss: 0.306041, loss_sup: 0.024156, loss_mps: 0.106452, loss_cps: 0.175434
[13:44:14.477] iteration 19590: total_loss: 0.694565, loss_sup: 0.065146, loss_mps: 0.207076, loss_cps: 0.422343
[13:44:14.623] iteration 19591: total_loss: 0.705808, loss_sup: 0.130926, loss_mps: 0.194771, loss_cps: 0.380111
[13:44:14.770] iteration 19592: total_loss: 0.648905, loss_sup: 0.058450, loss_mps: 0.199794, loss_cps: 0.390661
[13:44:14.916] iteration 19593: total_loss: 0.535075, loss_sup: 0.037678, loss_mps: 0.176487, loss_cps: 0.320910
[13:44:15.062] iteration 19594: total_loss: 0.412768, loss_sup: 0.015489, loss_mps: 0.143096, loss_cps: 0.254183
[13:44:15.208] iteration 19595: total_loss: 0.367418, loss_sup: 0.054158, loss_mps: 0.115145, loss_cps: 0.198115
[13:44:15.357] iteration 19596: total_loss: 0.505847, loss_sup: 0.032330, loss_mps: 0.159495, loss_cps: 0.314021
[13:44:15.503] iteration 19597: total_loss: 0.721749, loss_sup: 0.054248, loss_mps: 0.210018, loss_cps: 0.457483
[13:44:15.650] iteration 19598: total_loss: 0.313759, loss_sup: 0.039254, loss_mps: 0.101464, loss_cps: 0.173042
[13:44:15.801] iteration 19599: total_loss: 1.032374, loss_sup: 0.170577, loss_mps: 0.258248, loss_cps: 0.603549
[13:44:15.948] iteration 19600: total_loss: 0.599753, loss_sup: 0.022047, loss_mps: 0.194445, loss_cps: 0.383262
[13:44:15.948] Evaluation Started ==>
[13:44:27.277] ==> valid iteration 19600: unet metrics: {'dc': 0.6653315463151082, 'jc': 0.5509260422796004, 'pre': 0.7890428476107937, 'hd': 5.320268060937319}, ynet metrics: {'dc': 0.5790744692844764, 'jc': 0.46375866728074133, 'pre': 0.7878950213508318, 'hd': 5.6366255784876085}.
[13:44:27.279] Evaluation Finished!⏹️
[13:44:27.428] iteration 19601: total_loss: 0.597121, loss_sup: 0.019925, loss_mps: 0.186743, loss_cps: 0.390454
[13:44:27.578] iteration 19602: total_loss: 0.448510, loss_sup: 0.065388, loss_mps: 0.133668, loss_cps: 0.249455
[13:44:27.723] iteration 19603: total_loss: 0.538793, loss_sup: 0.045701, loss_mps: 0.162327, loss_cps: 0.330765
[13:44:27.869] iteration 19604: total_loss: 0.475933, loss_sup: 0.047579, loss_mps: 0.147586, loss_cps: 0.280767
[13:44:28.015] iteration 19605: total_loss: 0.664880, loss_sup: 0.234511, loss_mps: 0.143980, loss_cps: 0.286389
[13:44:28.160] iteration 19606: total_loss: 0.498898, loss_sup: 0.089551, loss_mps: 0.133535, loss_cps: 0.275811
[13:44:28.305] iteration 19607: total_loss: 0.694283, loss_sup: 0.266296, loss_mps: 0.143468, loss_cps: 0.284519
[13:44:28.451] iteration 19608: total_loss: 0.854496, loss_sup: 0.388138, loss_mps: 0.164194, loss_cps: 0.302164
[13:44:28.596] iteration 19609: total_loss: 0.505160, loss_sup: 0.026037, loss_mps: 0.160766, loss_cps: 0.318358
[13:44:28.742] iteration 19610: total_loss: 0.389608, loss_sup: 0.019694, loss_mps: 0.124402, loss_cps: 0.245512
[13:44:28.887] iteration 19611: total_loss: 0.615254, loss_sup: 0.064173, loss_mps: 0.179740, loss_cps: 0.371342
[13:44:29.034] iteration 19612: total_loss: 0.368579, loss_sup: 0.002972, loss_mps: 0.125323, loss_cps: 0.240284
[13:44:29.179] iteration 19613: total_loss: 0.549370, loss_sup: 0.161960, loss_mps: 0.138021, loss_cps: 0.249389
[13:44:29.324] iteration 19614: total_loss: 0.651606, loss_sup: 0.061000, loss_mps: 0.205882, loss_cps: 0.384724
[13:44:29.470] iteration 19615: total_loss: 0.404900, loss_sup: 0.051136, loss_mps: 0.127723, loss_cps: 0.226041
[13:44:29.616] iteration 19616: total_loss: 0.599432, loss_sup: 0.094915, loss_mps: 0.168999, loss_cps: 0.335518
[13:44:29.762] iteration 19617: total_loss: 0.412819, loss_sup: 0.101972, loss_mps: 0.114601, loss_cps: 0.196246
[13:44:29.908] iteration 19618: total_loss: 0.453021, loss_sup: 0.076507, loss_mps: 0.132425, loss_cps: 0.244089
[13:44:30.053] iteration 19619: total_loss: 0.471397, loss_sup: 0.137656, loss_mps: 0.122678, loss_cps: 0.211063
[13:44:30.198] iteration 19620: total_loss: 0.992821, loss_sup: 0.227044, loss_mps: 0.263867, loss_cps: 0.501910
[13:44:30.344] iteration 19621: total_loss: 0.525197, loss_sup: 0.126254, loss_mps: 0.138007, loss_cps: 0.260936
[13:44:30.489] iteration 19622: total_loss: 0.772046, loss_sup: 0.154047, loss_mps: 0.215106, loss_cps: 0.402893
[13:44:30.634] iteration 19623: total_loss: 0.740305, loss_sup: 0.271166, loss_mps: 0.157779, loss_cps: 0.311360
[13:44:30.780] iteration 19624: total_loss: 0.616615, loss_sup: 0.288016, loss_mps: 0.119235, loss_cps: 0.209364
[13:44:30.927] iteration 19625: total_loss: 0.881040, loss_sup: 0.075452, loss_mps: 0.248454, loss_cps: 0.557135
[13:44:31.072] iteration 19626: total_loss: 0.409215, loss_sup: 0.035188, loss_mps: 0.124411, loss_cps: 0.249616
[13:44:31.217] iteration 19627: total_loss: 0.700937, loss_sup: 0.156903, loss_mps: 0.187538, loss_cps: 0.356496
[13:44:31.367] iteration 19628: total_loss: 0.592437, loss_sup: 0.014572, loss_mps: 0.193853, loss_cps: 0.384012
[13:44:31.512] iteration 19629: total_loss: 0.628575, loss_sup: 0.021787, loss_mps: 0.205435, loss_cps: 0.401353
[13:44:31.658] iteration 19630: total_loss: 0.466208, loss_sup: 0.043212, loss_mps: 0.146307, loss_cps: 0.276688
[13:44:31.805] iteration 19631: total_loss: 1.040939, loss_sup: 0.414681, loss_mps: 0.206517, loss_cps: 0.419741
[13:44:31.950] iteration 19632: total_loss: 0.474402, loss_sup: 0.005382, loss_mps: 0.157691, loss_cps: 0.311329
[13:44:32.096] iteration 19633: total_loss: 0.408038, loss_sup: 0.022029, loss_mps: 0.141234, loss_cps: 0.244774
[13:44:32.241] iteration 19634: total_loss: 0.996263, loss_sup: 0.046228, loss_mps: 0.302398, loss_cps: 0.647637
[13:44:32.386] iteration 19635: total_loss: 0.443843, loss_sup: 0.021431, loss_mps: 0.142649, loss_cps: 0.279762
[13:44:32.531] iteration 19636: total_loss: 0.826275, loss_sup: 0.431892, loss_mps: 0.142054, loss_cps: 0.252329
[13:44:32.676] iteration 19637: total_loss: 0.417326, loss_sup: 0.111855, loss_mps: 0.110145, loss_cps: 0.195326
[13:44:32.822] iteration 19638: total_loss: 0.354932, loss_sup: 0.048540, loss_mps: 0.111779, loss_cps: 0.194613
[13:44:32.967] iteration 19639: total_loss: 0.412353, loss_sup: 0.004388, loss_mps: 0.148333, loss_cps: 0.259633
[13:44:33.113] iteration 19640: total_loss: 0.356117, loss_sup: 0.018396, loss_mps: 0.125256, loss_cps: 0.212466
[13:44:33.258] iteration 19641: total_loss: 0.748971, loss_sup: 0.063149, loss_mps: 0.235261, loss_cps: 0.450561
[13:44:33.403] iteration 19642: total_loss: 0.359347, loss_sup: 0.017843, loss_mps: 0.123612, loss_cps: 0.217892
[13:44:33.550] iteration 19643: total_loss: 0.697634, loss_sup: 0.189765, loss_mps: 0.175461, loss_cps: 0.332408
[13:44:33.695] iteration 19644: total_loss: 0.379613, loss_sup: 0.033900, loss_mps: 0.125268, loss_cps: 0.220446
[13:44:33.841] iteration 19645: total_loss: 0.336369, loss_sup: 0.006962, loss_mps: 0.118285, loss_cps: 0.211122
[13:44:33.904] iteration 19646: total_loss: 0.349755, loss_sup: 0.008751, loss_mps: 0.116735, loss_cps: 0.224269
[13:44:35.105] iteration 19647: total_loss: 0.419529, loss_sup: 0.081920, loss_mps: 0.120007, loss_cps: 0.217601
[13:44:35.253] iteration 19648: total_loss: 0.497605, loss_sup: 0.139716, loss_mps: 0.125064, loss_cps: 0.232825
[13:44:35.403] iteration 19649: total_loss: 0.416541, loss_sup: 0.042073, loss_mps: 0.142320, loss_cps: 0.232147
[13:44:35.556] iteration 19650: total_loss: 0.576989, loss_sup: 0.130544, loss_mps: 0.160081, loss_cps: 0.286364
[13:44:35.704] iteration 19651: total_loss: 0.595664, loss_sup: 0.010415, loss_mps: 0.183068, loss_cps: 0.402182
[13:44:35.849] iteration 19652: total_loss: 0.494850, loss_sup: 0.047115, loss_mps: 0.151578, loss_cps: 0.296157
[13:44:35.996] iteration 19653: total_loss: 0.341342, loss_sup: 0.027429, loss_mps: 0.114032, loss_cps: 0.199881
[13:44:36.142] iteration 19654: total_loss: 0.429831, loss_sup: 0.018318, loss_mps: 0.133951, loss_cps: 0.277563
[13:44:36.293] iteration 19655: total_loss: 0.581281, loss_sup: 0.132455, loss_mps: 0.156058, loss_cps: 0.292769
[13:44:36.442] iteration 19656: total_loss: 0.641850, loss_sup: 0.158278, loss_mps: 0.159192, loss_cps: 0.324380
[13:44:36.587] iteration 19657: total_loss: 0.583796, loss_sup: 0.056495, loss_mps: 0.182005, loss_cps: 0.345297
[13:44:36.734] iteration 19658: total_loss: 0.737489, loss_sup: 0.167931, loss_mps: 0.187334, loss_cps: 0.382224
[13:44:36.881] iteration 19659: total_loss: 0.298039, loss_sup: 0.013871, loss_mps: 0.104870, loss_cps: 0.179297
[13:44:37.029] iteration 19660: total_loss: 0.389464, loss_sup: 0.017616, loss_mps: 0.136113, loss_cps: 0.235736
[13:44:37.175] iteration 19661: total_loss: 0.473105, loss_sup: 0.080942, loss_mps: 0.142147, loss_cps: 0.250017
[13:44:37.322] iteration 19662: total_loss: 0.497450, loss_sup: 0.019393, loss_mps: 0.156939, loss_cps: 0.321118
[13:44:37.470] iteration 19663: total_loss: 0.774480, loss_sup: 0.337106, loss_mps: 0.153645, loss_cps: 0.283728
[13:44:37.616] iteration 19664: total_loss: 0.614666, loss_sup: 0.073573, loss_mps: 0.193593, loss_cps: 0.347499
[13:44:37.762] iteration 19665: total_loss: 0.258141, loss_sup: 0.001423, loss_mps: 0.099561, loss_cps: 0.157156
[13:44:37.908] iteration 19666: total_loss: 0.417497, loss_sup: 0.021717, loss_mps: 0.133734, loss_cps: 0.262046
[13:44:38.055] iteration 19667: total_loss: 0.443558, loss_sup: 0.026285, loss_mps: 0.144376, loss_cps: 0.272897
[13:44:38.202] iteration 19668: total_loss: 0.461048, loss_sup: 0.039374, loss_mps: 0.144655, loss_cps: 0.277019
[13:44:38.348] iteration 19669: total_loss: 0.246427, loss_sup: 0.012979, loss_mps: 0.091052, loss_cps: 0.142395
[13:44:38.494] iteration 19670: total_loss: 0.533219, loss_sup: 0.172953, loss_mps: 0.120708, loss_cps: 0.239559
[13:44:38.640] iteration 19671: total_loss: 0.384826, loss_sup: 0.073421, loss_mps: 0.111255, loss_cps: 0.200150
[13:44:38.790] iteration 19672: total_loss: 0.801519, loss_sup: 0.007125, loss_mps: 0.248475, loss_cps: 0.545920
[13:44:38.936] iteration 19673: total_loss: 0.485772, loss_sup: 0.113719, loss_mps: 0.123356, loss_cps: 0.248697
[13:44:39.082] iteration 19674: total_loss: 0.484543, loss_sup: 0.097035, loss_mps: 0.140350, loss_cps: 0.247158
[13:44:39.228] iteration 19675: total_loss: 0.728770, loss_sup: 0.343668, loss_mps: 0.136542, loss_cps: 0.248561
[13:44:39.374] iteration 19676: total_loss: 0.397528, loss_sup: 0.085671, loss_mps: 0.108574, loss_cps: 0.203284
[13:44:39.520] iteration 19677: total_loss: 0.324235, loss_sup: 0.031100, loss_mps: 0.108486, loss_cps: 0.184648
[13:44:39.666] iteration 19678: total_loss: 0.547666, loss_sup: 0.059990, loss_mps: 0.164862, loss_cps: 0.322814
[13:44:39.812] iteration 19679: total_loss: 0.309118, loss_sup: 0.014758, loss_mps: 0.108050, loss_cps: 0.186310
[13:44:39.958] iteration 19680: total_loss: 0.666531, loss_sup: 0.121221, loss_mps: 0.180963, loss_cps: 0.364347
[13:44:40.105] iteration 19681: total_loss: 0.349110, loss_sup: 0.047844, loss_mps: 0.111753, loss_cps: 0.189513
[13:44:40.251] iteration 19682: total_loss: 0.449107, loss_sup: 0.069414, loss_mps: 0.134148, loss_cps: 0.245544
[13:44:40.396] iteration 19683: total_loss: 0.526418, loss_sup: 0.021668, loss_mps: 0.169751, loss_cps: 0.334999
[13:44:40.543] iteration 19684: total_loss: 0.501724, loss_sup: 0.116019, loss_mps: 0.133187, loss_cps: 0.252518
[13:44:40.689] iteration 19685: total_loss: 0.351938, loss_sup: 0.052841, loss_mps: 0.101944, loss_cps: 0.197154
[13:44:40.836] iteration 19686: total_loss: 0.447586, loss_sup: 0.106035, loss_mps: 0.117777, loss_cps: 0.223774
[13:44:40.982] iteration 19687: total_loss: 0.522093, loss_sup: 0.146245, loss_mps: 0.128477, loss_cps: 0.247371
[13:44:41.128] iteration 19688: total_loss: 0.250450, loss_sup: 0.019477, loss_mps: 0.081571, loss_cps: 0.149402
[13:44:41.274] iteration 19689: total_loss: 0.345819, loss_sup: 0.055763, loss_mps: 0.101883, loss_cps: 0.188173
[13:44:41.420] iteration 19690: total_loss: 0.353611, loss_sup: 0.026760, loss_mps: 0.119780, loss_cps: 0.207071
[13:44:41.567] iteration 19691: total_loss: 0.716563, loss_sup: 0.294861, loss_mps: 0.137524, loss_cps: 0.284178
[13:44:41.713] iteration 19692: total_loss: 0.319467, loss_sup: 0.042760, loss_mps: 0.092482, loss_cps: 0.184226
[13:44:41.859] iteration 19693: total_loss: 0.578039, loss_sup: 0.110461, loss_mps: 0.159834, loss_cps: 0.307744
[13:44:42.005] iteration 19694: total_loss: 0.653376, loss_sup: 0.100326, loss_mps: 0.180923, loss_cps: 0.372127
[13:44:42.152] iteration 19695: total_loss: 0.462863, loss_sup: 0.025985, loss_mps: 0.158089, loss_cps: 0.278789
[13:44:42.300] iteration 19696: total_loss: 0.264309, loss_sup: 0.004322, loss_mps: 0.095110, loss_cps: 0.164877
[13:44:42.447] iteration 19697: total_loss: 0.328582, loss_sup: 0.091642, loss_mps: 0.086836, loss_cps: 0.150103
[13:44:42.592] iteration 19698: total_loss: 0.588342, loss_sup: 0.029605, loss_mps: 0.180036, loss_cps: 0.378702
[13:44:42.740] iteration 19699: total_loss: 0.266772, loss_sup: 0.011845, loss_mps: 0.093653, loss_cps: 0.161274
[13:44:42.886] iteration 19700: total_loss: 0.398688, loss_sup: 0.011350, loss_mps: 0.134690, loss_cps: 0.252647
[13:44:42.887] Evaluation Started ==>
[13:44:54.237] ==> valid iteration 19700: unet metrics: {'dc': 0.6744266832122622, 'jc': 0.5585802614304812, 'pre': 0.7800816982580536, 'hd': 5.49027768283833}, ynet metrics: {'dc': 0.5903496963582793, 'jc': 0.47768976208873454, 'pre': 0.8006163001077016, 'hd': 5.527493824575435}.
[13:44:54.238] Evaluation Finished!⏹️
[13:44:54.387] iteration 19701: total_loss: 0.520697, loss_sup: 0.032520, loss_mps: 0.158941, loss_cps: 0.329236
[13:44:54.535] iteration 19702: total_loss: 0.314345, loss_sup: 0.008946, loss_mps: 0.103798, loss_cps: 0.201601
[13:44:54.680] iteration 19703: total_loss: 0.450158, loss_sup: 0.055283, loss_mps: 0.131276, loss_cps: 0.263599
[13:44:54.826] iteration 19704: total_loss: 0.561670, loss_sup: 0.029327, loss_mps: 0.171660, loss_cps: 0.360683
[13:44:54.971] iteration 19705: total_loss: 0.448929, loss_sup: 0.083272, loss_mps: 0.122100, loss_cps: 0.243557
[13:44:55.116] iteration 19706: total_loss: 0.347273, loss_sup: 0.074243, loss_mps: 0.095518, loss_cps: 0.177512
[13:44:55.264] iteration 19707: total_loss: 0.401235, loss_sup: 0.041956, loss_mps: 0.142725, loss_cps: 0.216554
[13:44:55.408] iteration 19708: total_loss: 0.361730, loss_sup: 0.032176, loss_mps: 0.107385, loss_cps: 0.222169
[13:44:55.553] iteration 19709: total_loss: 0.303967, loss_sup: 0.018687, loss_mps: 0.101345, loss_cps: 0.183935
[13:44:55.698] iteration 19710: total_loss: 0.574264, loss_sup: 0.091403, loss_mps: 0.160884, loss_cps: 0.321977
[13:44:55.843] iteration 19711: total_loss: 0.459169, loss_sup: 0.059685, loss_mps: 0.132351, loss_cps: 0.267134
[13:44:55.988] iteration 19712: total_loss: 0.401359, loss_sup: 0.131412, loss_mps: 0.091119, loss_cps: 0.178829
[13:44:56.133] iteration 19713: total_loss: 0.427363, loss_sup: 0.026273, loss_mps: 0.129616, loss_cps: 0.271473
[13:44:56.280] iteration 19714: total_loss: 0.609935, loss_sup: 0.082723, loss_mps: 0.166400, loss_cps: 0.360812
[13:44:56.426] iteration 19715: total_loss: 0.575743, loss_sup: 0.084479, loss_mps: 0.168045, loss_cps: 0.323220
[13:44:56.573] iteration 19716: total_loss: 0.477147, loss_sup: 0.128236, loss_mps: 0.117756, loss_cps: 0.231155
[13:44:56.718] iteration 19717: total_loss: 0.834734, loss_sup: 0.119388, loss_mps: 0.230883, loss_cps: 0.484463
[13:44:56.863] iteration 19718: total_loss: 0.903475, loss_sup: 0.180354, loss_mps: 0.227756, loss_cps: 0.495365
[13:44:57.008] iteration 19719: total_loss: 0.454346, loss_sup: 0.018790, loss_mps: 0.144189, loss_cps: 0.291367
[13:44:57.159] iteration 19720: total_loss: 0.595133, loss_sup: 0.019398, loss_mps: 0.184547, loss_cps: 0.391188
[13:44:57.307] iteration 19721: total_loss: 0.495031, loss_sup: 0.074497, loss_mps: 0.134901, loss_cps: 0.285633
[13:44:57.455] iteration 19722: total_loss: 0.420337, loss_sup: 0.147552, loss_mps: 0.099205, loss_cps: 0.173580
[13:44:57.600] iteration 19723: total_loss: 0.695691, loss_sup: 0.106786, loss_mps: 0.190175, loss_cps: 0.398729
[13:44:57.746] iteration 19724: total_loss: 0.433824, loss_sup: 0.062143, loss_mps: 0.137740, loss_cps: 0.233941
[13:44:57.892] iteration 19725: total_loss: 0.539313, loss_sup: 0.199918, loss_mps: 0.116920, loss_cps: 0.222475
[13:44:58.037] iteration 19726: total_loss: 0.443210, loss_sup: 0.013332, loss_mps: 0.147485, loss_cps: 0.282394
[13:44:58.185] iteration 19727: total_loss: 0.509465, loss_sup: 0.091303, loss_mps: 0.132927, loss_cps: 0.285235
[13:44:58.337] iteration 19728: total_loss: 0.240052, loss_sup: 0.005096, loss_mps: 0.087397, loss_cps: 0.147559
[13:44:58.483] iteration 19729: total_loss: 0.885470, loss_sup: 0.075586, loss_mps: 0.250621, loss_cps: 0.559263
[13:44:58.629] iteration 19730: total_loss: 0.334515, loss_sup: 0.001482, loss_mps: 0.120666, loss_cps: 0.212367
[13:44:58.777] iteration 19731: total_loss: 0.534153, loss_sup: 0.126491, loss_mps: 0.144353, loss_cps: 0.263310
[13:44:58.923] iteration 19732: total_loss: 0.407653, loss_sup: 0.107850, loss_mps: 0.103527, loss_cps: 0.196276
[13:44:59.068] iteration 19733: total_loss: 0.836635, loss_sup: 0.151507, loss_mps: 0.211677, loss_cps: 0.473451
[13:44:59.216] iteration 19734: total_loss: 0.582006, loss_sup: 0.055764, loss_mps: 0.181377, loss_cps: 0.344865
[13:44:59.365] iteration 19735: total_loss: 0.620788, loss_sup: 0.104840, loss_mps: 0.165835, loss_cps: 0.350114
[13:44:59.511] iteration 19736: total_loss: 0.363836, loss_sup: 0.085558, loss_mps: 0.099372, loss_cps: 0.178906
[13:44:59.657] iteration 19737: total_loss: 0.266679, loss_sup: 0.014678, loss_mps: 0.092169, loss_cps: 0.159832
[13:44:59.802] iteration 19738: total_loss: 0.560257, loss_sup: 0.013456, loss_mps: 0.183311, loss_cps: 0.363491
[13:44:59.949] iteration 19739: total_loss: 1.136798, loss_sup: 0.102751, loss_mps: 0.308657, loss_cps: 0.725390
[13:45:00.095] iteration 19740: total_loss: 0.813310, loss_sup: 0.304185, loss_mps: 0.172072, loss_cps: 0.337052
[13:45:00.243] iteration 19741: total_loss: 0.489110, loss_sup: 0.120423, loss_mps: 0.136675, loss_cps: 0.232012
[13:45:00.390] iteration 19742: total_loss: 0.679267, loss_sup: 0.176188, loss_mps: 0.169845, loss_cps: 0.333234
[13:45:00.535] iteration 19743: total_loss: 0.679976, loss_sup: 0.100380, loss_mps: 0.194599, loss_cps: 0.384998
[13:45:00.681] iteration 19744: total_loss: 0.412754, loss_sup: 0.019105, loss_mps: 0.139729, loss_cps: 0.253921
[13:45:00.827] iteration 19745: total_loss: 0.784659, loss_sup: 0.107489, loss_mps: 0.223977, loss_cps: 0.453193
[13:45:00.974] iteration 19746: total_loss: 0.502252, loss_sup: 0.015901, loss_mps: 0.155304, loss_cps: 0.331048
[13:45:01.120] iteration 19747: total_loss: 0.263391, loss_sup: 0.041078, loss_mps: 0.085161, loss_cps: 0.137152
[13:45:01.265] iteration 19748: total_loss: 0.584656, loss_sup: 0.042356, loss_mps: 0.179208, loss_cps: 0.363091
[13:45:01.411] iteration 19749: total_loss: 0.648987, loss_sup: 0.194828, loss_mps: 0.152674, loss_cps: 0.301485
[13:45:01.558] iteration 19750: total_loss: 1.018091, loss_sup: 0.045934, loss_mps: 0.300732, loss_cps: 0.671425
[13:45:01.704] iteration 19751: total_loss: 0.411103, loss_sup: 0.072100, loss_mps: 0.128717, loss_cps: 0.210286
[13:45:01.852] iteration 19752: total_loss: 0.462072, loss_sup: 0.031982, loss_mps: 0.151781, loss_cps: 0.278310
[13:45:01.999] iteration 19753: total_loss: 0.290069, loss_sup: 0.009162, loss_mps: 0.097641, loss_cps: 0.183267
[13:45:02.147] iteration 19754: total_loss: 0.251637, loss_sup: 0.006310, loss_mps: 0.089259, loss_cps: 0.156067
[13:45:02.293] iteration 19755: total_loss: 1.228509, loss_sup: 0.221008, loss_mps: 0.308813, loss_cps: 0.698688
[13:45:02.439] iteration 19756: total_loss: 0.328251, loss_sup: 0.008362, loss_mps: 0.109042, loss_cps: 0.210847
[13:45:02.586] iteration 19757: total_loss: 0.400390, loss_sup: 0.128023, loss_mps: 0.100170, loss_cps: 0.172197
[13:45:02.733] iteration 19758: total_loss: 0.352319, loss_sup: 0.052305, loss_mps: 0.109766, loss_cps: 0.190248
[13:45:02.879] iteration 19759: total_loss: 0.805164, loss_sup: 0.052993, loss_mps: 0.242950, loss_cps: 0.509221
[13:45:03.025] iteration 19760: total_loss: 0.433672, loss_sup: 0.064145, loss_mps: 0.129649, loss_cps: 0.239879
[13:45:03.171] iteration 19761: total_loss: 0.787520, loss_sup: 0.140196, loss_mps: 0.211760, loss_cps: 0.435565
[13:45:03.317] iteration 19762: total_loss: 0.929912, loss_sup: 0.053698, loss_mps: 0.271440, loss_cps: 0.604774
[13:45:03.464] iteration 19763: total_loss: 0.779959, loss_sup: 0.164016, loss_mps: 0.210824, loss_cps: 0.405119
[13:45:03.610] iteration 19764: total_loss: 0.443441, loss_sup: 0.035647, loss_mps: 0.141513, loss_cps: 0.266281
[13:45:03.757] iteration 19765: total_loss: 0.688030, loss_sup: 0.082394, loss_mps: 0.198991, loss_cps: 0.406644
[13:45:03.904] iteration 19766: total_loss: 0.477807, loss_sup: 0.007920, loss_mps: 0.164776, loss_cps: 0.305110
[13:45:04.050] iteration 19767: total_loss: 0.350269, loss_sup: 0.056328, loss_mps: 0.107681, loss_cps: 0.186260
[13:45:04.197] iteration 19768: total_loss: 0.369142, loss_sup: 0.067609, loss_mps: 0.110541, loss_cps: 0.190992
[13:45:04.344] iteration 19769: total_loss: 0.781246, loss_sup: 0.006711, loss_mps: 0.233561, loss_cps: 0.540974
[13:45:04.492] iteration 19770: total_loss: 0.458803, loss_sup: 0.005963, loss_mps: 0.153395, loss_cps: 0.299445
[13:45:04.638] iteration 19771: total_loss: 0.421273, loss_sup: 0.120413, loss_mps: 0.110413, loss_cps: 0.190448
[13:45:04.784] iteration 19772: total_loss: 0.653780, loss_sup: 0.136525, loss_mps: 0.169865, loss_cps: 0.347390
[13:45:04.931] iteration 19773: total_loss: 0.617613, loss_sup: 0.110518, loss_mps: 0.164163, loss_cps: 0.342932
[13:45:05.077] iteration 19774: total_loss: 0.673443, loss_sup: 0.074433, loss_mps: 0.210577, loss_cps: 0.388433
[13:45:05.224] iteration 19775: total_loss: 0.442895, loss_sup: 0.119715, loss_mps: 0.116082, loss_cps: 0.207098
[13:45:05.370] iteration 19776: total_loss: 0.505099, loss_sup: 0.034822, loss_mps: 0.161094, loss_cps: 0.309183
[13:45:05.517] iteration 19777: total_loss: 0.425446, loss_sup: 0.087023, loss_mps: 0.126204, loss_cps: 0.212219
[13:45:05.662] iteration 19778: total_loss: 0.319841, loss_sup: 0.055158, loss_mps: 0.094685, loss_cps: 0.169998
[13:45:05.810] iteration 19779: total_loss: 0.511609, loss_sup: 0.069647, loss_mps: 0.151073, loss_cps: 0.290889
[13:45:05.956] iteration 19780: total_loss: 0.346150, loss_sup: 0.030810, loss_mps: 0.110245, loss_cps: 0.205095
[13:45:06.102] iteration 19781: total_loss: 0.573318, loss_sup: 0.063525, loss_mps: 0.175103, loss_cps: 0.334690
[13:45:06.249] iteration 19782: total_loss: 0.504325, loss_sup: 0.032743, loss_mps: 0.163577, loss_cps: 0.308005
[13:45:06.394] iteration 19783: total_loss: 0.549337, loss_sup: 0.191971, loss_mps: 0.127704, loss_cps: 0.229663
[13:45:06.542] iteration 19784: total_loss: 0.340468, loss_sup: 0.031636, loss_mps: 0.106563, loss_cps: 0.202269
[13:45:06.692] iteration 19785: total_loss: 0.301556, loss_sup: 0.002715, loss_mps: 0.104927, loss_cps: 0.193914
[13:45:06.840] iteration 19786: total_loss: 0.740560, loss_sup: 0.188484, loss_mps: 0.186005, loss_cps: 0.366071
[13:45:06.987] iteration 19787: total_loss: 0.964136, loss_sup: 0.111441, loss_mps: 0.267550, loss_cps: 0.585144
[13:45:07.133] iteration 19788: total_loss: 0.525743, loss_sup: 0.049165, loss_mps: 0.168091, loss_cps: 0.308487
[13:45:07.279] iteration 19789: total_loss: 0.422944, loss_sup: 0.021154, loss_mps: 0.138133, loss_cps: 0.263658
[13:45:07.425] iteration 19790: total_loss: 1.008750, loss_sup: 0.051811, loss_mps: 0.302739, loss_cps: 0.654200
[13:45:07.571] iteration 19791: total_loss: 0.599464, loss_sup: 0.135246, loss_mps: 0.161459, loss_cps: 0.302759
[13:45:07.720] iteration 19792: total_loss: 0.361520, loss_sup: 0.044641, loss_mps: 0.114346, loss_cps: 0.202534
[13:45:07.866] iteration 19793: total_loss: 0.717563, loss_sup: 0.040464, loss_mps: 0.226409, loss_cps: 0.450690
[13:45:08.013] iteration 19794: total_loss: 0.541169, loss_sup: 0.073096, loss_mps: 0.172233, loss_cps: 0.295840
[13:45:08.159] iteration 19795: total_loss: 0.537599, loss_sup: 0.091866, loss_mps: 0.158772, loss_cps: 0.286961
[13:45:08.305] iteration 19796: total_loss: 0.515374, loss_sup: 0.206165, loss_mps: 0.118866, loss_cps: 0.190343
[13:45:08.451] iteration 19797: total_loss: 0.302793, loss_sup: 0.005965, loss_mps: 0.107655, loss_cps: 0.189173
[13:45:08.598] iteration 19798: total_loss: 0.356236, loss_sup: 0.010363, loss_mps: 0.119578, loss_cps: 0.226295
[13:45:08.744] iteration 19799: total_loss: 0.486436, loss_sup: 0.012849, loss_mps: 0.160906, loss_cps: 0.312681
[13:45:08.891] iteration 19800: total_loss: 0.462517, loss_sup: 0.040260, loss_mps: 0.142670, loss_cps: 0.279587
[13:45:08.891] Evaluation Started ==>
[13:45:20.225] ==> valid iteration 19800: unet metrics: {'dc': 0.6126460544470829, 'jc': 0.49691530902442427, 'pre': 0.7753471195286953, 'hd': 5.57817032515395}, ynet metrics: {'dc': 0.6034525727295741, 'jc': 0.4896321992202155, 'pre': 0.7786963259381835, 'hd': 5.604588480190184}.
[13:45:20.227] Evaluation Finished!⏹️
[13:45:20.377] iteration 19801: total_loss: 0.459149, loss_sup: 0.068693, loss_mps: 0.135854, loss_cps: 0.254602
[13:45:20.524] iteration 19802: total_loss: 0.404774, loss_sup: 0.037505, loss_mps: 0.125035, loss_cps: 0.242234
[13:45:20.672] iteration 19803: total_loss: 0.552244, loss_sup: 0.081622, loss_mps: 0.161765, loss_cps: 0.308857
[13:45:20.817] iteration 19804: total_loss: 0.921589, loss_sup: 0.482370, loss_mps: 0.153882, loss_cps: 0.285336
[13:45:20.962] iteration 19805: total_loss: 0.590313, loss_sup: 0.087721, loss_mps: 0.155298, loss_cps: 0.347295
[13:45:21.108] iteration 19806: total_loss: 0.402639, loss_sup: 0.029359, loss_mps: 0.120563, loss_cps: 0.252716
[13:45:21.253] iteration 19807: total_loss: 0.425266, loss_sup: 0.045415, loss_mps: 0.132921, loss_cps: 0.246930
[13:45:21.399] iteration 19808: total_loss: 0.618867, loss_sup: 0.047848, loss_mps: 0.183380, loss_cps: 0.387639
[13:45:21.545] iteration 19809: total_loss: 0.403629, loss_sup: 0.089643, loss_mps: 0.115706, loss_cps: 0.198280
[13:45:21.690] iteration 19810: total_loss: 0.872652, loss_sup: 0.097459, loss_mps: 0.238128, loss_cps: 0.537064
[13:45:21.837] iteration 19811: total_loss: 0.648363, loss_sup: 0.096150, loss_mps: 0.182922, loss_cps: 0.369292
[13:45:21.983] iteration 19812: total_loss: 0.492323, loss_sup: 0.011744, loss_mps: 0.175456, loss_cps: 0.305123
[13:45:22.128] iteration 19813: total_loss: 0.455819, loss_sup: 0.082138, loss_mps: 0.128574, loss_cps: 0.245106
[13:45:22.274] iteration 19814: total_loss: 0.537583, loss_sup: 0.058579, loss_mps: 0.165041, loss_cps: 0.313964
[13:45:22.421] iteration 19815: total_loss: 0.922182, loss_sup: 0.067041, loss_mps: 0.281631, loss_cps: 0.573510
[13:45:22.567] iteration 19816: total_loss: 0.430787, loss_sup: 0.119207, loss_mps: 0.108442, loss_cps: 0.203138
[13:45:22.713] iteration 19817: total_loss: 0.829756, loss_sup: 0.199830, loss_mps: 0.195355, loss_cps: 0.434571
[13:45:22.858] iteration 19818: total_loss: 0.453597, loss_sup: 0.054526, loss_mps: 0.136950, loss_cps: 0.262121
[13:45:23.006] iteration 19819: total_loss: 0.600251, loss_sup: 0.058011, loss_mps: 0.171542, loss_cps: 0.370698
[13:45:23.155] iteration 19820: total_loss: 0.346386, loss_sup: 0.008979, loss_mps: 0.117511, loss_cps: 0.219896
[13:45:23.301] iteration 19821: total_loss: 0.528004, loss_sup: 0.156812, loss_mps: 0.135110, loss_cps: 0.236083
[13:45:23.448] iteration 19822: total_loss: 0.779925, loss_sup: 0.080686, loss_mps: 0.211376, loss_cps: 0.487863
[13:45:23.594] iteration 19823: total_loss: 0.424513, loss_sup: 0.035167, loss_mps: 0.133002, loss_cps: 0.256344
[13:45:23.740] iteration 19824: total_loss: 0.653300, loss_sup: 0.061394, loss_mps: 0.200247, loss_cps: 0.391660
[13:45:23.889] iteration 19825: total_loss: 0.465300, loss_sup: 0.094124, loss_mps: 0.140291, loss_cps: 0.230886
[13:45:24.034] iteration 19826: total_loss: 0.329096, loss_sup: 0.008760, loss_mps: 0.117981, loss_cps: 0.202356
[13:45:24.181] iteration 19827: total_loss: 0.448181, loss_sup: 0.031806, loss_mps: 0.152074, loss_cps: 0.264302
[13:45:24.327] iteration 19828: total_loss: 0.407902, loss_sup: 0.029118, loss_mps: 0.139134, loss_cps: 0.239649
[13:45:24.474] iteration 19829: total_loss: 0.387620, loss_sup: 0.038215, loss_mps: 0.124155, loss_cps: 0.225250
[13:45:24.621] iteration 19830: total_loss: 0.897851, loss_sup: 0.172381, loss_mps: 0.234661, loss_cps: 0.490810
[13:45:24.767] iteration 19831: total_loss: 0.602841, loss_sup: 0.076312, loss_mps: 0.164789, loss_cps: 0.361740
[13:45:24.912] iteration 19832: total_loss: 0.820310, loss_sup: 0.099916, loss_mps: 0.235283, loss_cps: 0.485111
[13:45:25.058] iteration 19833: total_loss: 0.817949, loss_sup: 0.365074, loss_mps: 0.159457, loss_cps: 0.293418
[13:45:25.204] iteration 19834: total_loss: 0.569962, loss_sup: 0.051879, loss_mps: 0.175352, loss_cps: 0.342731
[13:45:25.351] iteration 19835: total_loss: 0.352526, loss_sup: 0.048544, loss_mps: 0.105706, loss_cps: 0.198277
[13:45:25.497] iteration 19836: total_loss: 0.559809, loss_sup: 0.108032, loss_mps: 0.150267, loss_cps: 0.301510
[13:45:25.642] iteration 19837: total_loss: 0.650672, loss_sup: 0.070193, loss_mps: 0.188717, loss_cps: 0.391762
[13:45:25.788] iteration 19838: total_loss: 0.979041, loss_sup: 0.050192, loss_mps: 0.283035, loss_cps: 0.645814
[13:45:25.933] iteration 19839: total_loss: 0.999092, loss_sup: 0.225128, loss_mps: 0.248587, loss_cps: 0.525376
[13:45:26.079] iteration 19840: total_loss: 0.526297, loss_sup: 0.116177, loss_mps: 0.133330, loss_cps: 0.276790
[13:45:26.225] iteration 19841: total_loss: 0.627711, loss_sup: 0.032600, loss_mps: 0.196177, loss_cps: 0.398934
[13:45:26.371] iteration 19842: total_loss: 0.551289, loss_sup: 0.083740, loss_mps: 0.155160, loss_cps: 0.312389
[13:45:26.518] iteration 19843: total_loss: 1.019424, loss_sup: 0.020145, loss_mps: 0.308033, loss_cps: 0.691246
[13:45:26.663] iteration 19844: total_loss: 0.622725, loss_sup: 0.116337, loss_mps: 0.172349, loss_cps: 0.334039
[13:45:26.809] iteration 19845: total_loss: 0.477222, loss_sup: 0.049579, loss_mps: 0.144707, loss_cps: 0.282935
[13:45:26.954] iteration 19846: total_loss: 0.568808, loss_sup: 0.071832, loss_mps: 0.170711, loss_cps: 0.326265
[13:45:27.099] iteration 19847: total_loss: 1.644897, loss_sup: 0.266744, loss_mps: 0.421585, loss_cps: 0.956567
[13:45:27.245] iteration 19848: total_loss: 0.765035, loss_sup: 0.130504, loss_mps: 0.205320, loss_cps: 0.429212
[13:45:27.393] iteration 19849: total_loss: 0.662624, loss_sup: 0.115623, loss_mps: 0.181575, loss_cps: 0.365426
[13:45:27.539] iteration 19850: total_loss: 0.541962, loss_sup: 0.017597, loss_mps: 0.167779, loss_cps: 0.356585
[13:45:27.686] iteration 19851: total_loss: 0.694286, loss_sup: 0.093886, loss_mps: 0.204390, loss_cps: 0.396010
[13:45:27.832] iteration 19852: total_loss: 0.584694, loss_sup: 0.050335, loss_mps: 0.181724, loss_cps: 0.352636
[13:45:27.979] iteration 19853: total_loss: 0.484796, loss_sup: 0.034664, loss_mps: 0.154156, loss_cps: 0.295976
[13:45:28.126] iteration 19854: total_loss: 0.440450, loss_sup: 0.142182, loss_mps: 0.105398, loss_cps: 0.192870
[13:45:28.272] iteration 19855: total_loss: 0.714734, loss_sup: 0.259912, loss_mps: 0.167473, loss_cps: 0.287348
[13:45:28.418] iteration 19856: total_loss: 0.789254, loss_sup: 0.053057, loss_mps: 0.239457, loss_cps: 0.496740
[13:45:28.564] iteration 19857: total_loss: 0.699605, loss_sup: 0.078339, loss_mps: 0.199926, loss_cps: 0.421340
[13:45:28.712] iteration 19858: total_loss: 0.613149, loss_sup: 0.140222, loss_mps: 0.184090, loss_cps: 0.288838
[13:45:28.859] iteration 19859: total_loss: 0.559816, loss_sup: 0.028426, loss_mps: 0.187313, loss_cps: 0.344077
[13:45:29.005] iteration 19860: total_loss: 0.674008, loss_sup: 0.014169, loss_mps: 0.222176, loss_cps: 0.437662
[13:45:29.151] iteration 19861: total_loss: 0.651212, loss_sup: 0.059022, loss_mps: 0.204669, loss_cps: 0.387521
[13:45:29.297] iteration 19862: total_loss: 0.997730, loss_sup: 0.081058, loss_mps: 0.321989, loss_cps: 0.594683
[13:45:29.443] iteration 19863: total_loss: 0.960519, loss_sup: 0.136280, loss_mps: 0.274307, loss_cps: 0.549932
[13:45:29.589] iteration 19864: total_loss: 0.398575, loss_sup: 0.025439, loss_mps: 0.128089, loss_cps: 0.245047
[13:45:29.735] iteration 19865: total_loss: 0.616454, loss_sup: 0.047718, loss_mps: 0.200542, loss_cps: 0.368194
[13:45:29.882] iteration 19866: total_loss: 0.477139, loss_sup: 0.035403, loss_mps: 0.151814, loss_cps: 0.289922
[13:45:30.028] iteration 19867: total_loss: 0.363609, loss_sup: 0.076966, loss_mps: 0.109093, loss_cps: 0.177550
[13:45:30.175] iteration 19868: total_loss: 0.378996, loss_sup: 0.050616, loss_mps: 0.119559, loss_cps: 0.208821
[13:45:30.322] iteration 19869: total_loss: 0.602694, loss_sup: 0.079851, loss_mps: 0.194341, loss_cps: 0.328502
[13:45:30.472] iteration 19870: total_loss: 0.416766, loss_sup: 0.055209, loss_mps: 0.138873, loss_cps: 0.222684
[13:45:30.618] iteration 19871: total_loss: 0.606141, loss_sup: 0.102371, loss_mps: 0.171267, loss_cps: 0.332503
[13:45:30.764] iteration 19872: total_loss: 0.455066, loss_sup: 0.071359, loss_mps: 0.141110, loss_cps: 0.242597
[13:45:30.910] iteration 19873: total_loss: 0.532401, loss_sup: 0.096900, loss_mps: 0.147361, loss_cps: 0.288140
[13:45:31.056] iteration 19874: total_loss: 0.474680, loss_sup: 0.063948, loss_mps: 0.141380, loss_cps: 0.269353
[13:45:31.202] iteration 19875: total_loss: 0.277469, loss_sup: 0.067366, loss_mps: 0.083091, loss_cps: 0.127013
[13:45:31.349] iteration 19876: total_loss: 0.436102, loss_sup: 0.056167, loss_mps: 0.130668, loss_cps: 0.249266
[13:45:31.495] iteration 19877: total_loss: 0.559121, loss_sup: 0.130677, loss_mps: 0.143264, loss_cps: 0.285180
[13:45:31.641] iteration 19878: total_loss: 0.578914, loss_sup: 0.346533, loss_mps: 0.084518, loss_cps: 0.147862
[13:45:31.787] iteration 19879: total_loss: 0.705337, loss_sup: 0.181118, loss_mps: 0.173675, loss_cps: 0.350544
[13:45:31.936] iteration 19880: total_loss: 0.929781, loss_sup: 0.037153, loss_mps: 0.284548, loss_cps: 0.608080
[13:45:32.082] iteration 19881: total_loss: 0.381140, loss_sup: 0.052417, loss_mps: 0.117885, loss_cps: 0.210839
[13:45:32.230] iteration 19882: total_loss: 0.416139, loss_sup: 0.053858, loss_mps: 0.131191, loss_cps: 0.231090
[13:45:32.375] iteration 19883: total_loss: 0.309634, loss_sup: 0.012951, loss_mps: 0.102476, loss_cps: 0.194207
[13:45:32.522] iteration 19884: total_loss: 0.514361, loss_sup: 0.113879, loss_mps: 0.145357, loss_cps: 0.255124
[13:45:32.668] iteration 19885: total_loss: 0.651351, loss_sup: 0.081134, loss_mps: 0.189192, loss_cps: 0.381025
[13:45:32.814] iteration 19886: total_loss: 0.423283, loss_sup: 0.071103, loss_mps: 0.122837, loss_cps: 0.229343
[13:45:32.960] iteration 19887: total_loss: 0.550190, loss_sup: 0.020615, loss_mps: 0.182346, loss_cps: 0.347229
[13:45:33.106] iteration 19888: total_loss: 0.856684, loss_sup: 0.122590, loss_mps: 0.227143, loss_cps: 0.506950
[13:45:33.252] iteration 19889: total_loss: 0.749934, loss_sup: 0.048345, loss_mps: 0.213430, loss_cps: 0.488159
[13:45:33.398] iteration 19890: total_loss: 0.466906, loss_sup: 0.138133, loss_mps: 0.122968, loss_cps: 0.205805
[13:45:33.546] iteration 19891: total_loss: 0.683997, loss_sup: 0.186925, loss_mps: 0.161011, loss_cps: 0.336061
[13:45:33.692] iteration 19892: total_loss: 0.368814, loss_sup: 0.079068, loss_mps: 0.101622, loss_cps: 0.188124
[13:45:33.838] iteration 19893: total_loss: 1.095531, loss_sup: 0.085629, loss_mps: 0.320494, loss_cps: 0.689408
[13:45:33.984] iteration 19894: total_loss: 0.548751, loss_sup: 0.162695, loss_mps: 0.134660, loss_cps: 0.251396
[13:45:34.130] iteration 19895: total_loss: 0.327689, loss_sup: 0.007803, loss_mps: 0.117052, loss_cps: 0.202834
[13:45:34.276] iteration 19896: total_loss: 0.578830, loss_sup: 0.126794, loss_mps: 0.157070, loss_cps: 0.294966
[13:45:34.422] iteration 19897: total_loss: 0.341024, loss_sup: 0.009966, loss_mps: 0.126287, loss_cps: 0.204770
[13:45:34.570] iteration 19898: total_loss: 0.679896, loss_sup: 0.007719, loss_mps: 0.208544, loss_cps: 0.463632
[13:45:34.715] iteration 19899: total_loss: 0.418667, loss_sup: 0.032073, loss_mps: 0.139061, loss_cps: 0.247534
[13:45:34.862] iteration 19900: total_loss: 0.596518, loss_sup: 0.008867, loss_mps: 0.194006, loss_cps: 0.393645
[13:45:34.862] Evaluation Started ==>
[13:45:46.210] ==> valid iteration 19900: unet metrics: {'dc': 0.6076682261464225, 'jc': 0.49328021920867177, 'pre': 0.7681190920215629, 'hd': 5.435369305898459}, ynet metrics: {'dc': 0.5976429081814004, 'jc': 0.4813859082136044, 'pre': 0.7880068368340324, 'hd': 5.524410166081243}.
[13:45:46.212] Evaluation Finished!⏹️
[13:45:46.363] iteration 19901: total_loss: 0.362028, loss_sup: 0.020142, loss_mps: 0.121228, loss_cps: 0.220658
[13:45:46.513] iteration 19902: total_loss: 0.416710, loss_sup: 0.104411, loss_mps: 0.112260, loss_cps: 0.200038
[13:45:46.659] iteration 19903: total_loss: 0.555826, loss_sup: 0.042538, loss_mps: 0.170014, loss_cps: 0.343274
[13:45:46.804] iteration 19904: total_loss: 0.334452, loss_sup: 0.060643, loss_mps: 0.102434, loss_cps: 0.171375
[13:45:46.949] iteration 19905: total_loss: 1.096261, loss_sup: 0.231159, loss_mps: 0.277612, loss_cps: 0.587490
[13:45:47.095] iteration 19906: total_loss: 1.174257, loss_sup: 0.085537, loss_mps: 0.336772, loss_cps: 0.751947
[13:45:47.240] iteration 19907: total_loss: 0.706975, loss_sup: 0.133108, loss_mps: 0.188893, loss_cps: 0.384974
[13:45:47.386] iteration 19908: total_loss: 0.295758, loss_sup: 0.016382, loss_mps: 0.103771, loss_cps: 0.175605
[13:45:47.532] iteration 19909: total_loss: 0.909335, loss_sup: 0.192942, loss_mps: 0.231466, loss_cps: 0.484927
[13:45:47.678] iteration 19910: total_loss: 0.526771, loss_sup: 0.113504, loss_mps: 0.142970, loss_cps: 0.270297
[13:45:47.824] iteration 19911: total_loss: 0.362004, loss_sup: 0.030834, loss_mps: 0.114611, loss_cps: 0.216559
[13:45:47.971] iteration 19912: total_loss: 0.388599, loss_sup: 0.108438, loss_mps: 0.102818, loss_cps: 0.177343
[13:45:48.117] iteration 19913: total_loss: 0.692373, loss_sup: 0.082675, loss_mps: 0.193727, loss_cps: 0.415970
[13:45:48.262] iteration 19914: total_loss: 0.929416, loss_sup: 0.051685, loss_mps: 0.270325, loss_cps: 0.607406
[13:45:48.408] iteration 19915: total_loss: 0.837899, loss_sup: 0.071274, loss_mps: 0.232661, loss_cps: 0.533964
[13:45:48.555] iteration 19916: total_loss: 0.654677, loss_sup: 0.116329, loss_mps: 0.180232, loss_cps: 0.358116
[13:45:48.701] iteration 19917: total_loss: 0.430569, loss_sup: 0.021750, loss_mps: 0.138460, loss_cps: 0.270360
[13:45:48.848] iteration 19918: total_loss: 0.430930, loss_sup: 0.081966, loss_mps: 0.120047, loss_cps: 0.228917
[13:45:48.993] iteration 19919: total_loss: 0.885193, loss_sup: 0.177518, loss_mps: 0.224843, loss_cps: 0.482832
[13:45:49.139] iteration 19920: total_loss: 0.301359, loss_sup: 0.036608, loss_mps: 0.099443, loss_cps: 0.165309
[13:45:49.285] iteration 19921: total_loss: 0.592743, loss_sup: 0.138015, loss_mps: 0.162637, loss_cps: 0.292091
[13:45:49.431] iteration 19922: total_loss: 1.363157, loss_sup: 0.404440, loss_mps: 0.310346, loss_cps: 0.648372
[13:45:49.578] iteration 19923: total_loss: 0.731135, loss_sup: 0.231988, loss_mps: 0.171393, loss_cps: 0.327754
[13:45:49.724] iteration 19924: total_loss: 0.553174, loss_sup: 0.044061, loss_mps: 0.172001, loss_cps: 0.337112
[13:45:49.870] iteration 19925: total_loss: 0.522349, loss_sup: 0.020127, loss_mps: 0.170683, loss_cps: 0.331539
[13:45:50.019] iteration 19926: total_loss: 0.664623, loss_sup: 0.051841, loss_mps: 0.203936, loss_cps: 0.408847
[13:45:50.166] iteration 19927: total_loss: 0.897033, loss_sup: 0.177134, loss_mps: 0.224611, loss_cps: 0.495288
[13:45:50.311] iteration 19928: total_loss: 0.517097, loss_sup: 0.095179, loss_mps: 0.146829, loss_cps: 0.275089
[13:45:50.458] iteration 19929: total_loss: 0.597558, loss_sup: 0.144224, loss_mps: 0.162925, loss_cps: 0.290408
[13:45:50.604] iteration 19930: total_loss: 0.464394, loss_sup: 0.039236, loss_mps: 0.144209, loss_cps: 0.280949
[13:45:50.750] iteration 19931: total_loss: 0.719921, loss_sup: 0.192004, loss_mps: 0.178409, loss_cps: 0.349508
[13:45:50.897] iteration 19932: total_loss: 0.508402, loss_sup: 0.057212, loss_mps: 0.157065, loss_cps: 0.294124
[13:45:51.042] iteration 19933: total_loss: 0.672297, loss_sup: 0.059433, loss_mps: 0.208138, loss_cps: 0.404726
[13:45:51.188] iteration 19934: total_loss: 0.464539, loss_sup: 0.027060, loss_mps: 0.151899, loss_cps: 0.285580
[13:45:51.333] iteration 19935: total_loss: 0.383398, loss_sup: 0.020352, loss_mps: 0.129318, loss_cps: 0.233728
[13:45:51.478] iteration 19936: total_loss: 0.530474, loss_sup: 0.015910, loss_mps: 0.164442, loss_cps: 0.350122
[13:45:51.624] iteration 19937: total_loss: 0.643386, loss_sup: 0.055818, loss_mps: 0.203120, loss_cps: 0.384449
[13:45:51.770] iteration 19938: total_loss: 0.570741, loss_sup: 0.050678, loss_mps: 0.173854, loss_cps: 0.346209
[13:45:51.917] iteration 19939: total_loss: 0.981900, loss_sup: 0.250770, loss_mps: 0.265905, loss_cps: 0.465226
[13:45:52.063] iteration 19940: total_loss: 1.018519, loss_sup: 0.410603, loss_mps: 0.209102, loss_cps: 0.398814
[13:45:52.209] iteration 19941: total_loss: 0.630050, loss_sup: 0.187955, loss_mps: 0.163397, loss_cps: 0.278697
[13:45:52.355] iteration 19942: total_loss: 0.994525, loss_sup: 0.214680, loss_mps: 0.255847, loss_cps: 0.523998
[13:45:52.501] iteration 19943: total_loss: 1.110105, loss_sup: 0.252215, loss_mps: 0.270135, loss_cps: 0.587755
[13:45:52.647] iteration 19944: total_loss: 0.605870, loss_sup: 0.046064, loss_mps: 0.194454, loss_cps: 0.365353
[13:45:52.794] iteration 19945: total_loss: 0.448862, loss_sup: 0.050518, loss_mps: 0.149984, loss_cps: 0.248360
[13:45:52.941] iteration 19946: total_loss: 0.609414, loss_sup: 0.061585, loss_mps: 0.190526, loss_cps: 0.357303
[13:45:53.089] iteration 19947: total_loss: 0.688573, loss_sup: 0.154176, loss_mps: 0.185858, loss_cps: 0.348539
[13:45:53.235] iteration 19948: total_loss: 0.630464, loss_sup: 0.036884, loss_mps: 0.190661, loss_cps: 0.402919
[13:45:53.381] iteration 19949: total_loss: 0.531375, loss_sup: 0.055150, loss_mps: 0.165651, loss_cps: 0.310574
[13:45:53.527] iteration 19950: total_loss: 0.479509, loss_sup: 0.077305, loss_mps: 0.145591, loss_cps: 0.256613
[13:45:53.679] iteration 19951: total_loss: 0.561865, loss_sup: 0.084998, loss_mps: 0.169660, loss_cps: 0.307207
[13:45:53.825] iteration 19952: total_loss: 0.993716, loss_sup: 0.123895, loss_mps: 0.278574, loss_cps: 0.591247
[13:45:53.971] iteration 19953: total_loss: 0.387383, loss_sup: 0.057259, loss_mps: 0.126318, loss_cps: 0.203806
[13:45:54.118] iteration 19954: total_loss: 0.819558, loss_sup: 0.376608, loss_mps: 0.161033, loss_cps: 0.281917
[13:45:54.265] iteration 19955: total_loss: 0.714630, loss_sup: 0.152059, loss_mps: 0.185245, loss_cps: 0.377326
[13:45:54.410] iteration 19956: total_loss: 0.569161, loss_sup: 0.041095, loss_mps: 0.168698, loss_cps: 0.359368
[13:45:54.556] iteration 19957: total_loss: 0.500949, loss_sup: 0.061466, loss_mps: 0.159812, loss_cps: 0.279670
[13:45:54.704] iteration 19958: total_loss: 0.637059, loss_sup: 0.146960, loss_mps: 0.178454, loss_cps: 0.311645
[13:45:54.850] iteration 19959: total_loss: 0.513911, loss_sup: 0.172402, loss_mps: 0.123832, loss_cps: 0.217677
[13:45:54.997] iteration 19960: total_loss: 0.660503, loss_sup: 0.079062, loss_mps: 0.198326, loss_cps: 0.383115
[13:45:55.143] iteration 19961: total_loss: 0.587487, loss_sup: 0.128255, loss_mps: 0.160460, loss_cps: 0.298772
[13:45:55.289] iteration 19962: total_loss: 0.692473, loss_sup: 0.133740, loss_mps: 0.194509, loss_cps: 0.364224
[13:45:55.436] iteration 19963: total_loss: 0.459616, loss_sup: 0.048854, loss_mps: 0.149221, loss_cps: 0.261540
[13:45:55.582] iteration 19964: total_loss: 0.740736, loss_sup: 0.071773, loss_mps: 0.231275, loss_cps: 0.437688
[13:45:55.729] iteration 19965: total_loss: 0.396674, loss_sup: 0.016275, loss_mps: 0.131682, loss_cps: 0.248716
[13:45:55.878] iteration 19966: total_loss: 0.410701, loss_sup: 0.198648, loss_mps: 0.084409, loss_cps: 0.127644
[13:45:56.024] iteration 19967: total_loss: 0.419628, loss_sup: 0.110447, loss_mps: 0.118132, loss_cps: 0.191049
[13:45:56.169] iteration 19968: total_loss: 0.410061, loss_sup: 0.084888, loss_mps: 0.120609, loss_cps: 0.204564
[13:45:56.316] iteration 19969: total_loss: 0.574334, loss_sup: 0.107497, loss_mps: 0.161612, loss_cps: 0.305225
[13:45:56.463] iteration 19970: total_loss: 0.533007, loss_sup: 0.023873, loss_mps: 0.169089, loss_cps: 0.340045
[13:45:56.611] iteration 19971: total_loss: 0.360078, loss_sup: 0.063940, loss_mps: 0.108123, loss_cps: 0.188015
[13:45:56.758] iteration 19972: total_loss: 0.822013, loss_sup: 0.418201, loss_mps: 0.141853, loss_cps: 0.261959
[13:45:56.904] iteration 19973: total_loss: 0.884859, loss_sup: 0.112976, loss_mps: 0.252263, loss_cps: 0.519620
[13:45:57.054] iteration 19974: total_loss: 0.519251, loss_sup: 0.066193, loss_mps: 0.154418, loss_cps: 0.298639
[13:45:57.202] iteration 19975: total_loss: 0.392108, loss_sup: 0.029204, loss_mps: 0.122680, loss_cps: 0.240224
[13:45:57.348] iteration 19976: total_loss: 0.266107, loss_sup: 0.013510, loss_mps: 0.095798, loss_cps: 0.156799
[13:45:57.494] iteration 19977: total_loss: 0.847455, loss_sup: 0.272314, loss_mps: 0.204898, loss_cps: 0.370243
[13:45:57.641] iteration 19978: total_loss: 0.917517, loss_sup: 0.166201, loss_mps: 0.251541, loss_cps: 0.499774
[13:45:57.789] iteration 19979: total_loss: 0.451048, loss_sup: 0.140822, loss_mps: 0.111875, loss_cps: 0.198351
[13:45:57.935] iteration 19980: total_loss: 1.236820, loss_sup: 0.211118, loss_mps: 0.318375, loss_cps: 0.707327
[13:45:58.082] iteration 19981: total_loss: 0.454944, loss_sup: 0.034920, loss_mps: 0.148227, loss_cps: 0.271798
[13:45:58.229] iteration 19982: total_loss: 0.372900, loss_sup: 0.008556, loss_mps: 0.133566, loss_cps: 0.230778
[13:45:58.375] iteration 19983: total_loss: 0.860669, loss_sup: 0.055396, loss_mps: 0.262921, loss_cps: 0.542352
[13:45:58.521] iteration 19984: total_loss: 0.645893, loss_sup: 0.085116, loss_mps: 0.187988, loss_cps: 0.372789
[13:45:58.667] iteration 19985: total_loss: 0.778301, loss_sup: 0.111439, loss_mps: 0.229419, loss_cps: 0.437442
[13:45:58.815] iteration 19986: total_loss: 0.383556, loss_sup: 0.002911, loss_mps: 0.133999, loss_cps: 0.246646
[13:45:58.963] iteration 19987: total_loss: 0.453308, loss_sup: 0.076590, loss_mps: 0.131566, loss_cps: 0.245152
[13:45:59.109] iteration 19988: total_loss: 0.437057, loss_sup: 0.118334, loss_mps: 0.121283, loss_cps: 0.197440
[13:45:59.256] iteration 19989: total_loss: 0.342142, loss_sup: 0.042180, loss_mps: 0.107463, loss_cps: 0.192499
[13:45:59.402] iteration 19990: total_loss: 0.711092, loss_sup: 0.154599, loss_mps: 0.187546, loss_cps: 0.368947
[13:45:59.548] iteration 19991: total_loss: 0.651846, loss_sup: 0.262858, loss_mps: 0.142825, loss_cps: 0.246163
[13:45:59.694] iteration 19992: total_loss: 0.330747, loss_sup: 0.016182, loss_mps: 0.113589, loss_cps: 0.200976
[13:45:59.841] iteration 19993: total_loss: 0.462578, loss_sup: 0.116816, loss_mps: 0.125237, loss_cps: 0.220526
[13:45:59.988] iteration 19994: total_loss: 0.648329, loss_sup: 0.219107, loss_mps: 0.138116, loss_cps: 0.291106
[13:46:00.135] iteration 19995: total_loss: 0.379620, loss_sup: 0.160428, loss_mps: 0.084183, loss_cps: 0.135009
[13:46:00.281] iteration 19996: total_loss: 0.456732, loss_sup: 0.076412, loss_mps: 0.137891, loss_cps: 0.242429
[13:46:00.427] iteration 19997: total_loss: 0.754842, loss_sup: 0.062752, loss_mps: 0.223259, loss_cps: 0.468830
[13:46:00.574] iteration 19998: total_loss: 0.549476, loss_sup: 0.033827, loss_mps: 0.178859, loss_cps: 0.336790
[13:46:00.720] iteration 19999: total_loss: 0.521523, loss_sup: 0.040083, loss_mps: 0.162751, loss_cps: 0.318688
[13:46:00.867] iteration 20000: total_loss: 0.224910, loss_sup: 0.005427, loss_mps: 0.079299, loss_cps: 0.140185
[13:46:00.867] Evaluation Started ==>
[13:46:12.190] ==> valid iteration 20000: unet metrics: {'dc': 0.6389066575748428, 'jc': 0.522386756097211, 'pre': 0.7790353755719631, 'hd': 5.492773184552975}, ynet metrics: {'dc': 0.6144562895966833, 'jc': 0.497405750990133, 'pre': 0.8057467775680249, 'hd': 5.46229456622179}.
[13:46:12.192] Evaluation Finished!⏹️
[13:46:12.343] iteration 20001: total_loss: 0.283162, loss_sup: 0.016877, loss_mps: 0.103617, loss_cps: 0.162669
[13:46:12.492] iteration 20002: total_loss: 0.438479, loss_sup: 0.132797, loss_mps: 0.108031, loss_cps: 0.197652
[13:46:12.640] iteration 20003: total_loss: 0.363997, loss_sup: 0.065206, loss_mps: 0.107556, loss_cps: 0.191235
[13:46:12.785] iteration 20004: total_loss: 0.363955, loss_sup: 0.068964, loss_mps: 0.106801, loss_cps: 0.188190
[13:46:12.931] iteration 20005: total_loss: 0.699599, loss_sup: 0.148796, loss_mps: 0.187640, loss_cps: 0.363163
[13:46:13.077] iteration 20006: total_loss: 0.488420, loss_sup: 0.037783, loss_mps: 0.153095, loss_cps: 0.297542
[13:46:13.224] iteration 20007: total_loss: 0.464145, loss_sup: 0.090493, loss_mps: 0.129978, loss_cps: 0.243674
[13:46:13.371] iteration 20008: total_loss: 0.454949, loss_sup: 0.033378, loss_mps: 0.141763, loss_cps: 0.279809
[13:46:13.517] iteration 20009: total_loss: 0.691683, loss_sup: 0.136175, loss_mps: 0.175967, loss_cps: 0.379541
[13:46:13.663] iteration 20010: total_loss: 0.914655, loss_sup: 0.328083, loss_mps: 0.202910, loss_cps: 0.383662
[13:46:13.809] iteration 20011: total_loss: 0.290436, loss_sup: 0.040593, loss_mps: 0.093332, loss_cps: 0.156511
[13:46:13.955] iteration 20012: total_loss: 0.566105, loss_sup: 0.108365, loss_mps: 0.161733, loss_cps: 0.296008
[13:46:14.100] iteration 20013: total_loss: 0.503710, loss_sup: 0.141440, loss_mps: 0.129817, loss_cps: 0.232453
[13:46:14.246] iteration 20014: total_loss: 0.418712, loss_sup: 0.026142, loss_mps: 0.131087, loss_cps: 0.261483
[13:46:14.392] iteration 20015: total_loss: 0.529113, loss_sup: 0.105149, loss_mps: 0.149422, loss_cps: 0.274543
[13:46:14.537] iteration 20016: total_loss: 0.447372, loss_sup: 0.050465, loss_mps: 0.136169, loss_cps: 0.260738
[13:46:14.683] iteration 20017: total_loss: 0.326992, loss_sup: 0.084388, loss_mps: 0.086384, loss_cps: 0.156220
[13:46:14.829] iteration 20018: total_loss: 0.371497, loss_sup: 0.025483, loss_mps: 0.123320, loss_cps: 0.222693
[13:46:14.977] iteration 20019: total_loss: 0.634013, loss_sup: 0.199384, loss_mps: 0.145648, loss_cps: 0.288982
[13:46:15.123] iteration 20020: total_loss: 0.651681, loss_sup: 0.038592, loss_mps: 0.204260, loss_cps: 0.408828
[13:46:15.270] iteration 20021: total_loss: 0.515423, loss_sup: 0.072684, loss_mps: 0.154188, loss_cps: 0.288551
[13:46:15.419] iteration 20022: total_loss: 0.401672, loss_sup: 0.101661, loss_mps: 0.107120, loss_cps: 0.192890
[13:46:15.564] iteration 20023: total_loss: 0.596338, loss_sup: 0.168914, loss_mps: 0.140488, loss_cps: 0.286936
[13:46:15.710] iteration 20024: total_loss: 0.916510, loss_sup: 0.116899, loss_mps: 0.279044, loss_cps: 0.520568
[13:46:15.856] iteration 20025: total_loss: 0.997838, loss_sup: 0.094167, loss_mps: 0.282749, loss_cps: 0.620922
[13:46:16.001] iteration 20026: total_loss: 0.441726, loss_sup: 0.102788, loss_mps: 0.118489, loss_cps: 0.220449
[13:46:16.152] iteration 20027: total_loss: 0.329107, loss_sup: 0.083268, loss_mps: 0.092511, loss_cps: 0.153327
[13:46:16.297] iteration 20028: total_loss: 0.618802, loss_sup: 0.050813, loss_mps: 0.179141, loss_cps: 0.388847
[13:46:16.444] iteration 20029: total_loss: 0.621238, loss_sup: 0.034748, loss_mps: 0.201094, loss_cps: 0.385396
[13:46:16.590] iteration 20030: total_loss: 0.556445, loss_sup: 0.115268, loss_mps: 0.147785, loss_cps: 0.293393
[13:46:16.735] iteration 20031: total_loss: 0.887905, loss_sup: 0.220304, loss_mps: 0.221167, loss_cps: 0.446434
[13:46:16.881] iteration 20032: total_loss: 0.789557, loss_sup: 0.041864, loss_mps: 0.226765, loss_cps: 0.520927
[13:46:17.027] iteration 20033: total_loss: 0.512681, loss_sup: 0.099940, loss_mps: 0.143640, loss_cps: 0.269101
[13:46:17.172] iteration 20034: total_loss: 0.367947, loss_sup: 0.036564, loss_mps: 0.119068, loss_cps: 0.212315
[13:46:17.318] iteration 20035: total_loss: 0.886120, loss_sup: 0.124137, loss_mps: 0.232919, loss_cps: 0.529064
[13:46:17.463] iteration 20036: total_loss: 0.533733, loss_sup: 0.160502, loss_mps: 0.139803, loss_cps: 0.233427
[13:46:17.609] iteration 20037: total_loss: 0.917676, loss_sup: 0.023058, loss_mps: 0.269195, loss_cps: 0.625423
[13:46:17.755] iteration 20038: total_loss: 0.642294, loss_sup: 0.057262, loss_mps: 0.192645, loss_cps: 0.392387
[13:46:17.900] iteration 20039: total_loss: 0.396098, loss_sup: 0.022234, loss_mps: 0.129494, loss_cps: 0.244371
[13:46:18.045] iteration 20040: total_loss: 1.193214, loss_sup: 0.202305, loss_mps: 0.302498, loss_cps: 0.688411
[13:46:18.191] iteration 20041: total_loss: 0.442173, loss_sup: 0.059401, loss_mps: 0.133801, loss_cps: 0.248971
[13:46:18.337] iteration 20042: total_loss: 0.433633, loss_sup: 0.037317, loss_mps: 0.140477, loss_cps: 0.255839
[13:46:18.482] iteration 20043: total_loss: 0.464173, loss_sup: 0.076863, loss_mps: 0.143927, loss_cps: 0.243384
[13:46:18.628] iteration 20044: total_loss: 0.471832, loss_sup: 0.003993, loss_mps: 0.167688, loss_cps: 0.300150
[13:46:18.774] iteration 20045: total_loss: 0.527500, loss_sup: 0.187189, loss_mps: 0.125668, loss_cps: 0.214643
[13:46:18.919] iteration 20046: total_loss: 1.025533, loss_sup: 0.191391, loss_mps: 0.280918, loss_cps: 0.553224
[13:46:19.065] iteration 20047: total_loss: 0.658091, loss_sup: 0.067860, loss_mps: 0.191396, loss_cps: 0.398834
[13:46:19.210] iteration 20048: total_loss: 0.477384, loss_sup: 0.085429, loss_mps: 0.139715, loss_cps: 0.252240
[13:46:19.356] iteration 20049: total_loss: 0.741323, loss_sup: 0.090237, loss_mps: 0.218442, loss_cps: 0.432644
[13:46:19.502] iteration 20050: total_loss: 0.501607, loss_sup: 0.090215, loss_mps: 0.138474, loss_cps: 0.272917
[13:46:19.648] iteration 20051: total_loss: 0.521158, loss_sup: 0.076453, loss_mps: 0.159822, loss_cps: 0.284883
[13:46:19.793] iteration 20052: total_loss: 0.684948, loss_sup: 0.167264, loss_mps: 0.180030, loss_cps: 0.337654
[13:46:19.938] iteration 20053: total_loss: 0.414032, loss_sup: 0.037819, loss_mps: 0.135604, loss_cps: 0.240608
[13:46:20.084] iteration 20054: total_loss: 0.555833, loss_sup: 0.028834, loss_mps: 0.174910, loss_cps: 0.352088
[13:46:20.229] iteration 20055: total_loss: 0.529722, loss_sup: 0.089153, loss_mps: 0.150894, loss_cps: 0.289676
[13:46:20.375] iteration 20056: total_loss: 0.547627, loss_sup: 0.038841, loss_mps: 0.172733, loss_cps: 0.336053
[13:46:20.521] iteration 20057: total_loss: 0.480144, loss_sup: 0.040443, loss_mps: 0.142803, loss_cps: 0.296898
[13:46:20.667] iteration 20058: total_loss: 0.669157, loss_sup: 0.083619, loss_mps: 0.202214, loss_cps: 0.383324
[13:46:20.812] iteration 20059: total_loss: 0.483512, loss_sup: 0.068630, loss_mps: 0.143467, loss_cps: 0.271415
[13:46:20.957] iteration 20060: total_loss: 0.478055, loss_sup: 0.083927, loss_mps: 0.136787, loss_cps: 0.257341
[13:46:21.103] iteration 20061: total_loss: 0.506608, loss_sup: 0.032061, loss_mps: 0.158378, loss_cps: 0.316169
[13:46:21.250] iteration 20062: total_loss: 0.903127, loss_sup: 0.067789, loss_mps: 0.275147, loss_cps: 0.560191
[13:46:21.396] iteration 20063: total_loss: 0.812800, loss_sup: 0.085764, loss_mps: 0.233925, loss_cps: 0.493112
[13:46:21.459] iteration 20064: total_loss: 0.537959, loss_sup: 0.006881, loss_mps: 0.181580, loss_cps: 0.349498
[13:46:22.696] iteration 20065: total_loss: 0.467391, loss_sup: 0.143566, loss_mps: 0.121774, loss_cps: 0.202051
[13:46:22.848] iteration 20066: total_loss: 0.381609, loss_sup: 0.044948, loss_mps: 0.119738, loss_cps: 0.216923
[13:46:22.999] iteration 20067: total_loss: 0.590384, loss_sup: 0.037545, loss_mps: 0.192444, loss_cps: 0.360395
[13:46:23.146] iteration 20068: total_loss: 0.667539, loss_sup: 0.053933, loss_mps: 0.207140, loss_cps: 0.406466
[13:46:23.291] iteration 20069: total_loss: 0.449971, loss_sup: 0.144413, loss_mps: 0.105693, loss_cps: 0.199865
[13:46:23.437] iteration 20070: total_loss: 0.352291, loss_sup: 0.042471, loss_mps: 0.114227, loss_cps: 0.195594
[13:46:23.584] iteration 20071: total_loss: 0.323424, loss_sup: 0.029727, loss_mps: 0.109717, loss_cps: 0.183981
[13:46:23.733] iteration 20072: total_loss: 0.988558, loss_sup: 0.149126, loss_mps: 0.271071, loss_cps: 0.568362
[13:46:23.879] iteration 20073: total_loss: 0.427246, loss_sup: 0.043941, loss_mps: 0.135540, loss_cps: 0.247765
[13:46:24.025] iteration 20074: total_loss: 0.550118, loss_sup: 0.050112, loss_mps: 0.169547, loss_cps: 0.330459
[13:46:24.171] iteration 20075: total_loss: 0.399054, loss_sup: 0.031155, loss_mps: 0.128684, loss_cps: 0.239215
[13:46:24.318] iteration 20076: total_loss: 0.474591, loss_sup: 0.046187, loss_mps: 0.144815, loss_cps: 0.283589
[13:46:24.465] iteration 20077: total_loss: 0.883327, loss_sup: 0.204540, loss_mps: 0.216547, loss_cps: 0.462239
[13:46:24.611] iteration 20078: total_loss: 0.423769, loss_sup: 0.040841, loss_mps: 0.140026, loss_cps: 0.242902
[13:46:24.756] iteration 20079: total_loss: 0.839961, loss_sup: 0.127800, loss_mps: 0.230155, loss_cps: 0.482007
[13:46:24.902] iteration 20080: total_loss: 0.902321, loss_sup: 0.067751, loss_mps: 0.270660, loss_cps: 0.563910
[13:46:25.048] iteration 20081: total_loss: 0.867005, loss_sup: 0.205090, loss_mps: 0.215885, loss_cps: 0.446030
[13:46:25.195] iteration 20082: total_loss: 0.412394, loss_sup: 0.139141, loss_mps: 0.093646, loss_cps: 0.179607
[13:46:25.340] iteration 20083: total_loss: 0.465695, loss_sup: 0.088659, loss_mps: 0.130164, loss_cps: 0.246872
[13:46:25.488] iteration 20084: total_loss: 0.539996, loss_sup: 0.148152, loss_mps: 0.134819, loss_cps: 0.257025
[13:46:25.634] iteration 20085: total_loss: 0.522702, loss_sup: 0.082940, loss_mps: 0.148450, loss_cps: 0.291312
[13:46:25.780] iteration 20086: total_loss: 0.444072, loss_sup: 0.056113, loss_mps: 0.129289, loss_cps: 0.258670
[13:46:25.926] iteration 20087: total_loss: 0.417640, loss_sup: 0.011211, loss_mps: 0.139071, loss_cps: 0.267358
[13:46:26.072] iteration 20088: total_loss: 0.554217, loss_sup: 0.084682, loss_mps: 0.169768, loss_cps: 0.299766
[13:46:26.219] iteration 20089: total_loss: 0.635974, loss_sup: 0.060292, loss_mps: 0.190535, loss_cps: 0.385147
[13:46:26.365] iteration 20090: total_loss: 0.499058, loss_sup: 0.021277, loss_mps: 0.163113, loss_cps: 0.314667
[13:46:26.514] iteration 20091: total_loss: 0.596470, loss_sup: 0.199021, loss_mps: 0.143478, loss_cps: 0.253971
[13:46:26.660] iteration 20092: total_loss: 0.552318, loss_sup: 0.111633, loss_mps: 0.153908, loss_cps: 0.286777
[13:46:26.807] iteration 20093: total_loss: 0.663805, loss_sup: 0.129857, loss_mps: 0.175671, loss_cps: 0.358277
[13:46:26.953] iteration 20094: total_loss: 0.715081, loss_sup: 0.074706, loss_mps: 0.204351, loss_cps: 0.436024
[13:46:27.099] iteration 20095: total_loss: 0.423913, loss_sup: 0.096210, loss_mps: 0.117592, loss_cps: 0.210110
[13:46:27.246] iteration 20096: total_loss: 0.288630, loss_sup: 0.014681, loss_mps: 0.102883, loss_cps: 0.171067
[13:46:27.393] iteration 20097: total_loss: 0.512149, loss_sup: 0.017546, loss_mps: 0.167105, loss_cps: 0.327498
[13:46:27.539] iteration 20098: total_loss: 0.335886, loss_sup: 0.044462, loss_mps: 0.112379, loss_cps: 0.179045
[13:46:27.686] iteration 20099: total_loss: 0.759293, loss_sup: 0.052316, loss_mps: 0.238015, loss_cps: 0.468962
[13:46:27.832] iteration 20100: total_loss: 0.954223, loss_sup: 0.155211, loss_mps: 0.256482, loss_cps: 0.542530
[13:46:27.832] Evaluation Started ==>
[13:46:39.280] ==> valid iteration 20100: unet metrics: {'dc': 0.6751535413737975, 'jc': 0.5603673836114429, 'pre': 0.793368633663285, 'hd': 5.43441809115943}, ynet metrics: {'dc': 0.6162746303090002, 'jc': 0.500366242049761, 'pre': 0.8015503371888041, 'hd': 5.58687324216196}.
[13:46:39.282] Evaluation Finished!⏹️
[13:46:39.433] iteration 20101: total_loss: 0.910230, loss_sup: 0.312677, loss_mps: 0.213281, loss_cps: 0.384272
[13:46:39.580] iteration 20102: total_loss: 0.392553, loss_sup: 0.070950, loss_mps: 0.114089, loss_cps: 0.207514
[13:46:39.725] iteration 20103: total_loss: 0.635222, loss_sup: 0.032374, loss_mps: 0.210385, loss_cps: 0.392463
[13:46:39.871] iteration 20104: total_loss: 0.586105, loss_sup: 0.037890, loss_mps: 0.178176, loss_cps: 0.370040
[13:46:40.016] iteration 20105: total_loss: 1.336961, loss_sup: 0.257193, loss_mps: 0.335240, loss_cps: 0.744528
[13:46:40.161] iteration 20106: total_loss: 0.501280, loss_sup: 0.061125, loss_mps: 0.158928, loss_cps: 0.281227
[13:46:40.307] iteration 20107: total_loss: 0.401330, loss_sup: 0.071255, loss_mps: 0.114868, loss_cps: 0.215207
[13:46:40.453] iteration 20108: total_loss: 0.536045, loss_sup: 0.116542, loss_mps: 0.150957, loss_cps: 0.268546
[13:46:40.599] iteration 20109: total_loss: 0.711419, loss_sup: 0.070268, loss_mps: 0.205290, loss_cps: 0.435861
[13:46:40.747] iteration 20110: total_loss: 0.345941, loss_sup: 0.046982, loss_mps: 0.106296, loss_cps: 0.192664
[13:46:40.893] iteration 20111: total_loss: 0.716084, loss_sup: 0.070321, loss_mps: 0.228606, loss_cps: 0.417158
[13:46:41.039] iteration 20112: total_loss: 0.641481, loss_sup: 0.051990, loss_mps: 0.208017, loss_cps: 0.381474
[13:46:41.185] iteration 20113: total_loss: 0.895669, loss_sup: 0.072407, loss_mps: 0.260961, loss_cps: 0.562302
[13:46:41.332] iteration 20114: total_loss: 0.644263, loss_sup: 0.132098, loss_mps: 0.170505, loss_cps: 0.341661
[13:46:41.478] iteration 20115: total_loss: 0.786645, loss_sup: 0.168672, loss_mps: 0.212032, loss_cps: 0.405941
[13:46:41.625] iteration 20116: total_loss: 0.621726, loss_sup: 0.069082, loss_mps: 0.176144, loss_cps: 0.376500
[13:46:41.774] iteration 20117: total_loss: 0.328204, loss_sup: 0.041754, loss_mps: 0.101792, loss_cps: 0.184658
[13:46:41.920] iteration 20118: total_loss: 0.640736, loss_sup: 0.100999, loss_mps: 0.185439, loss_cps: 0.354298
[13:46:42.067] iteration 20119: total_loss: 0.325057, loss_sup: 0.085779, loss_mps: 0.092890, loss_cps: 0.146389
[13:46:42.215] iteration 20120: total_loss: 0.354544, loss_sup: 0.018557, loss_mps: 0.119658, loss_cps: 0.216329
[13:46:42.362] iteration 20121: total_loss: 0.653131, loss_sup: 0.011678, loss_mps: 0.198033, loss_cps: 0.443420
[13:46:42.508] iteration 20122: total_loss: 0.516389, loss_sup: 0.036872, loss_mps: 0.158014, loss_cps: 0.321503
[13:46:42.654] iteration 20123: total_loss: 0.823659, loss_sup: 0.100854, loss_mps: 0.236610, loss_cps: 0.486196
[13:46:42.800] iteration 20124: total_loss: 0.549840, loss_sup: 0.100135, loss_mps: 0.153946, loss_cps: 0.295759
[13:46:42.947] iteration 20125: total_loss: 0.557018, loss_sup: 0.173476, loss_mps: 0.135899, loss_cps: 0.247644
[13:46:43.094] iteration 20126: total_loss: 0.552616, loss_sup: 0.105512, loss_mps: 0.148075, loss_cps: 0.299029
[13:46:43.241] iteration 20127: total_loss: 0.827355, loss_sup: 0.069190, loss_mps: 0.248187, loss_cps: 0.509979
[13:46:43.390] iteration 20128: total_loss: 0.652230, loss_sup: 0.136949, loss_mps: 0.167326, loss_cps: 0.347954
[13:46:43.536] iteration 20129: total_loss: 0.782270, loss_sup: 0.080122, loss_mps: 0.222157, loss_cps: 0.479991
[13:46:43.681] iteration 20130: total_loss: 0.722556, loss_sup: 0.098659, loss_mps: 0.197963, loss_cps: 0.425933
[13:46:43.828] iteration 20131: total_loss: 0.571646, loss_sup: 0.237165, loss_mps: 0.129766, loss_cps: 0.204715
[13:46:43.974] iteration 20132: total_loss: 0.832067, loss_sup: 0.014152, loss_mps: 0.252664, loss_cps: 0.565250
[13:46:44.120] iteration 20133: total_loss: 0.694005, loss_sup: 0.277906, loss_mps: 0.140422, loss_cps: 0.275677
[13:46:44.266] iteration 20134: total_loss: 0.463502, loss_sup: 0.034924, loss_mps: 0.153532, loss_cps: 0.275045
[13:46:44.412] iteration 20135: total_loss: 0.331340, loss_sup: 0.069118, loss_mps: 0.097952, loss_cps: 0.164270
[13:46:44.561] iteration 20136: total_loss: 0.439524, loss_sup: 0.054890, loss_mps: 0.135733, loss_cps: 0.248900
[13:46:44.707] iteration 20137: total_loss: 0.549838, loss_sup: 0.060804, loss_mps: 0.170614, loss_cps: 0.318420
[13:46:44.853] iteration 20138: total_loss: 0.502362, loss_sup: 0.184595, loss_mps: 0.118695, loss_cps: 0.199073
[13:46:45.001] iteration 20139: total_loss: 0.646158, loss_sup: 0.088402, loss_mps: 0.185738, loss_cps: 0.372018
[13:46:45.147] iteration 20140: total_loss: 0.946327, loss_sup: 0.583637, loss_mps: 0.133176, loss_cps: 0.229514
[13:46:45.293] iteration 20141: total_loss: 0.662859, loss_sup: 0.045710, loss_mps: 0.202863, loss_cps: 0.414286
[13:46:45.438] iteration 20142: total_loss: 0.759313, loss_sup: 0.077040, loss_mps: 0.217953, loss_cps: 0.464320
[13:46:45.585] iteration 20143: total_loss: 0.713174, loss_sup: 0.073662, loss_mps: 0.212022, loss_cps: 0.427489
[13:46:45.731] iteration 20144: total_loss: 0.783611, loss_sup: 0.318850, loss_mps: 0.163674, loss_cps: 0.301088
[13:46:45.876] iteration 20145: total_loss: 0.551212, loss_sup: 0.157642, loss_mps: 0.141216, loss_cps: 0.252354
[13:46:46.022] iteration 20146: total_loss: 0.440491, loss_sup: 0.083131, loss_mps: 0.134091, loss_cps: 0.223269
[13:46:46.169] iteration 20147: total_loss: 0.451125, loss_sup: 0.020559, loss_mps: 0.146395, loss_cps: 0.284172
[13:46:46.315] iteration 20148: total_loss: 0.666261, loss_sup: 0.187241, loss_mps: 0.167068, loss_cps: 0.311953
[13:46:46.462] iteration 20149: total_loss: 0.609053, loss_sup: 0.033810, loss_mps: 0.193149, loss_cps: 0.382094
[13:46:46.608] iteration 20150: total_loss: 0.581394, loss_sup: 0.196434, loss_mps: 0.144927, loss_cps: 0.240033
[13:46:46.754] iteration 20151: total_loss: 0.478964, loss_sup: 0.075330, loss_mps: 0.145082, loss_cps: 0.258552
[13:46:46.900] iteration 20152: total_loss: 0.839987, loss_sup: 0.122719, loss_mps: 0.238773, loss_cps: 0.478495
[13:46:47.051] iteration 20153: total_loss: 0.643429, loss_sup: 0.063563, loss_mps: 0.194391, loss_cps: 0.385475
[13:46:47.198] iteration 20154: total_loss: 0.410752, loss_sup: 0.083863, loss_mps: 0.129258, loss_cps: 0.197631
[13:46:47.345] iteration 20155: total_loss: 0.628438, loss_sup: 0.118336, loss_mps: 0.177216, loss_cps: 0.332886
[13:46:47.491] iteration 20156: total_loss: 0.608763, loss_sup: 0.124098, loss_mps: 0.170030, loss_cps: 0.314635
[13:46:47.638] iteration 20157: total_loss: 0.373832, loss_sup: 0.003190, loss_mps: 0.128229, loss_cps: 0.242412
[13:46:47.785] iteration 20158: total_loss: 0.712840, loss_sup: 0.038688, loss_mps: 0.218114, loss_cps: 0.456039
[13:46:47.931] iteration 20159: total_loss: 0.378428, loss_sup: 0.065555, loss_mps: 0.116018, loss_cps: 0.196855
[13:46:48.077] iteration 20160: total_loss: 0.365237, loss_sup: 0.048653, loss_mps: 0.120109, loss_cps: 0.196475
[13:46:48.224] iteration 20161: total_loss: 0.609140, loss_sup: 0.039785, loss_mps: 0.192072, loss_cps: 0.377283
[13:46:48.371] iteration 20162: total_loss: 0.566163, loss_sup: 0.056528, loss_mps: 0.182808, loss_cps: 0.326827
[13:46:48.517] iteration 20163: total_loss: 0.443173, loss_sup: 0.043078, loss_mps: 0.138230, loss_cps: 0.261865
[13:46:48.663] iteration 20164: total_loss: 0.879376, loss_sup: 0.184864, loss_mps: 0.233888, loss_cps: 0.460624
[13:46:48.810] iteration 20165: total_loss: 0.571359, loss_sup: 0.104916, loss_mps: 0.162074, loss_cps: 0.304370
[13:46:48.960] iteration 20166: total_loss: 0.482094, loss_sup: 0.077312, loss_mps: 0.144464, loss_cps: 0.260317
[13:46:49.107] iteration 20167: total_loss: 0.771809, loss_sup: 0.126233, loss_mps: 0.206595, loss_cps: 0.438981
[13:46:49.253] iteration 20168: total_loss: 0.531163, loss_sup: 0.121772, loss_mps: 0.137550, loss_cps: 0.271840
[13:46:49.401] iteration 20169: total_loss: 0.475818, loss_sup: 0.095078, loss_mps: 0.137177, loss_cps: 0.243563
[13:46:49.547] iteration 20170: total_loss: 1.228550, loss_sup: 0.327766, loss_mps: 0.312498, loss_cps: 0.588286
[13:46:49.694] iteration 20171: total_loss: 0.486457, loss_sup: 0.093721, loss_mps: 0.143690, loss_cps: 0.249046
[13:46:49.841] iteration 20172: total_loss: 0.397889, loss_sup: 0.044065, loss_mps: 0.131337, loss_cps: 0.222488
[13:46:49.992] iteration 20173: total_loss: 0.346722, loss_sup: 0.023553, loss_mps: 0.116147, loss_cps: 0.207022
[13:46:50.139] iteration 20174: total_loss: 0.712971, loss_sup: 0.102255, loss_mps: 0.202366, loss_cps: 0.408349
[13:46:50.285] iteration 20175: total_loss: 0.536971, loss_sup: 0.030568, loss_mps: 0.189113, loss_cps: 0.317290
[13:46:50.431] iteration 20176: total_loss: 0.689574, loss_sup: 0.276592, loss_mps: 0.144587, loss_cps: 0.268395
[13:46:50.578] iteration 20177: total_loss: 0.609102, loss_sup: 0.178158, loss_mps: 0.159314, loss_cps: 0.271630
[13:46:50.724] iteration 20178: total_loss: 0.499106, loss_sup: 0.133895, loss_mps: 0.128398, loss_cps: 0.236812
[13:46:50.870] iteration 20179: total_loss: 0.480264, loss_sup: 0.018420, loss_mps: 0.167334, loss_cps: 0.294510
[13:46:51.017] iteration 20180: total_loss: 0.242513, loss_sup: 0.008825, loss_mps: 0.093663, loss_cps: 0.140025
[13:46:51.163] iteration 20181: total_loss: 0.423665, loss_sup: 0.014921, loss_mps: 0.141742, loss_cps: 0.267001
[13:46:51.309] iteration 20182: total_loss: 0.420402, loss_sup: 0.044368, loss_mps: 0.130725, loss_cps: 0.245310
[13:46:51.455] iteration 20183: total_loss: 0.562911, loss_sup: 0.098272, loss_mps: 0.160652, loss_cps: 0.303988
[13:46:51.602] iteration 20184: total_loss: 0.423712, loss_sup: 0.059516, loss_mps: 0.132551, loss_cps: 0.231644
[13:46:51.748] iteration 20185: total_loss: 0.315521, loss_sup: 0.042351, loss_mps: 0.096546, loss_cps: 0.176624
[13:46:51.894] iteration 20186: total_loss: 0.319668, loss_sup: 0.007762, loss_mps: 0.114165, loss_cps: 0.197742
[13:46:52.041] iteration 20187: total_loss: 0.798780, loss_sup: 0.055229, loss_mps: 0.235861, loss_cps: 0.507690
[13:46:52.195] iteration 20188: total_loss: 0.634043, loss_sup: 0.035423, loss_mps: 0.190997, loss_cps: 0.407622
[13:46:52.343] iteration 20189: total_loss: 0.451728, loss_sup: 0.027124, loss_mps: 0.149839, loss_cps: 0.274765
[13:46:52.489] iteration 20190: total_loss: 0.661204, loss_sup: 0.212341, loss_mps: 0.157211, loss_cps: 0.291653
[13:46:52.639] iteration 20191: total_loss: 0.817038, loss_sup: 0.075803, loss_mps: 0.242596, loss_cps: 0.498639
[13:46:52.789] iteration 20192: total_loss: 0.933172, loss_sup: 0.053013, loss_mps: 0.286911, loss_cps: 0.593248
[13:46:52.935] iteration 20193: total_loss: 0.335988, loss_sup: 0.013877, loss_mps: 0.112215, loss_cps: 0.209896
[13:46:53.081] iteration 20194: total_loss: 0.661404, loss_sup: 0.274170, loss_mps: 0.133921, loss_cps: 0.253313
[13:46:53.232] iteration 20195: total_loss: 0.644247, loss_sup: 0.069265, loss_mps: 0.178726, loss_cps: 0.396255
[13:46:53.379] iteration 20196: total_loss: 0.361140, loss_sup: 0.011764, loss_mps: 0.121102, loss_cps: 0.228273
[13:46:53.526] iteration 20197: total_loss: 0.446548, loss_sup: 0.094250, loss_mps: 0.123054, loss_cps: 0.229245
[13:46:53.673] iteration 20198: total_loss: 0.302397, loss_sup: 0.013006, loss_mps: 0.106570, loss_cps: 0.182820
[13:46:53.820] iteration 20199: total_loss: 0.440689, loss_sup: 0.082978, loss_mps: 0.124196, loss_cps: 0.233514
[13:46:53.968] iteration 20200: total_loss: 0.960399, loss_sup: 0.177676, loss_mps: 0.238728, loss_cps: 0.543995
[13:46:53.968] Evaluation Started ==>
[13:47:05.282] ==> valid iteration 20200: unet metrics: {'dc': 0.6443726889701512, 'jc': 0.5288414352023997, 'pre': 0.7873370247125394, 'hd': 5.370276448927744}, ynet metrics: {'dc': 0.6030264091558468, 'jc': 0.4870105641012107, 'pre': 0.7907678346866357, 'hd': 5.557475859870002}.
[13:47:05.285] Evaluation Finished!⏹️
[13:47:05.437] iteration 20201: total_loss: 0.500223, loss_sup: 0.095876, loss_mps: 0.138077, loss_cps: 0.266269
[13:47:05.584] iteration 20202: total_loss: 0.811940, loss_sup: 0.133594, loss_mps: 0.214064, loss_cps: 0.464282
[13:47:05.729] iteration 20203: total_loss: 0.533054, loss_sup: 0.029336, loss_mps: 0.164540, loss_cps: 0.339178
[13:47:05.874] iteration 20204: total_loss: 0.344419, loss_sup: 0.040533, loss_mps: 0.108477, loss_cps: 0.195409
[13:47:06.022] iteration 20205: total_loss: 0.549067, loss_sup: 0.069547, loss_mps: 0.155715, loss_cps: 0.323805
[13:47:06.167] iteration 20206: total_loss: 0.876706, loss_sup: 0.235751, loss_mps: 0.211729, loss_cps: 0.429226
[13:47:06.312] iteration 20207: total_loss: 1.112437, loss_sup: 0.102241, loss_mps: 0.301343, loss_cps: 0.708853
[13:47:06.458] iteration 20208: total_loss: 0.262383, loss_sup: 0.031968, loss_mps: 0.084692, loss_cps: 0.145723
[13:47:06.603] iteration 20209: total_loss: 0.480671, loss_sup: 0.081037, loss_mps: 0.143905, loss_cps: 0.255729
[13:47:06.748] iteration 20210: total_loss: 0.780086, loss_sup: 0.163874, loss_mps: 0.201914, loss_cps: 0.414298
[13:47:06.895] iteration 20211: total_loss: 0.466205, loss_sup: 0.045678, loss_mps: 0.142374, loss_cps: 0.278153
[13:47:07.041] iteration 20212: total_loss: 0.655506, loss_sup: 0.079563, loss_mps: 0.187768, loss_cps: 0.388176
[13:47:07.186] iteration 20213: total_loss: 0.643686, loss_sup: 0.074428, loss_mps: 0.194045, loss_cps: 0.375213
[13:47:07.331] iteration 20214: total_loss: 0.479608, loss_sup: 0.061645, loss_mps: 0.149107, loss_cps: 0.268856
[13:47:07.478] iteration 20215: total_loss: 0.390914, loss_sup: 0.016522, loss_mps: 0.124067, loss_cps: 0.250325
[13:47:07.624] iteration 20216: total_loss: 0.970003, loss_sup: 0.071595, loss_mps: 0.274203, loss_cps: 0.624205
[13:47:07.771] iteration 20217: total_loss: 0.398107, loss_sup: 0.111479, loss_mps: 0.102680, loss_cps: 0.183948
[13:47:07.916] iteration 20218: total_loss: 0.400107, loss_sup: 0.003668, loss_mps: 0.137692, loss_cps: 0.258747
[13:47:08.062] iteration 20219: total_loss: 0.375614, loss_sup: 0.024332, loss_mps: 0.124130, loss_cps: 0.227152
[13:47:08.208] iteration 20220: total_loss: 0.407935, loss_sup: 0.022169, loss_mps: 0.133138, loss_cps: 0.252627
[13:47:08.354] iteration 20221: total_loss: 0.386036, loss_sup: 0.027898, loss_mps: 0.128565, loss_cps: 0.229573
[13:47:08.500] iteration 20222: total_loss: 0.649252, loss_sup: 0.043946, loss_mps: 0.208035, loss_cps: 0.397271
[13:47:08.645] iteration 20223: total_loss: 0.497113, loss_sup: 0.104741, loss_mps: 0.133575, loss_cps: 0.258798
[13:47:08.791] iteration 20224: total_loss: 0.333622, loss_sup: 0.097366, loss_mps: 0.087751, loss_cps: 0.148504
[13:47:08.937] iteration 20225: total_loss: 0.928810, loss_sup: 0.250827, loss_mps: 0.222999, loss_cps: 0.454985
[13:47:09.082] iteration 20226: total_loss: 0.428317, loss_sup: 0.055384, loss_mps: 0.134698, loss_cps: 0.238236
[13:47:09.228] iteration 20227: total_loss: 0.523008, loss_sup: 0.068570, loss_mps: 0.159083, loss_cps: 0.295355
[13:47:09.374] iteration 20228: total_loss: 0.322244, loss_sup: 0.019262, loss_mps: 0.113466, loss_cps: 0.189516
[13:47:09.521] iteration 20229: total_loss: 0.510209, loss_sup: 0.057259, loss_mps: 0.159232, loss_cps: 0.293718
[13:47:09.666] iteration 20230: total_loss: 0.564451, loss_sup: 0.085646, loss_mps: 0.172780, loss_cps: 0.306025
[13:47:09.813] iteration 20231: total_loss: 0.696172, loss_sup: 0.149103, loss_mps: 0.178065, loss_cps: 0.369004
[13:47:09.960] iteration 20232: total_loss: 0.501455, loss_sup: 0.069878, loss_mps: 0.148199, loss_cps: 0.283378
[13:47:10.105] iteration 20233: total_loss: 0.409794, loss_sup: 0.057659, loss_mps: 0.124474, loss_cps: 0.227661
[13:47:10.252] iteration 20234: total_loss: 0.365861, loss_sup: 0.061783, loss_mps: 0.104614, loss_cps: 0.199464
[13:47:10.398] iteration 20235: total_loss: 0.863026, loss_sup: 0.109467, loss_mps: 0.241995, loss_cps: 0.511564
[13:47:10.545] iteration 20236: total_loss: 0.479726, loss_sup: 0.006149, loss_mps: 0.159996, loss_cps: 0.313580
[13:47:10.693] iteration 20237: total_loss: 0.899099, loss_sup: 0.129758, loss_mps: 0.247868, loss_cps: 0.521472
[13:47:10.839] iteration 20238: total_loss: 0.240583, loss_sup: 0.025792, loss_mps: 0.079210, loss_cps: 0.135581
[13:47:10.984] iteration 20239: total_loss: 0.393507, loss_sup: 0.076777, loss_mps: 0.114979, loss_cps: 0.201752
[13:47:11.130] iteration 20240: total_loss: 0.491844, loss_sup: 0.123279, loss_mps: 0.130265, loss_cps: 0.238300
[13:47:11.276] iteration 20241: total_loss: 0.742203, loss_sup: 0.056157, loss_mps: 0.219246, loss_cps: 0.466799
[13:47:11.422] iteration 20242: total_loss: 0.836425, loss_sup: 0.075567, loss_mps: 0.250538, loss_cps: 0.510320
[13:47:11.570] iteration 20243: total_loss: 0.392685, loss_sup: 0.004456, loss_mps: 0.140639, loss_cps: 0.247590
[13:47:11.718] iteration 20244: total_loss: 0.364838, loss_sup: 0.001898, loss_mps: 0.125263, loss_cps: 0.237677
[13:47:11.866] iteration 20245: total_loss: 0.754987, loss_sup: 0.119193, loss_mps: 0.204708, loss_cps: 0.431086
[13:47:12.012] iteration 20246: total_loss: 0.364916, loss_sup: 0.037935, loss_mps: 0.114406, loss_cps: 0.212574
[13:47:12.158] iteration 20247: total_loss: 0.484718, loss_sup: 0.021648, loss_mps: 0.156347, loss_cps: 0.306723
[13:47:12.303] iteration 20248: total_loss: 0.414335, loss_sup: 0.015933, loss_mps: 0.136637, loss_cps: 0.261765
[13:47:12.449] iteration 20249: total_loss: 0.359475, loss_sup: 0.019633, loss_mps: 0.112586, loss_cps: 0.227256
[13:47:12.596] iteration 20250: total_loss: 0.691304, loss_sup: 0.048764, loss_mps: 0.202757, loss_cps: 0.439783
[13:47:12.742] iteration 20251: total_loss: 0.298617, loss_sup: 0.017097, loss_mps: 0.101203, loss_cps: 0.180317
[13:47:12.890] iteration 20252: total_loss: 0.497894, loss_sup: 0.056343, loss_mps: 0.144395, loss_cps: 0.297156
[13:47:13.036] iteration 20253: total_loss: 0.336837, loss_sup: 0.085371, loss_mps: 0.093385, loss_cps: 0.158082
[13:47:13.181] iteration 20254: total_loss: 0.377653, loss_sup: 0.044783, loss_mps: 0.116560, loss_cps: 0.216310
[13:47:13.327] iteration 20255: total_loss: 0.599085, loss_sup: 0.078188, loss_mps: 0.173581, loss_cps: 0.347315
[13:47:13.473] iteration 20256: total_loss: 0.504078, loss_sup: 0.075716, loss_mps: 0.140420, loss_cps: 0.287943
[13:47:13.619] iteration 20257: total_loss: 0.719478, loss_sup: 0.086620, loss_mps: 0.200218, loss_cps: 0.432640
[13:47:13.764] iteration 20258: total_loss: 0.343675, loss_sup: 0.029225, loss_mps: 0.104845, loss_cps: 0.209605
[13:47:13.913] iteration 20259: total_loss: 0.573382, loss_sup: 0.101319, loss_mps: 0.166112, loss_cps: 0.305951
[13:47:14.060] iteration 20260: total_loss: 0.484752, loss_sup: 0.052913, loss_mps: 0.144246, loss_cps: 0.287593
[13:47:14.207] iteration 20261: total_loss: 0.541977, loss_sup: 0.026746, loss_mps: 0.170661, loss_cps: 0.344570
[13:47:14.353] iteration 20262: total_loss: 0.464557, loss_sup: 0.043080, loss_mps: 0.139727, loss_cps: 0.281751
[13:47:14.501] iteration 20263: total_loss: 0.429055, loss_sup: 0.062159, loss_mps: 0.124772, loss_cps: 0.242125
[13:47:14.648] iteration 20264: total_loss: 0.431886, loss_sup: 0.119321, loss_mps: 0.111350, loss_cps: 0.201215
[13:47:14.794] iteration 20265: total_loss: 0.587083, loss_sup: 0.133887, loss_mps: 0.162144, loss_cps: 0.291053
[13:47:14.941] iteration 20266: total_loss: 0.629068, loss_sup: 0.100074, loss_mps: 0.174272, loss_cps: 0.354721
[13:47:15.087] iteration 20267: total_loss: 0.788559, loss_sup: 0.048920, loss_mps: 0.222553, loss_cps: 0.517086
[13:47:15.235] iteration 20268: total_loss: 0.588927, loss_sup: 0.082074, loss_mps: 0.164117, loss_cps: 0.342735
[13:47:15.382] iteration 20269: total_loss: 0.365560, loss_sup: 0.091634, loss_mps: 0.104048, loss_cps: 0.169877
[13:47:15.527] iteration 20270: total_loss: 0.289752, loss_sup: 0.037047, loss_mps: 0.091780, loss_cps: 0.160926
[13:47:15.674] iteration 20271: total_loss: 0.394709, loss_sup: 0.021699, loss_mps: 0.134601, loss_cps: 0.238408
[13:47:15.820] iteration 20272: total_loss: 0.419509, loss_sup: 0.029035, loss_mps: 0.131763, loss_cps: 0.258712
[13:47:15.967] iteration 20273: total_loss: 0.571317, loss_sup: 0.141197, loss_mps: 0.153861, loss_cps: 0.276259
[13:47:16.114] iteration 20274: total_loss: 0.497700, loss_sup: 0.056970, loss_mps: 0.143054, loss_cps: 0.297676
[13:47:16.260] iteration 20275: total_loss: 0.649655, loss_sup: 0.015997, loss_mps: 0.197695, loss_cps: 0.435962
[13:47:16.407] iteration 20276: total_loss: 0.560003, loss_sup: 0.031752, loss_mps: 0.171629, loss_cps: 0.356621
[13:47:16.553] iteration 20277: total_loss: 0.587834, loss_sup: 0.148985, loss_mps: 0.158053, loss_cps: 0.280796
[13:47:16.700] iteration 20278: total_loss: 0.524942, loss_sup: 0.027002, loss_mps: 0.166079, loss_cps: 0.331861
[13:47:16.847] iteration 20279: total_loss: 0.808160, loss_sup: 0.059797, loss_mps: 0.246366, loss_cps: 0.501997
[13:47:16.993] iteration 20280: total_loss: 0.377079, loss_sup: 0.087769, loss_mps: 0.104971, loss_cps: 0.184339
[13:47:17.140] iteration 20281: total_loss: 0.446145, loss_sup: 0.045210, loss_mps: 0.139825, loss_cps: 0.261110
[13:47:17.286] iteration 20282: total_loss: 0.575673, loss_sup: 0.117482, loss_mps: 0.155444, loss_cps: 0.302747
[13:47:17.432] iteration 20283: total_loss: 0.446587, loss_sup: 0.046833, loss_mps: 0.140591, loss_cps: 0.259163
[13:47:17.579] iteration 20284: total_loss: 0.358125, loss_sup: 0.039773, loss_mps: 0.117363, loss_cps: 0.200989
[13:47:17.728] iteration 20285: total_loss: 0.901804, loss_sup: 0.169240, loss_mps: 0.247824, loss_cps: 0.484740
[13:47:17.875] iteration 20286: total_loss: 0.719902, loss_sup: 0.066553, loss_mps: 0.208219, loss_cps: 0.445130
[13:47:18.023] iteration 20287: total_loss: 0.505331, loss_sup: 0.112826, loss_mps: 0.138220, loss_cps: 0.254286
[13:47:18.169] iteration 20288: total_loss: 0.418421, loss_sup: 0.040082, loss_mps: 0.129603, loss_cps: 0.248737
[13:47:18.316] iteration 20289: total_loss: 0.624139, loss_sup: 0.133980, loss_mps: 0.157525, loss_cps: 0.332633
[13:47:18.463] iteration 20290: total_loss: 0.349495, loss_sup: 0.032900, loss_mps: 0.119211, loss_cps: 0.197384
[13:47:18.610] iteration 20291: total_loss: 0.840420, loss_sup: 0.073788, loss_mps: 0.250175, loss_cps: 0.516457
[13:47:18.758] iteration 20292: total_loss: 0.533612, loss_sup: 0.019848, loss_mps: 0.179831, loss_cps: 0.333933
[13:47:18.908] iteration 20293: total_loss: 0.799232, loss_sup: 0.007746, loss_mps: 0.250740, loss_cps: 0.540746
[13:47:19.055] iteration 20294: total_loss: 0.654569, loss_sup: 0.098171, loss_mps: 0.178562, loss_cps: 0.377836
[13:47:19.204] iteration 20295: total_loss: 0.332591, loss_sup: 0.021609, loss_mps: 0.104074, loss_cps: 0.206908
[13:47:19.353] iteration 20296: total_loss: 0.397279, loss_sup: 0.027742, loss_mps: 0.122458, loss_cps: 0.247080
[13:47:19.500] iteration 20297: total_loss: 0.909828, loss_sup: 0.100352, loss_mps: 0.255825, loss_cps: 0.553652
[13:47:19.648] iteration 20298: total_loss: 0.631022, loss_sup: 0.141845, loss_mps: 0.157183, loss_cps: 0.331994
[13:47:19.795] iteration 20299: total_loss: 0.596058, loss_sup: 0.125398, loss_mps: 0.155887, loss_cps: 0.314773
[13:47:19.941] iteration 20300: total_loss: 0.462847, loss_sup: 0.034600, loss_mps: 0.144395, loss_cps: 0.283852
[13:47:19.941] Evaluation Started ==>
[13:47:31.264] ==> valid iteration 20300: unet metrics: {'dc': 0.6692762440721981, 'jc': 0.552274254012443, 'pre': 0.8296212454413171, 'hd': 5.308238765577364}, ynet metrics: {'dc': 0.5911850939489864, 'jc': 0.47869858429747836, 'pre': 0.8074405936483556, 'hd': 5.540450351919369}.
[13:47:31.266] Evaluation Finished!⏹️
[13:47:31.419] iteration 20301: total_loss: 0.443012, loss_sup: 0.022415, loss_mps: 0.142857, loss_cps: 0.277739
[13:47:31.567] iteration 20302: total_loss: 0.912176, loss_sup: 0.125612, loss_mps: 0.247184, loss_cps: 0.539380
[13:47:31.713] iteration 20303: total_loss: 0.870039, loss_sup: 0.305769, loss_mps: 0.183189, loss_cps: 0.381081
[13:47:31.858] iteration 20304: total_loss: 0.388473, loss_sup: 0.045687, loss_mps: 0.128877, loss_cps: 0.213909
[13:47:32.003] iteration 20305: total_loss: 0.501834, loss_sup: 0.124375, loss_mps: 0.140847, loss_cps: 0.236611
[13:47:32.148] iteration 20306: total_loss: 0.465854, loss_sup: 0.009987, loss_mps: 0.149277, loss_cps: 0.306590
[13:47:32.293] iteration 20307: total_loss: 0.483973, loss_sup: 0.104228, loss_mps: 0.130560, loss_cps: 0.249186
[13:47:32.441] iteration 20308: total_loss: 0.935490, loss_sup: 0.188896, loss_mps: 0.249112, loss_cps: 0.497482
[13:47:32.589] iteration 20309: total_loss: 0.437764, loss_sup: 0.066809, loss_mps: 0.129749, loss_cps: 0.241207
[13:47:32.734] iteration 20310: total_loss: 0.420774, loss_sup: 0.014962, loss_mps: 0.131726, loss_cps: 0.274086
[13:47:32.879] iteration 20311: total_loss: 0.479349, loss_sup: 0.017890, loss_mps: 0.152969, loss_cps: 0.308489
[13:47:33.026] iteration 20312: total_loss: 0.877204, loss_sup: 0.056543, loss_mps: 0.248620, loss_cps: 0.572041
[13:47:33.172] iteration 20313: total_loss: 0.715101, loss_sup: 0.094412, loss_mps: 0.211300, loss_cps: 0.409388
[13:47:33.317] iteration 20314: total_loss: 0.502044, loss_sup: 0.055507, loss_mps: 0.162261, loss_cps: 0.284276
[13:47:33.462] iteration 20315: total_loss: 0.529837, loss_sup: 0.026460, loss_mps: 0.176148, loss_cps: 0.327229
[13:47:33.609] iteration 20316: total_loss: 0.313251, loss_sup: 0.012731, loss_mps: 0.107729, loss_cps: 0.192791
[13:47:33.759] iteration 20317: total_loss: 0.458216, loss_sup: 0.047671, loss_mps: 0.132994, loss_cps: 0.277552
[13:47:33.905] iteration 20318: total_loss: 0.244799, loss_sup: 0.006751, loss_mps: 0.094331, loss_cps: 0.143717
[13:47:34.052] iteration 20319: total_loss: 0.313417, loss_sup: 0.041113, loss_mps: 0.098930, loss_cps: 0.173374
[13:47:34.199] iteration 20320: total_loss: 0.387928, loss_sup: 0.007746, loss_mps: 0.132323, loss_cps: 0.247859
[13:47:34.345] iteration 20321: total_loss: 0.824632, loss_sup: 0.210673, loss_mps: 0.204465, loss_cps: 0.409493
[13:47:34.492] iteration 20322: total_loss: 0.624419, loss_sup: 0.119085, loss_mps: 0.168113, loss_cps: 0.337220
[13:47:34.637] iteration 20323: total_loss: 0.467840, loss_sup: 0.061784, loss_mps: 0.129338, loss_cps: 0.276718
[13:47:34.783] iteration 20324: total_loss: 0.690535, loss_sup: 0.061229, loss_mps: 0.207439, loss_cps: 0.421867
[13:47:34.929] iteration 20325: total_loss: 0.509829, loss_sup: 0.061118, loss_mps: 0.147110, loss_cps: 0.301601
[13:47:35.076] iteration 20326: total_loss: 1.054361, loss_sup: 0.171154, loss_mps: 0.292735, loss_cps: 0.590472
[13:47:35.221] iteration 20327: total_loss: 0.448073, loss_sup: 0.022163, loss_mps: 0.146689, loss_cps: 0.279221
[13:47:35.367] iteration 20328: total_loss: 0.516253, loss_sup: 0.136573, loss_mps: 0.134711, loss_cps: 0.244969
[13:47:35.512] iteration 20329: total_loss: 0.344981, loss_sup: 0.031728, loss_mps: 0.113960, loss_cps: 0.199293
[13:47:35.657] iteration 20330: total_loss: 0.870389, loss_sup: 0.208100, loss_mps: 0.205595, loss_cps: 0.456694
[13:47:35.803] iteration 20331: total_loss: 0.755714, loss_sup: 0.079720, loss_mps: 0.216837, loss_cps: 0.459157
[13:47:35.948] iteration 20332: total_loss: 1.037504, loss_sup: 0.199346, loss_mps: 0.264849, loss_cps: 0.573309
[13:47:36.096] iteration 20333: total_loss: 0.362281, loss_sup: 0.031676, loss_mps: 0.112791, loss_cps: 0.217815
[13:47:36.241] iteration 20334: total_loss: 0.489947, loss_sup: 0.112219, loss_mps: 0.135902, loss_cps: 0.241825
[13:47:36.387] iteration 20335: total_loss: 0.795303, loss_sup: 0.019880, loss_mps: 0.242182, loss_cps: 0.533240
[13:47:36.532] iteration 20336: total_loss: 0.341028, loss_sup: 0.008157, loss_mps: 0.119886, loss_cps: 0.212986
[13:47:36.677] iteration 20337: total_loss: 0.509125, loss_sup: 0.115264, loss_mps: 0.137840, loss_cps: 0.256021
[13:47:36.823] iteration 20338: total_loss: 0.479614, loss_sup: 0.064712, loss_mps: 0.146291, loss_cps: 0.268611
[13:47:36.969] iteration 20339: total_loss: 0.585912, loss_sup: 0.061586, loss_mps: 0.172688, loss_cps: 0.351639
[13:47:37.117] iteration 20340: total_loss: 0.601560, loss_sup: 0.033335, loss_mps: 0.199646, loss_cps: 0.368579
[13:47:37.262] iteration 20341: total_loss: 0.573833, loss_sup: 0.122749, loss_mps: 0.152407, loss_cps: 0.298677
[13:47:37.408] iteration 20342: total_loss: 0.466865, loss_sup: 0.126034, loss_mps: 0.119355, loss_cps: 0.221476
[13:47:37.553] iteration 20343: total_loss: 0.590577, loss_sup: 0.016299, loss_mps: 0.189726, loss_cps: 0.384551
[13:47:37.699] iteration 20344: total_loss: 0.421086, loss_sup: 0.043312, loss_mps: 0.123596, loss_cps: 0.254178
[13:47:37.848] iteration 20345: total_loss: 0.372407, loss_sup: 0.056935, loss_mps: 0.114273, loss_cps: 0.201199
[13:47:37.996] iteration 20346: total_loss: 0.972774, loss_sup: 0.079863, loss_mps: 0.280773, loss_cps: 0.612138
[13:47:38.142] iteration 20347: total_loss: 0.444629, loss_sup: 0.089186, loss_mps: 0.131038, loss_cps: 0.224405
[13:47:38.289] iteration 20348: total_loss: 0.460285, loss_sup: 0.019843, loss_mps: 0.151031, loss_cps: 0.289411
[13:47:38.436] iteration 20349: total_loss: 0.515777, loss_sup: 0.076563, loss_mps: 0.147275, loss_cps: 0.291939
[13:47:38.583] iteration 20350: total_loss: 0.539175, loss_sup: 0.036684, loss_mps: 0.170359, loss_cps: 0.332132
[13:47:38.730] iteration 20351: total_loss: 0.616108, loss_sup: 0.085136, loss_mps: 0.169696, loss_cps: 0.361276
[13:47:38.876] iteration 20352: total_loss: 0.498324, loss_sup: 0.147375, loss_mps: 0.125298, loss_cps: 0.225652
[13:47:39.023] iteration 20353: total_loss: 0.580027, loss_sup: 0.115993, loss_mps: 0.152868, loss_cps: 0.311166
[13:47:39.169] iteration 20354: total_loss: 0.463488, loss_sup: 0.056991, loss_mps: 0.139305, loss_cps: 0.267192
[13:47:39.317] iteration 20355: total_loss: 0.414131, loss_sup: 0.105476, loss_mps: 0.108331, loss_cps: 0.200324
[13:47:39.464] iteration 20356: total_loss: 0.669880, loss_sup: 0.081792, loss_mps: 0.190877, loss_cps: 0.397210
[13:47:39.610] iteration 20357: total_loss: 0.870487, loss_sup: 0.056860, loss_mps: 0.256231, loss_cps: 0.557397
[13:47:39.757] iteration 20358: total_loss: 0.662747, loss_sup: 0.106933, loss_mps: 0.183551, loss_cps: 0.372264
[13:47:39.905] iteration 20359: total_loss: 0.343579, loss_sup: 0.116934, loss_mps: 0.089540, loss_cps: 0.137104
[13:47:40.051] iteration 20360: total_loss: 0.448445, loss_sup: 0.026862, loss_mps: 0.143265, loss_cps: 0.278317
[13:47:40.197] iteration 20361: total_loss: 0.969229, loss_sup: 0.038351, loss_mps: 0.286045, loss_cps: 0.644834
[13:47:40.344] iteration 20362: total_loss: 0.441642, loss_sup: 0.042311, loss_mps: 0.142416, loss_cps: 0.256915
[13:47:40.489] iteration 20363: total_loss: 0.418384, loss_sup: 0.010646, loss_mps: 0.141102, loss_cps: 0.266635
[13:47:40.635] iteration 20364: total_loss: 1.026034, loss_sup: 0.214437, loss_mps: 0.254706, loss_cps: 0.556890
[13:47:40.783] iteration 20365: total_loss: 1.136779, loss_sup: 0.416533, loss_mps: 0.251985, loss_cps: 0.468261
[13:47:40.929] iteration 20366: total_loss: 0.299658, loss_sup: 0.042867, loss_mps: 0.094148, loss_cps: 0.162644
[13:47:41.077] iteration 20367: total_loss: 0.429777, loss_sup: 0.018726, loss_mps: 0.144155, loss_cps: 0.266896
[13:47:41.227] iteration 20368: total_loss: 0.341731, loss_sup: 0.031852, loss_mps: 0.106799, loss_cps: 0.203079
[13:47:41.374] iteration 20369: total_loss: 0.744502, loss_sup: 0.106501, loss_mps: 0.206679, loss_cps: 0.431322
[13:47:41.521] iteration 20370: total_loss: 0.441647, loss_sup: 0.030245, loss_mps: 0.142767, loss_cps: 0.268635
[13:47:41.670] iteration 20371: total_loss: 2.127240, loss_sup: 0.166196, loss_mps: 0.597452, loss_cps: 1.363591
[13:47:41.816] iteration 20372: total_loss: 0.568938, loss_sup: 0.139984, loss_mps: 0.151413, loss_cps: 0.277542
[13:47:41.964] iteration 20373: total_loss: 0.657863, loss_sup: 0.132284, loss_mps: 0.174257, loss_cps: 0.351323
[13:47:42.111] iteration 20374: total_loss: 0.272391, loss_sup: 0.009448, loss_mps: 0.094600, loss_cps: 0.168344
[13:47:42.256] iteration 20375: total_loss: 0.611585, loss_sup: 0.021370, loss_mps: 0.195801, loss_cps: 0.394414
[13:47:42.403] iteration 20376: total_loss: 0.404573, loss_sup: 0.064775, loss_mps: 0.126532, loss_cps: 0.213266
[13:47:42.550] iteration 20377: total_loss: 0.564881, loss_sup: 0.083821, loss_mps: 0.163991, loss_cps: 0.317069
[13:47:42.697] iteration 20378: total_loss: 0.484822, loss_sup: 0.023905, loss_mps: 0.164247, loss_cps: 0.296670
[13:47:42.846] iteration 20379: total_loss: 0.763828, loss_sup: 0.108988, loss_mps: 0.229959, loss_cps: 0.424881
[13:47:42.992] iteration 20380: total_loss: 0.517747, loss_sup: 0.016138, loss_mps: 0.165088, loss_cps: 0.336520
[13:47:43.138] iteration 20381: total_loss: 0.329341, loss_sup: 0.082863, loss_mps: 0.095240, loss_cps: 0.151238
[13:47:43.284] iteration 20382: total_loss: 0.443619, loss_sup: 0.114729, loss_mps: 0.123572, loss_cps: 0.205318
[13:47:43.431] iteration 20383: total_loss: 0.373295, loss_sup: 0.026826, loss_mps: 0.129161, loss_cps: 0.217308
[13:47:43.576] iteration 20384: total_loss: 0.585488, loss_sup: 0.149522, loss_mps: 0.151200, loss_cps: 0.284766
[13:47:43.723] iteration 20385: total_loss: 0.383812, loss_sup: 0.056538, loss_mps: 0.117190, loss_cps: 0.210084
[13:47:43.874] iteration 20386: total_loss: 0.598495, loss_sup: 0.167276, loss_mps: 0.141292, loss_cps: 0.289927
[13:47:44.023] iteration 20387: total_loss: 0.666642, loss_sup: 0.082807, loss_mps: 0.194129, loss_cps: 0.389706
[13:47:44.169] iteration 20388: total_loss: 0.430293, loss_sup: 0.048770, loss_mps: 0.130157, loss_cps: 0.251367
[13:47:44.315] iteration 20389: total_loss: 0.869726, loss_sup: 0.355578, loss_mps: 0.177972, loss_cps: 0.336175
[13:47:44.461] iteration 20390: total_loss: 0.595569, loss_sup: 0.087707, loss_mps: 0.173516, loss_cps: 0.334346
[13:47:44.608] iteration 20391: total_loss: 0.661472, loss_sup: 0.185860, loss_mps: 0.169344, loss_cps: 0.306268
[13:47:44.758] iteration 20392: total_loss: 0.406384, loss_sup: 0.012169, loss_mps: 0.133128, loss_cps: 0.261087
[13:47:44.905] iteration 20393: total_loss: 1.203018, loss_sup: 0.315905, loss_mps: 0.280046, loss_cps: 0.607067
[13:47:45.050] iteration 20394: total_loss: 0.858656, loss_sup: 0.213104, loss_mps: 0.208574, loss_cps: 0.436977
[13:47:45.196] iteration 20395: total_loss: 0.351278, loss_sup: 0.040985, loss_mps: 0.107156, loss_cps: 0.203137
[13:47:45.342] iteration 20396: total_loss: 0.430254, loss_sup: 0.018407, loss_mps: 0.145311, loss_cps: 0.266535
[13:47:45.488] iteration 20397: total_loss: 0.756026, loss_sup: 0.130199, loss_mps: 0.204893, loss_cps: 0.420934
[13:47:45.636] iteration 20398: total_loss: 0.531423, loss_sup: 0.080681, loss_mps: 0.159092, loss_cps: 0.291650
[13:47:45.782] iteration 20399: total_loss: 0.873459, loss_sup: 0.136622, loss_mps: 0.246590, loss_cps: 0.490247
[13:47:45.928] iteration 20400: total_loss: 0.627803, loss_sup: 0.079100, loss_mps: 0.173567, loss_cps: 0.375136
[13:47:45.929] Evaluation Started ==>
[13:47:57.293] ==> valid iteration 20400: unet metrics: {'dc': 0.6224466632109839, 'jc': 0.5063505275585395, 'pre': 0.8031592683216072, 'hd': 5.418613728905046}, ynet metrics: {'dc': 0.5799801158186302, 'jc': 0.46537766934631775, 'pre': 0.7863148773304526, 'hd': 5.697785700375746}.
[13:47:57.295] Evaluation Finished!⏹️
[13:47:57.448] iteration 20401: total_loss: 0.539599, loss_sup: 0.095565, loss_mps: 0.140521, loss_cps: 0.303513
[13:47:57.599] iteration 20402: total_loss: 0.668553, loss_sup: 0.026680, loss_mps: 0.211784, loss_cps: 0.430090
[13:47:57.745] iteration 20403: total_loss: 0.590412, loss_sup: 0.039921, loss_mps: 0.180327, loss_cps: 0.370164
[13:47:57.890] iteration 20404: total_loss: 0.388740, loss_sup: 0.071931, loss_mps: 0.118135, loss_cps: 0.198674
[13:47:58.035] iteration 20405: total_loss: 0.653443, loss_sup: 0.150814, loss_mps: 0.173484, loss_cps: 0.329145
[13:47:58.180] iteration 20406: total_loss: 0.362378, loss_sup: 0.018026, loss_mps: 0.120002, loss_cps: 0.224350
[13:47:58.325] iteration 20407: total_loss: 0.709869, loss_sup: 0.008274, loss_mps: 0.229234, loss_cps: 0.472361
[13:47:58.470] iteration 20408: total_loss: 0.361297, loss_sup: 0.016951, loss_mps: 0.121209, loss_cps: 0.223137
[13:47:58.615] iteration 20409: total_loss: 1.011735, loss_sup: 0.323408, loss_mps: 0.236963, loss_cps: 0.451365
[13:47:58.763] iteration 20410: total_loss: 0.872147, loss_sup: 0.252510, loss_mps: 0.211063, loss_cps: 0.408574
[13:47:58.913] iteration 20411: total_loss: 0.326314, loss_sup: 0.031955, loss_mps: 0.109641, loss_cps: 0.184718
[13:47:59.059] iteration 20412: total_loss: 0.675881, loss_sup: 0.313275, loss_mps: 0.137671, loss_cps: 0.224934
[13:47:59.205] iteration 20413: total_loss: 0.485180, loss_sup: 0.020471, loss_mps: 0.161666, loss_cps: 0.303044
[13:47:59.354] iteration 20414: total_loss: 0.286633, loss_sup: 0.004855, loss_mps: 0.106089, loss_cps: 0.175689
[13:47:59.502] iteration 20415: total_loss: 0.928914, loss_sup: 0.097658, loss_mps: 0.254001, loss_cps: 0.577255
[13:47:59.648] iteration 20416: total_loss: 0.408314, loss_sup: 0.053023, loss_mps: 0.132172, loss_cps: 0.223119
[13:47:59.797] iteration 20417: total_loss: 0.426060, loss_sup: 0.060553, loss_mps: 0.126274, loss_cps: 0.239233
[13:47:59.944] iteration 20418: total_loss: 0.710932, loss_sup: 0.212901, loss_mps: 0.171268, loss_cps: 0.326763
[13:48:00.093] iteration 20419: total_loss: 0.404343, loss_sup: 0.013061, loss_mps: 0.139149, loss_cps: 0.252133
[13:48:00.239] iteration 20420: total_loss: 0.623279, loss_sup: 0.048351, loss_mps: 0.186734, loss_cps: 0.388194
[13:48:00.385] iteration 20421: total_loss: 0.458859, loss_sup: 0.029522, loss_mps: 0.146620, loss_cps: 0.282716
[13:48:00.531] iteration 20422: total_loss: 0.374100, loss_sup: 0.097415, loss_mps: 0.103469, loss_cps: 0.173216
[13:48:00.677] iteration 20423: total_loss: 0.378028, loss_sup: 0.077340, loss_mps: 0.107152, loss_cps: 0.193535
[13:48:00.824] iteration 20424: total_loss: 0.491196, loss_sup: 0.084431, loss_mps: 0.143907, loss_cps: 0.262858
[13:48:00.970] iteration 20425: total_loss: 0.344373, loss_sup: 0.055892, loss_mps: 0.102852, loss_cps: 0.185629
[13:48:01.116] iteration 20426: total_loss: 0.519480, loss_sup: 0.048063, loss_mps: 0.169248, loss_cps: 0.302169
[13:48:01.262] iteration 20427: total_loss: 0.431676, loss_sup: 0.024057, loss_mps: 0.145028, loss_cps: 0.262591
[13:48:01.407] iteration 20428: total_loss: 0.299190, loss_sup: 0.030791, loss_mps: 0.105663, loss_cps: 0.162736
[13:48:01.553] iteration 20429: total_loss: 0.382717, loss_sup: 0.019449, loss_mps: 0.130950, loss_cps: 0.232318
[13:48:01.699] iteration 20430: total_loss: 0.643699, loss_sup: 0.057977, loss_mps: 0.193618, loss_cps: 0.392104
[13:48:01.846] iteration 20431: total_loss: 1.371297, loss_sup: 0.050769, loss_mps: 0.395340, loss_cps: 0.925187
[13:48:01.992] iteration 20432: total_loss: 0.539715, loss_sup: 0.124681, loss_mps: 0.146201, loss_cps: 0.268833
[13:48:02.138] iteration 20433: total_loss: 0.393310, loss_sup: 0.080107, loss_mps: 0.113103, loss_cps: 0.200100
[13:48:02.284] iteration 20434: total_loss: 0.488758, loss_sup: 0.151578, loss_mps: 0.124820, loss_cps: 0.212361
[13:48:02.430] iteration 20435: total_loss: 0.533217, loss_sup: 0.051822, loss_mps: 0.163747, loss_cps: 0.317648
[13:48:02.576] iteration 20436: total_loss: 0.416180, loss_sup: 0.035434, loss_mps: 0.130653, loss_cps: 0.250093
[13:48:02.723] iteration 20437: total_loss: 0.402496, loss_sup: 0.028808, loss_mps: 0.126105, loss_cps: 0.247583
[13:48:02.869] iteration 20438: total_loss: 0.602213, loss_sup: 0.152457, loss_mps: 0.149752, loss_cps: 0.300005
[13:48:03.016] iteration 20439: total_loss: 0.632090, loss_sup: 0.015922, loss_mps: 0.194191, loss_cps: 0.421976
[13:48:03.163] iteration 20440: total_loss: 0.388674, loss_sup: 0.028643, loss_mps: 0.123923, loss_cps: 0.236109
[13:48:03.312] iteration 20441: total_loss: 0.778805, loss_sup: 0.156542, loss_mps: 0.194762, loss_cps: 0.427502
[13:48:03.458] iteration 20442: total_loss: 0.433495, loss_sup: 0.028936, loss_mps: 0.142157, loss_cps: 0.262403
[13:48:03.604] iteration 20443: total_loss: 0.687177, loss_sup: 0.135282, loss_mps: 0.183949, loss_cps: 0.367946
[13:48:03.750] iteration 20444: total_loss: 0.486232, loss_sup: 0.069561, loss_mps: 0.142776, loss_cps: 0.273895
[13:48:03.897] iteration 20445: total_loss: 1.642961, loss_sup: 0.684350, loss_mps: 0.292230, loss_cps: 0.666381
[13:48:04.044] iteration 20446: total_loss: 0.785264, loss_sup: 0.381228, loss_mps: 0.143514, loss_cps: 0.260522
[13:48:04.189] iteration 20447: total_loss: 0.539151, loss_sup: 0.070094, loss_mps: 0.158700, loss_cps: 0.310358
[13:48:04.339] iteration 20448: total_loss: 0.272059, loss_sup: 0.007796, loss_mps: 0.094119, loss_cps: 0.170144
[13:48:04.485] iteration 20449: total_loss: 0.587179, loss_sup: 0.008487, loss_mps: 0.193225, loss_cps: 0.385467
[13:48:04.631] iteration 20450: total_loss: 0.433085, loss_sup: 0.035154, loss_mps: 0.137237, loss_cps: 0.260693
[13:48:04.778] iteration 20451: total_loss: 0.357981, loss_sup: 0.108440, loss_mps: 0.098523, loss_cps: 0.151017
[13:48:04.924] iteration 20452: total_loss: 0.471665, loss_sup: 0.110600, loss_mps: 0.130637, loss_cps: 0.230427
[13:48:05.070] iteration 20453: total_loss: 0.689060, loss_sup: 0.083770, loss_mps: 0.204982, loss_cps: 0.400308
[13:48:05.216] iteration 20454: total_loss: 0.596829, loss_sup: 0.184678, loss_mps: 0.152156, loss_cps: 0.259994
[13:48:05.363] iteration 20455: total_loss: 0.317130, loss_sup: 0.006636, loss_mps: 0.113538, loss_cps: 0.196956
[13:48:05.509] iteration 20456: total_loss: 0.365771, loss_sup: 0.030724, loss_mps: 0.119420, loss_cps: 0.215627
[13:48:05.655] iteration 20457: total_loss: 0.932262, loss_sup: 0.169456, loss_mps: 0.251315, loss_cps: 0.511491
[13:48:05.802] iteration 20458: total_loss: 0.559344, loss_sup: 0.036660, loss_mps: 0.175346, loss_cps: 0.347338
[13:48:05.947] iteration 20459: total_loss: 0.546848, loss_sup: 0.190581, loss_mps: 0.132771, loss_cps: 0.223496
[13:48:06.093] iteration 20460: total_loss: 0.528276, loss_sup: 0.126161, loss_mps: 0.142867, loss_cps: 0.259249
[13:48:06.239] iteration 20461: total_loss: 0.382270, loss_sup: 0.030107, loss_mps: 0.122755, loss_cps: 0.229408
[13:48:06.387] iteration 20462: total_loss: 0.550065, loss_sup: 0.075764, loss_mps: 0.162326, loss_cps: 0.311975
[13:48:06.532] iteration 20463: total_loss: 0.523954, loss_sup: 0.046860, loss_mps: 0.159659, loss_cps: 0.317436
[13:48:06.679] iteration 20464: total_loss: 0.356886, loss_sup: 0.007100, loss_mps: 0.121644, loss_cps: 0.228143
[13:48:06.825] iteration 20465: total_loss: 0.567090, loss_sup: 0.035713, loss_mps: 0.170956, loss_cps: 0.360421
[13:48:06.971] iteration 20466: total_loss: 0.356033, loss_sup: 0.031864, loss_mps: 0.122289, loss_cps: 0.201881
[13:48:07.117] iteration 20467: total_loss: 0.437313, loss_sup: 0.062821, loss_mps: 0.122993, loss_cps: 0.251499
[13:48:07.263] iteration 20468: total_loss: 0.341505, loss_sup: 0.007962, loss_mps: 0.118429, loss_cps: 0.215114
[13:48:07.408] iteration 20469: total_loss: 0.706772, loss_sup: 0.196162, loss_mps: 0.170770, loss_cps: 0.339840
[13:48:07.554] iteration 20470: total_loss: 0.515916, loss_sup: 0.048853, loss_mps: 0.148010, loss_cps: 0.319054
[13:48:07.701] iteration 20471: total_loss: 0.427070, loss_sup: 0.034184, loss_mps: 0.134060, loss_cps: 0.258826
[13:48:07.847] iteration 20472: total_loss: 0.577077, loss_sup: 0.019745, loss_mps: 0.190056, loss_cps: 0.367276
[13:48:07.993] iteration 20473: total_loss: 0.501593, loss_sup: 0.096435, loss_mps: 0.141584, loss_cps: 0.263573
[13:48:08.139] iteration 20474: total_loss: 0.311990, loss_sup: 0.053324, loss_mps: 0.099693, loss_cps: 0.158972
[13:48:08.284] iteration 20475: total_loss: 0.658778, loss_sup: 0.085979, loss_mps: 0.184579, loss_cps: 0.388220
[13:48:08.430] iteration 20476: total_loss: 0.282607, loss_sup: 0.042750, loss_mps: 0.087959, loss_cps: 0.151899
[13:48:08.576] iteration 20477: total_loss: 1.286917, loss_sup: 0.319702, loss_mps: 0.304689, loss_cps: 0.662526
[13:48:08.722] iteration 20478: total_loss: 0.831954, loss_sup: 0.151697, loss_mps: 0.239345, loss_cps: 0.440911
[13:48:08.867] iteration 20479: total_loss: 0.522843, loss_sup: 0.029488, loss_mps: 0.157550, loss_cps: 0.335805
[13:48:09.014] iteration 20480: total_loss: 0.529089, loss_sup: 0.184586, loss_mps: 0.129113, loss_cps: 0.215389
[13:48:09.159] iteration 20481: total_loss: 0.392791, loss_sup: 0.095841, loss_mps: 0.107906, loss_cps: 0.189044
[13:48:09.220] iteration 20482: total_loss: 0.511528, loss_sup: 0.024185, loss_mps: 0.157345, loss_cps: 0.329999
[13:48:10.612] iteration 20483: total_loss: 0.314011, loss_sup: 0.082402, loss_mps: 0.080968, loss_cps: 0.150641
[13:48:10.760] iteration 20484: total_loss: 0.539005, loss_sup: 0.069802, loss_mps: 0.159026, loss_cps: 0.310177
[13:48:10.907] iteration 20485: total_loss: 0.475487, loss_sup: 0.077443, loss_mps: 0.139555, loss_cps: 0.258490
[13:48:11.058] iteration 20486: total_loss: 0.366526, loss_sup: 0.024536, loss_mps: 0.112866, loss_cps: 0.229125
[13:48:11.204] iteration 20487: total_loss: 0.447836, loss_sup: 0.018569, loss_mps: 0.139295, loss_cps: 0.289972
[13:48:11.350] iteration 20488: total_loss: 0.320080, loss_sup: 0.014205, loss_mps: 0.106012, loss_cps: 0.199863
[13:48:11.496] iteration 20489: total_loss: 0.437234, loss_sup: 0.071925, loss_mps: 0.131461, loss_cps: 0.233848
[13:48:11.650] iteration 20490: total_loss: 0.532398, loss_sup: 0.057101, loss_mps: 0.156026, loss_cps: 0.319270
[13:48:11.798] iteration 20491: total_loss: 0.523334, loss_sup: 0.110438, loss_mps: 0.139327, loss_cps: 0.273568
[13:48:11.946] iteration 20492: total_loss: 0.365209, loss_sup: 0.011068, loss_mps: 0.126608, loss_cps: 0.227533
[13:48:12.094] iteration 20493: total_loss: 0.576482, loss_sup: 0.148353, loss_mps: 0.148689, loss_cps: 0.279440
[13:48:12.240] iteration 20494: total_loss: 0.614660, loss_sup: 0.147117, loss_mps: 0.157513, loss_cps: 0.310030
[13:48:12.386] iteration 20495: total_loss: 0.517345, loss_sup: 0.043881, loss_mps: 0.160024, loss_cps: 0.313441
[13:48:12.533] iteration 20496: total_loss: 0.411136, loss_sup: 0.021847, loss_mps: 0.140779, loss_cps: 0.248509
[13:48:12.679] iteration 20497: total_loss: 0.669193, loss_sup: 0.114206, loss_mps: 0.181195, loss_cps: 0.373793
[13:48:12.825] iteration 20498: total_loss: 0.546898, loss_sup: 0.087633, loss_mps: 0.157050, loss_cps: 0.302215
[13:48:12.971] iteration 20499: total_loss: 0.549114, loss_sup: 0.099343, loss_mps: 0.156123, loss_cps: 0.293648
[13:48:13.119] iteration 20500: total_loss: 0.365208, loss_sup: 0.019049, loss_mps: 0.115171, loss_cps: 0.230988
[13:48:13.119] Evaluation Started ==>
[13:48:24.451] ==> valid iteration 20500: unet metrics: {'dc': 0.6314312714437728, 'jc': 0.5160995311480511, 'pre': 0.8046769366478412, 'hd': 5.372976667236573}, ynet metrics: {'dc': 0.5827450399705281, 'jc': 0.47062131144585717, 'pre': 0.7903010380853726, 'hd': 5.698105699343658}.
[13:48:24.453] Evaluation Finished!⏹️
[13:48:24.606] iteration 20501: total_loss: 0.538440, loss_sup: 0.072812, loss_mps: 0.165449, loss_cps: 0.300179
[13:48:24.753] iteration 20502: total_loss: 1.059066, loss_sup: 0.545127, loss_mps: 0.177957, loss_cps: 0.335982
[13:48:24.898] iteration 20503: total_loss: 0.503438, loss_sup: 0.013718, loss_mps: 0.166970, loss_cps: 0.322750
[13:48:25.043] iteration 20504: total_loss: 0.340088, loss_sup: 0.008826, loss_mps: 0.114248, loss_cps: 0.217015
[13:48:25.188] iteration 20505: total_loss: 0.380626, loss_sup: 0.043171, loss_mps: 0.124394, loss_cps: 0.213060
[13:48:25.338] iteration 20506: total_loss: 0.639501, loss_sup: 0.082949, loss_mps: 0.182193, loss_cps: 0.374358
[13:48:25.483] iteration 20507: total_loss: 0.560790, loss_sup: 0.136901, loss_mps: 0.149040, loss_cps: 0.274849
[13:48:25.631] iteration 20508: total_loss: 0.299411, loss_sup: 0.017405, loss_mps: 0.102805, loss_cps: 0.179202
[13:48:25.777] iteration 20509: total_loss: 0.841471, loss_sup: 0.034457, loss_mps: 0.248437, loss_cps: 0.558578
[13:48:25.921] iteration 20510: total_loss: 0.388881, loss_sup: 0.077463, loss_mps: 0.110211, loss_cps: 0.201206
[13:48:26.067] iteration 20511: total_loss: 0.292220, loss_sup: 0.015393, loss_mps: 0.099548, loss_cps: 0.177278
[13:48:26.213] iteration 20512: total_loss: 0.624587, loss_sup: 0.150181, loss_mps: 0.154190, loss_cps: 0.320217
[13:48:26.359] iteration 20513: total_loss: 0.417396, loss_sup: 0.026064, loss_mps: 0.133581, loss_cps: 0.257750
[13:48:26.507] iteration 20514: total_loss: 0.733923, loss_sup: 0.073949, loss_mps: 0.201366, loss_cps: 0.458609
[13:48:26.654] iteration 20515: total_loss: 0.577678, loss_sup: 0.089267, loss_mps: 0.164071, loss_cps: 0.324339
[13:48:26.799] iteration 20516: total_loss: 0.455222, loss_sup: 0.052621, loss_mps: 0.144618, loss_cps: 0.257983
[13:48:26.946] iteration 20517: total_loss: 0.701709, loss_sup: 0.116109, loss_mps: 0.200392, loss_cps: 0.385208
[13:48:27.092] iteration 20518: total_loss: 1.442267, loss_sup: 0.303887, loss_mps: 0.356796, loss_cps: 0.781583
[13:48:27.237] iteration 20519: total_loss: 0.797607, loss_sup: 0.177162, loss_mps: 0.189913, loss_cps: 0.430532
[13:48:27.383] iteration 20520: total_loss: 1.101567, loss_sup: 0.178935, loss_mps: 0.293687, loss_cps: 0.628945
[13:48:27.529] iteration 20521: total_loss: 0.421741, loss_sup: 0.013930, loss_mps: 0.146340, loss_cps: 0.261471
[13:48:27.676] iteration 20522: total_loss: 0.715227, loss_sup: 0.107873, loss_mps: 0.195755, loss_cps: 0.411600
[13:48:27.821] iteration 20523: total_loss: 0.819164, loss_sup: 0.087295, loss_mps: 0.237873, loss_cps: 0.493997
[13:48:27.967] iteration 20524: total_loss: 1.359417, loss_sup: 0.152402, loss_mps: 0.379922, loss_cps: 0.827093
[13:48:28.113] iteration 20525: total_loss: 0.356891, loss_sup: 0.068918, loss_mps: 0.110227, loss_cps: 0.177746
[13:48:28.258] iteration 20526: total_loss: 1.392257, loss_sup: 0.044315, loss_mps: 0.400269, loss_cps: 0.947673
[13:48:28.405] iteration 20527: total_loss: 0.654376, loss_sup: 0.020703, loss_mps: 0.209762, loss_cps: 0.423911
[13:48:28.551] iteration 20528: total_loss: 0.483962, loss_sup: 0.062242, loss_mps: 0.149405, loss_cps: 0.272315
[13:48:28.699] iteration 20529: total_loss: 0.502572, loss_sup: 0.052613, loss_mps: 0.163817, loss_cps: 0.286143
[13:48:28.847] iteration 20530: total_loss: 0.299054, loss_sup: 0.018744, loss_mps: 0.106982, loss_cps: 0.173329
[13:48:28.994] iteration 20531: total_loss: 0.790991, loss_sup: 0.054370, loss_mps: 0.246963, loss_cps: 0.489657
[13:48:29.140] iteration 20532: total_loss: 0.510882, loss_sup: 0.056019, loss_mps: 0.157554, loss_cps: 0.297309
[13:48:29.286] iteration 20533: total_loss: 0.917118, loss_sup: 0.390894, loss_mps: 0.188018, loss_cps: 0.338207
[13:48:29.433] iteration 20534: total_loss: 0.488613, loss_sup: 0.010881, loss_mps: 0.160206, loss_cps: 0.317526
[13:48:29.580] iteration 20535: total_loss: 0.483535, loss_sup: 0.095332, loss_mps: 0.147237, loss_cps: 0.240966
[13:48:29.727] iteration 20536: total_loss: 0.654769, loss_sup: 0.134627, loss_mps: 0.187622, loss_cps: 0.332520
[13:48:29.874] iteration 20537: total_loss: 0.679970, loss_sup: 0.133996, loss_mps: 0.180339, loss_cps: 0.365634
[13:48:30.021] iteration 20538: total_loss: 0.591087, loss_sup: 0.012797, loss_mps: 0.194204, loss_cps: 0.384086
[13:48:30.172] iteration 20539: total_loss: 0.352435, loss_sup: 0.082531, loss_mps: 0.101610, loss_cps: 0.168294
[13:48:30.319] iteration 20540: total_loss: 0.906944, loss_sup: 0.137006, loss_mps: 0.254531, loss_cps: 0.515406
[13:48:30.466] iteration 20541: total_loss: 0.729266, loss_sup: 0.121947, loss_mps: 0.205680, loss_cps: 0.401639
[13:48:30.613] iteration 20542: total_loss: 0.385817, loss_sup: 0.031958, loss_mps: 0.125352, loss_cps: 0.228506
[13:48:30.760] iteration 20543: total_loss: 0.702779, loss_sup: 0.113892, loss_mps: 0.190924, loss_cps: 0.397963
[13:48:30.907] iteration 20544: total_loss: 1.115800, loss_sup: 0.049503, loss_mps: 0.323692, loss_cps: 0.742605
[13:48:31.055] iteration 20545: total_loss: 0.266784, loss_sup: 0.006628, loss_mps: 0.099165, loss_cps: 0.160991
[13:48:31.201] iteration 20546: total_loss: 0.450891, loss_sup: 0.102453, loss_mps: 0.125864, loss_cps: 0.222573
[13:48:31.349] iteration 20547: total_loss: 0.654396, loss_sup: 0.062422, loss_mps: 0.202230, loss_cps: 0.389744
[13:48:31.496] iteration 20548: total_loss: 0.562821, loss_sup: 0.102871, loss_mps: 0.172208, loss_cps: 0.287743
[13:48:31.642] iteration 20549: total_loss: 0.312658, loss_sup: 0.018453, loss_mps: 0.104438, loss_cps: 0.189768
[13:48:31.788] iteration 20550: total_loss: 0.552226, loss_sup: 0.076953, loss_mps: 0.163722, loss_cps: 0.311550
[13:48:31.934] iteration 20551: total_loss: 0.884141, loss_sup: 0.136863, loss_mps: 0.242278, loss_cps: 0.505000
[13:48:32.080] iteration 20552: total_loss: 0.355806, loss_sup: 0.018425, loss_mps: 0.123835, loss_cps: 0.213547
[13:48:32.226] iteration 20553: total_loss: 0.464314, loss_sup: 0.035417, loss_mps: 0.149439, loss_cps: 0.279458
[13:48:32.372] iteration 20554: total_loss: 0.912594, loss_sup: 0.157188, loss_mps: 0.237865, loss_cps: 0.517541
[13:48:32.518] iteration 20555: total_loss: 0.383106, loss_sup: 0.102326, loss_mps: 0.100748, loss_cps: 0.180031
[13:48:32.663] iteration 20556: total_loss: 0.636046, loss_sup: 0.150740, loss_mps: 0.155721, loss_cps: 0.329584
[13:48:32.809] iteration 20557: total_loss: 0.925341, loss_sup: 0.186096, loss_mps: 0.230981, loss_cps: 0.508264
[13:48:32.955] iteration 20558: total_loss: 0.433348, loss_sup: 0.027238, loss_mps: 0.141045, loss_cps: 0.265065
[13:48:33.101] iteration 20559: total_loss: 0.723372, loss_sup: 0.087871, loss_mps: 0.199930, loss_cps: 0.435571
[13:48:33.248] iteration 20560: total_loss: 0.468111, loss_sup: 0.041249, loss_mps: 0.151247, loss_cps: 0.275615
[13:48:33.394] iteration 20561: total_loss: 0.550383, loss_sup: 0.071348, loss_mps: 0.158862, loss_cps: 0.320173
[13:48:33.540] iteration 20562: total_loss: 0.846003, loss_sup: 0.044659, loss_mps: 0.271586, loss_cps: 0.529758
[13:48:33.685] iteration 20563: total_loss: 0.620474, loss_sup: 0.093669, loss_mps: 0.171961, loss_cps: 0.354843
[13:48:33.831] iteration 20564: total_loss: 0.804823, loss_sup: 0.123069, loss_mps: 0.229378, loss_cps: 0.452376
[13:48:33.977] iteration 20565: total_loss: 0.529689, loss_sup: 0.166049, loss_mps: 0.136311, loss_cps: 0.227329
[13:48:34.123] iteration 20566: total_loss: 0.427249, loss_sup: 0.027212, loss_mps: 0.141003, loss_cps: 0.259034
[13:48:34.269] iteration 20567: total_loss: 0.444097, loss_sup: 0.045064, loss_mps: 0.128624, loss_cps: 0.270409
[13:48:34.414] iteration 20568: total_loss: 0.993527, loss_sup: 0.175843, loss_mps: 0.256413, loss_cps: 0.561270
[13:48:34.560] iteration 20569: total_loss: 0.429566, loss_sup: 0.124880, loss_mps: 0.106522, loss_cps: 0.198164
[13:48:34.708] iteration 20570: total_loss: 0.308673, loss_sup: 0.013561, loss_mps: 0.105092, loss_cps: 0.190020
[13:48:34.854] iteration 20571: total_loss: 0.472306, loss_sup: 0.192559, loss_mps: 0.098955, loss_cps: 0.180792
[13:48:34.999] iteration 20572: total_loss: 0.562114, loss_sup: 0.057940, loss_mps: 0.172227, loss_cps: 0.331947
[13:48:35.147] iteration 20573: total_loss: 0.524265, loss_sup: 0.078539, loss_mps: 0.153625, loss_cps: 0.292101
[13:48:35.295] iteration 20574: total_loss: 0.749991, loss_sup: 0.170393, loss_mps: 0.190663, loss_cps: 0.388935
[13:48:35.441] iteration 20575: total_loss: 0.506893, loss_sup: 0.020230, loss_mps: 0.159985, loss_cps: 0.326678
[13:48:35.587] iteration 20576: total_loss: 0.716130, loss_sup: 0.056598, loss_mps: 0.219962, loss_cps: 0.439570
[13:48:35.733] iteration 20577: total_loss: 0.366755, loss_sup: 0.035984, loss_mps: 0.120732, loss_cps: 0.210040
[13:48:35.879] iteration 20578: total_loss: 0.421364, loss_sup: 0.113887, loss_mps: 0.124106, loss_cps: 0.183371
[13:48:36.025] iteration 20579: total_loss: 0.533528, loss_sup: 0.092631, loss_mps: 0.150124, loss_cps: 0.290773
[13:48:36.171] iteration 20580: total_loss: 0.490312, loss_sup: 0.032753, loss_mps: 0.156941, loss_cps: 0.300618
[13:48:36.316] iteration 20581: total_loss: 1.066024, loss_sup: 0.054629, loss_mps: 0.327178, loss_cps: 0.684217
[13:48:36.462] iteration 20582: total_loss: 0.554165, loss_sup: 0.009395, loss_mps: 0.181101, loss_cps: 0.363669
[13:48:36.609] iteration 20583: total_loss: 0.555477, loss_sup: 0.119742, loss_mps: 0.153118, loss_cps: 0.282616
[13:48:36.755] iteration 20584: total_loss: 0.577102, loss_sup: 0.148363, loss_mps: 0.145612, loss_cps: 0.283126
[13:48:36.901] iteration 20585: total_loss: 0.591373, loss_sup: 0.019079, loss_mps: 0.183968, loss_cps: 0.388326
[13:48:37.047] iteration 20586: total_loss: 1.199625, loss_sup: 0.249359, loss_mps: 0.297816, loss_cps: 0.652449
[13:48:37.193] iteration 20587: total_loss: 0.461486, loss_sup: 0.058287, loss_mps: 0.150747, loss_cps: 0.252452
[13:48:37.338] iteration 20588: total_loss: 0.420679, loss_sup: 0.018560, loss_mps: 0.130026, loss_cps: 0.272093
[13:48:37.484] iteration 20589: total_loss: 0.373704, loss_sup: 0.062197, loss_mps: 0.116606, loss_cps: 0.194902
[13:48:37.630] iteration 20590: total_loss: 0.733687, loss_sup: 0.061022, loss_mps: 0.211128, loss_cps: 0.461538
[13:48:37.777] iteration 20591: total_loss: 1.011507, loss_sup: 0.129917, loss_mps: 0.280369, loss_cps: 0.601222
[13:48:37.923] iteration 20592: total_loss: 0.823088, loss_sup: 0.065638, loss_mps: 0.235865, loss_cps: 0.521585
[13:48:38.068] iteration 20593: total_loss: 0.597883, loss_sup: 0.105641, loss_mps: 0.169289, loss_cps: 0.322954
[13:48:38.217] iteration 20594: total_loss: 0.448004, loss_sup: 0.041353, loss_mps: 0.139325, loss_cps: 0.267327
[13:48:38.363] iteration 20595: total_loss: 0.446792, loss_sup: 0.020619, loss_mps: 0.149212, loss_cps: 0.276960
[13:48:38.510] iteration 20596: total_loss: 0.650450, loss_sup: 0.043779, loss_mps: 0.203849, loss_cps: 0.402822
[13:48:38.657] iteration 20597: total_loss: 0.501154, loss_sup: 0.017500, loss_mps: 0.161310, loss_cps: 0.322344
[13:48:38.803] iteration 20598: total_loss: 0.743453, loss_sup: 0.069287, loss_mps: 0.221957, loss_cps: 0.452209
[13:48:38.950] iteration 20599: total_loss: 0.407269, loss_sup: 0.075403, loss_mps: 0.114492, loss_cps: 0.217375
[13:48:39.096] iteration 20600: total_loss: 0.994873, loss_sup: 0.162810, loss_mps: 0.267983, loss_cps: 0.564080
[13:48:39.096] Evaluation Started ==>
[13:48:50.450] ==> valid iteration 20600: unet metrics: {'dc': 0.643293907604136, 'jc': 0.5252962660006235, 'pre': 0.8119011764403601, 'hd': 5.427172227891485}, ynet metrics: {'dc': 0.5822705984073729, 'jc': 0.46723975164546233, 'pre': 0.7926211112695829, 'hd': 5.687669698931491}.
[13:48:50.453] Evaluation Finished!⏹️
[13:48:50.608] iteration 20601: total_loss: 0.478568, loss_sup: 0.122270, loss_mps: 0.123947, loss_cps: 0.232351
[13:48:50.759] iteration 20602: total_loss: 0.530302, loss_sup: 0.051240, loss_mps: 0.168716, loss_cps: 0.310345
[13:48:50.905] iteration 20603: total_loss: 0.377474, loss_sup: 0.021117, loss_mps: 0.127971, loss_cps: 0.228386
[13:48:51.050] iteration 20604: total_loss: 0.318588, loss_sup: 0.017318, loss_mps: 0.109561, loss_cps: 0.191709
[13:48:51.194] iteration 20605: total_loss: 0.783636, loss_sup: 0.402505, loss_mps: 0.132821, loss_cps: 0.248310
[13:48:51.339] iteration 20606: total_loss: 0.489350, loss_sup: 0.104512, loss_mps: 0.132014, loss_cps: 0.252824
[13:48:51.485] iteration 20607: total_loss: 0.837391, loss_sup: 0.093412, loss_mps: 0.235404, loss_cps: 0.508576
[13:48:51.630] iteration 20608: total_loss: 0.562095, loss_sup: 0.032714, loss_mps: 0.173434, loss_cps: 0.355947
[13:48:51.775] iteration 20609: total_loss: 1.118841, loss_sup: 0.158949, loss_mps: 0.290543, loss_cps: 0.669349
[13:48:51.921] iteration 20610: total_loss: 0.327150, loss_sup: 0.040098, loss_mps: 0.100726, loss_cps: 0.186325
[13:48:52.066] iteration 20611: total_loss: 0.702266, loss_sup: 0.248496, loss_mps: 0.152608, loss_cps: 0.301162
[13:48:52.214] iteration 20612: total_loss: 0.330945, loss_sup: 0.030894, loss_mps: 0.113122, loss_cps: 0.186929
[13:48:52.360] iteration 20613: total_loss: 0.413624, loss_sup: 0.008711, loss_mps: 0.142091, loss_cps: 0.262821
[13:48:52.507] iteration 20614: total_loss: 0.624208, loss_sup: 0.168140, loss_mps: 0.152411, loss_cps: 0.303657
[13:48:52.653] iteration 20615: total_loss: 0.854860, loss_sup: 0.104441, loss_mps: 0.243660, loss_cps: 0.506759
[13:48:52.798] iteration 20616: total_loss: 0.428899, loss_sup: 0.052940, loss_mps: 0.137279, loss_cps: 0.238680
[13:48:52.943] iteration 20617: total_loss: 0.448624, loss_sup: 0.067598, loss_mps: 0.138690, loss_cps: 0.242336
[13:48:53.090] iteration 20618: total_loss: 0.647663, loss_sup: 0.132712, loss_mps: 0.172531, loss_cps: 0.342420
[13:48:53.236] iteration 20619: total_loss: 0.461804, loss_sup: 0.112404, loss_mps: 0.125552, loss_cps: 0.223849
[13:48:53.381] iteration 20620: total_loss: 0.596070, loss_sup: 0.074384, loss_mps: 0.175469, loss_cps: 0.346218
[13:48:53.526] iteration 20621: total_loss: 0.449717, loss_sup: 0.062601, loss_mps: 0.141124, loss_cps: 0.245993
[13:48:53.674] iteration 20622: total_loss: 0.529128, loss_sup: 0.245895, loss_mps: 0.108794, loss_cps: 0.174439
[13:48:53.821] iteration 20623: total_loss: 0.411916, loss_sup: 0.091593, loss_mps: 0.125106, loss_cps: 0.195218
[13:48:53.967] iteration 20624: total_loss: 0.441141, loss_sup: 0.125497, loss_mps: 0.115940, loss_cps: 0.199704
[13:48:54.112] iteration 20625: total_loss: 0.399274, loss_sup: 0.156196, loss_mps: 0.093750, loss_cps: 0.149328
[13:48:54.258] iteration 20626: total_loss: 0.790457, loss_sup: 0.083693, loss_mps: 0.232370, loss_cps: 0.474394
[13:48:54.404] iteration 20627: total_loss: 0.366861, loss_sup: 0.072508, loss_mps: 0.110918, loss_cps: 0.183435
[13:48:54.552] iteration 20628: total_loss: 0.432303, loss_sup: 0.014402, loss_mps: 0.143317, loss_cps: 0.274584
[13:48:54.698] iteration 20629: total_loss: 0.996037, loss_sup: 0.218306, loss_mps: 0.268296, loss_cps: 0.509435
[13:48:54.845] iteration 20630: total_loss: 0.368526, loss_sup: 0.067373, loss_mps: 0.114705, loss_cps: 0.186448
[13:48:54.993] iteration 20631: total_loss: 0.772419, loss_sup: 0.136525, loss_mps: 0.222215, loss_cps: 0.413678
[13:48:55.139] iteration 20632: total_loss: 0.404450, loss_sup: 0.016115, loss_mps: 0.141517, loss_cps: 0.246818
[13:48:55.288] iteration 20633: total_loss: 0.395159, loss_sup: 0.147351, loss_mps: 0.096725, loss_cps: 0.151083
[13:48:55.438] iteration 20634: total_loss: 0.427117, loss_sup: 0.024681, loss_mps: 0.141999, loss_cps: 0.260437
[13:48:55.585] iteration 20635: total_loss: 0.454189, loss_sup: 0.141292, loss_mps: 0.114070, loss_cps: 0.198827
[13:48:55.732] iteration 20636: total_loss: 0.449822, loss_sup: 0.047201, loss_mps: 0.156291, loss_cps: 0.246330
[13:48:55.878] iteration 20637: total_loss: 0.242137, loss_sup: 0.007257, loss_mps: 0.090322, loss_cps: 0.144558
[13:48:56.024] iteration 20638: total_loss: 0.392108, loss_sup: 0.052569, loss_mps: 0.121607, loss_cps: 0.217931
[13:48:56.169] iteration 20639: total_loss: 0.567872, loss_sup: 0.015552, loss_mps: 0.194968, loss_cps: 0.357352
[13:48:56.319] iteration 20640: total_loss: 0.470143, loss_sup: 0.062584, loss_mps: 0.148549, loss_cps: 0.259010
[13:48:56.464] iteration 20641: total_loss: 0.513087, loss_sup: 0.031667, loss_mps: 0.171203, loss_cps: 0.310217
[13:48:56.610] iteration 20642: total_loss: 0.719142, loss_sup: 0.200304, loss_mps: 0.181330, loss_cps: 0.337507
[13:48:56.759] iteration 20643: total_loss: 0.451897, loss_sup: 0.110667, loss_mps: 0.116936, loss_cps: 0.224294
[13:48:56.905] iteration 20644: total_loss: 0.272908, loss_sup: 0.011848, loss_mps: 0.096079, loss_cps: 0.164982
[13:48:57.052] iteration 20645: total_loss: 0.402832, loss_sup: 0.042263, loss_mps: 0.130924, loss_cps: 0.229644
[13:48:57.199] iteration 20646: total_loss: 0.410906, loss_sup: 0.018993, loss_mps: 0.134286, loss_cps: 0.257627
[13:48:57.344] iteration 20647: total_loss: 0.635108, loss_sup: 0.137642, loss_mps: 0.167874, loss_cps: 0.329592
[13:48:57.492] iteration 20648: total_loss: 0.329025, loss_sup: 0.045506, loss_mps: 0.103125, loss_cps: 0.180394
[13:48:57.638] iteration 20649: total_loss: 0.357773, loss_sup: 0.026856, loss_mps: 0.111853, loss_cps: 0.219064
[13:48:57.784] iteration 20650: total_loss: 0.256773, loss_sup: 0.031752, loss_mps: 0.080763, loss_cps: 0.144257
[13:48:57.930] iteration 20651: total_loss: 0.332636, loss_sup: 0.013860, loss_mps: 0.115179, loss_cps: 0.203597
[13:48:58.075] iteration 20652: total_loss: 0.401408, loss_sup: 0.015645, loss_mps: 0.137700, loss_cps: 0.248062
[13:48:58.221] iteration 20653: total_loss: 0.503387, loss_sup: 0.047250, loss_mps: 0.155977, loss_cps: 0.300161
[13:48:58.367] iteration 20654: total_loss: 0.276656, loss_sup: 0.017050, loss_mps: 0.094717, loss_cps: 0.164890
[13:48:58.512] iteration 20655: total_loss: 0.414399, loss_sup: 0.108848, loss_mps: 0.113744, loss_cps: 0.191806
[13:48:58.658] iteration 20656: total_loss: 0.273603, loss_sup: 0.103945, loss_mps: 0.068223, loss_cps: 0.101435
[13:48:58.805] iteration 20657: total_loss: 0.287854, loss_sup: 0.029572, loss_mps: 0.099781, loss_cps: 0.158501
[13:48:58.950] iteration 20658: total_loss: 0.394080, loss_sup: 0.014863, loss_mps: 0.135190, loss_cps: 0.244026
[13:48:59.097] iteration 20659: total_loss: 0.486497, loss_sup: 0.077881, loss_mps: 0.136259, loss_cps: 0.272356
[13:48:59.243] iteration 20660: total_loss: 0.719994, loss_sup: 0.056025, loss_mps: 0.213060, loss_cps: 0.450909
[13:48:59.391] iteration 20661: total_loss: 0.690589, loss_sup: 0.087380, loss_mps: 0.189280, loss_cps: 0.413928
[13:48:59.537] iteration 20662: total_loss: 0.401789, loss_sup: 0.103161, loss_mps: 0.107736, loss_cps: 0.190892
[13:48:59.685] iteration 20663: total_loss: 0.238365, loss_sup: 0.012950, loss_mps: 0.083704, loss_cps: 0.141710
[13:48:59.831] iteration 20664: total_loss: 0.383756, loss_sup: 0.113105, loss_mps: 0.094468, loss_cps: 0.176183
[13:48:59.980] iteration 20665: total_loss: 0.386088, loss_sup: 0.083404, loss_mps: 0.098731, loss_cps: 0.203954
[13:49:00.128] iteration 20666: total_loss: 0.294239, loss_sup: 0.015770, loss_mps: 0.101274, loss_cps: 0.177195
[13:49:00.275] iteration 20667: total_loss: 0.775870, loss_sup: 0.124119, loss_mps: 0.211051, loss_cps: 0.440701
[13:49:00.421] iteration 20668: total_loss: 0.494042, loss_sup: 0.065717, loss_mps: 0.138112, loss_cps: 0.290213
[13:49:00.569] iteration 20669: total_loss: 0.529362, loss_sup: 0.168078, loss_mps: 0.121019, loss_cps: 0.240265
[13:49:00.719] iteration 20670: total_loss: 0.739198, loss_sup: 0.195124, loss_mps: 0.183797, loss_cps: 0.360277
[13:49:00.865] iteration 20671: total_loss: 0.674120, loss_sup: 0.125378, loss_mps: 0.173814, loss_cps: 0.374928
[13:49:01.011] iteration 20672: total_loss: 0.290291, loss_sup: 0.018343, loss_mps: 0.095491, loss_cps: 0.176456
[13:49:01.157] iteration 20673: total_loss: 0.326071, loss_sup: 0.029023, loss_mps: 0.109688, loss_cps: 0.187360
[13:49:01.304] iteration 20674: total_loss: 0.453992, loss_sup: 0.077173, loss_mps: 0.133424, loss_cps: 0.243395
[13:49:01.453] iteration 20675: total_loss: 0.295876, loss_sup: 0.042610, loss_mps: 0.093303, loss_cps: 0.159962
[13:49:01.599] iteration 20676: total_loss: 0.777784, loss_sup: 0.027060, loss_mps: 0.239072, loss_cps: 0.511652
[13:49:01.745] iteration 20677: total_loss: 0.379794, loss_sup: 0.007677, loss_mps: 0.132412, loss_cps: 0.239705
[13:49:01.891] iteration 20678: total_loss: 0.314968, loss_sup: 0.018943, loss_mps: 0.102410, loss_cps: 0.193615
[13:49:02.037] iteration 20679: total_loss: 0.318553, loss_sup: 0.037602, loss_mps: 0.098916, loss_cps: 0.182035
[13:49:02.183] iteration 20680: total_loss: 0.642716, loss_sup: 0.055107, loss_mps: 0.190908, loss_cps: 0.396700
[13:49:02.328] iteration 20681: total_loss: 0.383907, loss_sup: 0.156152, loss_mps: 0.087660, loss_cps: 0.140095
[13:49:02.474] iteration 20682: total_loss: 0.477052, loss_sup: 0.031585, loss_mps: 0.148870, loss_cps: 0.296597
[13:49:02.620] iteration 20683: total_loss: 0.510400, loss_sup: 0.007084, loss_mps: 0.164667, loss_cps: 0.338648
[13:49:02.766] iteration 20684: total_loss: 0.678129, loss_sup: 0.164185, loss_mps: 0.172699, loss_cps: 0.341245
[13:49:02.912] iteration 20685: total_loss: 0.865080, loss_sup: 0.078235, loss_mps: 0.244256, loss_cps: 0.542590
[13:49:03.058] iteration 20686: total_loss: 0.626143, loss_sup: 0.135735, loss_mps: 0.168813, loss_cps: 0.321594
[13:49:03.204] iteration 20687: total_loss: 0.549682, loss_sup: 0.129752, loss_mps: 0.153477, loss_cps: 0.266453
[13:49:03.349] iteration 20688: total_loss: 0.388330, loss_sup: 0.061587, loss_mps: 0.115999, loss_cps: 0.210743
[13:49:03.495] iteration 20689: total_loss: 0.367924, loss_sup: 0.074276, loss_mps: 0.105198, loss_cps: 0.188450
[13:49:03.642] iteration 20690: total_loss: 0.646965, loss_sup: 0.119859, loss_mps: 0.175297, loss_cps: 0.351809
[13:49:03.787] iteration 20691: total_loss: 0.417816, loss_sup: 0.085445, loss_mps: 0.115730, loss_cps: 0.216641
[13:49:03.933] iteration 20692: total_loss: 1.448938, loss_sup: 0.503727, loss_mps: 0.315867, loss_cps: 0.629343
[13:49:04.083] iteration 20693: total_loss: 0.538984, loss_sup: 0.040204, loss_mps: 0.162360, loss_cps: 0.336420
[13:49:04.236] iteration 20694: total_loss: 0.732719, loss_sup: 0.011430, loss_mps: 0.236217, loss_cps: 0.485071
[13:49:04.382] iteration 20695: total_loss: 0.582251, loss_sup: 0.158315, loss_mps: 0.142733, loss_cps: 0.281204
[13:49:04.529] iteration 20696: total_loss: 0.583331, loss_sup: 0.159319, loss_mps: 0.155921, loss_cps: 0.268091
[13:49:04.675] iteration 20697: total_loss: 0.309991, loss_sup: 0.017081, loss_mps: 0.107734, loss_cps: 0.185176
[13:49:04.821] iteration 20698: total_loss: 0.833923, loss_sup: 0.042455, loss_mps: 0.245153, loss_cps: 0.546316
[13:49:04.967] iteration 20699: total_loss: 0.561200, loss_sup: 0.088402, loss_mps: 0.169104, loss_cps: 0.303694
[13:49:05.113] iteration 20700: total_loss: 0.815899, loss_sup: 0.024247, loss_mps: 0.251950, loss_cps: 0.539702
[13:49:05.113] Evaluation Started ==>
[13:49:16.426] ==> valid iteration 20700: unet metrics: {'dc': 0.6890934362292983, 'jc': 0.574451884303917, 'pre': 0.786678026359179, 'hd': 5.476378538052641}, ynet metrics: {'dc': 0.575434912461956, 'jc': 0.45977811560950305, 'pre': 0.7909181539497084, 'hd': 5.6416776203064085}.
[13:49:16.490] ==> New best valid dice for unet: 0.689093, at iteration 20700
[13:49:16.492] Evaluation Finished!⏹️
[13:49:16.645] iteration 20701: total_loss: 0.531751, loss_sup: 0.074126, loss_mps: 0.151410, loss_cps: 0.306214
[13:49:16.794] iteration 20702: total_loss: 0.334478, loss_sup: 0.008246, loss_mps: 0.114513, loss_cps: 0.211719
[13:49:16.941] iteration 20703: total_loss: 0.522402, loss_sup: 0.060052, loss_mps: 0.140776, loss_cps: 0.321575
[13:49:17.090] iteration 20704: total_loss: 0.564315, loss_sup: 0.019477, loss_mps: 0.181730, loss_cps: 0.363108
[13:49:17.237] iteration 20705: total_loss: 0.644132, loss_sup: 0.019114, loss_mps: 0.215092, loss_cps: 0.409926
[13:49:17.383] iteration 20706: total_loss: 0.594452, loss_sup: 0.010732, loss_mps: 0.194033, loss_cps: 0.389687
[13:49:17.528] iteration 20707: total_loss: 0.574397, loss_sup: 0.178856, loss_mps: 0.140414, loss_cps: 0.255128
[13:49:17.676] iteration 20708: total_loss: 0.808987, loss_sup: 0.147660, loss_mps: 0.217072, loss_cps: 0.444254
[13:49:17.822] iteration 20709: total_loss: 0.409734, loss_sup: 0.100682, loss_mps: 0.108751, loss_cps: 0.200301
[13:49:17.967] iteration 20710: total_loss: 0.976902, loss_sup: 0.077605, loss_mps: 0.274590, loss_cps: 0.624707
[13:49:18.112] iteration 20711: total_loss: 0.842682, loss_sup: 0.119036, loss_mps: 0.235230, loss_cps: 0.488415
[13:49:18.258] iteration 20712: total_loss: 0.483717, loss_sup: 0.004908, loss_mps: 0.163409, loss_cps: 0.315400
[13:49:18.403] iteration 20713: total_loss: 0.370047, loss_sup: 0.007277, loss_mps: 0.123959, loss_cps: 0.238811
[13:49:18.548] iteration 20714: total_loss: 0.657248, loss_sup: 0.058956, loss_mps: 0.189441, loss_cps: 0.408850
[13:49:18.694] iteration 20715: total_loss: 0.334578, loss_sup: 0.015631, loss_mps: 0.107931, loss_cps: 0.211015
[13:49:18.841] iteration 20716: total_loss: 0.695023, loss_sup: 0.150550, loss_mps: 0.183613, loss_cps: 0.360860
[13:49:18.987] iteration 20717: total_loss: 0.374544, loss_sup: 0.014461, loss_mps: 0.121444, loss_cps: 0.238639
[13:49:19.132] iteration 20718: total_loss: 0.334929, loss_sup: 0.035571, loss_mps: 0.109196, loss_cps: 0.190162
[13:49:19.278] iteration 20719: total_loss: 0.501489, loss_sup: 0.054290, loss_mps: 0.151454, loss_cps: 0.295745
[13:49:19.423] iteration 20720: total_loss: 0.269988, loss_sup: 0.002112, loss_mps: 0.101187, loss_cps: 0.166688
[13:49:19.568] iteration 20721: total_loss: 0.376592, loss_sup: 0.069278, loss_mps: 0.103404, loss_cps: 0.203910
[13:49:19.713] iteration 20722: total_loss: 0.667545, loss_sup: 0.063850, loss_mps: 0.197394, loss_cps: 0.406301
[13:49:19.859] iteration 20723: total_loss: 0.378602, loss_sup: 0.010691, loss_mps: 0.126791, loss_cps: 0.241120
[13:49:20.007] iteration 20724: total_loss: 0.549453, loss_sup: 0.014298, loss_mps: 0.182600, loss_cps: 0.352555
[13:49:20.153] iteration 20725: total_loss: 0.470884, loss_sup: 0.110989, loss_mps: 0.127324, loss_cps: 0.232571
[13:49:20.299] iteration 20726: total_loss: 0.622963, loss_sup: 0.021528, loss_mps: 0.189496, loss_cps: 0.411939
[13:49:20.445] iteration 20727: total_loss: 0.648904, loss_sup: 0.178722, loss_mps: 0.163303, loss_cps: 0.306879
[13:49:20.591] iteration 20728: total_loss: 0.579031, loss_sup: 0.226170, loss_mps: 0.128891, loss_cps: 0.223970
[13:49:20.737] iteration 20729: total_loss: 0.504404, loss_sup: 0.009243, loss_mps: 0.169176, loss_cps: 0.325985
[13:49:20.883] iteration 20730: total_loss: 0.746029, loss_sup: 0.052629, loss_mps: 0.220452, loss_cps: 0.472948
[13:49:21.028] iteration 20731: total_loss: 0.879496, loss_sup: 0.178743, loss_mps: 0.215155, loss_cps: 0.485598
[13:49:21.174] iteration 20732: total_loss: 0.308590, loss_sup: 0.040885, loss_mps: 0.099584, loss_cps: 0.168121
[13:49:21.321] iteration 20733: total_loss: 0.723003, loss_sup: 0.182513, loss_mps: 0.163458, loss_cps: 0.377032
[13:49:21.466] iteration 20734: total_loss: 0.684788, loss_sup: 0.155696, loss_mps: 0.168509, loss_cps: 0.360583
[13:49:21.613] iteration 20735: total_loss: 0.799663, loss_sup: 0.039793, loss_mps: 0.234322, loss_cps: 0.525548
[13:49:21.759] iteration 20736: total_loss: 0.748400, loss_sup: 0.316604, loss_mps: 0.145944, loss_cps: 0.285851
[13:49:21.904] iteration 20737: total_loss: 0.328197, loss_sup: 0.024386, loss_mps: 0.108497, loss_cps: 0.195315
[13:49:22.054] iteration 20738: total_loss: 0.532168, loss_sup: 0.081706, loss_mps: 0.156156, loss_cps: 0.294305
[13:49:22.200] iteration 20739: total_loss: 0.722708, loss_sup: 0.104336, loss_mps: 0.211097, loss_cps: 0.407275
[13:49:22.346] iteration 20740: total_loss: 0.369348, loss_sup: 0.036333, loss_mps: 0.118978, loss_cps: 0.214037
[13:49:22.491] iteration 20741: total_loss: 0.603937, loss_sup: 0.130115, loss_mps: 0.164661, loss_cps: 0.309161
[13:49:22.639] iteration 20742: total_loss: 0.390460, loss_sup: 0.084420, loss_mps: 0.117412, loss_cps: 0.188628
[13:49:22.786] iteration 20743: total_loss: 0.500681, loss_sup: 0.041890, loss_mps: 0.151200, loss_cps: 0.307590
[13:49:22.932] iteration 20744: total_loss: 0.604074, loss_sup: 0.028570, loss_mps: 0.185132, loss_cps: 0.390373
[13:49:23.079] iteration 20745: total_loss: 0.546082, loss_sup: 0.058194, loss_mps: 0.159472, loss_cps: 0.328416
[13:49:23.231] iteration 20746: total_loss: 0.378547, loss_sup: 0.025080, loss_mps: 0.128450, loss_cps: 0.225017
[13:49:23.378] iteration 20747: total_loss: 0.531282, loss_sup: 0.151077, loss_mps: 0.128305, loss_cps: 0.251900
[13:49:23.525] iteration 20748: total_loss: 0.389846, loss_sup: 0.016846, loss_mps: 0.131175, loss_cps: 0.241825
[13:49:23.673] iteration 20749: total_loss: 0.371583, loss_sup: 0.012221, loss_mps: 0.130837, loss_cps: 0.228525
[13:49:23.818] iteration 20750: total_loss: 0.659103, loss_sup: 0.236635, loss_mps: 0.149543, loss_cps: 0.272925
[13:49:23.964] iteration 20751: total_loss: 0.381266, loss_sup: 0.025283, loss_mps: 0.128931, loss_cps: 0.227053
[13:49:24.110] iteration 20752: total_loss: 0.456348, loss_sup: 0.136845, loss_mps: 0.118086, loss_cps: 0.201418
[13:49:24.257] iteration 20753: total_loss: 0.355883, loss_sup: 0.140472, loss_mps: 0.085546, loss_cps: 0.129865
[13:49:24.403] iteration 20754: total_loss: 0.539409, loss_sup: 0.107855, loss_mps: 0.144348, loss_cps: 0.287206
[13:49:24.549] iteration 20755: total_loss: 0.499375, loss_sup: 0.119913, loss_mps: 0.131526, loss_cps: 0.247936
[13:49:24.694] iteration 20756: total_loss: 0.340283, loss_sup: 0.043258, loss_mps: 0.108661, loss_cps: 0.188364
[13:49:24.841] iteration 20757: total_loss: 0.586451, loss_sup: 0.070819, loss_mps: 0.174040, loss_cps: 0.341591
[13:49:24.987] iteration 20758: total_loss: 0.601727, loss_sup: 0.046390, loss_mps: 0.174787, loss_cps: 0.380551
[13:49:25.134] iteration 20759: total_loss: 0.434645, loss_sup: 0.086268, loss_mps: 0.126647, loss_cps: 0.221730
[13:49:25.280] iteration 20760: total_loss: 0.512347, loss_sup: 0.097215, loss_mps: 0.143778, loss_cps: 0.271354
[13:49:25.426] iteration 20761: total_loss: 0.451283, loss_sup: 0.052995, loss_mps: 0.138038, loss_cps: 0.260251
[13:49:25.572] iteration 20762: total_loss: 0.676871, loss_sup: 0.117406, loss_mps: 0.196390, loss_cps: 0.363075
[13:49:25.718] iteration 20763: total_loss: 0.517446, loss_sup: 0.031325, loss_mps: 0.161780, loss_cps: 0.324341
[13:49:25.864] iteration 20764: total_loss: 0.479065, loss_sup: 0.068948, loss_mps: 0.147240, loss_cps: 0.262878
[13:49:26.013] iteration 20765: total_loss: 0.919119, loss_sup: 0.064316, loss_mps: 0.257291, loss_cps: 0.597512
[13:49:26.159] iteration 20766: total_loss: 0.302582, loss_sup: 0.003697, loss_mps: 0.107817, loss_cps: 0.191068
[13:49:26.304] iteration 20767: total_loss: 0.610718, loss_sup: 0.146421, loss_mps: 0.162274, loss_cps: 0.302024
[13:49:26.452] iteration 20768: total_loss: 0.574363, loss_sup: 0.046097, loss_mps: 0.176275, loss_cps: 0.351991
[13:49:26.603] iteration 20769: total_loss: 0.898661, loss_sup: 0.129108, loss_mps: 0.248082, loss_cps: 0.521471
[13:49:26.749] iteration 20770: total_loss: 1.092412, loss_sup: 0.515373, loss_mps: 0.197498, loss_cps: 0.379541
[13:49:26.895] iteration 20771: total_loss: 0.358699, loss_sup: 0.012544, loss_mps: 0.119548, loss_cps: 0.226606
[13:49:27.040] iteration 20772: total_loss: 0.622411, loss_sup: 0.093179, loss_mps: 0.179272, loss_cps: 0.349960
[13:49:27.186] iteration 20773: total_loss: 0.784043, loss_sup: 0.090421, loss_mps: 0.224007, loss_cps: 0.469615
[13:49:27.333] iteration 20774: total_loss: 0.680392, loss_sup: 0.080701, loss_mps: 0.204075, loss_cps: 0.395616
[13:49:27.479] iteration 20775: total_loss: 0.518095, loss_sup: 0.193250, loss_mps: 0.118752, loss_cps: 0.206093
[13:49:27.627] iteration 20776: total_loss: 0.432787, loss_sup: 0.061824, loss_mps: 0.130896, loss_cps: 0.240067
[13:49:27.775] iteration 20777: total_loss: 0.751861, loss_sup: 0.186576, loss_mps: 0.196329, loss_cps: 0.368956
[13:49:27.921] iteration 20778: total_loss: 0.586613, loss_sup: 0.100821, loss_mps: 0.162048, loss_cps: 0.323744
[13:49:28.072] iteration 20779: total_loss: 0.954847, loss_sup: 0.143797, loss_mps: 0.261948, loss_cps: 0.549102
[13:49:28.218] iteration 20780: total_loss: 0.638751, loss_sup: 0.110866, loss_mps: 0.177547, loss_cps: 0.350338
[13:49:28.364] iteration 20781: total_loss: 0.800330, loss_sup: 0.047735, loss_mps: 0.240454, loss_cps: 0.512141
[13:49:28.510] iteration 20782: total_loss: 0.638630, loss_sup: 0.099644, loss_mps: 0.179686, loss_cps: 0.359301
[13:49:28.656] iteration 20783: total_loss: 0.777449, loss_sup: 0.078153, loss_mps: 0.241776, loss_cps: 0.457519
[13:49:28.803] iteration 20784: total_loss: 0.763623, loss_sup: 0.042764, loss_mps: 0.221044, loss_cps: 0.499815
[13:49:28.951] iteration 20785: total_loss: 0.726482, loss_sup: 0.077695, loss_mps: 0.203892, loss_cps: 0.444895
[13:49:29.097] iteration 20786: total_loss: 0.861201, loss_sup: 0.028865, loss_mps: 0.256724, loss_cps: 0.575612
[13:49:29.244] iteration 20787: total_loss: 1.426670, loss_sup: 0.053126, loss_mps: 0.415944, loss_cps: 0.957600
[13:49:29.390] iteration 20788: total_loss: 0.405155, loss_sup: 0.037868, loss_mps: 0.126149, loss_cps: 0.241138
[13:49:29.536] iteration 20789: total_loss: 0.771938, loss_sup: 0.180308, loss_mps: 0.202739, loss_cps: 0.388891
[13:49:29.682] iteration 20790: total_loss: 0.348104, loss_sup: 0.025559, loss_mps: 0.119533, loss_cps: 0.203012
[13:49:29.829] iteration 20791: total_loss: 0.451739, loss_sup: 0.021738, loss_mps: 0.153228, loss_cps: 0.276774
[13:49:29.976] iteration 20792: total_loss: 0.801344, loss_sup: 0.065884, loss_mps: 0.245041, loss_cps: 0.490419
[13:49:30.122] iteration 20793: total_loss: 0.871782, loss_sup: 0.126225, loss_mps: 0.240470, loss_cps: 0.505086
[13:49:30.269] iteration 20794: total_loss: 1.112782, loss_sup: 0.322276, loss_mps: 0.246189, loss_cps: 0.544318
[13:49:30.419] iteration 20795: total_loss: 0.938293, loss_sup: 0.168854, loss_mps: 0.253562, loss_cps: 0.515878
[13:49:30.566] iteration 20796: total_loss: 1.220955, loss_sup: 0.404352, loss_mps: 0.280193, loss_cps: 0.536410
[13:49:30.713] iteration 20797: total_loss: 0.448732, loss_sup: 0.023831, loss_mps: 0.143823, loss_cps: 0.281079
[13:49:30.859] iteration 20798: total_loss: 0.655691, loss_sup: 0.018000, loss_mps: 0.209839, loss_cps: 0.427851
[13:49:31.005] iteration 20799: total_loss: 0.496064, loss_sup: 0.109455, loss_mps: 0.150189, loss_cps: 0.236420
[13:49:31.154] iteration 20800: total_loss: 0.475873, loss_sup: 0.061914, loss_mps: 0.145570, loss_cps: 0.268389
[13:49:31.154] Evaluation Started ==>
[13:49:42.476] ==> valid iteration 20800: unet metrics: {'dc': 0.5945585741816878, 'jc': 0.479800659883813, 'pre': 0.7930907981801162, 'hd': 5.389521585195002}, ynet metrics: {'dc': 0.5813165807074963, 'jc': 0.46629404143044983, 'pre': 0.776237445338919, 'hd': 5.5854257242940415}.
[13:49:42.478] Evaluation Finished!⏹️
[13:49:42.631] iteration 20801: total_loss: 0.379138, loss_sup: 0.034209, loss_mps: 0.134312, loss_cps: 0.210618
[13:49:42.781] iteration 20802: total_loss: 0.814154, loss_sup: 0.039282, loss_mps: 0.258616, loss_cps: 0.516256
[13:49:42.926] iteration 20803: total_loss: 0.373581, loss_sup: 0.017047, loss_mps: 0.131120, loss_cps: 0.225413
[13:49:43.071] iteration 20804: total_loss: 0.765025, loss_sup: 0.152373, loss_mps: 0.211782, loss_cps: 0.400870
[13:49:43.217] iteration 20805: total_loss: 0.599609, loss_sup: 0.028445, loss_mps: 0.199185, loss_cps: 0.371979
[13:49:43.363] iteration 20806: total_loss: 0.686350, loss_sup: 0.095428, loss_mps: 0.193699, loss_cps: 0.397223
[13:49:43.508] iteration 20807: total_loss: 0.712859, loss_sup: 0.087285, loss_mps: 0.203796, loss_cps: 0.421778
[13:49:43.655] iteration 20808: total_loss: 0.498773, loss_sup: 0.063965, loss_mps: 0.148672, loss_cps: 0.286137
[13:49:43.801] iteration 20809: total_loss: 0.663075, loss_sup: 0.115719, loss_mps: 0.200118, loss_cps: 0.347239
[13:49:43.946] iteration 20810: total_loss: 0.901245, loss_sup: 0.175069, loss_mps: 0.248357, loss_cps: 0.477819
[13:49:44.092] iteration 20811: total_loss: 0.550520, loss_sup: 0.056397, loss_mps: 0.175028, loss_cps: 0.319095
[13:49:44.239] iteration 20812: total_loss: 0.624967, loss_sup: 0.044401, loss_mps: 0.190054, loss_cps: 0.390512
[13:49:44.385] iteration 20813: total_loss: 0.388734, loss_sup: 0.005896, loss_mps: 0.130593, loss_cps: 0.252245
[13:49:44.531] iteration 20814: total_loss: 0.320177, loss_sup: 0.029791, loss_mps: 0.110889, loss_cps: 0.179497
[13:49:44.676] iteration 20815: total_loss: 0.415701, loss_sup: 0.197745, loss_mps: 0.087048, loss_cps: 0.130908
[13:49:44.824] iteration 20816: total_loss: 0.431392, loss_sup: 0.110605, loss_mps: 0.111726, loss_cps: 0.209061
[13:49:44.971] iteration 20817: total_loss: 0.402942, loss_sup: 0.011935, loss_mps: 0.144175, loss_cps: 0.246832
[13:49:45.117] iteration 20818: total_loss: 1.295204, loss_sup: 0.119675, loss_mps: 0.353763, loss_cps: 0.821766
[13:49:45.264] iteration 20819: total_loss: 0.486489, loss_sup: 0.033738, loss_mps: 0.169370, loss_cps: 0.283382
[13:49:45.409] iteration 20820: total_loss: 0.483294, loss_sup: 0.101471, loss_mps: 0.138869, loss_cps: 0.242953
[13:49:45.554] iteration 20821: total_loss: 0.579256, loss_sup: 0.100423, loss_mps: 0.169365, loss_cps: 0.309467
[13:49:45.699] iteration 20822: total_loss: 0.297309, loss_sup: 0.015262, loss_mps: 0.108976, loss_cps: 0.173071
[13:49:45.845] iteration 20823: total_loss: 0.510605, loss_sup: 0.029105, loss_mps: 0.156828, loss_cps: 0.324672
[13:49:45.991] iteration 20824: total_loss: 0.792623, loss_sup: 0.296743, loss_mps: 0.170016, loss_cps: 0.325864
[13:49:46.138] iteration 20825: total_loss: 0.740673, loss_sup: 0.209920, loss_mps: 0.172041, loss_cps: 0.358712
[13:49:46.286] iteration 20826: total_loss: 0.826059, loss_sup: 0.171686, loss_mps: 0.215082, loss_cps: 0.439291
[13:49:46.432] iteration 20827: total_loss: 0.398544, loss_sup: 0.082993, loss_mps: 0.110709, loss_cps: 0.204842
[13:49:46.577] iteration 20828: total_loss: 0.584615, loss_sup: 0.151663, loss_mps: 0.150598, loss_cps: 0.282355
[13:49:46.724] iteration 20829: total_loss: 0.570291, loss_sup: 0.030642, loss_mps: 0.189143, loss_cps: 0.350506
[13:49:46.870] iteration 20830: total_loss: 0.552518, loss_sup: 0.035237, loss_mps: 0.177679, loss_cps: 0.339603
[13:49:47.016] iteration 20831: total_loss: 0.691839, loss_sup: 0.109865, loss_mps: 0.190397, loss_cps: 0.391576
[13:49:47.162] iteration 20832: total_loss: 0.616514, loss_sup: 0.224867, loss_mps: 0.140023, loss_cps: 0.251624
[13:49:47.308] iteration 20833: total_loss: 0.453933, loss_sup: 0.020593, loss_mps: 0.166933, loss_cps: 0.266406
[13:49:47.456] iteration 20834: total_loss: 0.451397, loss_sup: 0.095342, loss_mps: 0.129885, loss_cps: 0.226170
[13:49:47.601] iteration 20835: total_loss: 0.758660, loss_sup: 0.163404, loss_mps: 0.203504, loss_cps: 0.391753
[13:49:47.747] iteration 20836: total_loss: 0.425933, loss_sup: 0.024468, loss_mps: 0.138981, loss_cps: 0.262483
[13:49:47.893] iteration 20837: total_loss: 0.505903, loss_sup: 0.035101, loss_mps: 0.152668, loss_cps: 0.318133
[13:49:48.041] iteration 20838: total_loss: 0.598061, loss_sup: 0.047848, loss_mps: 0.189661, loss_cps: 0.360553
[13:49:48.186] iteration 20839: total_loss: 0.400972, loss_sup: 0.091581, loss_mps: 0.110107, loss_cps: 0.199284
[13:49:48.333] iteration 20840: total_loss: 0.595243, loss_sup: 0.060085, loss_mps: 0.183113, loss_cps: 0.352045
[13:49:48.479] iteration 20841: total_loss: 0.414382, loss_sup: 0.044476, loss_mps: 0.130730, loss_cps: 0.239176
[13:49:48.627] iteration 20842: total_loss: 0.341244, loss_sup: 0.067429, loss_mps: 0.105106, loss_cps: 0.168709
[13:49:48.773] iteration 20843: total_loss: 0.717795, loss_sup: 0.165489, loss_mps: 0.187002, loss_cps: 0.365304
[13:49:48.921] iteration 20844: total_loss: 0.370493, loss_sup: 0.022322, loss_mps: 0.122434, loss_cps: 0.225737
[13:49:49.069] iteration 20845: total_loss: 0.684083, loss_sup: 0.082507, loss_mps: 0.194824, loss_cps: 0.406751
[13:49:49.214] iteration 20846: total_loss: 0.359674, loss_sup: 0.010297, loss_mps: 0.129825, loss_cps: 0.219552
[13:49:49.362] iteration 20847: total_loss: 0.539878, loss_sup: 0.051113, loss_mps: 0.168981, loss_cps: 0.319784
[13:49:49.509] iteration 20848: total_loss: 0.400828, loss_sup: 0.070921, loss_mps: 0.126340, loss_cps: 0.203566
[13:49:49.655] iteration 20849: total_loss: 0.603220, loss_sup: 0.039463, loss_mps: 0.194344, loss_cps: 0.369413
[13:49:49.802] iteration 20850: total_loss: 0.347716, loss_sup: 0.006050, loss_mps: 0.119931, loss_cps: 0.221735
[13:49:49.947] iteration 20851: total_loss: 0.682144, loss_sup: 0.156119, loss_mps: 0.178478, loss_cps: 0.347547
[13:49:50.095] iteration 20852: total_loss: 0.402534, loss_sup: 0.088381, loss_mps: 0.118596, loss_cps: 0.195557
[13:49:50.240] iteration 20853: total_loss: 0.388850, loss_sup: 0.047273, loss_mps: 0.122164, loss_cps: 0.219413
[13:49:50.387] iteration 20854: total_loss: 0.602526, loss_sup: 0.030693, loss_mps: 0.181395, loss_cps: 0.390438
[13:49:50.534] iteration 20855: total_loss: 0.495692, loss_sup: 0.050756, loss_mps: 0.154092, loss_cps: 0.290844
[13:49:50.681] iteration 20856: total_loss: 0.375720, loss_sup: 0.037150, loss_mps: 0.122079, loss_cps: 0.216491
[13:49:50.828] iteration 20857: total_loss: 0.422637, loss_sup: 0.027633, loss_mps: 0.130671, loss_cps: 0.264332
[13:49:50.973] iteration 20858: total_loss: 0.541665, loss_sup: 0.023843, loss_mps: 0.180561, loss_cps: 0.337261
[13:49:51.121] iteration 20859: total_loss: 0.409512, loss_sup: 0.079736, loss_mps: 0.124220, loss_cps: 0.205556
[13:49:51.267] iteration 20860: total_loss: 0.678175, loss_sup: 0.168583, loss_mps: 0.166392, loss_cps: 0.343200
[13:49:51.413] iteration 20861: total_loss: 0.454011, loss_sup: 0.024760, loss_mps: 0.151928, loss_cps: 0.277324
[13:49:51.559] iteration 20862: total_loss: 0.424565, loss_sup: 0.009509, loss_mps: 0.143634, loss_cps: 0.271422
[13:49:51.705] iteration 20863: total_loss: 0.674065, loss_sup: 0.112100, loss_mps: 0.183852, loss_cps: 0.378112
[13:49:51.850] iteration 20864: total_loss: 0.752918, loss_sup: 0.140837, loss_mps: 0.203041, loss_cps: 0.409040
[13:49:51.997] iteration 20865: total_loss: 0.749843, loss_sup: 0.097294, loss_mps: 0.221527, loss_cps: 0.431022
[13:49:52.144] iteration 20866: total_loss: 0.349900, loss_sup: 0.081460, loss_mps: 0.093291, loss_cps: 0.175149
[13:49:52.290] iteration 20867: total_loss: 0.477241, loss_sup: 0.075921, loss_mps: 0.143384, loss_cps: 0.257935
[13:49:52.438] iteration 20868: total_loss: 1.172503, loss_sup: 0.086303, loss_mps: 0.342284, loss_cps: 0.743915
[13:49:52.583] iteration 20869: total_loss: 1.203547, loss_sup: 0.187585, loss_mps: 0.313731, loss_cps: 0.702232
[13:49:52.731] iteration 20870: total_loss: 0.393418, loss_sup: 0.023153, loss_mps: 0.140028, loss_cps: 0.230237
[13:49:52.877] iteration 20871: total_loss: 0.571157, loss_sup: 0.062319, loss_mps: 0.164064, loss_cps: 0.344773
[13:49:53.023] iteration 20872: total_loss: 0.599503, loss_sup: 0.092797, loss_mps: 0.176853, loss_cps: 0.329853
[13:49:53.169] iteration 20873: total_loss: 0.485031, loss_sup: 0.176647, loss_mps: 0.106457, loss_cps: 0.201927
[13:49:53.314] iteration 20874: total_loss: 0.496701, loss_sup: 0.055512, loss_mps: 0.148345, loss_cps: 0.292845
[13:49:53.461] iteration 20875: total_loss: 0.388027, loss_sup: 0.046420, loss_mps: 0.117248, loss_cps: 0.224359
[13:49:53.607] iteration 20876: total_loss: 0.474124, loss_sup: 0.031703, loss_mps: 0.145590, loss_cps: 0.296831
[13:49:53.754] iteration 20877: total_loss: 0.597588, loss_sup: 0.073687, loss_mps: 0.166406, loss_cps: 0.357495
[13:49:53.899] iteration 20878: total_loss: 0.408985, loss_sup: 0.089185, loss_mps: 0.113863, loss_cps: 0.205936
[13:49:54.045] iteration 20879: total_loss: 0.589331, loss_sup: 0.095689, loss_mps: 0.168281, loss_cps: 0.325362
[13:49:54.190] iteration 20880: total_loss: 0.438929, loss_sup: 0.039948, loss_mps: 0.141352, loss_cps: 0.257629
[13:49:54.336] iteration 20881: total_loss: 0.454605, loss_sup: 0.054811, loss_mps: 0.138379, loss_cps: 0.261415
[13:49:54.483] iteration 20882: total_loss: 0.452691, loss_sup: 0.048297, loss_mps: 0.151102, loss_cps: 0.253291
[13:49:54.629] iteration 20883: total_loss: 0.470467, loss_sup: 0.044138, loss_mps: 0.148515, loss_cps: 0.277815
[13:49:54.775] iteration 20884: total_loss: 0.449981, loss_sup: 0.018788, loss_mps: 0.141601, loss_cps: 0.289591
[13:49:54.922] iteration 20885: total_loss: 0.403662, loss_sup: 0.005532, loss_mps: 0.140051, loss_cps: 0.258079
[13:49:55.067] iteration 20886: total_loss: 0.413135, loss_sup: 0.052742, loss_mps: 0.127047, loss_cps: 0.233347
[13:49:55.216] iteration 20887: total_loss: 0.732195, loss_sup: 0.044696, loss_mps: 0.225662, loss_cps: 0.461837
[13:49:55.361] iteration 20888: total_loss: 0.325089, loss_sup: 0.079219, loss_mps: 0.092835, loss_cps: 0.153035
[13:49:55.508] iteration 20889: total_loss: 0.413450, loss_sup: 0.043218, loss_mps: 0.123749, loss_cps: 0.246483
[13:49:55.653] iteration 20890: total_loss: 0.436994, loss_sup: 0.084124, loss_mps: 0.122495, loss_cps: 0.230375
[13:49:55.800] iteration 20891: total_loss: 0.344879, loss_sup: 0.027822, loss_mps: 0.118288, loss_cps: 0.198769
[13:49:55.946] iteration 20892: total_loss: 0.426021, loss_sup: 0.031796, loss_mps: 0.134532, loss_cps: 0.259693
[13:49:56.091] iteration 20893: total_loss: 0.577492, loss_sup: 0.062377, loss_mps: 0.170213, loss_cps: 0.344903
[13:49:56.237] iteration 20894: total_loss: 0.560069, loss_sup: 0.215907, loss_mps: 0.121148, loss_cps: 0.223014
[13:49:56.383] iteration 20895: total_loss: 0.409155, loss_sup: 0.015639, loss_mps: 0.134977, loss_cps: 0.258539
[13:49:56.530] iteration 20896: total_loss: 0.523266, loss_sup: 0.061004, loss_mps: 0.154190, loss_cps: 0.308072
[13:49:56.675] iteration 20897: total_loss: 0.458549, loss_sup: 0.024098, loss_mps: 0.156568, loss_cps: 0.277883
[13:49:56.821] iteration 20898: total_loss: 0.536717, loss_sup: 0.050058, loss_mps: 0.165142, loss_cps: 0.321517
[13:49:56.967] iteration 20899: total_loss: 1.200614, loss_sup: 0.229058, loss_mps: 0.293800, loss_cps: 0.677755
[13:49:57.027] iteration 20900: total_loss: 1.432028, loss_sup: 0.099936, loss_mps: 0.412187, loss_cps: 0.919905
[13:49:57.028] Evaluation Started ==>
[13:50:08.451] ==> valid iteration 20900: unet metrics: {'dc': 0.6598329860115522, 'jc': 0.5446827851008554, 'pre': 0.7830211942208775, 'hd': 5.42856133496695}, ynet metrics: {'dc': 0.6047669142482902, 'jc': 0.4880334630812305, 'pre': 0.7882422859136301, 'hd': 5.5789457292751665}.
[13:50:08.453] Evaluation Finished!⏹️
[13:50:09.656] iteration 20901: total_loss: 0.423470, loss_sup: 0.057565, loss_mps: 0.132534, loss_cps: 0.233371
[13:50:09.805] iteration 20902: total_loss: 0.763345, loss_sup: 0.243567, loss_mps: 0.169436, loss_cps: 0.350342
[13:50:09.951] iteration 20903: total_loss: 0.374880, loss_sup: 0.032769, loss_mps: 0.117557, loss_cps: 0.224555
[13:50:10.097] iteration 20904: total_loss: 0.970233, loss_sup: 0.084017, loss_mps: 0.274380, loss_cps: 0.611837
[13:50:10.242] iteration 20905: total_loss: 0.596352, loss_sup: 0.047915, loss_mps: 0.185169, loss_cps: 0.363269
[13:50:10.388] iteration 20906: total_loss: 0.630205, loss_sup: 0.011935, loss_mps: 0.195162, loss_cps: 0.423108
[13:50:10.533] iteration 20907: total_loss: 0.509052, loss_sup: 0.014992, loss_mps: 0.166390, loss_cps: 0.327670
[13:50:10.679] iteration 20908: total_loss: 0.644242, loss_sup: 0.116004, loss_mps: 0.173000, loss_cps: 0.355237
[13:50:10.827] iteration 20909: total_loss: 0.651891, loss_sup: 0.168498, loss_mps: 0.166340, loss_cps: 0.317053
[13:50:10.974] iteration 20910: total_loss: 0.816669, loss_sup: 0.169115, loss_mps: 0.207661, loss_cps: 0.439893
[13:50:11.120] iteration 20911: total_loss: 1.414049, loss_sup: 0.216325, loss_mps: 0.346592, loss_cps: 0.851132
[13:50:11.265] iteration 20912: total_loss: 0.526338, loss_sup: 0.020949, loss_mps: 0.162551, loss_cps: 0.342839
[13:50:11.411] iteration 20913: total_loss: 0.253129, loss_sup: 0.006649, loss_mps: 0.092836, loss_cps: 0.153643
[13:50:11.556] iteration 20914: total_loss: 1.101470, loss_sup: 0.112546, loss_mps: 0.299479, loss_cps: 0.689446
[13:50:11.702] iteration 20915: total_loss: 0.556357, loss_sup: 0.020073, loss_mps: 0.185918, loss_cps: 0.350365
[13:50:11.850] iteration 20916: total_loss: 0.969258, loss_sup: 0.135744, loss_mps: 0.269553, loss_cps: 0.563962
[13:50:11.995] iteration 20917: total_loss: 0.542938, loss_sup: 0.126646, loss_mps: 0.140252, loss_cps: 0.276040
[13:50:12.142] iteration 20918: total_loss: 0.372052, loss_sup: 0.035900, loss_mps: 0.118662, loss_cps: 0.217490
[13:50:12.288] iteration 20919: total_loss: 0.847238, loss_sup: 0.267802, loss_mps: 0.194863, loss_cps: 0.384574
[13:50:12.434] iteration 20920: total_loss: 0.500965, loss_sup: 0.026721, loss_mps: 0.157001, loss_cps: 0.317243
[13:50:12.579] iteration 20921: total_loss: 0.921669, loss_sup: 0.203831, loss_mps: 0.236430, loss_cps: 0.481408
[13:50:12.730] iteration 20922: total_loss: 0.313066, loss_sup: 0.022100, loss_mps: 0.107551, loss_cps: 0.183415
[13:50:12.877] iteration 20923: total_loss: 1.229597, loss_sup: 0.265521, loss_mps: 0.311279, loss_cps: 0.652797
[13:50:13.026] iteration 20924: total_loss: 0.523978, loss_sup: 0.183612, loss_mps: 0.121185, loss_cps: 0.219182
[13:50:13.174] iteration 20925: total_loss: 0.451212, loss_sup: 0.030194, loss_mps: 0.148955, loss_cps: 0.272063
[13:50:13.320] iteration 20926: total_loss: 1.015476, loss_sup: 0.269275, loss_mps: 0.245372, loss_cps: 0.500829
[13:50:13.468] iteration 20927: total_loss: 0.475075, loss_sup: 0.076452, loss_mps: 0.141019, loss_cps: 0.257603
[13:50:13.617] iteration 20928: total_loss: 0.817908, loss_sup: 0.216630, loss_mps: 0.207629, loss_cps: 0.393650
[13:50:13.764] iteration 20929: total_loss: 0.840584, loss_sup: 0.077549, loss_mps: 0.258716, loss_cps: 0.504319
[13:50:13.911] iteration 20930: total_loss: 1.038386, loss_sup: 0.123098, loss_mps: 0.286716, loss_cps: 0.628572
[13:50:14.057] iteration 20931: total_loss: 0.547879, loss_sup: 0.112699, loss_mps: 0.152507, loss_cps: 0.282673
[13:50:14.203] iteration 20932: total_loss: 0.861554, loss_sup: 0.078752, loss_mps: 0.258268, loss_cps: 0.524534
[13:50:14.350] iteration 20933: total_loss: 0.597386, loss_sup: 0.090213, loss_mps: 0.175800, loss_cps: 0.331373
[13:50:14.496] iteration 20934: total_loss: 0.434488, loss_sup: 0.053592, loss_mps: 0.137422, loss_cps: 0.243474
[13:50:14.642] iteration 20935: total_loss: 0.496623, loss_sup: 0.012644, loss_mps: 0.163457, loss_cps: 0.320522
[13:50:14.789] iteration 20936: total_loss: 0.487024, loss_sup: 0.152728, loss_mps: 0.123931, loss_cps: 0.210364
[13:50:14.935] iteration 20937: total_loss: 0.857807, loss_sup: 0.063834, loss_mps: 0.256041, loss_cps: 0.537932
[13:50:15.083] iteration 20938: total_loss: 0.659256, loss_sup: 0.065793, loss_mps: 0.200970, loss_cps: 0.392493
[13:50:15.229] iteration 20939: total_loss: 0.662078, loss_sup: 0.099751, loss_mps: 0.198896, loss_cps: 0.363430
[13:50:15.375] iteration 20940: total_loss: 0.361657, loss_sup: 0.033503, loss_mps: 0.119158, loss_cps: 0.208995
[13:50:15.521] iteration 20941: total_loss: 0.861018, loss_sup: 0.085350, loss_mps: 0.243740, loss_cps: 0.531928
[13:50:15.672] iteration 20942: total_loss: 0.468401, loss_sup: 0.053162, loss_mps: 0.149542, loss_cps: 0.265697
[13:50:15.818] iteration 20943: total_loss: 0.730449, loss_sup: 0.164382, loss_mps: 0.194022, loss_cps: 0.372045
[13:50:15.964] iteration 20944: total_loss: 0.664787, loss_sup: 0.032106, loss_mps: 0.216066, loss_cps: 0.416615
[13:50:16.110] iteration 20945: total_loss: 0.662305, loss_sup: 0.133311, loss_mps: 0.179679, loss_cps: 0.349316
[13:50:16.256] iteration 20946: total_loss: 0.530696, loss_sup: 0.100724, loss_mps: 0.147548, loss_cps: 0.282424
[13:50:16.403] iteration 20947: total_loss: 0.670062, loss_sup: 0.023639, loss_mps: 0.211660, loss_cps: 0.434763
[13:50:16.550] iteration 20948: total_loss: 0.535889, loss_sup: 0.029441, loss_mps: 0.173108, loss_cps: 0.333340
[13:50:16.697] iteration 20949: total_loss: 0.686157, loss_sup: 0.041895, loss_mps: 0.211703, loss_cps: 0.432559
[13:50:16.843] iteration 20950: total_loss: 0.350618, loss_sup: 0.029990, loss_mps: 0.122864, loss_cps: 0.197764
[13:50:16.989] iteration 20951: total_loss: 0.414345, loss_sup: 0.006381, loss_mps: 0.150619, loss_cps: 0.257345
[13:50:17.135] iteration 20952: total_loss: 0.485299, loss_sup: 0.003148, loss_mps: 0.155054, loss_cps: 0.327098
[13:50:17.281] iteration 20953: total_loss: 0.583708, loss_sup: 0.154803, loss_mps: 0.158465, loss_cps: 0.270439
[13:50:17.427] iteration 20954: total_loss: 0.762380, loss_sup: 0.110310, loss_mps: 0.217311, loss_cps: 0.434759
[13:50:17.574] iteration 20955: total_loss: 0.655130, loss_sup: 0.235997, loss_mps: 0.145207, loss_cps: 0.273926
[13:50:17.720] iteration 20956: total_loss: 0.628361, loss_sup: 0.066560, loss_mps: 0.190065, loss_cps: 0.371737
[13:50:17.866] iteration 20957: total_loss: 0.612761, loss_sup: 0.062949, loss_mps: 0.191161, loss_cps: 0.358650
[13:50:18.013] iteration 20958: total_loss: 0.795012, loss_sup: 0.186970, loss_mps: 0.201171, loss_cps: 0.406871
[13:50:18.162] iteration 20959: total_loss: 0.948807, loss_sup: 0.205796, loss_mps: 0.249655, loss_cps: 0.493356
[13:50:18.308] iteration 20960: total_loss: 0.356088, loss_sup: 0.025660, loss_mps: 0.117951, loss_cps: 0.212477
[13:50:18.455] iteration 20961: total_loss: 0.704077, loss_sup: 0.124692, loss_mps: 0.206943, loss_cps: 0.372443
[13:50:18.603] iteration 20962: total_loss: 0.756771, loss_sup: 0.122827, loss_mps: 0.209067, loss_cps: 0.424878
[13:50:18.754] iteration 20963: total_loss: 1.014480, loss_sup: 0.031284, loss_mps: 0.298416, loss_cps: 0.684780
[13:50:18.900] iteration 20964: total_loss: 0.849408, loss_sup: 0.068730, loss_mps: 0.247370, loss_cps: 0.533309
[13:50:19.046] iteration 20965: total_loss: 0.395911, loss_sup: 0.039351, loss_mps: 0.135397, loss_cps: 0.221164
[13:50:19.192] iteration 20966: total_loss: 0.510425, loss_sup: 0.054320, loss_mps: 0.166536, loss_cps: 0.289569
[13:50:19.338] iteration 20967: total_loss: 0.467929, loss_sup: 0.084560, loss_mps: 0.139768, loss_cps: 0.243601
[13:50:19.484] iteration 20968: total_loss: 0.473000, loss_sup: 0.091611, loss_mps: 0.139894, loss_cps: 0.241495
[13:50:19.629] iteration 20969: total_loss: 0.777019, loss_sup: 0.256759, loss_mps: 0.177960, loss_cps: 0.342301
[13:50:19.777] iteration 20970: total_loss: 0.755317, loss_sup: 0.020403, loss_mps: 0.232458, loss_cps: 0.502456
[13:50:19.923] iteration 20971: total_loss: 0.338205, loss_sup: 0.035945, loss_mps: 0.119522, loss_cps: 0.182737
[13:50:20.068] iteration 20972: total_loss: 0.579396, loss_sup: 0.053067, loss_mps: 0.189944, loss_cps: 0.336385
[13:50:20.217] iteration 20973: total_loss: 0.293654, loss_sup: 0.021388, loss_mps: 0.104500, loss_cps: 0.167765
[13:50:20.363] iteration 20974: total_loss: 0.392444, loss_sup: 0.016303, loss_mps: 0.133819, loss_cps: 0.242321
[13:50:20.511] iteration 20975: total_loss: 0.483491, loss_sup: 0.067529, loss_mps: 0.148093, loss_cps: 0.267869
[13:50:20.657] iteration 20976: total_loss: 0.420066, loss_sup: 0.020071, loss_mps: 0.135387, loss_cps: 0.264607
[13:50:20.803] iteration 20977: total_loss: 0.623799, loss_sup: 0.183264, loss_mps: 0.161018, loss_cps: 0.279517
[13:50:20.949] iteration 20978: total_loss: 0.851709, loss_sup: 0.180611, loss_mps: 0.240214, loss_cps: 0.430884
[13:50:21.098] iteration 20979: total_loss: 0.544078, loss_sup: 0.090686, loss_mps: 0.154865, loss_cps: 0.298527
[13:50:21.244] iteration 20980: total_loss: 0.435569, loss_sup: 0.041543, loss_mps: 0.137202, loss_cps: 0.256824
[13:50:21.390] iteration 20981: total_loss: 0.539269, loss_sup: 0.106398, loss_mps: 0.153838, loss_cps: 0.279033
[13:50:21.536] iteration 20982: total_loss: 0.352223, loss_sup: 0.046481, loss_mps: 0.113546, loss_cps: 0.192196
[13:50:21.682] iteration 20983: total_loss: 0.476376, loss_sup: 0.038298, loss_mps: 0.144517, loss_cps: 0.293562
[13:50:21.828] iteration 20984: total_loss: 0.681835, loss_sup: 0.148986, loss_mps: 0.175863, loss_cps: 0.356987
[13:50:21.974] iteration 20985: total_loss: 0.670726, loss_sup: 0.030881, loss_mps: 0.219974, loss_cps: 0.419871
[13:50:22.120] iteration 20986: total_loss: 0.722106, loss_sup: 0.091668, loss_mps: 0.205664, loss_cps: 0.424774
[13:50:22.267] iteration 20987: total_loss: 0.429052, loss_sup: 0.093889, loss_mps: 0.122813, loss_cps: 0.212350
[13:50:22.413] iteration 20988: total_loss: 0.619417, loss_sup: 0.120327, loss_mps: 0.166466, loss_cps: 0.332624
[13:50:22.561] iteration 20989: total_loss: 0.501847, loss_sup: 0.041205, loss_mps: 0.159627, loss_cps: 0.301016
[13:50:22.706] iteration 20990: total_loss: 0.504769, loss_sup: 0.026529, loss_mps: 0.160956, loss_cps: 0.317284
[13:50:22.853] iteration 20991: total_loss: 0.453319, loss_sup: 0.030003, loss_mps: 0.142987, loss_cps: 0.280330
[13:50:22.998] iteration 20992: total_loss: 0.456833, loss_sup: 0.076304, loss_mps: 0.136417, loss_cps: 0.244111
[13:50:23.144] iteration 20993: total_loss: 0.574265, loss_sup: 0.155869, loss_mps: 0.142093, loss_cps: 0.276302
[13:50:23.290] iteration 20994: total_loss: 0.554283, loss_sup: 0.055327, loss_mps: 0.169719, loss_cps: 0.329237
[13:50:23.438] iteration 20995: total_loss: 0.526778, loss_sup: 0.121182, loss_mps: 0.136478, loss_cps: 0.269118
[13:50:23.585] iteration 20996: total_loss: 0.600057, loss_sup: 0.032328, loss_mps: 0.185898, loss_cps: 0.381831
[13:50:23.731] iteration 20997: total_loss: 0.307072, loss_sup: 0.013187, loss_mps: 0.107606, loss_cps: 0.186278
[13:50:23.878] iteration 20998: total_loss: 0.696672, loss_sup: 0.033905, loss_mps: 0.214764, loss_cps: 0.448003
[13:50:24.026] iteration 20999: total_loss: 0.644459, loss_sup: 0.088330, loss_mps: 0.185162, loss_cps: 0.370967
[13:50:24.172] iteration 21000: total_loss: 0.223249, loss_sup: 0.004439, loss_mps: 0.084598, loss_cps: 0.134211
[13:50:24.172] Evaluation Started ==>
[13:50:35.541] ==> valid iteration 21000: unet metrics: {'dc': 0.6481649101613667, 'jc': 0.5348360849643173, 'pre': 0.7944131516476489, 'hd': 5.381132641083925}, ynet metrics: {'dc': 0.6289656469124913, 'jc': 0.5099060391484953, 'pre': 0.790882747790816, 'hd': 5.657762515270243}.
[13:50:35.544] Evaluation Finished!⏹️
[13:50:35.694] iteration 21001: total_loss: 0.601579, loss_sup: 0.033939, loss_mps: 0.186359, loss_cps: 0.381280
[13:50:35.842] iteration 21002: total_loss: 0.855066, loss_sup: 0.308864, loss_mps: 0.187038, loss_cps: 0.359163
[13:50:35.987] iteration 21003: total_loss: 0.790066, loss_sup: 0.313757, loss_mps: 0.163848, loss_cps: 0.312461
[13:50:36.133] iteration 21004: total_loss: 0.377195, loss_sup: 0.163106, loss_mps: 0.077731, loss_cps: 0.136358
[13:50:36.279] iteration 21005: total_loss: 0.379754, loss_sup: 0.046135, loss_mps: 0.119722, loss_cps: 0.213897
[13:50:36.424] iteration 21006: total_loss: 0.542998, loss_sup: 0.050711, loss_mps: 0.169789, loss_cps: 0.322499
[13:50:36.569] iteration 21007: total_loss: 0.402746, loss_sup: 0.022230, loss_mps: 0.132103, loss_cps: 0.248413
[13:50:36.715] iteration 21008: total_loss: 0.683254, loss_sup: 0.116175, loss_mps: 0.199715, loss_cps: 0.367363
[13:50:36.861] iteration 21009: total_loss: 0.612482, loss_sup: 0.058359, loss_mps: 0.188775, loss_cps: 0.365349
[13:50:37.006] iteration 21010: total_loss: 0.379549, loss_sup: 0.051020, loss_mps: 0.115812, loss_cps: 0.212718
[13:50:37.151] iteration 21011: total_loss: 0.390400, loss_sup: 0.027453, loss_mps: 0.122334, loss_cps: 0.240613
[13:50:37.296] iteration 21012: total_loss: 0.641709, loss_sup: 0.075124, loss_mps: 0.191210, loss_cps: 0.375376
[13:50:37.442] iteration 21013: total_loss: 0.496133, loss_sup: 0.061242, loss_mps: 0.147516, loss_cps: 0.287375
[13:50:37.589] iteration 21014: total_loss: 0.578261, loss_sup: 0.082982, loss_mps: 0.160531, loss_cps: 0.334748
[13:50:37.735] iteration 21015: total_loss: 0.508591, loss_sup: 0.217867, loss_mps: 0.107695, loss_cps: 0.183030
[13:50:37.882] iteration 21016: total_loss: 0.614240, loss_sup: 0.106928, loss_mps: 0.168985, loss_cps: 0.338327
[13:50:38.029] iteration 21017: total_loss: 0.367364, loss_sup: 0.047643, loss_mps: 0.117982, loss_cps: 0.201740
[13:50:38.175] iteration 21018: total_loss: 0.324457, loss_sup: 0.004945, loss_mps: 0.117281, loss_cps: 0.202230
[13:50:38.320] iteration 21019: total_loss: 0.344452, loss_sup: 0.067923, loss_mps: 0.106756, loss_cps: 0.169773
[13:50:38.468] iteration 21020: total_loss: 0.521155, loss_sup: 0.042961, loss_mps: 0.164656, loss_cps: 0.313538
[13:50:38.614] iteration 21021: total_loss: 0.521236, loss_sup: 0.023114, loss_mps: 0.168681, loss_cps: 0.329441
[13:50:38.762] iteration 21022: total_loss: 0.253594, loss_sup: 0.027502, loss_mps: 0.083101, loss_cps: 0.142990
[13:50:38.908] iteration 21023: total_loss: 0.438513, loss_sup: 0.063251, loss_mps: 0.135182, loss_cps: 0.240080
[13:50:39.054] iteration 21024: total_loss: 0.595289, loss_sup: 0.039264, loss_mps: 0.185749, loss_cps: 0.370276
[13:50:39.200] iteration 21025: total_loss: 0.898261, loss_sup: 0.225272, loss_mps: 0.216684, loss_cps: 0.456305
[13:50:39.347] iteration 21026: total_loss: 0.449921, loss_sup: 0.065016, loss_mps: 0.128730, loss_cps: 0.256175
[13:50:39.493] iteration 21027: total_loss: 0.432077, loss_sup: 0.071934, loss_mps: 0.126383, loss_cps: 0.233760
[13:50:39.640] iteration 21028: total_loss: 0.419146, loss_sup: 0.033091, loss_mps: 0.136312, loss_cps: 0.249743
[13:50:39.791] iteration 21029: total_loss: 0.664514, loss_sup: 0.233038, loss_mps: 0.147069, loss_cps: 0.284407
[13:50:39.940] iteration 21030: total_loss: 0.318639, loss_sup: 0.033851, loss_mps: 0.100438, loss_cps: 0.184349
[13:50:40.086] iteration 21031: total_loss: 0.576967, loss_sup: 0.123374, loss_mps: 0.147983, loss_cps: 0.305609
[13:50:40.233] iteration 21032: total_loss: 0.485058, loss_sup: 0.083970, loss_mps: 0.140561, loss_cps: 0.260527
[13:50:40.380] iteration 21033: total_loss: 0.297296, loss_sup: 0.049878, loss_mps: 0.092346, loss_cps: 0.155073
[13:50:40.526] iteration 21034: total_loss: 0.340960, loss_sup: 0.072303, loss_mps: 0.103231, loss_cps: 0.165427
[13:50:40.672] iteration 21035: total_loss: 0.524899, loss_sup: 0.016998, loss_mps: 0.168514, loss_cps: 0.339387
[13:50:40.817] iteration 21036: total_loss: 1.367506, loss_sup: 0.117891, loss_mps: 0.369755, loss_cps: 0.879860
[13:50:40.962] iteration 21037: total_loss: 0.836570, loss_sup: 0.076649, loss_mps: 0.248483, loss_cps: 0.511438
[13:50:41.110] iteration 21038: total_loss: 0.340480, loss_sup: 0.050427, loss_mps: 0.101413, loss_cps: 0.188639
[13:50:41.256] iteration 21039: total_loss: 0.815956, loss_sup: 0.247768, loss_mps: 0.193664, loss_cps: 0.374524
[13:50:41.401] iteration 21040: total_loss: 0.652569, loss_sup: 0.048743, loss_mps: 0.196560, loss_cps: 0.407266
[13:50:41.547] iteration 21041: total_loss: 0.367360, loss_sup: 0.020331, loss_mps: 0.123454, loss_cps: 0.223575
[13:50:41.693] iteration 21042: total_loss: 0.861576, loss_sup: 0.056768, loss_mps: 0.255397, loss_cps: 0.549410
[13:50:41.839] iteration 21043: total_loss: 0.632381, loss_sup: 0.039611, loss_mps: 0.188199, loss_cps: 0.404572
[13:50:41.985] iteration 21044: total_loss: 0.397538, loss_sup: 0.094613, loss_mps: 0.104679, loss_cps: 0.198245
[13:50:42.131] iteration 21045: total_loss: 0.420193, loss_sup: 0.051162, loss_mps: 0.130774, loss_cps: 0.238257
[13:50:42.277] iteration 21046: total_loss: 0.748224, loss_sup: 0.119248, loss_mps: 0.214255, loss_cps: 0.414721
[13:50:42.422] iteration 21047: total_loss: 0.411134, loss_sup: 0.021342, loss_mps: 0.138267, loss_cps: 0.251525
[13:50:42.568] iteration 21048: total_loss: 0.859705, loss_sup: 0.257108, loss_mps: 0.192040, loss_cps: 0.410558
[13:50:42.714] iteration 21049: total_loss: 0.526782, loss_sup: 0.062524, loss_mps: 0.144356, loss_cps: 0.319902
[13:50:42.865] iteration 21050: total_loss: 0.842633, loss_sup: 0.229002, loss_mps: 0.199174, loss_cps: 0.414456
[13:50:43.012] iteration 21051: total_loss: 0.583009, loss_sup: 0.125622, loss_mps: 0.150592, loss_cps: 0.306795
[13:50:43.158] iteration 21052: total_loss: 0.872526, loss_sup: 0.180237, loss_mps: 0.222048, loss_cps: 0.470242
[13:50:43.304] iteration 21053: total_loss: 0.523266, loss_sup: 0.021699, loss_mps: 0.171300, loss_cps: 0.330267
[13:50:43.451] iteration 21054: total_loss: 0.321777, loss_sup: 0.015337, loss_mps: 0.106981, loss_cps: 0.199459
[13:50:43.596] iteration 21055: total_loss: 0.468115, loss_sup: 0.039329, loss_mps: 0.147126, loss_cps: 0.281660
[13:50:43.743] iteration 21056: total_loss: 0.523363, loss_sup: 0.026412, loss_mps: 0.166180, loss_cps: 0.330771
[13:50:43.892] iteration 21057: total_loss: 0.383011, loss_sup: 0.157351, loss_mps: 0.084269, loss_cps: 0.141390
[13:50:44.037] iteration 21058: total_loss: 0.647082, loss_sup: 0.060229, loss_mps: 0.190855, loss_cps: 0.395998
[13:50:44.183] iteration 21059: total_loss: 0.472376, loss_sup: 0.061297, loss_mps: 0.138168, loss_cps: 0.272912
[13:50:44.329] iteration 21060: total_loss: 0.323791, loss_sup: 0.019375, loss_mps: 0.108685, loss_cps: 0.195731
[13:50:44.477] iteration 21061: total_loss: 0.450302, loss_sup: 0.061357, loss_mps: 0.138286, loss_cps: 0.250660
[13:50:44.623] iteration 21062: total_loss: 0.727326, loss_sup: 0.132276, loss_mps: 0.206880, loss_cps: 0.388169
[13:50:44.769] iteration 21063: total_loss: 0.652933, loss_sup: 0.220448, loss_mps: 0.150292, loss_cps: 0.282193
[13:50:44.915] iteration 21064: total_loss: 0.357257, loss_sup: 0.011127, loss_mps: 0.121345, loss_cps: 0.224785
[13:50:45.061] iteration 21065: total_loss: 0.741964, loss_sup: 0.101990, loss_mps: 0.219909, loss_cps: 0.420065
[13:50:45.207] iteration 21066: total_loss: 0.382528, loss_sup: 0.018367, loss_mps: 0.131407, loss_cps: 0.232755
[13:50:45.354] iteration 21067: total_loss: 0.551644, loss_sup: 0.035299, loss_mps: 0.169982, loss_cps: 0.346363
[13:50:45.500] iteration 21068: total_loss: 1.063918, loss_sup: 0.195287, loss_mps: 0.280042, loss_cps: 0.588589
[13:50:45.647] iteration 21069: total_loss: 1.911009, loss_sup: 0.216684, loss_mps: 0.513104, loss_cps: 1.181221
[13:50:45.795] iteration 21070: total_loss: 0.321293, loss_sup: 0.034336, loss_mps: 0.103544, loss_cps: 0.183413
[13:50:45.941] iteration 21071: total_loss: 0.490677, loss_sup: 0.011224, loss_mps: 0.166383, loss_cps: 0.313071
[13:50:46.088] iteration 21072: total_loss: 0.422028, loss_sup: 0.067663, loss_mps: 0.123778, loss_cps: 0.230587
[13:50:46.235] iteration 21073: total_loss: 0.477454, loss_sup: 0.050691, loss_mps: 0.146086, loss_cps: 0.280677
[13:50:46.380] iteration 21074: total_loss: 0.740517, loss_sup: 0.064371, loss_mps: 0.212937, loss_cps: 0.463209
[13:50:46.527] iteration 21075: total_loss: 0.699885, loss_sup: 0.149411, loss_mps: 0.181535, loss_cps: 0.368939
[13:50:46.673] iteration 21076: total_loss: 0.666348, loss_sup: 0.040435, loss_mps: 0.197829, loss_cps: 0.428084
[13:50:46.821] iteration 21077: total_loss: 1.321086, loss_sup: 0.121240, loss_mps: 0.364529, loss_cps: 0.835318
[13:50:46.967] iteration 21078: total_loss: 0.893688, loss_sup: 0.117110, loss_mps: 0.244103, loss_cps: 0.532476
[13:50:47.113] iteration 21079: total_loss: 0.699766, loss_sup: 0.125220, loss_mps: 0.193098, loss_cps: 0.381448
[13:50:47.260] iteration 21080: total_loss: 0.894778, loss_sup: 0.048648, loss_mps: 0.271995, loss_cps: 0.574135
[13:50:47.407] iteration 21081: total_loss: 0.493977, loss_sup: 0.050847, loss_mps: 0.150107, loss_cps: 0.293023
[13:50:47.556] iteration 21082: total_loss: 0.680138, loss_sup: 0.070283, loss_mps: 0.187500, loss_cps: 0.422354
[13:50:47.705] iteration 21083: total_loss: 0.491700, loss_sup: 0.084522, loss_mps: 0.136487, loss_cps: 0.270691
[13:50:47.854] iteration 21084: total_loss: 0.568712, loss_sup: 0.109685, loss_mps: 0.156480, loss_cps: 0.302547
[13:50:48.000] iteration 21085: total_loss: 0.498427, loss_sup: 0.087930, loss_mps: 0.138277, loss_cps: 0.272220
[13:50:48.148] iteration 21086: total_loss: 0.910910, loss_sup: 0.111857, loss_mps: 0.275000, loss_cps: 0.524052
[13:50:48.296] iteration 21087: total_loss: 0.689323, loss_sup: 0.073984, loss_mps: 0.213431, loss_cps: 0.401908
[13:50:48.443] iteration 21088: total_loss: 0.807530, loss_sup: 0.172138, loss_mps: 0.218272, loss_cps: 0.417120
[13:50:48.589] iteration 21089: total_loss: 0.549896, loss_sup: 0.041116, loss_mps: 0.170913, loss_cps: 0.337867
[13:50:48.736] iteration 21090: total_loss: 0.290620, loss_sup: 0.013312, loss_mps: 0.108296, loss_cps: 0.169013
[13:50:48.882] iteration 21091: total_loss: 0.717215, loss_sup: 0.051718, loss_mps: 0.235500, loss_cps: 0.429998
[13:50:49.030] iteration 21092: total_loss: 0.626123, loss_sup: 0.079521, loss_mps: 0.192474, loss_cps: 0.354128
[13:50:49.177] iteration 21093: total_loss: 0.559836, loss_sup: 0.044979, loss_mps: 0.185860, loss_cps: 0.328997
[13:50:49.323] iteration 21094: total_loss: 0.851408, loss_sup: 0.177560, loss_mps: 0.234024, loss_cps: 0.439824
[13:50:49.469] iteration 21095: total_loss: 0.380447, loss_sup: 0.063251, loss_mps: 0.117971, loss_cps: 0.199224
[13:50:49.615] iteration 21096: total_loss: 0.734792, loss_sup: 0.026942, loss_mps: 0.218273, loss_cps: 0.489577
[13:50:49.763] iteration 21097: total_loss: 0.466593, loss_sup: 0.030739, loss_mps: 0.155261, loss_cps: 0.280594
[13:50:49.910] iteration 21098: total_loss: 1.039844, loss_sup: 0.100719, loss_mps: 0.287623, loss_cps: 0.651502
[13:50:50.057] iteration 21099: total_loss: 0.594093, loss_sup: 0.020336, loss_mps: 0.182681, loss_cps: 0.391077
[13:50:50.204] iteration 21100: total_loss: 0.470856, loss_sup: 0.051081, loss_mps: 0.145504, loss_cps: 0.274271
[13:50:50.204] Evaluation Started ==>
[13:51:01.539] ==> valid iteration 21100: unet metrics: {'dc': 0.6452934392666266, 'jc': 0.5292443117270983, 'pre': 0.7837509612010155, 'hd': 5.359324232300427}, ynet metrics: {'dc': 0.5652791637835205, 'jc': 0.4538868588954835, 'pre': 0.7720326070744934, 'hd': 5.515635794073346}.
[13:51:01.542] Evaluation Finished!⏹️
[13:51:01.692] iteration 21101: total_loss: 0.357289, loss_sup: 0.028197, loss_mps: 0.119780, loss_cps: 0.209313
[13:51:01.840] iteration 21102: total_loss: 0.642391, loss_sup: 0.114365, loss_mps: 0.186751, loss_cps: 0.341276
[13:51:01.986] iteration 21103: total_loss: 0.702497, loss_sup: 0.144706, loss_mps: 0.188150, loss_cps: 0.369641
[13:51:02.133] iteration 21104: total_loss: 0.291689, loss_sup: 0.006658, loss_mps: 0.101007, loss_cps: 0.184024
[13:51:02.280] iteration 21105: total_loss: 0.550024, loss_sup: 0.071074, loss_mps: 0.158675, loss_cps: 0.320275
[13:51:02.426] iteration 21106: total_loss: 0.803383, loss_sup: 0.070890, loss_mps: 0.237764, loss_cps: 0.494728
[13:51:02.573] iteration 21107: total_loss: 0.552569, loss_sup: 0.136475, loss_mps: 0.141194, loss_cps: 0.274900
[13:51:02.718] iteration 21108: total_loss: 0.732900, loss_sup: 0.118823, loss_mps: 0.200576, loss_cps: 0.413501
[13:51:02.863] iteration 21109: total_loss: 0.483154, loss_sup: 0.047626, loss_mps: 0.154162, loss_cps: 0.281366
[13:51:03.009] iteration 21110: total_loss: 0.701033, loss_sup: 0.094273, loss_mps: 0.203994, loss_cps: 0.402766
[13:51:03.155] iteration 21111: total_loss: 0.487990, loss_sup: 0.104855, loss_mps: 0.146206, loss_cps: 0.236929
[13:51:03.302] iteration 21112: total_loss: 0.543592, loss_sup: 0.141552, loss_mps: 0.146762, loss_cps: 0.255278
[13:51:03.448] iteration 21113: total_loss: 0.595198, loss_sup: 0.204592, loss_mps: 0.152211, loss_cps: 0.238395
[13:51:03.599] iteration 21114: total_loss: 0.597065, loss_sup: 0.060180, loss_mps: 0.189721, loss_cps: 0.347163
[13:51:03.745] iteration 21115: total_loss: 0.992328, loss_sup: 0.109908, loss_mps: 0.285515, loss_cps: 0.596906
[13:51:03.890] iteration 21116: total_loss: 0.445664, loss_sup: 0.034745, loss_mps: 0.139902, loss_cps: 0.271017
[13:51:04.037] iteration 21117: total_loss: 0.255609, loss_sup: 0.047718, loss_mps: 0.078006, loss_cps: 0.129885
[13:51:04.184] iteration 21118: total_loss: 0.937630, loss_sup: 0.168958, loss_mps: 0.250616, loss_cps: 0.518056
[13:51:04.332] iteration 21119: total_loss: 0.443319, loss_sup: 0.094446, loss_mps: 0.123879, loss_cps: 0.224994
[13:51:04.479] iteration 21120: total_loss: 0.398662, loss_sup: 0.017311, loss_mps: 0.142148, loss_cps: 0.239203
[13:51:04.627] iteration 21121: total_loss: 0.473343, loss_sup: 0.019867, loss_mps: 0.166729, loss_cps: 0.286746
[13:51:04.776] iteration 21122: total_loss: 0.520273, loss_sup: 0.013065, loss_mps: 0.168880, loss_cps: 0.338329
[13:51:04.922] iteration 21123: total_loss: 0.294964, loss_sup: 0.034870, loss_mps: 0.093922, loss_cps: 0.166172
[13:51:05.067] iteration 21124: total_loss: 0.659523, loss_sup: 0.210642, loss_mps: 0.157000, loss_cps: 0.291882
[13:51:05.213] iteration 21125: total_loss: 0.531013, loss_sup: 0.081198, loss_mps: 0.159586, loss_cps: 0.290228
[13:51:05.358] iteration 21126: total_loss: 0.476953, loss_sup: 0.131413, loss_mps: 0.124906, loss_cps: 0.220634
[13:51:05.504] iteration 21127: total_loss: 0.523606, loss_sup: 0.047265, loss_mps: 0.162166, loss_cps: 0.314174
[13:51:05.650] iteration 21128: total_loss: 0.423530, loss_sup: 0.122419, loss_mps: 0.112238, loss_cps: 0.188873
[13:51:05.795] iteration 21129: total_loss: 0.790967, loss_sup: 0.289211, loss_mps: 0.181020, loss_cps: 0.320735
[13:51:05.942] iteration 21130: total_loss: 0.557521, loss_sup: 0.139859, loss_mps: 0.149364, loss_cps: 0.268298
[13:51:06.088] iteration 21131: total_loss: 0.556132, loss_sup: 0.077752, loss_mps: 0.162441, loss_cps: 0.315939
[13:51:06.234] iteration 21132: total_loss: 0.865485, loss_sup: 0.441861, loss_mps: 0.145196, loss_cps: 0.278429
[13:51:06.381] iteration 21133: total_loss: 0.249071, loss_sup: 0.010116, loss_mps: 0.087796, loss_cps: 0.151159
[13:51:06.529] iteration 21134: total_loss: 0.339954, loss_sup: 0.005833, loss_mps: 0.118081, loss_cps: 0.216041
[13:51:06.675] iteration 21135: total_loss: 0.822443, loss_sup: 0.281229, loss_mps: 0.186847, loss_cps: 0.354367
[13:51:06.821] iteration 21136: total_loss: 0.397196, loss_sup: 0.095520, loss_mps: 0.112809, loss_cps: 0.188866
[13:51:06.970] iteration 21137: total_loss: 0.874523, loss_sup: 0.135239, loss_mps: 0.244787, loss_cps: 0.494498
[13:51:07.116] iteration 21138: total_loss: 0.391874, loss_sup: 0.024123, loss_mps: 0.130394, loss_cps: 0.237357
[13:51:07.262] iteration 21139: total_loss: 0.340109, loss_sup: 0.034072, loss_mps: 0.111679, loss_cps: 0.194358
[13:51:07.407] iteration 21140: total_loss: 0.592634, loss_sup: 0.064142, loss_mps: 0.190197, loss_cps: 0.338295
[13:51:07.553] iteration 21141: total_loss: 0.465605, loss_sup: 0.002187, loss_mps: 0.155692, loss_cps: 0.307726
[13:51:07.700] iteration 21142: total_loss: 0.635780, loss_sup: 0.089747, loss_mps: 0.176845, loss_cps: 0.369188
[13:51:07.846] iteration 21143: total_loss: 0.667908, loss_sup: 0.177467, loss_mps: 0.162483, loss_cps: 0.327958
[13:51:07.994] iteration 21144: total_loss: 0.470943, loss_sup: 0.101339, loss_mps: 0.129451, loss_cps: 0.240153
[13:51:08.146] iteration 21145: total_loss: 0.307954, loss_sup: 0.060958, loss_mps: 0.095900, loss_cps: 0.151097
[13:51:08.292] iteration 21146: total_loss: 1.459927, loss_sup: 0.119205, loss_mps: 0.419362, loss_cps: 0.921361
[13:51:08.441] iteration 21147: total_loss: 0.584063, loss_sup: 0.006614, loss_mps: 0.184892, loss_cps: 0.392558
[13:51:08.590] iteration 21148: total_loss: 0.445376, loss_sup: 0.040234, loss_mps: 0.145174, loss_cps: 0.259968
[13:51:08.735] iteration 21149: total_loss: 0.459521, loss_sup: 0.041352, loss_mps: 0.138066, loss_cps: 0.280102
[13:51:08.881] iteration 21150: total_loss: 0.775235, loss_sup: 0.075491, loss_mps: 0.220317, loss_cps: 0.479427
[13:51:09.027] iteration 21151: total_loss: 0.414888, loss_sup: 0.003388, loss_mps: 0.145394, loss_cps: 0.266106
[13:51:09.173] iteration 21152: total_loss: 0.404688, loss_sup: 0.076431, loss_mps: 0.120460, loss_cps: 0.207797
[13:51:09.319] iteration 21153: total_loss: 0.453665, loss_sup: 0.042703, loss_mps: 0.141530, loss_cps: 0.269432
[13:51:09.465] iteration 21154: total_loss: 0.661187, loss_sup: 0.123313, loss_mps: 0.178689, loss_cps: 0.359184
[13:51:09.612] iteration 21155: total_loss: 0.808901, loss_sup: 0.140018, loss_mps: 0.210770, loss_cps: 0.458114
[13:51:09.758] iteration 21156: total_loss: 0.709076, loss_sup: 0.080099, loss_mps: 0.215763, loss_cps: 0.413214
[13:51:09.905] iteration 21157: total_loss: 0.665690, loss_sup: 0.081064, loss_mps: 0.183198, loss_cps: 0.401427
[13:51:10.052] iteration 21158: total_loss: 0.430432, loss_sup: 0.128934, loss_mps: 0.109470, loss_cps: 0.192028
[13:51:10.198] iteration 21159: total_loss: 0.617034, loss_sup: 0.051174, loss_mps: 0.194113, loss_cps: 0.371747
[13:51:10.344] iteration 21160: total_loss: 0.391284, loss_sup: 0.027640, loss_mps: 0.132680, loss_cps: 0.230964
[13:51:10.489] iteration 21161: total_loss: 0.405020, loss_sup: 0.094041, loss_mps: 0.111618, loss_cps: 0.199361
[13:51:10.634] iteration 21162: total_loss: 0.462193, loss_sup: 0.004203, loss_mps: 0.152675, loss_cps: 0.305315
[13:51:10.781] iteration 21163: total_loss: 0.837096, loss_sup: 0.236132, loss_mps: 0.194280, loss_cps: 0.406684
[13:51:10.927] iteration 21164: total_loss: 0.706116, loss_sup: 0.057483, loss_mps: 0.212912, loss_cps: 0.435721
[13:51:11.074] iteration 21165: total_loss: 0.834319, loss_sup: 0.074600, loss_mps: 0.242367, loss_cps: 0.517352
[13:51:11.220] iteration 21166: total_loss: 0.458909, loss_sup: 0.047994, loss_mps: 0.135253, loss_cps: 0.275662
[13:51:11.366] iteration 21167: total_loss: 0.512859, loss_sup: 0.081899, loss_mps: 0.150161, loss_cps: 0.280799
[13:51:11.514] iteration 21168: total_loss: 0.407279, loss_sup: 0.002951, loss_mps: 0.142271, loss_cps: 0.262057
[13:51:11.660] iteration 21169: total_loss: 0.571835, loss_sup: 0.041016, loss_mps: 0.183879, loss_cps: 0.346939
[13:51:11.806] iteration 21170: total_loss: 0.634375, loss_sup: 0.120840, loss_mps: 0.172221, loss_cps: 0.341315
[13:51:11.952] iteration 21171: total_loss: 0.604338, loss_sup: 0.037567, loss_mps: 0.175089, loss_cps: 0.391682
[13:51:12.099] iteration 21172: total_loss: 0.421215, loss_sup: 0.112193, loss_mps: 0.108069, loss_cps: 0.200952
[13:51:12.245] iteration 21173: total_loss: 0.461221, loss_sup: 0.121257, loss_mps: 0.122524, loss_cps: 0.217440
[13:51:12.390] iteration 21174: total_loss: 0.365067, loss_sup: 0.019139, loss_mps: 0.119757, loss_cps: 0.226171
[13:51:12.537] iteration 21175: total_loss: 0.767369, loss_sup: 0.065976, loss_mps: 0.237392, loss_cps: 0.464001
[13:51:12.683] iteration 21176: total_loss: 0.425343, loss_sup: 0.048845, loss_mps: 0.127042, loss_cps: 0.249455
[13:51:12.829] iteration 21177: total_loss: 0.436702, loss_sup: 0.017706, loss_mps: 0.150485, loss_cps: 0.268511
[13:51:12.975] iteration 21178: total_loss: 0.537370, loss_sup: 0.059475, loss_mps: 0.162593, loss_cps: 0.315303
[13:51:13.121] iteration 21179: total_loss: 0.355157, loss_sup: 0.035679, loss_mps: 0.115591, loss_cps: 0.203886
[13:51:13.268] iteration 21180: total_loss: 0.333476, loss_sup: 0.055089, loss_mps: 0.096661, loss_cps: 0.181725
[13:51:13.414] iteration 21181: total_loss: 0.505397, loss_sup: 0.023862, loss_mps: 0.170550, loss_cps: 0.310985
[13:51:13.560] iteration 21182: total_loss: 0.594264, loss_sup: 0.087763, loss_mps: 0.171787, loss_cps: 0.334713
[13:51:13.706] iteration 21183: total_loss: 0.573468, loss_sup: 0.063918, loss_mps: 0.173967, loss_cps: 0.335584
[13:51:13.852] iteration 21184: total_loss: 0.384488, loss_sup: 0.021652, loss_mps: 0.126027, loss_cps: 0.236809
[13:51:13.997] iteration 21185: total_loss: 0.598549, loss_sup: 0.178692, loss_mps: 0.147267, loss_cps: 0.272591
[13:51:14.144] iteration 21186: total_loss: 0.629440, loss_sup: 0.039760, loss_mps: 0.205188, loss_cps: 0.384492
[13:51:14.291] iteration 21187: total_loss: 0.352650, loss_sup: 0.098857, loss_mps: 0.095307, loss_cps: 0.158486
[13:51:14.440] iteration 21188: total_loss: 0.806773, loss_sup: 0.241118, loss_mps: 0.196791, loss_cps: 0.368863
[13:51:14.592] iteration 21189: total_loss: 0.496303, loss_sup: 0.052858, loss_mps: 0.152832, loss_cps: 0.290613
[13:51:14.739] iteration 21190: total_loss: 0.525862, loss_sup: 0.051026, loss_mps: 0.161996, loss_cps: 0.312840
[13:51:14.885] iteration 21191: total_loss: 0.341277, loss_sup: 0.017063, loss_mps: 0.110092, loss_cps: 0.214123
[13:51:15.031] iteration 21192: total_loss: 0.357787, loss_sup: 0.038959, loss_mps: 0.110717, loss_cps: 0.208112
[13:51:15.178] iteration 21193: total_loss: 0.485269, loss_sup: 0.083856, loss_mps: 0.143331, loss_cps: 0.258082
[13:51:15.325] iteration 21194: total_loss: 0.319156, loss_sup: 0.059252, loss_mps: 0.096472, loss_cps: 0.163432
[13:51:15.472] iteration 21195: total_loss: 1.113411, loss_sup: 0.403212, loss_mps: 0.240316, loss_cps: 0.469883
[13:51:15.620] iteration 21196: total_loss: 0.615303, loss_sup: 0.068934, loss_mps: 0.179517, loss_cps: 0.366852
[13:51:15.767] iteration 21197: total_loss: 0.338545, loss_sup: 0.022662, loss_mps: 0.114781, loss_cps: 0.201102
[13:51:15.914] iteration 21198: total_loss: 0.704577, loss_sup: 0.030838, loss_mps: 0.224224, loss_cps: 0.449516
[13:51:16.060] iteration 21199: total_loss: 0.758440, loss_sup: 0.162259, loss_mps: 0.197819, loss_cps: 0.398361
[13:51:16.207] iteration 21200: total_loss: 0.482985, loss_sup: 0.096685, loss_mps: 0.134482, loss_cps: 0.251817
[13:51:16.207] Evaluation Started ==>
[13:51:27.526] ==> valid iteration 21200: unet metrics: {'dc': 0.650332232717399, 'jc': 0.5372686355307464, 'pre': 0.8029633823848356, 'hd': 5.445149100551559}, ynet metrics: {'dc': 0.6476965079988486, 'jc': 0.5300854791477789, 'pre': 0.8025728986738017, 'hd': 5.577813935282664}.
[13:51:27.528] Evaluation Finished!⏹️
[13:51:27.679] iteration 21201: total_loss: 0.414485, loss_sup: 0.005641, loss_mps: 0.135034, loss_cps: 0.273809
[13:51:27.826] iteration 21202: total_loss: 0.976961, loss_sup: 0.076075, loss_mps: 0.278899, loss_cps: 0.621987
[13:51:27.972] iteration 21203: total_loss: 0.419977, loss_sup: 0.012115, loss_mps: 0.137949, loss_cps: 0.269913
[13:51:28.118] iteration 21204: total_loss: 0.494301, loss_sup: 0.017835, loss_mps: 0.158570, loss_cps: 0.317896
[13:51:28.263] iteration 21205: total_loss: 0.676823, loss_sup: 0.056193, loss_mps: 0.201219, loss_cps: 0.419410
[13:51:28.410] iteration 21206: total_loss: 0.433759, loss_sup: 0.047005, loss_mps: 0.133258, loss_cps: 0.253497
[13:51:28.557] iteration 21207: total_loss: 0.483736, loss_sup: 0.044841, loss_mps: 0.152245, loss_cps: 0.286650
[13:51:28.702] iteration 21208: total_loss: 0.726959, loss_sup: 0.173609, loss_mps: 0.183976, loss_cps: 0.369373
[13:51:28.849] iteration 21209: total_loss: 0.536960, loss_sup: 0.046244, loss_mps: 0.164813, loss_cps: 0.325903
[13:51:28.995] iteration 21210: total_loss: 0.551495, loss_sup: 0.026894, loss_mps: 0.172652, loss_cps: 0.351950
[13:51:29.141] iteration 21211: total_loss: 0.650793, loss_sup: 0.107217, loss_mps: 0.180348, loss_cps: 0.363227
[13:51:29.286] iteration 21212: total_loss: 0.557469, loss_sup: 0.121915, loss_mps: 0.148994, loss_cps: 0.286559
[13:51:29.432] iteration 21213: total_loss: 0.346017, loss_sup: 0.063831, loss_mps: 0.100709, loss_cps: 0.181477
[13:51:29.578] iteration 21214: total_loss: 0.818257, loss_sup: 0.041986, loss_mps: 0.246473, loss_cps: 0.529797
[13:51:29.724] iteration 21215: total_loss: 1.109413, loss_sup: 0.031043, loss_mps: 0.326391, loss_cps: 0.751979
[13:51:29.869] iteration 21216: total_loss: 0.718447, loss_sup: 0.158097, loss_mps: 0.176867, loss_cps: 0.383484
[13:51:30.015] iteration 21217: total_loss: 0.537489, loss_sup: 0.086930, loss_mps: 0.158405, loss_cps: 0.292153
[13:51:30.161] iteration 21218: total_loss: 0.560383, loss_sup: 0.003129, loss_mps: 0.183546, loss_cps: 0.373708
[13:51:30.307] iteration 21219: total_loss: 0.484889, loss_sup: 0.030940, loss_mps: 0.155398, loss_cps: 0.298551
[13:51:30.453] iteration 21220: total_loss: 0.650303, loss_sup: 0.092166, loss_mps: 0.181082, loss_cps: 0.377055
[13:51:30.598] iteration 21221: total_loss: 0.739289, loss_sup: 0.005357, loss_mps: 0.217573, loss_cps: 0.516359
[13:51:30.745] iteration 21222: total_loss: 1.069426, loss_sup: 0.247081, loss_mps: 0.250154, loss_cps: 0.572191
[13:51:30.891] iteration 21223: total_loss: 0.659278, loss_sup: 0.031941, loss_mps: 0.200025, loss_cps: 0.427312
[13:51:31.039] iteration 21224: total_loss: 0.737991, loss_sup: 0.242506, loss_mps: 0.161371, loss_cps: 0.334114
[13:51:31.185] iteration 21225: total_loss: 0.506237, loss_sup: 0.010100, loss_mps: 0.165182, loss_cps: 0.330955
[13:51:31.330] iteration 21226: total_loss: 0.910953, loss_sup: 0.067612, loss_mps: 0.272479, loss_cps: 0.570862
[13:51:31.476] iteration 21227: total_loss: 0.657905, loss_sup: 0.079308, loss_mps: 0.182671, loss_cps: 0.395926
[13:51:31.621] iteration 21228: total_loss: 0.726136, loss_sup: 0.043964, loss_mps: 0.218072, loss_cps: 0.464100
[13:51:31.767] iteration 21229: total_loss: 0.524959, loss_sup: 0.088555, loss_mps: 0.150101, loss_cps: 0.286303
[13:51:31.913] iteration 21230: total_loss: 0.621830, loss_sup: 0.107740, loss_mps: 0.184903, loss_cps: 0.329187
[13:51:32.058] iteration 21231: total_loss: 0.686044, loss_sup: 0.158683, loss_mps: 0.186338, loss_cps: 0.341023
[13:51:32.204] iteration 21232: total_loss: 0.325508, loss_sup: 0.020278, loss_mps: 0.115401, loss_cps: 0.189829
[13:51:32.351] iteration 21233: total_loss: 0.696862, loss_sup: 0.076453, loss_mps: 0.197872, loss_cps: 0.422537
[13:51:32.497] iteration 21234: total_loss: 0.373982, loss_sup: 0.122029, loss_mps: 0.096595, loss_cps: 0.155358
[13:51:32.645] iteration 21235: total_loss: 0.304152, loss_sup: 0.054541, loss_mps: 0.093264, loss_cps: 0.156347
[13:51:32.792] iteration 21236: total_loss: 0.527649, loss_sup: 0.162557, loss_mps: 0.137682, loss_cps: 0.227409
[13:51:32.938] iteration 21237: total_loss: 0.719677, loss_sup: 0.040334, loss_mps: 0.223330, loss_cps: 0.456014
[13:51:33.085] iteration 21238: total_loss: 0.439215, loss_sup: 0.030408, loss_mps: 0.141012, loss_cps: 0.267795
[13:51:33.231] iteration 21239: total_loss: 0.474218, loss_sup: 0.148177, loss_mps: 0.118012, loss_cps: 0.208029
[13:51:33.378] iteration 21240: total_loss: 0.508455, loss_sup: 0.017807, loss_mps: 0.169535, loss_cps: 0.321113
[13:51:33.524] iteration 21241: total_loss: 1.115210, loss_sup: 0.385983, loss_mps: 0.253362, loss_cps: 0.475866
[13:51:33.670] iteration 21242: total_loss: 0.499290, loss_sup: 0.066914, loss_mps: 0.150423, loss_cps: 0.281953
[13:51:33.817] iteration 21243: total_loss: 0.591944, loss_sup: 0.112234, loss_mps: 0.172523, loss_cps: 0.307187
[13:51:33.963] iteration 21244: total_loss: 0.462583, loss_sup: 0.051883, loss_mps: 0.143458, loss_cps: 0.267241
[13:51:34.110] iteration 21245: total_loss: 0.824456, loss_sup: 0.032129, loss_mps: 0.260040, loss_cps: 0.532287
[13:51:34.257] iteration 21246: total_loss: 0.357364, loss_sup: 0.039601, loss_mps: 0.119938, loss_cps: 0.197826
[13:51:34.403] iteration 21247: total_loss: 0.765731, loss_sup: 0.096117, loss_mps: 0.215098, loss_cps: 0.454516
[13:51:34.548] iteration 21248: total_loss: 0.842103, loss_sup: 0.108560, loss_mps: 0.237492, loss_cps: 0.496051
[13:51:34.694] iteration 21249: total_loss: 0.563634, loss_sup: 0.117524, loss_mps: 0.151569, loss_cps: 0.294541
[13:51:34.842] iteration 21250: total_loss: 0.506141, loss_sup: 0.027410, loss_mps: 0.168862, loss_cps: 0.309869
[13:51:34.989] iteration 21251: total_loss: 0.324780, loss_sup: 0.060272, loss_mps: 0.102465, loss_cps: 0.162042
[13:51:35.134] iteration 21252: total_loss: 0.291158, loss_sup: 0.027699, loss_mps: 0.102023, loss_cps: 0.161436
[13:51:35.280] iteration 21253: total_loss: 0.334450, loss_sup: 0.027800, loss_mps: 0.111777, loss_cps: 0.194874
[13:51:35.425] iteration 21254: total_loss: 1.016917, loss_sup: 0.153482, loss_mps: 0.276162, loss_cps: 0.587273
[13:51:35.571] iteration 21255: total_loss: 0.254948, loss_sup: 0.020979, loss_mps: 0.087875, loss_cps: 0.146095
[13:51:35.717] iteration 21256: total_loss: 0.638296, loss_sup: 0.067269, loss_mps: 0.189963, loss_cps: 0.381064
[13:51:35.864] iteration 21257: total_loss: 0.305499, loss_sup: 0.005745, loss_mps: 0.113289, loss_cps: 0.186465
[13:51:36.010] iteration 21258: total_loss: 0.386019, loss_sup: 0.033501, loss_mps: 0.126764, loss_cps: 0.225753
[13:51:36.156] iteration 21259: total_loss: 0.309223, loss_sup: 0.004542, loss_mps: 0.113063, loss_cps: 0.191618
[13:51:36.302] iteration 21260: total_loss: 0.571889, loss_sup: 0.082221, loss_mps: 0.176794, loss_cps: 0.312874
[13:51:36.447] iteration 21261: total_loss: 0.516379, loss_sup: 0.013985, loss_mps: 0.177464, loss_cps: 0.324930
[13:51:36.593] iteration 21262: total_loss: 0.499618, loss_sup: 0.160464, loss_mps: 0.117051, loss_cps: 0.222103
[13:51:36.739] iteration 21263: total_loss: 0.313514, loss_sup: 0.031351, loss_mps: 0.105697, loss_cps: 0.176467
[13:51:36.885] iteration 21264: total_loss: 0.531413, loss_sup: 0.078748, loss_mps: 0.154300, loss_cps: 0.298365
[13:51:37.031] iteration 21265: total_loss: 0.541853, loss_sup: 0.062795, loss_mps: 0.159500, loss_cps: 0.319558
[13:51:37.177] iteration 21266: total_loss: 0.508593, loss_sup: 0.074288, loss_mps: 0.144378, loss_cps: 0.289927
[13:51:37.323] iteration 21267: total_loss: 0.464993, loss_sup: 0.017868, loss_mps: 0.146559, loss_cps: 0.300566
[13:51:37.468] iteration 21268: total_loss: 0.643339, loss_sup: 0.047254, loss_mps: 0.195568, loss_cps: 0.400516
[13:51:37.614] iteration 21269: total_loss: 0.744829, loss_sup: 0.332237, loss_mps: 0.138234, loss_cps: 0.274358
[13:51:37.761] iteration 21270: total_loss: 0.328535, loss_sup: 0.078434, loss_mps: 0.094186, loss_cps: 0.155915
[13:51:37.907] iteration 21271: total_loss: 0.306779, loss_sup: 0.028545, loss_mps: 0.102021, loss_cps: 0.176212
[13:51:38.053] iteration 21272: total_loss: 0.723777, loss_sup: 0.151205, loss_mps: 0.184719, loss_cps: 0.387853
[13:51:38.199] iteration 21273: total_loss: 0.436937, loss_sup: 0.057576, loss_mps: 0.143938, loss_cps: 0.235423
[13:51:38.344] iteration 21274: total_loss: 0.348509, loss_sup: 0.035075, loss_mps: 0.118023, loss_cps: 0.195412
[13:51:38.490] iteration 21275: total_loss: 0.331160, loss_sup: 0.068009, loss_mps: 0.097187, loss_cps: 0.165963
[13:51:38.636] iteration 21276: total_loss: 0.622554, loss_sup: 0.055702, loss_mps: 0.188377, loss_cps: 0.378474
[13:51:38.782] iteration 21277: total_loss: 0.635039, loss_sup: 0.142875, loss_mps: 0.160782, loss_cps: 0.331383
[13:51:38.931] iteration 21278: total_loss: 0.549986, loss_sup: 0.171359, loss_mps: 0.138225, loss_cps: 0.240402
[13:51:39.077] iteration 21279: total_loss: 0.570433, loss_sup: 0.106765, loss_mps: 0.159801, loss_cps: 0.303867
[13:51:39.223] iteration 21280: total_loss: 0.865714, loss_sup: 0.058794, loss_mps: 0.255040, loss_cps: 0.551879
[13:51:39.369] iteration 21281: total_loss: 0.411802, loss_sup: 0.106620, loss_mps: 0.107857, loss_cps: 0.197324
[13:51:39.517] iteration 21282: total_loss: 0.399999, loss_sup: 0.062303, loss_mps: 0.120106, loss_cps: 0.217590
[13:51:39.662] iteration 21283: total_loss: 1.167132, loss_sup: 0.515419, loss_mps: 0.217616, loss_cps: 0.434097
[13:51:39.808] iteration 21284: total_loss: 0.206586, loss_sup: 0.002924, loss_mps: 0.078019, loss_cps: 0.125642
[13:51:39.954] iteration 21285: total_loss: 0.401422, loss_sup: 0.044379, loss_mps: 0.124972, loss_cps: 0.232071
[13:51:40.099] iteration 21286: total_loss: 0.337624, loss_sup: 0.075367, loss_mps: 0.103301, loss_cps: 0.158955
[13:51:40.247] iteration 21287: total_loss: 0.382432, loss_sup: 0.090364, loss_mps: 0.105060, loss_cps: 0.187008
[13:51:40.395] iteration 21288: total_loss: 0.227456, loss_sup: 0.003034, loss_mps: 0.084388, loss_cps: 0.140033
[13:51:40.543] iteration 21289: total_loss: 0.315896, loss_sup: 0.041548, loss_mps: 0.096168, loss_cps: 0.178180
[13:51:40.689] iteration 21290: total_loss: 0.262941, loss_sup: 0.034950, loss_mps: 0.088324, loss_cps: 0.139667
[13:51:40.836] iteration 21291: total_loss: 0.764036, loss_sup: 0.038261, loss_mps: 0.225316, loss_cps: 0.500459
[13:51:40.982] iteration 21292: total_loss: 0.297487, loss_sup: 0.012800, loss_mps: 0.104332, loss_cps: 0.180355
[13:51:41.128] iteration 21293: total_loss: 0.497615, loss_sup: 0.104446, loss_mps: 0.132053, loss_cps: 0.261115
[13:51:41.274] iteration 21294: total_loss: 0.285198, loss_sup: 0.021070, loss_mps: 0.091164, loss_cps: 0.172964
[13:51:41.422] iteration 21295: total_loss: 0.593529, loss_sup: 0.148751, loss_mps: 0.158217, loss_cps: 0.286562
[13:51:41.570] iteration 21296: total_loss: 0.227583, loss_sup: 0.044383, loss_mps: 0.070333, loss_cps: 0.112867
[13:51:41.717] iteration 21297: total_loss: 0.343711, loss_sup: 0.052171, loss_mps: 0.103553, loss_cps: 0.187987
[13:51:41.863] iteration 21298: total_loss: 0.415038, loss_sup: 0.095092, loss_mps: 0.115031, loss_cps: 0.204915
[13:51:42.009] iteration 21299: total_loss: 0.407395, loss_sup: 0.024689, loss_mps: 0.134342, loss_cps: 0.248365
[13:51:42.155] iteration 21300: total_loss: 0.694341, loss_sup: 0.036535, loss_mps: 0.206092, loss_cps: 0.451714
[13:51:42.156] Evaluation Started ==>
[13:51:53.552] ==> valid iteration 21300: unet metrics: {'dc': 0.6667630848528343, 'jc': 0.5526223948907738, 'pre': 0.7853322343152347, 'hd': 5.44564710750606}, ynet metrics: {'dc': 0.6003835930636652, 'jc': 0.4854119432696044, 'pre': 0.8126127357830027, 'hd': 5.36094225963301}.
[13:51:53.554] Evaluation Finished!⏹️
[13:51:53.706] iteration 21301: total_loss: 0.273403, loss_sup: 0.033038, loss_mps: 0.086940, loss_cps: 0.153426
[13:51:53.853] iteration 21302: total_loss: 0.513158, loss_sup: 0.102796, loss_mps: 0.133743, loss_cps: 0.276619
[13:51:54.001] iteration 21303: total_loss: 0.532519, loss_sup: 0.039244, loss_mps: 0.169092, loss_cps: 0.324184
[13:51:54.146] iteration 21304: total_loss: 0.477086, loss_sup: 0.015687, loss_mps: 0.151468, loss_cps: 0.309931
[13:51:54.297] iteration 21305: total_loss: 0.572238, loss_sup: 0.043534, loss_mps: 0.179043, loss_cps: 0.349662
[13:51:54.442] iteration 21306: total_loss: 0.402260, loss_sup: 0.028385, loss_mps: 0.128504, loss_cps: 0.245371
[13:51:54.587] iteration 21307: total_loss: 0.417145, loss_sup: 0.086254, loss_mps: 0.117307, loss_cps: 0.213585
[13:51:54.733] iteration 21308: total_loss: 1.034598, loss_sup: 0.326530, loss_mps: 0.224227, loss_cps: 0.483841
[13:51:54.878] iteration 21309: total_loss: 0.507246, loss_sup: 0.087531, loss_mps: 0.146794, loss_cps: 0.272921
[13:51:55.023] iteration 21310: total_loss: 0.478902, loss_sup: 0.140797, loss_mps: 0.124351, loss_cps: 0.213753
[13:51:55.170] iteration 21311: total_loss: 0.882236, loss_sup: 0.350419, loss_mps: 0.183891, loss_cps: 0.347925
[13:51:55.315] iteration 21312: total_loss: 0.434310, loss_sup: 0.072624, loss_mps: 0.124605, loss_cps: 0.237081
[13:51:55.460] iteration 21313: total_loss: 0.615318, loss_sup: 0.123973, loss_mps: 0.161692, loss_cps: 0.329653
[13:51:55.605] iteration 21314: total_loss: 0.783836, loss_sup: 0.064969, loss_mps: 0.195316, loss_cps: 0.523552
[13:51:55.751] iteration 21315: total_loss: 0.295519, loss_sup: 0.018738, loss_mps: 0.098959, loss_cps: 0.177822
[13:51:55.897] iteration 21316: total_loss: 0.555316, loss_sup: 0.072065, loss_mps: 0.154574, loss_cps: 0.328677
[13:51:56.042] iteration 21317: total_loss: 0.530180, loss_sup: 0.055354, loss_mps: 0.162387, loss_cps: 0.312438
[13:51:56.105] iteration 21318: total_loss: 0.503158, loss_sup: 0.056047, loss_mps: 0.169256, loss_cps: 0.277855
[13:51:57.350] iteration 21319: total_loss: 0.790629, loss_sup: 0.213875, loss_mps: 0.190376, loss_cps: 0.386379
[13:51:57.498] iteration 21320: total_loss: 0.558532, loss_sup: 0.135881, loss_mps: 0.148017, loss_cps: 0.274634
[13:51:57.644] iteration 21321: total_loss: 0.438994, loss_sup: 0.135906, loss_mps: 0.110529, loss_cps: 0.192559
[13:51:57.791] iteration 21322: total_loss: 0.332119, loss_sup: 0.014581, loss_mps: 0.114210, loss_cps: 0.203328
[13:51:57.937] iteration 21323: total_loss: 0.342760, loss_sup: 0.024156, loss_mps: 0.114363, loss_cps: 0.204241
[13:51:58.083] iteration 21324: total_loss: 0.435888, loss_sup: 0.040698, loss_mps: 0.135401, loss_cps: 0.259790
[13:51:58.229] iteration 21325: total_loss: 0.754976, loss_sup: 0.021888, loss_mps: 0.236730, loss_cps: 0.496358
[13:51:58.377] iteration 21326: total_loss: 0.356172, loss_sup: 0.027930, loss_mps: 0.115777, loss_cps: 0.212464
[13:51:58.524] iteration 21327: total_loss: 0.821299, loss_sup: 0.055445, loss_mps: 0.256225, loss_cps: 0.509628
[13:51:58.669] iteration 21328: total_loss: 0.690260, loss_sup: 0.180044, loss_mps: 0.187598, loss_cps: 0.322618
[13:51:58.815] iteration 21329: total_loss: 0.487289, loss_sup: 0.035324, loss_mps: 0.153773, loss_cps: 0.298193
[13:51:58.964] iteration 21330: total_loss: 0.626890, loss_sup: 0.103908, loss_mps: 0.181239, loss_cps: 0.341743
[13:51:59.109] iteration 21331: total_loss: 0.934132, loss_sup: 0.062425, loss_mps: 0.261327, loss_cps: 0.610379
[13:51:59.255] iteration 21332: total_loss: 0.926970, loss_sup: 0.170650, loss_mps: 0.245814, loss_cps: 0.510507
[13:51:59.400] iteration 21333: total_loss: 0.400485, loss_sup: 0.025038, loss_mps: 0.140818, loss_cps: 0.234629
[13:51:59.545] iteration 21334: total_loss: 0.718616, loss_sup: 0.019129, loss_mps: 0.231121, loss_cps: 0.468366
[13:51:59.691] iteration 21335: total_loss: 0.627335, loss_sup: 0.073010, loss_mps: 0.191607, loss_cps: 0.362717
[13:51:59.837] iteration 21336: total_loss: 0.475348, loss_sup: 0.044408, loss_mps: 0.145333, loss_cps: 0.285607
[13:51:59.983] iteration 21337: total_loss: 1.160039, loss_sup: 0.363544, loss_mps: 0.258197, loss_cps: 0.538297
[13:52:00.129] iteration 21338: total_loss: 0.693988, loss_sup: 0.077532, loss_mps: 0.212391, loss_cps: 0.404065
[13:52:00.274] iteration 21339: total_loss: 0.264449, loss_sup: 0.001531, loss_mps: 0.092686, loss_cps: 0.170232
[13:52:00.420] iteration 21340: total_loss: 0.417584, loss_sup: 0.019013, loss_mps: 0.146215, loss_cps: 0.252356
[13:52:00.565] iteration 21341: total_loss: 0.415561, loss_sup: 0.018557, loss_mps: 0.139815, loss_cps: 0.257189
[13:52:00.711] iteration 21342: total_loss: 1.641764, loss_sup: 0.058777, loss_mps: 0.450525, loss_cps: 1.132463
[13:52:00.857] iteration 21343: total_loss: 0.399267, loss_sup: 0.021721, loss_mps: 0.135330, loss_cps: 0.242216
[13:52:01.002] iteration 21344: total_loss: 0.456322, loss_sup: 0.049390, loss_mps: 0.139751, loss_cps: 0.267181
[13:52:01.152] iteration 21345: total_loss: 0.325471, loss_sup: 0.067811, loss_mps: 0.097229, loss_cps: 0.160431
[13:52:01.298] iteration 21346: total_loss: 0.305416, loss_sup: 0.023910, loss_mps: 0.100400, loss_cps: 0.181106
[13:52:01.444] iteration 21347: total_loss: 0.559400, loss_sup: 0.053109, loss_mps: 0.163061, loss_cps: 0.343229
[13:52:01.590] iteration 21348: total_loss: 0.495112, loss_sup: 0.054192, loss_mps: 0.155996, loss_cps: 0.284924
[13:52:01.736] iteration 21349: total_loss: 0.860672, loss_sup: 0.116479, loss_mps: 0.232442, loss_cps: 0.511751
[13:52:01.882] iteration 21350: total_loss: 0.443192, loss_sup: 0.048947, loss_mps: 0.135331, loss_cps: 0.258913
[13:52:02.028] iteration 21351: total_loss: 0.728323, loss_sup: 0.146127, loss_mps: 0.195849, loss_cps: 0.386347
[13:52:02.173] iteration 21352: total_loss: 0.619066, loss_sup: 0.042450, loss_mps: 0.186562, loss_cps: 0.390054
[13:52:02.320] iteration 21353: total_loss: 0.571735, loss_sup: 0.031457, loss_mps: 0.195161, loss_cps: 0.345118
[13:52:02.467] iteration 21354: total_loss: 0.848725, loss_sup: 0.187911, loss_mps: 0.211445, loss_cps: 0.449369
[13:52:02.613] iteration 21355: total_loss: 0.470063, loss_sup: 0.045434, loss_mps: 0.146363, loss_cps: 0.278266
[13:52:02.758] iteration 21356: total_loss: 0.901046, loss_sup: 0.241896, loss_mps: 0.213704, loss_cps: 0.445446
[13:52:02.904] iteration 21357: total_loss: 0.300952, loss_sup: 0.012477, loss_mps: 0.102857, loss_cps: 0.185618
[13:52:03.052] iteration 21358: total_loss: 1.123959, loss_sup: 0.270871, loss_mps: 0.272585, loss_cps: 0.580503
[13:52:03.197] iteration 21359: total_loss: 0.370328, loss_sup: 0.061683, loss_mps: 0.108984, loss_cps: 0.199661
[13:52:03.343] iteration 21360: total_loss: 0.810271, loss_sup: 0.073369, loss_mps: 0.234331, loss_cps: 0.502571
[13:52:03.489] iteration 21361: total_loss: 0.193496, loss_sup: 0.005084, loss_mps: 0.068986, loss_cps: 0.119426
[13:52:03.635] iteration 21362: total_loss: 0.611554, loss_sup: 0.121980, loss_mps: 0.166189, loss_cps: 0.323385
[13:52:03.781] iteration 21363: total_loss: 0.499637, loss_sup: 0.103776, loss_mps: 0.134929, loss_cps: 0.260932
[13:52:03.927] iteration 21364: total_loss: 0.562718, loss_sup: 0.095266, loss_mps: 0.161623, loss_cps: 0.305830
[13:52:04.073] iteration 21365: total_loss: 0.468658, loss_sup: 0.042211, loss_mps: 0.159701, loss_cps: 0.266746
[13:52:04.219] iteration 21366: total_loss: 0.283583, loss_sup: 0.012242, loss_mps: 0.099116, loss_cps: 0.172225
[13:52:04.365] iteration 21367: total_loss: 0.515720, loss_sup: 0.069675, loss_mps: 0.156469, loss_cps: 0.289576
[13:52:04.511] iteration 21368: total_loss: 0.331299, loss_sup: 0.027414, loss_mps: 0.113009, loss_cps: 0.190876
[13:52:04.657] iteration 21369: total_loss: 0.475256, loss_sup: 0.026813, loss_mps: 0.161320, loss_cps: 0.287123
[13:52:04.803] iteration 21370: total_loss: 1.401084, loss_sup: 0.293250, loss_mps: 0.348956, loss_cps: 0.758877
[13:52:04.949] iteration 21371: total_loss: 0.762643, loss_sup: 0.046199, loss_mps: 0.231917, loss_cps: 0.484526
[13:52:05.096] iteration 21372: total_loss: 0.449116, loss_sup: 0.140052, loss_mps: 0.114864, loss_cps: 0.194200
[13:52:05.243] iteration 21373: total_loss: 0.349564, loss_sup: 0.102013, loss_mps: 0.092742, loss_cps: 0.154809
[13:52:05.391] iteration 21374: total_loss: 0.542504, loss_sup: 0.052273, loss_mps: 0.161069, loss_cps: 0.329162
[13:52:05.537] iteration 21375: total_loss: 0.377614, loss_sup: 0.137718, loss_mps: 0.092368, loss_cps: 0.147528
[13:52:05.683] iteration 21376: total_loss: 0.511801, loss_sup: 0.044507, loss_mps: 0.168892, loss_cps: 0.298402
[13:52:05.829] iteration 21377: total_loss: 0.512196, loss_sup: 0.017207, loss_mps: 0.169324, loss_cps: 0.325666
[13:52:05.975] iteration 21378: total_loss: 1.221984, loss_sup: 0.285230, loss_mps: 0.279114, loss_cps: 0.657639
[13:52:06.121] iteration 21379: total_loss: 0.638161, loss_sup: 0.062490, loss_mps: 0.190729, loss_cps: 0.384942
[13:52:06.267] iteration 21380: total_loss: 0.778796, loss_sup: 0.146540, loss_mps: 0.213474, loss_cps: 0.418782
[13:52:06.413] iteration 21381: total_loss: 0.348610, loss_sup: 0.048651, loss_mps: 0.113876, loss_cps: 0.186083
[13:52:06.559] iteration 21382: total_loss: 0.252800, loss_sup: 0.007838, loss_mps: 0.093521, loss_cps: 0.151441
[13:52:06.706] iteration 21383: total_loss: 0.872117, loss_sup: 0.186194, loss_mps: 0.217813, loss_cps: 0.468110
[13:52:06.852] iteration 21384: total_loss: 0.433034, loss_sup: 0.074670, loss_mps: 0.125725, loss_cps: 0.232640
[13:52:06.999] iteration 21385: total_loss: 0.450288, loss_sup: 0.091761, loss_mps: 0.126141, loss_cps: 0.232385
[13:52:07.147] iteration 21386: total_loss: 0.395359, loss_sup: 0.042019, loss_mps: 0.132847, loss_cps: 0.220494
[13:52:07.296] iteration 21387: total_loss: 0.491995, loss_sup: 0.039255, loss_mps: 0.166204, loss_cps: 0.286535
[13:52:07.443] iteration 21388: total_loss: 0.466627, loss_sup: 0.072939, loss_mps: 0.139759, loss_cps: 0.253930
[13:52:07.591] iteration 21389: total_loss: 0.453899, loss_sup: 0.027575, loss_mps: 0.147987, loss_cps: 0.278337
[13:52:07.736] iteration 21390: total_loss: 0.522701, loss_sup: 0.046491, loss_mps: 0.165346, loss_cps: 0.310863
[13:52:07.882] iteration 21391: total_loss: 0.585178, loss_sup: 0.244312, loss_mps: 0.125433, loss_cps: 0.215432
[13:52:08.029] iteration 21392: total_loss: 0.366887, loss_sup: 0.022438, loss_mps: 0.134733, loss_cps: 0.209716
[13:52:08.175] iteration 21393: total_loss: 0.380733, loss_sup: 0.011473, loss_mps: 0.135696, loss_cps: 0.233564
[13:52:08.322] iteration 21394: total_loss: 0.633971, loss_sup: 0.124579, loss_mps: 0.172611, loss_cps: 0.336781
[13:52:08.468] iteration 21395: total_loss: 0.664329, loss_sup: 0.070891, loss_mps: 0.195842, loss_cps: 0.397596
[13:52:08.613] iteration 21396: total_loss: 0.559736, loss_sup: 0.060494, loss_mps: 0.168196, loss_cps: 0.331046
[13:52:08.759] iteration 21397: total_loss: 0.615323, loss_sup: 0.187858, loss_mps: 0.150970, loss_cps: 0.276494
[13:52:08.906] iteration 21398: total_loss: 0.620602, loss_sup: 0.197739, loss_mps: 0.137830, loss_cps: 0.285033
[13:52:09.052] iteration 21399: total_loss: 0.349971, loss_sup: 0.042682, loss_mps: 0.114687, loss_cps: 0.192601
[13:52:09.199] iteration 21400: total_loss: 0.595865, loss_sup: 0.076742, loss_mps: 0.177586, loss_cps: 0.341538
[13:52:09.199] Evaluation Started ==>
[13:52:20.540] ==> valid iteration 21400: unet metrics: {'dc': 0.6188364520357178, 'jc': 0.49945922916154983, 'pre': 0.794728806486992, 'hd': 5.580320836675987}, ynet metrics: {'dc': 0.5979817141693948, 'jc': 0.48392835973242426, 'pre': 0.7827838050282754, 'hd': 5.558467053216571}.
[13:52:20.542] Evaluation Finished!⏹️
[13:52:20.692] iteration 21401: total_loss: 0.506078, loss_sup: 0.077948, loss_mps: 0.145823, loss_cps: 0.282307
[13:52:20.840] iteration 21402: total_loss: 0.446774, loss_sup: 0.004946, loss_mps: 0.149561, loss_cps: 0.292267
[13:52:20.987] iteration 21403: total_loss: 0.843346, loss_sup: 0.376563, loss_mps: 0.163628, loss_cps: 0.303154
[13:52:21.132] iteration 21404: total_loss: 0.556313, loss_sup: 0.114958, loss_mps: 0.150456, loss_cps: 0.290899
[13:52:21.278] iteration 21405: total_loss: 0.727447, loss_sup: 0.107014, loss_mps: 0.213733, loss_cps: 0.406701
[13:52:21.426] iteration 21406: total_loss: 0.558612, loss_sup: 0.194922, loss_mps: 0.128017, loss_cps: 0.235673
[13:52:21.575] iteration 21407: total_loss: 0.468866, loss_sup: 0.082473, loss_mps: 0.136377, loss_cps: 0.250016
[13:52:21.721] iteration 21408: total_loss: 0.390393, loss_sup: 0.008824, loss_mps: 0.142962, loss_cps: 0.238607
[13:52:21.868] iteration 21409: total_loss: 0.378130, loss_sup: 0.031538, loss_mps: 0.125535, loss_cps: 0.221057
[13:52:22.017] iteration 21410: total_loss: 0.296179, loss_sup: 0.018617, loss_mps: 0.099881, loss_cps: 0.177680
[13:52:22.163] iteration 21411: total_loss: 0.614330, loss_sup: 0.138126, loss_mps: 0.165604, loss_cps: 0.310599
[13:52:22.308] iteration 21412: total_loss: 0.486135, loss_sup: 0.041669, loss_mps: 0.138151, loss_cps: 0.306314
[13:52:22.454] iteration 21413: total_loss: 0.279790, loss_sup: 0.006527, loss_mps: 0.105137, loss_cps: 0.168126
[13:52:22.599] iteration 21414: total_loss: 0.725533, loss_sup: 0.072198, loss_mps: 0.206338, loss_cps: 0.446997
[13:52:22.745] iteration 21415: total_loss: 0.469575, loss_sup: 0.062229, loss_mps: 0.147262, loss_cps: 0.260084
[13:52:22.891] iteration 21416: total_loss: 0.416747, loss_sup: 0.038970, loss_mps: 0.134161, loss_cps: 0.243616
[13:52:23.038] iteration 21417: total_loss: 0.499953, loss_sup: 0.047860, loss_mps: 0.155049, loss_cps: 0.297043
[13:52:23.184] iteration 21418: total_loss: 0.458084, loss_sup: 0.029052, loss_mps: 0.152527, loss_cps: 0.276505
[13:52:23.330] iteration 21419: total_loss: 0.264385, loss_sup: 0.006337, loss_mps: 0.096401, loss_cps: 0.161647
[13:52:23.475] iteration 21420: total_loss: 0.604491, loss_sup: 0.039052, loss_mps: 0.184907, loss_cps: 0.380533
[13:52:23.626] iteration 21421: total_loss: 0.970234, loss_sup: 0.223748, loss_mps: 0.238888, loss_cps: 0.507598
[13:52:23.772] iteration 21422: total_loss: 0.394863, loss_sup: 0.078937, loss_mps: 0.110838, loss_cps: 0.205088
[13:52:23.918] iteration 21423: total_loss: 0.385960, loss_sup: 0.024836, loss_mps: 0.127349, loss_cps: 0.233775
[13:52:24.066] iteration 21424: total_loss: 0.438551, loss_sup: 0.040545, loss_mps: 0.143195, loss_cps: 0.254810
[13:52:24.211] iteration 21425: total_loss: 0.754195, loss_sup: 0.407592, loss_mps: 0.125218, loss_cps: 0.221385
[13:52:24.357] iteration 21426: total_loss: 0.429846, loss_sup: 0.020920, loss_mps: 0.147764, loss_cps: 0.261162
[13:52:24.503] iteration 21427: total_loss: 0.517654, loss_sup: 0.051495, loss_mps: 0.164604, loss_cps: 0.301555
[13:52:24.652] iteration 21428: total_loss: 0.496675, loss_sup: 0.051027, loss_mps: 0.158009, loss_cps: 0.287639
[13:52:24.799] iteration 21429: total_loss: 0.531948, loss_sup: 0.009014, loss_mps: 0.177473, loss_cps: 0.345461
[13:52:24.945] iteration 21430: total_loss: 0.642947, loss_sup: 0.070592, loss_mps: 0.186841, loss_cps: 0.385514
[13:52:25.091] iteration 21431: total_loss: 0.366244, loss_sup: 0.025373, loss_mps: 0.120276, loss_cps: 0.220594
[13:52:25.237] iteration 21432: total_loss: 0.629883, loss_sup: 0.049506, loss_mps: 0.194290, loss_cps: 0.386087
[13:52:25.383] iteration 21433: total_loss: 0.358731, loss_sup: 0.046020, loss_mps: 0.108655, loss_cps: 0.204056
[13:52:25.529] iteration 21434: total_loss: 0.528098, loss_sup: 0.033595, loss_mps: 0.169594, loss_cps: 0.324909
[13:52:25.676] iteration 21435: total_loss: 0.777816, loss_sup: 0.162760, loss_mps: 0.202927, loss_cps: 0.412130
[13:52:25.822] iteration 21436: total_loss: 0.410596, loss_sup: 0.049229, loss_mps: 0.125806, loss_cps: 0.235561
[13:52:25.969] iteration 21437: total_loss: 0.314494, loss_sup: 0.036151, loss_mps: 0.093884, loss_cps: 0.184460
[13:52:26.117] iteration 21438: total_loss: 0.329038, loss_sup: 0.056119, loss_mps: 0.100849, loss_cps: 0.172069
[13:52:26.263] iteration 21439: total_loss: 0.465658, loss_sup: 0.110682, loss_mps: 0.126179, loss_cps: 0.228796
[13:52:26.409] iteration 21440: total_loss: 0.319883, loss_sup: 0.018504, loss_mps: 0.103179, loss_cps: 0.198200
[13:52:26.562] iteration 21441: total_loss: 0.332674, loss_sup: 0.068057, loss_mps: 0.093005, loss_cps: 0.171611
[13:52:26.712] iteration 21442: total_loss: 0.399698, loss_sup: 0.036137, loss_mps: 0.123913, loss_cps: 0.239647
[13:52:26.858] iteration 21443: total_loss: 0.463003, loss_sup: 0.062420, loss_mps: 0.140180, loss_cps: 0.260403
[13:52:27.004] iteration 21444: total_loss: 0.302451, loss_sup: 0.022045, loss_mps: 0.099350, loss_cps: 0.181057
[13:52:27.150] iteration 21445: total_loss: 0.702028, loss_sup: 0.031406, loss_mps: 0.209718, loss_cps: 0.460904
[13:52:27.297] iteration 21446: total_loss: 0.289839, loss_sup: 0.029798, loss_mps: 0.090525, loss_cps: 0.169516
[13:52:27.443] iteration 21447: total_loss: 0.361676, loss_sup: 0.082267, loss_mps: 0.099860, loss_cps: 0.179549
[13:52:27.589] iteration 21448: total_loss: 0.619742, loss_sup: 0.115024, loss_mps: 0.171662, loss_cps: 0.333056
[13:52:27.734] iteration 21449: total_loss: 0.303663, loss_sup: 0.033124, loss_mps: 0.100268, loss_cps: 0.170271
[13:52:27.881] iteration 21450: total_loss: 0.606259, loss_sup: 0.206230, loss_mps: 0.137281, loss_cps: 0.262749
[13:52:28.026] iteration 21451: total_loss: 0.637325, loss_sup: 0.135917, loss_mps: 0.166003, loss_cps: 0.335406
[13:52:28.174] iteration 21452: total_loss: 0.374971, loss_sup: 0.022255, loss_mps: 0.121661, loss_cps: 0.231054
[13:52:28.321] iteration 21453: total_loss: 0.301727, loss_sup: 0.029959, loss_mps: 0.099453, loss_cps: 0.172316
[13:52:28.468] iteration 21454: total_loss: 0.410699, loss_sup: 0.085327, loss_mps: 0.119638, loss_cps: 0.205734
[13:52:28.614] iteration 21455: total_loss: 0.420555, loss_sup: 0.075538, loss_mps: 0.121482, loss_cps: 0.223535
[13:52:28.760] iteration 21456: total_loss: 0.331919, loss_sup: 0.050007, loss_mps: 0.106678, loss_cps: 0.175234
[13:52:28.906] iteration 21457: total_loss: 0.596417, loss_sup: 0.124638, loss_mps: 0.157598, loss_cps: 0.314181
[13:52:29.052] iteration 21458: total_loss: 0.346391, loss_sup: 0.071875, loss_mps: 0.098945, loss_cps: 0.175571
[13:52:29.199] iteration 21459: total_loss: 0.910291, loss_sup: 0.065655, loss_mps: 0.263933, loss_cps: 0.580704
[13:52:29.345] iteration 21460: total_loss: 0.445702, loss_sup: 0.019043, loss_mps: 0.142416, loss_cps: 0.284243
[13:52:29.492] iteration 21461: total_loss: 0.704813, loss_sup: 0.067283, loss_mps: 0.196704, loss_cps: 0.440826
[13:52:29.639] iteration 21462: total_loss: 0.577101, loss_sup: 0.078786, loss_mps: 0.179856, loss_cps: 0.318459
[13:52:29.787] iteration 21463: total_loss: 0.438973, loss_sup: 0.137369, loss_mps: 0.106608, loss_cps: 0.194997
[13:52:29.933] iteration 21464: total_loss: 0.727134, loss_sup: 0.017860, loss_mps: 0.232188, loss_cps: 0.477086
[13:52:30.079] iteration 21465: total_loss: 0.645791, loss_sup: 0.099868, loss_mps: 0.177751, loss_cps: 0.368171
[13:52:30.227] iteration 21466: total_loss: 0.581891, loss_sup: 0.090582, loss_mps: 0.162780, loss_cps: 0.328529
[13:52:30.373] iteration 21467: total_loss: 0.641347, loss_sup: 0.083822, loss_mps: 0.181952, loss_cps: 0.375574
[13:52:30.519] iteration 21468: total_loss: 0.475579, loss_sup: 0.054123, loss_mps: 0.141555, loss_cps: 0.279901
[13:52:30.665] iteration 21469: total_loss: 0.891159, loss_sup: 0.237097, loss_mps: 0.222028, loss_cps: 0.432034
[13:52:30.810] iteration 21470: total_loss: 0.420609, loss_sup: 0.072850, loss_mps: 0.127680, loss_cps: 0.220079
[13:52:30.956] iteration 21471: total_loss: 0.417809, loss_sup: 0.119025, loss_mps: 0.106872, loss_cps: 0.191912
[13:52:31.103] iteration 21472: total_loss: 0.585715, loss_sup: 0.050287, loss_mps: 0.182040, loss_cps: 0.353388
[13:52:31.250] iteration 21473: total_loss: 0.524971, loss_sup: 0.144329, loss_mps: 0.127766, loss_cps: 0.252876
[13:52:31.398] iteration 21474: total_loss: 0.905670, loss_sup: 0.173323, loss_mps: 0.233295, loss_cps: 0.499053
[13:52:31.545] iteration 21475: total_loss: 0.278704, loss_sup: 0.001174, loss_mps: 0.100884, loss_cps: 0.176646
[13:52:31.692] iteration 21476: total_loss: 0.963318, loss_sup: 0.087049, loss_mps: 0.267595, loss_cps: 0.608674
[13:52:31.838] iteration 21477: total_loss: 0.228190, loss_sup: 0.004957, loss_mps: 0.083419, loss_cps: 0.139814
[13:52:31.985] iteration 21478: total_loss: 0.606753, loss_sup: 0.051606, loss_mps: 0.177958, loss_cps: 0.377189
[13:52:32.130] iteration 21479: total_loss: 0.859873, loss_sup: 0.317774, loss_mps: 0.184922, loss_cps: 0.357177
[13:52:32.277] iteration 21480: total_loss: 0.738104, loss_sup: 0.042219, loss_mps: 0.222857, loss_cps: 0.473028
[13:52:32.423] iteration 21481: total_loss: 0.567866, loss_sup: 0.038069, loss_mps: 0.173751, loss_cps: 0.356046
[13:52:32.570] iteration 21482: total_loss: 1.069718, loss_sup: 0.086962, loss_mps: 0.277903, loss_cps: 0.704853
[13:52:32.716] iteration 21483: total_loss: 0.909953, loss_sup: 0.094620, loss_mps: 0.257329, loss_cps: 0.558004
[13:52:32.862] iteration 21484: total_loss: 0.238876, loss_sup: 0.007617, loss_mps: 0.082679, loss_cps: 0.148580
[13:52:33.010] iteration 21485: total_loss: 0.757060, loss_sup: 0.046984, loss_mps: 0.229992, loss_cps: 0.480084
[13:52:33.157] iteration 21486: total_loss: 0.352057, loss_sup: 0.011682, loss_mps: 0.117811, loss_cps: 0.222565
[13:52:33.303] iteration 21487: total_loss: 0.561502, loss_sup: 0.087405, loss_mps: 0.156484, loss_cps: 0.317613
[13:52:33.449] iteration 21488: total_loss: 0.533451, loss_sup: 0.007237, loss_mps: 0.174067, loss_cps: 0.352147
[13:52:33.595] iteration 21489: total_loss: 0.640085, loss_sup: 0.021134, loss_mps: 0.207556, loss_cps: 0.411395
[13:52:33.741] iteration 21490: total_loss: 0.600379, loss_sup: 0.079806, loss_mps: 0.175227, loss_cps: 0.345346
[13:52:33.886] iteration 21491: total_loss: 0.580065, loss_sup: 0.099291, loss_mps: 0.163888, loss_cps: 0.316885
[13:52:34.033] iteration 21492: total_loss: 0.631750, loss_sup: 0.212165, loss_mps: 0.148930, loss_cps: 0.270655
[13:52:34.181] iteration 21493: total_loss: 0.623855, loss_sup: 0.081185, loss_mps: 0.184221, loss_cps: 0.358449
[13:52:34.328] iteration 21494: total_loss: 0.499760, loss_sup: 0.073196, loss_mps: 0.149278, loss_cps: 0.277286
[13:52:34.474] iteration 21495: total_loss: 0.380686, loss_sup: 0.056361, loss_mps: 0.116656, loss_cps: 0.207669
[13:52:34.620] iteration 21496: total_loss: 0.571992, loss_sup: 0.109892, loss_mps: 0.156524, loss_cps: 0.305576
[13:52:34.766] iteration 21497: total_loss: 0.710630, loss_sup: 0.069976, loss_mps: 0.219146, loss_cps: 0.421507
[13:52:34.912] iteration 21498: total_loss: 0.623830, loss_sup: 0.097355, loss_mps: 0.175924, loss_cps: 0.350552
[13:52:35.058] iteration 21499: total_loss: 0.493668, loss_sup: 0.026395, loss_mps: 0.159127, loss_cps: 0.308146
[13:52:35.204] iteration 21500: total_loss: 0.917854, loss_sup: 0.059711, loss_mps: 0.272140, loss_cps: 0.586003
[13:52:35.204] Evaluation Started ==>
[13:52:46.626] ==> valid iteration 21500: unet metrics: {'dc': 0.6213964300069004, 'jc': 0.5052452478715437, 'pre': 0.7840836938608658, 'hd': 5.4848718628259645}, ynet metrics: {'dc': 0.573353935380261, 'jc': 0.4599741095175163, 'pre': 0.8043187017362093, 'hd': 5.749910216878823}.
[13:52:46.628] Evaluation Finished!⏹️
[13:52:46.781] iteration 21501: total_loss: 0.629680, loss_sup: 0.029131, loss_mps: 0.196214, loss_cps: 0.404335
[13:52:46.931] iteration 21502: total_loss: 0.649134, loss_sup: 0.036450, loss_mps: 0.200052, loss_cps: 0.412631
[13:52:47.078] iteration 21503: total_loss: 0.743098, loss_sup: 0.158736, loss_mps: 0.193305, loss_cps: 0.391057
[13:52:47.223] iteration 21504: total_loss: 1.218468, loss_sup: 0.090402, loss_mps: 0.350454, loss_cps: 0.777611
[13:52:47.369] iteration 21505: total_loss: 0.325642, loss_sup: 0.025732, loss_mps: 0.108562, loss_cps: 0.191348
[13:52:47.515] iteration 21506: total_loss: 0.888239, loss_sup: 0.453696, loss_mps: 0.165236, loss_cps: 0.269306
[13:52:47.661] iteration 21507: total_loss: 0.429558, loss_sup: 0.029980, loss_mps: 0.139295, loss_cps: 0.260283
[13:52:47.806] iteration 21508: total_loss: 0.555252, loss_sup: 0.080964, loss_mps: 0.164764, loss_cps: 0.309523
[13:52:47.951] iteration 21509: total_loss: 0.377228, loss_sup: 0.049668, loss_mps: 0.116705, loss_cps: 0.210854
[13:52:48.096] iteration 21510: total_loss: 0.384710, loss_sup: 0.035768, loss_mps: 0.122627, loss_cps: 0.226315
[13:52:48.242] iteration 21511: total_loss: 0.679678, loss_sup: 0.178425, loss_mps: 0.173211, loss_cps: 0.328042
[13:52:48.387] iteration 21512: total_loss: 0.641199, loss_sup: 0.117697, loss_mps: 0.184132, loss_cps: 0.339371
[13:52:48.533] iteration 21513: total_loss: 0.506219, loss_sup: 0.067487, loss_mps: 0.156186, loss_cps: 0.282547
[13:52:48.680] iteration 21514: total_loss: 0.594123, loss_sup: 0.045431, loss_mps: 0.184549, loss_cps: 0.364143
[13:52:48.826] iteration 21515: total_loss: 0.550862, loss_sup: 0.057390, loss_mps: 0.179101, loss_cps: 0.314371
[13:52:48.974] iteration 21516: total_loss: 0.963004, loss_sup: 0.269576, loss_mps: 0.222929, loss_cps: 0.470499
[13:52:49.120] iteration 21517: total_loss: 0.409258, loss_sup: 0.046838, loss_mps: 0.124744, loss_cps: 0.237676
[13:52:49.266] iteration 21518: total_loss: 0.996362, loss_sup: 0.217934, loss_mps: 0.261220, loss_cps: 0.517208
[13:52:49.412] iteration 21519: total_loss: 0.811173, loss_sup: 0.094598, loss_mps: 0.238149, loss_cps: 0.478427
[13:52:49.557] iteration 21520: total_loss: 0.358947, loss_sup: 0.043201, loss_mps: 0.117006, loss_cps: 0.198739
[13:52:49.702] iteration 21521: total_loss: 0.383012, loss_sup: 0.057819, loss_mps: 0.121696, loss_cps: 0.203497
[13:52:49.850] iteration 21522: total_loss: 1.146519, loss_sup: 0.223366, loss_mps: 0.297472, loss_cps: 0.625681
[13:52:49.995] iteration 21523: total_loss: 0.604983, loss_sup: 0.071625, loss_mps: 0.181720, loss_cps: 0.351638
[13:52:50.141] iteration 21524: total_loss: 0.447233, loss_sup: 0.031747, loss_mps: 0.138058, loss_cps: 0.277428
[13:52:50.287] iteration 21525: total_loss: 0.621593, loss_sup: 0.214407, loss_mps: 0.142925, loss_cps: 0.264261
[13:52:50.432] iteration 21526: total_loss: 0.601227, loss_sup: 0.010356, loss_mps: 0.186138, loss_cps: 0.404733
[13:52:50.578] iteration 21527: total_loss: 0.675045, loss_sup: 0.271300, loss_mps: 0.144501, loss_cps: 0.259245
[13:52:50.723] iteration 21528: total_loss: 0.634363, loss_sup: 0.160675, loss_mps: 0.171076, loss_cps: 0.302612
[13:52:50.870] iteration 21529: total_loss: 0.580610, loss_sup: 0.100002, loss_mps: 0.167665, loss_cps: 0.312943
[13:52:51.015] iteration 21530: total_loss: 0.354326, loss_sup: 0.006653, loss_mps: 0.125445, loss_cps: 0.222227
[13:52:51.160] iteration 21531: total_loss: 0.445444, loss_sup: 0.046561, loss_mps: 0.142041, loss_cps: 0.256842
[13:52:51.307] iteration 21532: total_loss: 0.541060, loss_sup: 0.020035, loss_mps: 0.172583, loss_cps: 0.348442
[13:52:51.453] iteration 21533: total_loss: 0.197830, loss_sup: 0.012089, loss_mps: 0.073719, loss_cps: 0.112022
[13:52:51.599] iteration 21534: total_loss: 0.384764, loss_sup: 0.041627, loss_mps: 0.123358, loss_cps: 0.219779
[13:52:51.745] iteration 21535: total_loss: 0.393508, loss_sup: 0.104069, loss_mps: 0.108175, loss_cps: 0.181263
[13:52:51.891] iteration 21536: total_loss: 0.494990, loss_sup: 0.092718, loss_mps: 0.148716, loss_cps: 0.253555
[13:52:52.038] iteration 21537: total_loss: 0.529049, loss_sup: 0.150649, loss_mps: 0.140409, loss_cps: 0.237991
[13:52:52.186] iteration 21538: total_loss: 0.554510, loss_sup: 0.029886, loss_mps: 0.175247, loss_cps: 0.349377
[13:52:52.332] iteration 21539: total_loss: 0.598126, loss_sup: 0.113223, loss_mps: 0.160814, loss_cps: 0.324089
[13:52:52.479] iteration 21540: total_loss: 0.544919, loss_sup: 0.030544, loss_mps: 0.172216, loss_cps: 0.342160
[13:52:52.625] iteration 21541: total_loss: 0.725563, loss_sup: 0.019749, loss_mps: 0.221650, loss_cps: 0.484165
[13:52:52.772] iteration 21542: total_loss: 0.465876, loss_sup: 0.090211, loss_mps: 0.135020, loss_cps: 0.240645
[13:52:52.918] iteration 21543: total_loss: 0.392136, loss_sup: 0.026087, loss_mps: 0.131702, loss_cps: 0.234347
[13:52:53.063] iteration 21544: total_loss: 0.502528, loss_sup: 0.056781, loss_mps: 0.147824, loss_cps: 0.297923
[13:52:53.210] iteration 21545: total_loss: 0.780415, loss_sup: 0.051848, loss_mps: 0.232028, loss_cps: 0.496539
[13:52:53.356] iteration 21546: total_loss: 0.806197, loss_sup: 0.126365, loss_mps: 0.220637, loss_cps: 0.459195
[13:52:53.502] iteration 21547: total_loss: 0.595927, loss_sup: 0.052845, loss_mps: 0.182518, loss_cps: 0.360564
[13:52:53.648] iteration 21548: total_loss: 0.381839, loss_sup: 0.022805, loss_mps: 0.127760, loss_cps: 0.231274
[13:52:53.796] iteration 21549: total_loss: 0.633635, loss_sup: 0.073872, loss_mps: 0.195660, loss_cps: 0.364103
[13:52:53.943] iteration 21550: total_loss: 0.287928, loss_sup: 0.008938, loss_mps: 0.102973, loss_cps: 0.176017
[13:52:54.089] iteration 21551: total_loss: 0.846457, loss_sup: 0.201634, loss_mps: 0.215274, loss_cps: 0.429549
[13:52:54.235] iteration 21552: total_loss: 0.354356, loss_sup: 0.033688, loss_mps: 0.112805, loss_cps: 0.207864
[13:52:54.385] iteration 21553: total_loss: 0.831426, loss_sup: 0.041824, loss_mps: 0.253286, loss_cps: 0.536316
[13:52:54.531] iteration 21554: total_loss: 0.580063, loss_sup: 0.090980, loss_mps: 0.161741, loss_cps: 0.327342
[13:52:54.677] iteration 21555: total_loss: 0.321473, loss_sup: 0.023459, loss_mps: 0.110796, loss_cps: 0.187219
[13:52:54.823] iteration 21556: total_loss: 0.868780, loss_sup: 0.102197, loss_mps: 0.243309, loss_cps: 0.523274
[13:52:54.969] iteration 21557: total_loss: 0.385422, loss_sup: 0.038419, loss_mps: 0.122236, loss_cps: 0.224768
[13:52:55.115] iteration 21558: total_loss: 0.514244, loss_sup: 0.031609, loss_mps: 0.175101, loss_cps: 0.307534
[13:52:55.260] iteration 21559: total_loss: 0.732807, loss_sup: 0.154819, loss_mps: 0.184051, loss_cps: 0.393938
[13:52:55.406] iteration 21560: total_loss: 0.494732, loss_sup: 0.084368, loss_mps: 0.143551, loss_cps: 0.266814
[13:52:55.552] iteration 21561: total_loss: 1.087812, loss_sup: 0.406190, loss_mps: 0.220720, loss_cps: 0.460903
[13:52:55.700] iteration 21562: total_loss: 0.526949, loss_sup: 0.053275, loss_mps: 0.157381, loss_cps: 0.316293
[13:52:55.848] iteration 21563: total_loss: 0.320582, loss_sup: 0.012568, loss_mps: 0.109663, loss_cps: 0.198351
[13:52:55.994] iteration 21564: total_loss: 0.572136, loss_sup: 0.097779, loss_mps: 0.160146, loss_cps: 0.314211
[13:52:56.139] iteration 21565: total_loss: 0.984255, loss_sup: 0.102555, loss_mps: 0.286028, loss_cps: 0.595673
[13:52:56.286] iteration 21566: total_loss: 0.722290, loss_sup: 0.122825, loss_mps: 0.202593, loss_cps: 0.396871
[13:52:56.432] iteration 21567: total_loss: 0.624584, loss_sup: 0.034929, loss_mps: 0.192784, loss_cps: 0.396872
[13:52:56.578] iteration 21568: total_loss: 0.689365, loss_sup: 0.119413, loss_mps: 0.195709, loss_cps: 0.374242
[13:52:56.725] iteration 21569: total_loss: 0.451259, loss_sup: 0.093871, loss_mps: 0.128166, loss_cps: 0.229223
[13:52:56.878] iteration 21570: total_loss: 0.481192, loss_sup: 0.072882, loss_mps: 0.144751, loss_cps: 0.263560
[13:52:57.024] iteration 21571: total_loss: 0.281757, loss_sup: 0.025342, loss_mps: 0.096753, loss_cps: 0.159662
[13:52:57.170] iteration 21572: total_loss: 0.393399, loss_sup: 0.024170, loss_mps: 0.131527, loss_cps: 0.237702
[13:52:57.316] iteration 21573: total_loss: 0.589656, loss_sup: 0.088207, loss_mps: 0.174247, loss_cps: 0.327202
[13:52:57.462] iteration 21574: total_loss: 0.287962, loss_sup: 0.025462, loss_mps: 0.099743, loss_cps: 0.162757
[13:52:57.608] iteration 21575: total_loss: 0.406496, loss_sup: 0.096568, loss_mps: 0.108424, loss_cps: 0.201504
[13:52:57.755] iteration 21576: total_loss: 0.647372, loss_sup: 0.052784, loss_mps: 0.216789, loss_cps: 0.377800
[13:52:57.900] iteration 21577: total_loss: 0.516853, loss_sup: 0.116373, loss_mps: 0.141683, loss_cps: 0.258796
[13:52:58.048] iteration 21578: total_loss: 0.523219, loss_sup: 0.035982, loss_mps: 0.171570, loss_cps: 0.315667
[13:52:58.195] iteration 21579: total_loss: 0.418908, loss_sup: 0.023981, loss_mps: 0.133681, loss_cps: 0.261247
[13:52:58.341] iteration 21580: total_loss: 0.368559, loss_sup: 0.049605, loss_mps: 0.116666, loss_cps: 0.202288
[13:52:58.487] iteration 21581: total_loss: 0.348846, loss_sup: 0.035059, loss_mps: 0.113314, loss_cps: 0.200473
[13:52:58.634] iteration 21582: total_loss: 0.317715, loss_sup: 0.005150, loss_mps: 0.111971, loss_cps: 0.200595
[13:52:58.780] iteration 21583: total_loss: 0.694418, loss_sup: 0.051264, loss_mps: 0.211785, loss_cps: 0.431369
[13:52:58.926] iteration 21584: total_loss: 0.515436, loss_sup: 0.076753, loss_mps: 0.152297, loss_cps: 0.286387
[13:52:59.073] iteration 21585: total_loss: 0.658362, loss_sup: 0.190997, loss_mps: 0.170591, loss_cps: 0.296773
[13:52:59.218] iteration 21586: total_loss: 0.550585, loss_sup: 0.174412, loss_mps: 0.130953, loss_cps: 0.245220
[13:52:59.377] iteration 21587: total_loss: 0.687698, loss_sup: 0.139930, loss_mps: 0.182857, loss_cps: 0.364911
[13:52:59.523] iteration 21588: total_loss: 0.328863, loss_sup: 0.004214, loss_mps: 0.120243, loss_cps: 0.204407
[13:52:59.669] iteration 21589: total_loss: 0.354648, loss_sup: 0.060198, loss_mps: 0.102695, loss_cps: 0.191754
[13:52:59.816] iteration 21590: total_loss: 0.365897, loss_sup: 0.020022, loss_mps: 0.122130, loss_cps: 0.223745
[13:52:59.964] iteration 21591: total_loss: 0.527189, loss_sup: 0.023723, loss_mps: 0.166888, loss_cps: 0.336578
[13:53:00.111] iteration 21592: total_loss: 1.231810, loss_sup: 0.091360, loss_mps: 0.325792, loss_cps: 0.814658
[13:53:00.259] iteration 21593: total_loss: 0.317905, loss_sup: 0.025968, loss_mps: 0.104214, loss_cps: 0.187723
[13:53:00.405] iteration 21594: total_loss: 0.656175, loss_sup: 0.045854, loss_mps: 0.200229, loss_cps: 0.410092
[13:53:00.552] iteration 21595: total_loss: 0.648644, loss_sup: 0.060334, loss_mps: 0.192563, loss_cps: 0.395746
[13:53:00.697] iteration 21596: total_loss: 0.413006, loss_sup: 0.077537, loss_mps: 0.124565, loss_cps: 0.210904
[13:53:00.843] iteration 21597: total_loss: 0.517924, loss_sup: 0.069227, loss_mps: 0.152251, loss_cps: 0.296446
[13:53:00.990] iteration 21598: total_loss: 0.919861, loss_sup: 0.119717, loss_mps: 0.233921, loss_cps: 0.566223
[13:53:01.136] iteration 21599: total_loss: 0.406508, loss_sup: 0.016012, loss_mps: 0.131727, loss_cps: 0.258769
[13:53:01.283] iteration 21600: total_loss: 0.820517, loss_sup: 0.052182, loss_mps: 0.244465, loss_cps: 0.523869
[13:53:01.283] Evaluation Started ==>
[13:53:12.650] ==> valid iteration 21600: unet metrics: {'dc': 0.635197979856043, 'jc': 0.5204321541757799, 'pre': 0.8052893848757814, 'hd': 5.336898476620164}, ynet metrics: {'dc': 0.5389628352761727, 'jc': 0.42700029525020294, 'pre': 0.7623590956740698, 'hd': 5.700156043687725}.
[13:53:12.653] Evaluation Finished!⏹️
[13:53:12.812] iteration 21601: total_loss: 0.733544, loss_sup: 0.114342, loss_mps: 0.203159, loss_cps: 0.416043
[13:53:12.960] iteration 21602: total_loss: 0.603552, loss_sup: 0.086168, loss_mps: 0.180746, loss_cps: 0.336639
[13:53:13.106] iteration 21603: total_loss: 0.673749, loss_sup: 0.157373, loss_mps: 0.164301, loss_cps: 0.352075
[13:53:13.253] iteration 21604: total_loss: 0.951556, loss_sup: 0.105919, loss_mps: 0.260010, loss_cps: 0.585627
[13:53:13.399] iteration 21605: total_loss: 0.515531, loss_sup: 0.095345, loss_mps: 0.140087, loss_cps: 0.280098
[13:53:13.545] iteration 21606: total_loss: 0.463338, loss_sup: 0.026839, loss_mps: 0.143617, loss_cps: 0.292883
[13:53:13.691] iteration 21607: total_loss: 0.456911, loss_sup: 0.052423, loss_mps: 0.137735, loss_cps: 0.266753
[13:53:13.837] iteration 21608: total_loss: 1.177107, loss_sup: 0.332485, loss_mps: 0.261575, loss_cps: 0.583047
[13:53:13.986] iteration 21609: total_loss: 1.004011, loss_sup: 0.106964, loss_mps: 0.285634, loss_cps: 0.611414
[13:53:14.132] iteration 21610: total_loss: 0.540700, loss_sup: 0.056567, loss_mps: 0.167785, loss_cps: 0.316348
[13:53:14.278] iteration 21611: total_loss: 0.800452, loss_sup: 0.200706, loss_mps: 0.194015, loss_cps: 0.405731
[13:53:14.425] iteration 21612: total_loss: 0.541059, loss_sup: 0.072212, loss_mps: 0.161958, loss_cps: 0.306889
[13:53:14.573] iteration 21613: total_loss: 0.454234, loss_sup: 0.039671, loss_mps: 0.141576, loss_cps: 0.272987
[13:53:14.723] iteration 21614: total_loss: 0.536882, loss_sup: 0.076948, loss_mps: 0.156315, loss_cps: 0.303619
[13:53:14.868] iteration 21615: total_loss: 0.785194, loss_sup: 0.207228, loss_mps: 0.201414, loss_cps: 0.376552
[13:53:15.014] iteration 21616: total_loss: 0.586858, loss_sup: 0.061311, loss_mps: 0.177070, loss_cps: 0.348477
[13:53:15.161] iteration 21617: total_loss: 0.371902, loss_sup: 0.045663, loss_mps: 0.121218, loss_cps: 0.205022
[13:53:15.308] iteration 21618: total_loss: 0.562906, loss_sup: 0.140270, loss_mps: 0.150957, loss_cps: 0.271679
[13:53:15.453] iteration 21619: total_loss: 0.449227, loss_sup: 0.041438, loss_mps: 0.145992, loss_cps: 0.261797
[13:53:15.600] iteration 21620: total_loss: 0.532561, loss_sup: 0.016268, loss_mps: 0.169239, loss_cps: 0.347053
[13:53:15.749] iteration 21621: total_loss: 0.525445, loss_sup: 0.023840, loss_mps: 0.167976, loss_cps: 0.333630
[13:53:15.895] iteration 21622: total_loss: 0.383642, loss_sup: 0.028406, loss_mps: 0.124743, loss_cps: 0.230493
[13:53:16.041] iteration 21623: total_loss: 0.835348, loss_sup: 0.309934, loss_mps: 0.188066, loss_cps: 0.337347
[13:53:16.187] iteration 21624: total_loss: 0.423252, loss_sup: 0.049863, loss_mps: 0.139517, loss_cps: 0.233872
[13:53:16.333] iteration 21625: total_loss: 1.736635, loss_sup: 0.107614, loss_mps: 0.500188, loss_cps: 1.128833
[13:53:16.480] iteration 21626: total_loss: 0.726564, loss_sup: 0.070944, loss_mps: 0.224004, loss_cps: 0.431617
[13:53:16.629] iteration 21627: total_loss: 0.743454, loss_sup: 0.147679, loss_mps: 0.204756, loss_cps: 0.391019
[13:53:16.774] iteration 21628: total_loss: 0.355362, loss_sup: 0.034806, loss_mps: 0.119416, loss_cps: 0.201140
[13:53:16.920] iteration 21629: total_loss: 0.358818, loss_sup: 0.034634, loss_mps: 0.114924, loss_cps: 0.209260
[13:53:17.067] iteration 21630: total_loss: 0.741865, loss_sup: 0.169536, loss_mps: 0.197930, loss_cps: 0.374399
[13:53:17.213] iteration 21631: total_loss: 0.369339, loss_sup: 0.077917, loss_mps: 0.112615, loss_cps: 0.178807
[13:53:17.359] iteration 21632: total_loss: 0.997196, loss_sup: 0.077042, loss_mps: 0.283469, loss_cps: 0.636685
[13:53:17.505] iteration 21633: total_loss: 0.306390, loss_sup: 0.010455, loss_mps: 0.110962, loss_cps: 0.184973
[13:53:17.651] iteration 21634: total_loss: 0.474009, loss_sup: 0.021579, loss_mps: 0.157522, loss_cps: 0.294908
[13:53:17.797] iteration 21635: total_loss: 0.723354, loss_sup: 0.012652, loss_mps: 0.232512, loss_cps: 0.478190
[13:53:17.943] iteration 21636: total_loss: 0.343799, loss_sup: 0.032773, loss_mps: 0.114481, loss_cps: 0.196545
[13:53:18.089] iteration 21637: total_loss: 0.370260, loss_sup: 0.032484, loss_mps: 0.124196, loss_cps: 0.213581
[13:53:18.235] iteration 21638: total_loss: 0.741888, loss_sup: 0.049422, loss_mps: 0.225127, loss_cps: 0.467338
[13:53:18.381] iteration 21639: total_loss: 0.358657, loss_sup: 0.020000, loss_mps: 0.122411, loss_cps: 0.216246
[13:53:18.527] iteration 21640: total_loss: 0.526193, loss_sup: 0.091833, loss_mps: 0.148934, loss_cps: 0.285426
[13:53:18.673] iteration 21641: total_loss: 0.237119, loss_sup: 0.008063, loss_mps: 0.086035, loss_cps: 0.143021
[13:53:18.819] iteration 21642: total_loss: 0.400881, loss_sup: 0.054931, loss_mps: 0.128950, loss_cps: 0.217001
[13:53:18.966] iteration 21643: total_loss: 0.233988, loss_sup: 0.014431, loss_mps: 0.085749, loss_cps: 0.133809
[13:53:19.112] iteration 21644: total_loss: 0.601162, loss_sup: 0.055217, loss_mps: 0.180912, loss_cps: 0.365034
[13:53:19.259] iteration 21645: total_loss: 0.399373, loss_sup: 0.125501, loss_mps: 0.098992, loss_cps: 0.174879
[13:53:19.409] iteration 21646: total_loss: 0.710157, loss_sup: 0.107725, loss_mps: 0.207281, loss_cps: 0.395151
[13:53:19.555] iteration 21647: total_loss: 0.697089, loss_sup: 0.095759, loss_mps: 0.196237, loss_cps: 0.405093
[13:53:19.701] iteration 21648: total_loss: 0.647890, loss_sup: 0.031031, loss_mps: 0.203849, loss_cps: 0.413010
[13:53:19.850] iteration 21649: total_loss: 0.558191, loss_sup: 0.094771, loss_mps: 0.161086, loss_cps: 0.302334
[13:53:19.997] iteration 21650: total_loss: 0.705648, loss_sup: 0.148627, loss_mps: 0.173023, loss_cps: 0.383997
[13:53:20.144] iteration 21651: total_loss: 1.091303, loss_sup: 0.138909, loss_mps: 0.295245, loss_cps: 0.657149
[13:53:20.290] iteration 21652: total_loss: 0.532030, loss_sup: 0.010252, loss_mps: 0.157157, loss_cps: 0.364622
[13:53:20.437] iteration 21653: total_loss: 0.382430, loss_sup: 0.022792, loss_mps: 0.123003, loss_cps: 0.236635
[13:53:20.584] iteration 21654: total_loss: 0.482335, loss_sup: 0.065122, loss_mps: 0.137109, loss_cps: 0.280104
[13:53:20.730] iteration 21655: total_loss: 0.445183, loss_sup: 0.132422, loss_mps: 0.112290, loss_cps: 0.200471
[13:53:20.876] iteration 21656: total_loss: 0.378993, loss_sup: 0.003339, loss_mps: 0.130908, loss_cps: 0.244745
[13:53:21.021] iteration 21657: total_loss: 0.347334, loss_sup: 0.025288, loss_mps: 0.124044, loss_cps: 0.198002
[13:53:21.168] iteration 21658: total_loss: 0.368652, loss_sup: 0.066409, loss_mps: 0.113822, loss_cps: 0.188422
[13:53:21.313] iteration 21659: total_loss: 0.439355, loss_sup: 0.079131, loss_mps: 0.122862, loss_cps: 0.237363
[13:53:21.460] iteration 21660: total_loss: 0.877963, loss_sup: 0.217828, loss_mps: 0.211379, loss_cps: 0.448756
[13:53:21.606] iteration 21661: total_loss: 0.557174, loss_sup: 0.005693, loss_mps: 0.175386, loss_cps: 0.376095
[13:53:21.755] iteration 21662: total_loss: 0.438352, loss_sup: 0.061769, loss_mps: 0.129396, loss_cps: 0.247187
[13:53:21.901] iteration 21663: total_loss: 0.624235, loss_sup: 0.144060, loss_mps: 0.169243, loss_cps: 0.310932
[13:53:22.048] iteration 21664: total_loss: 0.623543, loss_sup: 0.092227, loss_mps: 0.165335, loss_cps: 0.365981
[13:53:22.194] iteration 21665: total_loss: 0.524972, loss_sup: 0.101465, loss_mps: 0.149373, loss_cps: 0.274134
[13:53:22.343] iteration 21666: total_loss: 0.822477, loss_sup: 0.320709, loss_mps: 0.169398, loss_cps: 0.332369
[13:53:22.489] iteration 21667: total_loss: 0.647242, loss_sup: 0.213448, loss_mps: 0.158361, loss_cps: 0.275433
[13:53:22.636] iteration 21668: total_loss: 0.833716, loss_sup: 0.216235, loss_mps: 0.212408, loss_cps: 0.405073
[13:53:22.784] iteration 21669: total_loss: 0.392172, loss_sup: 0.021891, loss_mps: 0.127599, loss_cps: 0.242682
[13:53:22.930] iteration 21670: total_loss: 0.494782, loss_sup: 0.067130, loss_mps: 0.146349, loss_cps: 0.281302
[13:53:23.077] iteration 21671: total_loss: 0.807600, loss_sup: 0.211380, loss_mps: 0.203668, loss_cps: 0.392552
[13:53:23.224] iteration 21672: total_loss: 0.878881, loss_sup: 0.014428, loss_mps: 0.258395, loss_cps: 0.606057
[13:53:23.372] iteration 21673: total_loss: 0.429726, loss_sup: 0.025202, loss_mps: 0.143533, loss_cps: 0.260991
[13:53:23.518] iteration 21674: total_loss: 0.480334, loss_sup: 0.058145, loss_mps: 0.145349, loss_cps: 0.276840
[13:53:23.665] iteration 21675: total_loss: 0.302113, loss_sup: 0.019972, loss_mps: 0.102529, loss_cps: 0.179611
[13:53:23.811] iteration 21676: total_loss: 0.963744, loss_sup: 0.153779, loss_mps: 0.252587, loss_cps: 0.557377
[13:53:23.957] iteration 21677: total_loss: 0.809271, loss_sup: 0.198626, loss_mps: 0.194273, loss_cps: 0.416373
[13:53:24.103] iteration 21678: total_loss: 0.234364, loss_sup: 0.020023, loss_mps: 0.081174, loss_cps: 0.133167
[13:53:24.250] iteration 21679: total_loss: 0.638590, loss_sup: 0.324002, loss_mps: 0.108445, loss_cps: 0.206143
[13:53:24.397] iteration 21680: total_loss: 0.643277, loss_sup: 0.072220, loss_mps: 0.199065, loss_cps: 0.371992
[13:53:24.543] iteration 21681: total_loss: 0.367190, loss_sup: 0.078789, loss_mps: 0.116455, loss_cps: 0.171946
[13:53:24.691] iteration 21682: total_loss: 0.312545, loss_sup: 0.051204, loss_mps: 0.098630, loss_cps: 0.162711
[13:53:24.839] iteration 21683: total_loss: 0.884021, loss_sup: 0.208293, loss_mps: 0.237984, loss_cps: 0.437744
[13:53:24.986] iteration 21684: total_loss: 0.498991, loss_sup: 0.013480, loss_mps: 0.165819, loss_cps: 0.319691
[13:53:25.132] iteration 21685: total_loss: 0.317033, loss_sup: 0.007281, loss_mps: 0.112630, loss_cps: 0.197122
[13:53:25.279] iteration 21686: total_loss: 1.006801, loss_sup: 0.134716, loss_mps: 0.285164, loss_cps: 0.586921
[13:53:25.426] iteration 21687: total_loss: 0.629846, loss_sup: 0.021206, loss_mps: 0.204082, loss_cps: 0.404558
[13:53:25.572] iteration 21688: total_loss: 0.194562, loss_sup: 0.003782, loss_mps: 0.073592, loss_cps: 0.117188
[13:53:25.719] iteration 21689: total_loss: 0.505587, loss_sup: 0.033647, loss_mps: 0.171870, loss_cps: 0.300070
[13:53:25.865] iteration 21690: total_loss: 0.403991, loss_sup: 0.075285, loss_mps: 0.121282, loss_cps: 0.207424
[13:53:26.012] iteration 21691: total_loss: 0.578595, loss_sup: 0.113139, loss_mps: 0.161086, loss_cps: 0.304371
[13:53:26.158] iteration 21692: total_loss: 0.733378, loss_sup: 0.051789, loss_mps: 0.218171, loss_cps: 0.463418
[13:53:26.306] iteration 21693: total_loss: 0.358016, loss_sup: 0.025286, loss_mps: 0.125226, loss_cps: 0.207504
[13:53:26.452] iteration 21694: total_loss: 0.396175, loss_sup: 0.043366, loss_mps: 0.127809, loss_cps: 0.225000
[13:53:26.599] iteration 21695: total_loss: 0.513524, loss_sup: 0.022925, loss_mps: 0.165915, loss_cps: 0.324684
[13:53:26.745] iteration 21696: total_loss: 0.978100, loss_sup: 0.241496, loss_mps: 0.240205, loss_cps: 0.496399
[13:53:26.891] iteration 21697: total_loss: 0.543790, loss_sup: 0.185188, loss_mps: 0.133540, loss_cps: 0.225063
[13:53:27.038] iteration 21698: total_loss: 0.467990, loss_sup: 0.082591, loss_mps: 0.134568, loss_cps: 0.250831
[13:53:27.184] iteration 21699: total_loss: 0.498994, loss_sup: 0.031342, loss_mps: 0.159705, loss_cps: 0.307947
[13:53:27.331] iteration 21700: total_loss: 0.441442, loss_sup: 0.035761, loss_mps: 0.142559, loss_cps: 0.263122
[13:53:27.331] Evaluation Started ==>
[13:53:38.644] ==> valid iteration 21700: unet metrics: {'dc': 0.627398230901651, 'jc': 0.5106033014318536, 'pre': 0.7969966734017617, 'hd': 5.4420290932799285}, ynet metrics: {'dc': 0.585709968849085, 'jc': 0.4730753762478682, 'pre': 0.812616506465055, 'hd': 5.508983017340028}.
[13:53:38.646] Evaluation Finished!⏹️
[13:53:38.799] iteration 21701: total_loss: 0.798434, loss_sup: 0.021323, loss_mps: 0.252708, loss_cps: 0.524403
[13:53:38.947] iteration 21702: total_loss: 0.602954, loss_sup: 0.262857, loss_mps: 0.123585, loss_cps: 0.216511
[13:53:39.094] iteration 21703: total_loss: 0.610645, loss_sup: 0.051294, loss_mps: 0.187068, loss_cps: 0.372283
[13:53:39.244] iteration 21704: total_loss: 0.301562, loss_sup: 0.011824, loss_mps: 0.107573, loss_cps: 0.182164
[13:53:39.390] iteration 21705: total_loss: 0.961535, loss_sup: 0.090654, loss_mps: 0.274470, loss_cps: 0.596411
[13:53:39.535] iteration 21706: total_loss: 0.402952, loss_sup: 0.120009, loss_mps: 0.108606, loss_cps: 0.174337
[13:53:39.681] iteration 21707: total_loss: 0.276064, loss_sup: 0.079763, loss_mps: 0.075052, loss_cps: 0.121249
[13:53:39.826] iteration 21708: total_loss: 0.355487, loss_sup: 0.084305, loss_mps: 0.102592, loss_cps: 0.168590
[13:53:39.971] iteration 21709: total_loss: 0.478027, loss_sup: 0.178320, loss_mps: 0.117134, loss_cps: 0.182572
[13:53:40.118] iteration 21710: total_loss: 0.716581, loss_sup: 0.186073, loss_mps: 0.171717, loss_cps: 0.358791
[13:53:40.263] iteration 21711: total_loss: 0.673974, loss_sup: 0.080846, loss_mps: 0.196319, loss_cps: 0.396810
[13:53:40.409] iteration 21712: total_loss: 0.334170, loss_sup: 0.046190, loss_mps: 0.102928, loss_cps: 0.185052
[13:53:40.555] iteration 21713: total_loss: 0.333762, loss_sup: 0.073245, loss_mps: 0.097113, loss_cps: 0.163405
[13:53:40.700] iteration 21714: total_loss: 0.809057, loss_sup: 0.230217, loss_mps: 0.195359, loss_cps: 0.383480
[13:53:40.845] iteration 21715: total_loss: 0.359052, loss_sup: 0.081772, loss_mps: 0.105299, loss_cps: 0.171982
[13:53:40.992] iteration 21716: total_loss: 0.552707, loss_sup: 0.078084, loss_mps: 0.151664, loss_cps: 0.322958
[13:53:41.137] iteration 21717: total_loss: 0.410426, loss_sup: 0.025163, loss_mps: 0.129143, loss_cps: 0.256120
[13:53:41.284] iteration 21718: total_loss: 0.636324, loss_sup: 0.034960, loss_mps: 0.195286, loss_cps: 0.406079
[13:53:41.432] iteration 21719: total_loss: 0.881196, loss_sup: 0.065703, loss_mps: 0.251603, loss_cps: 0.563890
[13:53:41.579] iteration 21720: total_loss: 0.243353, loss_sup: 0.019844, loss_mps: 0.083220, loss_cps: 0.140289
[13:53:41.725] iteration 21721: total_loss: 0.660305, loss_sup: 0.171127, loss_mps: 0.165120, loss_cps: 0.324058
[13:53:41.871] iteration 21722: total_loss: 0.347346, loss_sup: 0.126292, loss_mps: 0.081377, loss_cps: 0.139677
[13:53:42.017] iteration 21723: total_loss: 0.316446, loss_sup: 0.090896, loss_mps: 0.083744, loss_cps: 0.141805
[13:53:42.163] iteration 21724: total_loss: 0.560197, loss_sup: 0.145813, loss_mps: 0.134311, loss_cps: 0.280073
[13:53:42.310] iteration 21725: total_loss: 0.370125, loss_sup: 0.008294, loss_mps: 0.122253, loss_cps: 0.239578
[13:53:42.456] iteration 21726: total_loss: 0.363283, loss_sup: 0.082869, loss_mps: 0.104439, loss_cps: 0.175974
[13:53:42.601] iteration 21727: total_loss: 0.718061, loss_sup: 0.100210, loss_mps: 0.211709, loss_cps: 0.406142
[13:53:42.747] iteration 21728: total_loss: 0.525327, loss_sup: 0.058678, loss_mps: 0.178910, loss_cps: 0.287740
[13:53:42.895] iteration 21729: total_loss: 0.834993, loss_sup: 0.106933, loss_mps: 0.230014, loss_cps: 0.498046
[13:53:43.040] iteration 21730: total_loss: 0.664011, loss_sup: 0.153161, loss_mps: 0.182590, loss_cps: 0.328261
[13:53:43.186] iteration 21731: total_loss: 0.296112, loss_sup: 0.012745, loss_mps: 0.102169, loss_cps: 0.181197
[13:53:43.332] iteration 21732: total_loss: 0.420104, loss_sup: 0.046277, loss_mps: 0.129571, loss_cps: 0.244256
[13:53:43.478] iteration 21733: total_loss: 1.048965, loss_sup: 0.179027, loss_mps: 0.272599, loss_cps: 0.597339
[13:53:43.623] iteration 21734: total_loss: 0.384373, loss_sup: 0.036980, loss_mps: 0.127128, loss_cps: 0.220265
[13:53:43.769] iteration 21735: total_loss: 0.607584, loss_sup: 0.072123, loss_mps: 0.176849, loss_cps: 0.358612
[13:53:43.830] iteration 21736: total_loss: 0.343036, loss_sup: 0.073674, loss_mps: 0.098336, loss_cps: 0.171026
[13:53:44.996] iteration 21737: total_loss: 0.801556, loss_sup: 0.246815, loss_mps: 0.172280, loss_cps: 0.382461
[13:53:45.147] iteration 21738: total_loss: 0.794578, loss_sup: 0.330369, loss_mps: 0.171543, loss_cps: 0.292667
[13:53:45.293] iteration 21739: total_loss: 0.472509, loss_sup: 0.058106, loss_mps: 0.148307, loss_cps: 0.266096
[13:53:45.441] iteration 21740: total_loss: 0.385051, loss_sup: 0.047261, loss_mps: 0.126439, loss_cps: 0.211351
[13:53:45.593] iteration 21741: total_loss: 0.295220, loss_sup: 0.031861, loss_mps: 0.099071, loss_cps: 0.164288
[13:53:45.740] iteration 21742: total_loss: 0.638228, loss_sup: 0.106902, loss_mps: 0.183068, loss_cps: 0.348258
[13:53:45.886] iteration 21743: total_loss: 0.412948, loss_sup: 0.031345, loss_mps: 0.130164, loss_cps: 0.251439
[13:53:46.039] iteration 21744: total_loss: 0.266456, loss_sup: 0.022587, loss_mps: 0.091748, loss_cps: 0.152121
[13:53:46.185] iteration 21745: total_loss: 0.400554, loss_sup: 0.014408, loss_mps: 0.131837, loss_cps: 0.254309
[13:53:46.332] iteration 21746: total_loss: 0.435262, loss_sup: 0.067406, loss_mps: 0.131196, loss_cps: 0.236660
[13:53:46.478] iteration 21747: total_loss: 0.465519, loss_sup: 0.019659, loss_mps: 0.150832, loss_cps: 0.295029
[13:53:46.625] iteration 21748: total_loss: 0.601937, loss_sup: 0.121929, loss_mps: 0.172516, loss_cps: 0.307492
[13:53:46.771] iteration 21749: total_loss: 0.391376, loss_sup: 0.038243, loss_mps: 0.125148, loss_cps: 0.227984
[13:53:46.918] iteration 21750: total_loss: 0.641419, loss_sup: 0.011593, loss_mps: 0.199463, loss_cps: 0.430363
[13:53:47.065] iteration 21751: total_loss: 0.565689, loss_sup: 0.070376, loss_mps: 0.163017, loss_cps: 0.332296
[13:53:47.215] iteration 21752: total_loss: 0.597911, loss_sup: 0.125379, loss_mps: 0.158083, loss_cps: 0.314450
[13:53:47.361] iteration 21753: total_loss: 0.512430, loss_sup: 0.058466, loss_mps: 0.153423, loss_cps: 0.300541
[13:53:47.508] iteration 21754: total_loss: 0.448467, loss_sup: 0.025661, loss_mps: 0.152995, loss_cps: 0.269812
[13:53:47.654] iteration 21755: total_loss: 0.374761, loss_sup: 0.008601, loss_mps: 0.124179, loss_cps: 0.241980
[13:53:47.800] iteration 21756: total_loss: 0.478659, loss_sup: 0.015110, loss_mps: 0.161419, loss_cps: 0.302130
[13:53:47.947] iteration 21757: total_loss: 0.596199, loss_sup: 0.129932, loss_mps: 0.161417, loss_cps: 0.304851
[13:53:48.097] iteration 21758: total_loss: 0.321173, loss_sup: 0.035888, loss_mps: 0.102948, loss_cps: 0.182338
[13:53:48.243] iteration 21759: total_loss: 0.507897, loss_sup: 0.018108, loss_mps: 0.167116, loss_cps: 0.322672
[13:53:48.390] iteration 21760: total_loss: 0.716431, loss_sup: 0.075505, loss_mps: 0.206429, loss_cps: 0.434498
[13:53:48.540] iteration 21761: total_loss: 0.462893, loss_sup: 0.018334, loss_mps: 0.148634, loss_cps: 0.295925
[13:53:48.693] iteration 21762: total_loss: 0.200155, loss_sup: 0.024492, loss_mps: 0.065436, loss_cps: 0.110226
[13:53:48.839] iteration 21763: total_loss: 0.600148, loss_sup: 0.045083, loss_mps: 0.184266, loss_cps: 0.370798
[13:53:48.986] iteration 21764: total_loss: 0.485330, loss_sup: 0.073779, loss_mps: 0.140373, loss_cps: 0.271177
[13:53:49.133] iteration 21765: total_loss: 0.484132, loss_sup: 0.066949, loss_mps: 0.145597, loss_cps: 0.271586
[13:53:49.280] iteration 21766: total_loss: 0.676317, loss_sup: 0.200439, loss_mps: 0.158808, loss_cps: 0.317070
[13:53:49.427] iteration 21767: total_loss: 0.420608, loss_sup: 0.065092, loss_mps: 0.122475, loss_cps: 0.233042
[13:53:49.577] iteration 21768: total_loss: 0.338805, loss_sup: 0.027282, loss_mps: 0.114694, loss_cps: 0.196830
[13:53:49.724] iteration 21769: total_loss: 0.905848, loss_sup: 0.143332, loss_mps: 0.224750, loss_cps: 0.537766
[13:53:49.870] iteration 21770: total_loss: 0.431490, loss_sup: 0.012187, loss_mps: 0.138009, loss_cps: 0.281294
[13:53:50.017] iteration 21771: total_loss: 0.247275, loss_sup: 0.068049, loss_mps: 0.067129, loss_cps: 0.112098
[13:53:50.163] iteration 21772: total_loss: 0.416422, loss_sup: 0.035227, loss_mps: 0.130642, loss_cps: 0.250553
[13:53:50.310] iteration 21773: total_loss: 0.466710, loss_sup: 0.065786, loss_mps: 0.135432, loss_cps: 0.265492
[13:53:50.457] iteration 21774: total_loss: 0.469009, loss_sup: 0.060306, loss_mps: 0.139716, loss_cps: 0.268987
[13:53:50.603] iteration 21775: total_loss: 0.542122, loss_sup: 0.045797, loss_mps: 0.159815, loss_cps: 0.336511
[13:53:50.750] iteration 21776: total_loss: 0.963836, loss_sup: 0.280096, loss_mps: 0.217483, loss_cps: 0.466257
[13:53:50.897] iteration 21777: total_loss: 0.722513, loss_sup: 0.274563, loss_mps: 0.152543, loss_cps: 0.295407
[13:53:51.048] iteration 21778: total_loss: 0.714937, loss_sup: 0.054106, loss_mps: 0.208380, loss_cps: 0.452452
[13:53:51.194] iteration 21779: total_loss: 0.277767, loss_sup: 0.016109, loss_mps: 0.096611, loss_cps: 0.165047
[13:53:51.345] iteration 21780: total_loss: 0.474161, loss_sup: 0.148993, loss_mps: 0.118611, loss_cps: 0.206557
[13:53:51.492] iteration 21781: total_loss: 0.364190, loss_sup: 0.028606, loss_mps: 0.129478, loss_cps: 0.206106
[13:53:51.641] iteration 21782: total_loss: 0.287253, loss_sup: 0.049629, loss_mps: 0.086829, loss_cps: 0.150796
[13:53:51.787] iteration 21783: total_loss: 0.554718, loss_sup: 0.058642, loss_mps: 0.164133, loss_cps: 0.331944
[13:53:51.934] iteration 21784: total_loss: 0.647225, loss_sup: 0.118027, loss_mps: 0.195851, loss_cps: 0.333348
[13:53:52.081] iteration 21785: total_loss: 0.317462, loss_sup: 0.026998, loss_mps: 0.107442, loss_cps: 0.183021
[13:53:52.227] iteration 21786: total_loss: 0.671216, loss_sup: 0.068337, loss_mps: 0.190746, loss_cps: 0.412133
[13:53:52.374] iteration 21787: total_loss: 0.595520, loss_sup: 0.062515, loss_mps: 0.185783, loss_cps: 0.347223
[13:53:52.521] iteration 21788: total_loss: 0.898021, loss_sup: 0.208086, loss_mps: 0.229362, loss_cps: 0.460573
[13:53:52.669] iteration 21789: total_loss: 0.551951, loss_sup: 0.056979, loss_mps: 0.166220, loss_cps: 0.328753
[13:53:52.816] iteration 21790: total_loss: 0.397489, loss_sup: 0.031791, loss_mps: 0.137345, loss_cps: 0.228353
[13:53:52.964] iteration 21791: total_loss: 0.334631, loss_sup: 0.021845, loss_mps: 0.122084, loss_cps: 0.190702
[13:53:53.112] iteration 21792: total_loss: 0.257992, loss_sup: 0.008670, loss_mps: 0.092830, loss_cps: 0.156493
[13:53:53.259] iteration 21793: total_loss: 0.397503, loss_sup: 0.105235, loss_mps: 0.110701, loss_cps: 0.181567
[13:53:53.407] iteration 21794: total_loss: 0.467690, loss_sup: 0.063115, loss_mps: 0.134688, loss_cps: 0.269887
[13:53:53.556] iteration 21795: total_loss: 0.920220, loss_sup: 0.011301, loss_mps: 0.274999, loss_cps: 0.633921
[13:53:53.704] iteration 21796: total_loss: 0.372550, loss_sup: 0.008256, loss_mps: 0.129244, loss_cps: 0.235050
[13:53:53.851] iteration 21797: total_loss: 0.295961, loss_sup: 0.003758, loss_mps: 0.099103, loss_cps: 0.193100
[13:53:54.001] iteration 21798: total_loss: 0.383532, loss_sup: 0.020457, loss_mps: 0.126962, loss_cps: 0.236113
[13:53:54.149] iteration 21799: total_loss: 0.564072, loss_sup: 0.161172, loss_mps: 0.137608, loss_cps: 0.265292
[13:53:54.296] iteration 21800: total_loss: 0.493572, loss_sup: 0.042181, loss_mps: 0.160867, loss_cps: 0.290524
[13:53:54.296] Evaluation Started ==>
[13:54:05.687] ==> valid iteration 21800: unet metrics: {'dc': 0.6348611252716206, 'jc': 0.5196000871719232, 'pre': 0.7872237259089236, 'hd': 5.453538581823504}, ynet metrics: {'dc': 0.5854586102264452, 'jc': 0.4714056444403303, 'pre': 0.8085677540761754, 'hd': 5.392145243172053}.
[13:54:05.688] Evaluation Finished!⏹️
[13:54:05.838] iteration 21801: total_loss: 0.684531, loss_sup: 0.080589, loss_mps: 0.188069, loss_cps: 0.415873
[13:54:05.986] iteration 21802: total_loss: 0.406830, loss_sup: 0.121789, loss_mps: 0.109545, loss_cps: 0.175496
[13:54:06.132] iteration 21803: total_loss: 0.557407, loss_sup: 0.066043, loss_mps: 0.170755, loss_cps: 0.320608
[13:54:06.278] iteration 21804: total_loss: 0.491205, loss_sup: 0.146447, loss_mps: 0.126876, loss_cps: 0.217882
[13:54:06.423] iteration 21805: total_loss: 0.450757, loss_sup: 0.032905, loss_mps: 0.138880, loss_cps: 0.278972
[13:54:06.570] iteration 21806: total_loss: 0.396386, loss_sup: 0.023982, loss_mps: 0.140180, loss_cps: 0.232224
[13:54:06.716] iteration 21807: total_loss: 0.900236, loss_sup: 0.060057, loss_mps: 0.261821, loss_cps: 0.578357
[13:54:06.862] iteration 21808: total_loss: 0.309191, loss_sup: 0.007216, loss_mps: 0.101222, loss_cps: 0.200753
[13:54:07.011] iteration 21809: total_loss: 0.476344, loss_sup: 0.158344, loss_mps: 0.115305, loss_cps: 0.202695
[13:54:07.157] iteration 21810: total_loss: 0.838390, loss_sup: 0.104226, loss_mps: 0.225039, loss_cps: 0.509125
[13:54:07.302] iteration 21811: total_loss: 0.648093, loss_sup: 0.081278, loss_mps: 0.190483, loss_cps: 0.376332
[13:54:07.448] iteration 21812: total_loss: 0.531852, loss_sup: 0.113318, loss_mps: 0.146262, loss_cps: 0.272272
[13:54:07.595] iteration 21813: total_loss: 0.488908, loss_sup: 0.042817, loss_mps: 0.153521, loss_cps: 0.292571
[13:54:07.739] iteration 21814: total_loss: 0.351463, loss_sup: 0.027165, loss_mps: 0.114428, loss_cps: 0.209870
[13:54:07.885] iteration 21815: total_loss: 0.787891, loss_sup: 0.200154, loss_mps: 0.203241, loss_cps: 0.384497
[13:54:08.030] iteration 21816: total_loss: 0.216586, loss_sup: 0.016527, loss_mps: 0.078238, loss_cps: 0.121821
[13:54:08.175] iteration 21817: total_loss: 0.324463, loss_sup: 0.023204, loss_mps: 0.107618, loss_cps: 0.193641
[13:54:08.321] iteration 21818: total_loss: 0.294769, loss_sup: 0.021328, loss_mps: 0.095941, loss_cps: 0.177500
[13:54:08.466] iteration 21819: total_loss: 0.549779, loss_sup: 0.084906, loss_mps: 0.157138, loss_cps: 0.307734
[13:54:08.613] iteration 21820: total_loss: 0.659946, loss_sup: 0.117271, loss_mps: 0.183819, loss_cps: 0.358856
[13:54:08.758] iteration 21821: total_loss: 0.745802, loss_sup: 0.049973, loss_mps: 0.227347, loss_cps: 0.468482
[13:54:08.904] iteration 21822: total_loss: 0.622599, loss_sup: 0.036278, loss_mps: 0.194687, loss_cps: 0.391635
[13:54:09.049] iteration 21823: total_loss: 0.384967, loss_sup: 0.032418, loss_mps: 0.127696, loss_cps: 0.224853
[13:54:09.195] iteration 21824: total_loss: 0.276118, loss_sup: 0.042591, loss_mps: 0.091417, loss_cps: 0.142110
[13:54:09.340] iteration 21825: total_loss: 0.979649, loss_sup: 0.082457, loss_mps: 0.268614, loss_cps: 0.628578
[13:54:09.486] iteration 21826: total_loss: 0.536175, loss_sup: 0.052665, loss_mps: 0.162155, loss_cps: 0.321355
[13:54:09.631] iteration 21827: total_loss: 0.671935, loss_sup: 0.070048, loss_mps: 0.197346, loss_cps: 0.404542
[13:54:09.776] iteration 21828: total_loss: 0.434488, loss_sup: 0.053823, loss_mps: 0.135753, loss_cps: 0.244912
[13:54:09.921] iteration 21829: total_loss: 0.458985, loss_sup: 0.049863, loss_mps: 0.138567, loss_cps: 0.270555
[13:54:10.067] iteration 21830: total_loss: 0.604094, loss_sup: 0.106455, loss_mps: 0.165011, loss_cps: 0.332629
[13:54:10.212] iteration 21831: total_loss: 0.526581, loss_sup: 0.088543, loss_mps: 0.154015, loss_cps: 0.284024
[13:54:10.357] iteration 21832: total_loss: 0.842849, loss_sup: 0.082895, loss_mps: 0.239804, loss_cps: 0.520150
[13:54:10.502] iteration 21833: total_loss: 0.477357, loss_sup: 0.022185, loss_mps: 0.149464, loss_cps: 0.305708
[13:54:10.648] iteration 21834: total_loss: 0.695798, loss_sup: 0.190644, loss_mps: 0.165306, loss_cps: 0.339848
[13:54:10.794] iteration 21835: total_loss: 0.690617, loss_sup: 0.035734, loss_mps: 0.203556, loss_cps: 0.451326
[13:54:10.939] iteration 21836: total_loss: 0.618936, loss_sup: 0.036266, loss_mps: 0.184302, loss_cps: 0.398368
[13:54:11.085] iteration 21837: total_loss: 0.766748, loss_sup: 0.047378, loss_mps: 0.228828, loss_cps: 0.490542
[13:54:11.230] iteration 21838: total_loss: 0.368627, loss_sup: 0.053062, loss_mps: 0.108849, loss_cps: 0.206717
[13:54:11.375] iteration 21839: total_loss: 0.814113, loss_sup: 0.179976, loss_mps: 0.205083, loss_cps: 0.429054
[13:54:11.521] iteration 21840: total_loss: 0.453310, loss_sup: 0.084899, loss_mps: 0.130295, loss_cps: 0.238116
[13:54:11.667] iteration 21841: total_loss: 0.547133, loss_sup: 0.097107, loss_mps: 0.153198, loss_cps: 0.296828
[13:54:11.813] iteration 21842: total_loss: 0.752823, loss_sup: 0.152763, loss_mps: 0.195763, loss_cps: 0.404296
[13:54:11.958] iteration 21843: total_loss: 0.899951, loss_sup: 0.172283, loss_mps: 0.226242, loss_cps: 0.501426
[13:54:12.103] iteration 21844: total_loss: 0.434507, loss_sup: 0.035862, loss_mps: 0.142486, loss_cps: 0.256159
[13:54:12.248] iteration 21845: total_loss: 0.404563, loss_sup: 0.089981, loss_mps: 0.112327, loss_cps: 0.202256
[13:54:12.394] iteration 21846: total_loss: 0.277126, loss_sup: 0.022250, loss_mps: 0.093041, loss_cps: 0.161835
[13:54:12.539] iteration 21847: total_loss: 0.588885, loss_sup: 0.082839, loss_mps: 0.170643, loss_cps: 0.335404
[13:54:12.686] iteration 21848: total_loss: 0.541391, loss_sup: 0.234346, loss_mps: 0.109895, loss_cps: 0.197150
[13:54:12.832] iteration 21849: total_loss: 0.499776, loss_sup: 0.043182, loss_mps: 0.162996, loss_cps: 0.293598
[13:54:12.977] iteration 21850: total_loss: 0.670034, loss_sup: 0.146595, loss_mps: 0.179763, loss_cps: 0.343676
[13:54:13.128] iteration 21851: total_loss: 0.640372, loss_sup: 0.102845, loss_mps: 0.189479, loss_cps: 0.348048
[13:54:13.273] iteration 21852: total_loss: 0.424432, loss_sup: 0.073340, loss_mps: 0.124832, loss_cps: 0.226260
[13:54:13.419] iteration 21853: total_loss: 0.789384, loss_sup: 0.200575, loss_mps: 0.188424, loss_cps: 0.400386
[13:54:13.564] iteration 21854: total_loss: 0.792634, loss_sup: 0.104745, loss_mps: 0.226388, loss_cps: 0.461501
[13:54:13.711] iteration 21855: total_loss: 0.372949, loss_sup: 0.056108, loss_mps: 0.112570, loss_cps: 0.204270
[13:54:13.857] iteration 21856: total_loss: 0.597384, loss_sup: 0.046742, loss_mps: 0.175439, loss_cps: 0.375203
[13:54:14.002] iteration 21857: total_loss: 0.569127, loss_sup: 0.159926, loss_mps: 0.149645, loss_cps: 0.259555
[13:54:14.149] iteration 21858: total_loss: 0.295857, loss_sup: 0.034017, loss_mps: 0.101825, loss_cps: 0.160015
[13:54:14.294] iteration 21859: total_loss: 0.447962, loss_sup: 0.080850, loss_mps: 0.128790, loss_cps: 0.238321
[13:54:14.440] iteration 21860: total_loss: 0.431146, loss_sup: 0.019265, loss_mps: 0.147567, loss_cps: 0.264314
[13:54:14.586] iteration 21861: total_loss: 0.527546, loss_sup: 0.030893, loss_mps: 0.168217, loss_cps: 0.328436
[13:54:14.731] iteration 21862: total_loss: 0.591252, loss_sup: 0.106942, loss_mps: 0.159960, loss_cps: 0.324350
[13:54:14.877] iteration 21863: total_loss: 0.808055, loss_sup: 0.120685, loss_mps: 0.219681, loss_cps: 0.467690
[13:54:15.022] iteration 21864: total_loss: 0.320588, loss_sup: 0.017180, loss_mps: 0.110653, loss_cps: 0.192755
[13:54:15.168] iteration 21865: total_loss: 0.969816, loss_sup: 0.172365, loss_mps: 0.251460, loss_cps: 0.545991
[13:54:15.314] iteration 21866: total_loss: 0.501258, loss_sup: 0.059456, loss_mps: 0.149487, loss_cps: 0.292315
[13:54:15.459] iteration 21867: total_loss: 0.266681, loss_sup: 0.050749, loss_mps: 0.082789, loss_cps: 0.133143
[13:54:15.607] iteration 21868: total_loss: 0.887046, loss_sup: 0.164058, loss_mps: 0.230383, loss_cps: 0.492605
[13:54:15.754] iteration 21869: total_loss: 0.810902, loss_sup: 0.310393, loss_mps: 0.165111, loss_cps: 0.335397
[13:54:15.900] iteration 21870: total_loss: 0.416481, loss_sup: 0.037429, loss_mps: 0.128632, loss_cps: 0.250420
[13:54:16.046] iteration 21871: total_loss: 0.435013, loss_sup: 0.168753, loss_mps: 0.089870, loss_cps: 0.176390
[13:54:16.191] iteration 21872: total_loss: 0.512781, loss_sup: 0.053662, loss_mps: 0.153897, loss_cps: 0.305221
[13:54:16.336] iteration 21873: total_loss: 0.304965, loss_sup: 0.064790, loss_mps: 0.088338, loss_cps: 0.151837
[13:54:16.482] iteration 21874: total_loss: 0.478031, loss_sup: 0.039726, loss_mps: 0.151872, loss_cps: 0.286432
[13:54:16.629] iteration 21875: total_loss: 0.567310, loss_sup: 0.041750, loss_mps: 0.180355, loss_cps: 0.345205
[13:54:16.775] iteration 21876: total_loss: 0.248935, loss_sup: 0.042578, loss_mps: 0.083174, loss_cps: 0.123183
[13:54:16.921] iteration 21877: total_loss: 0.370238, loss_sup: 0.041811, loss_mps: 0.122872, loss_cps: 0.205554
[13:54:17.068] iteration 21878: total_loss: 0.463841, loss_sup: 0.053978, loss_mps: 0.140025, loss_cps: 0.269838
[13:54:17.214] iteration 21879: total_loss: 0.682864, loss_sup: 0.127813, loss_mps: 0.187855, loss_cps: 0.367197
[13:54:17.359] iteration 21880: total_loss: 0.934921, loss_sup: 0.003637, loss_mps: 0.263786, loss_cps: 0.667498
[13:54:17.505] iteration 21881: total_loss: 0.229597, loss_sup: 0.010015, loss_mps: 0.083903, loss_cps: 0.135679
[13:54:17.655] iteration 21882: total_loss: 0.924206, loss_sup: 0.144812, loss_mps: 0.249212, loss_cps: 0.530182
[13:54:17.802] iteration 21883: total_loss: 0.786003, loss_sup: 0.146437, loss_mps: 0.207258, loss_cps: 0.432308
[13:54:17.948] iteration 21884: total_loss: 0.604052, loss_sup: 0.043712, loss_mps: 0.189155, loss_cps: 0.371185
[13:54:18.094] iteration 21885: total_loss: 0.762543, loss_sup: 0.102289, loss_mps: 0.205567, loss_cps: 0.454687
[13:54:18.240] iteration 21886: total_loss: 0.517727, loss_sup: 0.061434, loss_mps: 0.149885, loss_cps: 0.306408
[13:54:18.386] iteration 21887: total_loss: 0.475980, loss_sup: 0.055646, loss_mps: 0.136574, loss_cps: 0.283760
[13:54:18.532] iteration 21888: total_loss: 0.666236, loss_sup: 0.050910, loss_mps: 0.192880, loss_cps: 0.422446
[13:54:18.678] iteration 21889: total_loss: 0.590398, loss_sup: 0.022377, loss_mps: 0.193111, loss_cps: 0.374911
[13:54:18.828] iteration 21890: total_loss: 0.352009, loss_sup: 0.010345, loss_mps: 0.111417, loss_cps: 0.230247
[13:54:18.974] iteration 21891: total_loss: 0.282199, loss_sup: 0.006649, loss_mps: 0.102356, loss_cps: 0.173195
[13:54:19.120] iteration 21892: total_loss: 0.595441, loss_sup: 0.138388, loss_mps: 0.153908, loss_cps: 0.303145
[13:54:19.266] iteration 21893: total_loss: 0.535928, loss_sup: 0.160195, loss_mps: 0.136266, loss_cps: 0.239467
[13:54:19.411] iteration 21894: total_loss: 0.316190, loss_sup: 0.030896, loss_mps: 0.103899, loss_cps: 0.181395
[13:54:19.557] iteration 21895: total_loss: 0.659108, loss_sup: 0.054523, loss_mps: 0.203714, loss_cps: 0.400871
[13:54:19.703] iteration 21896: total_loss: 0.611233, loss_sup: 0.049933, loss_mps: 0.195550, loss_cps: 0.365751
[13:54:19.851] iteration 21897: total_loss: 0.725190, loss_sup: 0.050559, loss_mps: 0.222988, loss_cps: 0.451644
[13:54:19.997] iteration 21898: total_loss: 0.773575, loss_sup: 0.173507, loss_mps: 0.204174, loss_cps: 0.395894
[13:54:20.143] iteration 21899: total_loss: 0.829307, loss_sup: 0.100646, loss_mps: 0.236454, loss_cps: 0.492206
[13:54:20.289] iteration 21900: total_loss: 0.415093, loss_sup: 0.112562, loss_mps: 0.105352, loss_cps: 0.197179
[13:54:20.290] Evaluation Started ==>
[13:54:31.685] ==> valid iteration 21900: unet metrics: {'dc': 0.5935815112854548, 'jc': 0.47837007917291263, 'pre': 0.7975226293113201, 'hd': 5.469266796884456}, ynet metrics: {'dc': 0.600785327041245, 'jc': 0.4849762974401931, 'pre': 0.793600618955159, 'hd': 5.481561592876594}.
[13:54:31.687] Evaluation Finished!⏹️
[13:54:31.842] iteration 21901: total_loss: 0.431470, loss_sup: 0.078465, loss_mps: 0.129383, loss_cps: 0.223622
[13:54:31.990] iteration 21902: total_loss: 0.537001, loss_sup: 0.067178, loss_mps: 0.156202, loss_cps: 0.313621
[13:54:32.138] iteration 21903: total_loss: 0.657385, loss_sup: 0.073166, loss_mps: 0.203115, loss_cps: 0.381105
[13:54:32.285] iteration 21904: total_loss: 0.419367, loss_sup: 0.048633, loss_mps: 0.124176, loss_cps: 0.246559
[13:54:32.431] iteration 21905: total_loss: 0.704827, loss_sup: 0.005759, loss_mps: 0.218434, loss_cps: 0.480634
[13:54:32.576] iteration 21906: total_loss: 0.495811, loss_sup: 0.006092, loss_mps: 0.173486, loss_cps: 0.316233
[13:54:32.722] iteration 21907: total_loss: 0.603715, loss_sup: 0.084433, loss_mps: 0.189340, loss_cps: 0.329942
[13:54:32.868] iteration 21908: total_loss: 0.927702, loss_sup: 0.041684, loss_mps: 0.285919, loss_cps: 0.600099
[13:54:33.015] iteration 21909: total_loss: 0.499376, loss_sup: 0.108539, loss_mps: 0.139475, loss_cps: 0.251363
[13:54:33.160] iteration 21910: total_loss: 0.290121, loss_sup: 0.032760, loss_mps: 0.092966, loss_cps: 0.164396
[13:54:33.306] iteration 21911: total_loss: 0.683017, loss_sup: 0.057812, loss_mps: 0.202982, loss_cps: 0.422223
[13:54:33.454] iteration 21912: total_loss: 0.731920, loss_sup: 0.124302, loss_mps: 0.213138, loss_cps: 0.394479
[13:54:33.601] iteration 21913: total_loss: 0.350436, loss_sup: 0.014224, loss_mps: 0.122900, loss_cps: 0.213311
[13:54:33.747] iteration 21914: total_loss: 0.595109, loss_sup: 0.094345, loss_mps: 0.171173, loss_cps: 0.329592
[13:54:33.895] iteration 21915: total_loss: 0.519973, loss_sup: 0.109580, loss_mps: 0.154144, loss_cps: 0.256249
[13:54:34.041] iteration 21916: total_loss: 0.273692, loss_sup: 0.006110, loss_mps: 0.098614, loss_cps: 0.168968
[13:54:34.188] iteration 21917: total_loss: 0.473050, loss_sup: 0.015071, loss_mps: 0.162500, loss_cps: 0.295479
[13:54:34.334] iteration 21918: total_loss: 0.498764, loss_sup: 0.043688, loss_mps: 0.153390, loss_cps: 0.301686
[13:54:34.480] iteration 21919: total_loss: 0.471824, loss_sup: 0.054141, loss_mps: 0.148136, loss_cps: 0.269547
[13:54:34.626] iteration 21920: total_loss: 0.752362, loss_sup: 0.073120, loss_mps: 0.215769, loss_cps: 0.463474
[13:54:34.772] iteration 21921: total_loss: 0.585997, loss_sup: 0.060919, loss_mps: 0.172885, loss_cps: 0.352193
[13:54:34.919] iteration 21922: total_loss: 0.400521, loss_sup: 0.075599, loss_mps: 0.117103, loss_cps: 0.207819
[13:54:35.065] iteration 21923: total_loss: 0.520133, loss_sup: 0.048067, loss_mps: 0.151653, loss_cps: 0.320413
[13:54:35.212] iteration 21924: total_loss: 0.350893, loss_sup: 0.046628, loss_mps: 0.100292, loss_cps: 0.203973
[13:54:35.358] iteration 21925: total_loss: 0.311967, loss_sup: 0.021337, loss_mps: 0.104244, loss_cps: 0.186387
[13:54:35.504] iteration 21926: total_loss: 0.547805, loss_sup: 0.083827, loss_mps: 0.156436, loss_cps: 0.307542
[13:54:35.649] iteration 21927: total_loss: 0.583614, loss_sup: 0.043205, loss_mps: 0.173577, loss_cps: 0.366832
[13:54:35.795] iteration 21928: total_loss: 0.658370, loss_sup: 0.116603, loss_mps: 0.182205, loss_cps: 0.359563
[13:54:35.942] iteration 21929: total_loss: 0.674438, loss_sup: 0.072443, loss_mps: 0.203112, loss_cps: 0.398883
[13:54:36.088] iteration 21930: total_loss: 0.227530, loss_sup: 0.014473, loss_mps: 0.080760, loss_cps: 0.132298
[13:54:36.234] iteration 21931: total_loss: 0.250428, loss_sup: 0.030132, loss_mps: 0.079019, loss_cps: 0.141276
[13:54:36.380] iteration 21932: total_loss: 0.500679, loss_sup: 0.126935, loss_mps: 0.122069, loss_cps: 0.251675
[13:54:36.526] iteration 21933: total_loss: 0.408849, loss_sup: 0.072048, loss_mps: 0.122444, loss_cps: 0.214358
[13:54:36.672] iteration 21934: total_loss: 0.466946, loss_sup: 0.030356, loss_mps: 0.132024, loss_cps: 0.304566
[13:54:36.817] iteration 21935: total_loss: 0.427278, loss_sup: 0.024630, loss_mps: 0.130737, loss_cps: 0.271910
[13:54:36.963] iteration 21936: total_loss: 0.505984, loss_sup: 0.112111, loss_mps: 0.135141, loss_cps: 0.258733
[13:54:37.109] iteration 21937: total_loss: 1.008216, loss_sup: 0.204943, loss_mps: 0.256316, loss_cps: 0.546957
[13:54:37.256] iteration 21938: total_loss: 0.608498, loss_sup: 0.277113, loss_mps: 0.115283, loss_cps: 0.216102
[13:54:37.402] iteration 21939: total_loss: 0.327193, loss_sup: 0.007959, loss_mps: 0.106937, loss_cps: 0.212297
[13:54:37.548] iteration 21940: total_loss: 0.409409, loss_sup: 0.081234, loss_mps: 0.121250, loss_cps: 0.206925
[13:54:37.695] iteration 21941: total_loss: 0.802194, loss_sup: 0.329253, loss_mps: 0.163827, loss_cps: 0.309114
[13:54:37.841] iteration 21942: total_loss: 0.556874, loss_sup: 0.103328, loss_mps: 0.147433, loss_cps: 0.306113
[13:54:37.987] iteration 21943: total_loss: 0.571622, loss_sup: 0.127821, loss_mps: 0.149591, loss_cps: 0.294210
[13:54:38.136] iteration 21944: total_loss: 0.340292, loss_sup: 0.020066, loss_mps: 0.115379, loss_cps: 0.204847
[13:54:38.283] iteration 21945: total_loss: 0.251713, loss_sup: 0.025546, loss_mps: 0.088121, loss_cps: 0.138046
[13:54:38.429] iteration 21946: total_loss: 0.413741, loss_sup: 0.059458, loss_mps: 0.138031, loss_cps: 0.216252
[13:54:38.576] iteration 21947: total_loss: 0.556792, loss_sup: 0.082398, loss_mps: 0.157947, loss_cps: 0.316447
[13:54:38.723] iteration 21948: total_loss: 0.667589, loss_sup: 0.166256, loss_mps: 0.168551, loss_cps: 0.332782
[13:54:38.869] iteration 21949: total_loss: 0.511068, loss_sup: 0.146118, loss_mps: 0.133962, loss_cps: 0.230988
[13:54:39.015] iteration 21950: total_loss: 0.300360, loss_sup: 0.016161, loss_mps: 0.104313, loss_cps: 0.179887
[13:54:39.161] iteration 21951: total_loss: 0.489547, loss_sup: 0.026761, loss_mps: 0.164106, loss_cps: 0.298681
[13:54:39.307] iteration 21952: total_loss: 0.342503, loss_sup: 0.025582, loss_mps: 0.114314, loss_cps: 0.202607
[13:54:39.454] iteration 21953: total_loss: 0.340011, loss_sup: 0.015793, loss_mps: 0.117346, loss_cps: 0.206872
[13:54:39.600] iteration 21954: total_loss: 0.341760, loss_sup: 0.008043, loss_mps: 0.118871, loss_cps: 0.214846
[13:54:39.747] iteration 21955: total_loss: 0.609405, loss_sup: 0.085286, loss_mps: 0.174969, loss_cps: 0.349151
[13:54:39.895] iteration 21956: total_loss: 0.302131, loss_sup: 0.029620, loss_mps: 0.103789, loss_cps: 0.168722
[13:54:40.041] iteration 21957: total_loss: 0.251006, loss_sup: 0.031135, loss_mps: 0.082648, loss_cps: 0.137222
[13:54:40.187] iteration 21958: total_loss: 0.489841, loss_sup: 0.146917, loss_mps: 0.123789, loss_cps: 0.219135
[13:54:40.334] iteration 21959: total_loss: 0.795814, loss_sup: 0.112018, loss_mps: 0.223230, loss_cps: 0.460566
[13:54:40.483] iteration 21960: total_loss: 0.260714, loss_sup: 0.009054, loss_mps: 0.095954, loss_cps: 0.155706
[13:54:40.630] iteration 21961: total_loss: 0.687976, loss_sup: 0.149150, loss_mps: 0.196435, loss_cps: 0.342392
[13:54:40.779] iteration 21962: total_loss: 0.520187, loss_sup: 0.023279, loss_mps: 0.169674, loss_cps: 0.327234
[13:54:40.926] iteration 21963: total_loss: 0.254290, loss_sup: 0.021812, loss_mps: 0.091929, loss_cps: 0.140550
[13:54:41.073] iteration 21964: total_loss: 0.502749, loss_sup: 0.057922, loss_mps: 0.150898, loss_cps: 0.293930
[13:54:41.219] iteration 21965: total_loss: 0.485359, loss_sup: 0.009483, loss_mps: 0.161710, loss_cps: 0.314166
[13:54:41.366] iteration 21966: total_loss: 0.357179, loss_sup: 0.015531, loss_mps: 0.122632, loss_cps: 0.219016
[13:54:41.511] iteration 21967: total_loss: 0.541238, loss_sup: 0.137288, loss_mps: 0.137691, loss_cps: 0.266259
[13:54:41.657] iteration 21968: total_loss: 0.386070, loss_sup: 0.076033, loss_mps: 0.108576, loss_cps: 0.201461
[13:54:41.804] iteration 21969: total_loss: 1.363947, loss_sup: 0.171248, loss_mps: 0.383213, loss_cps: 0.809486
[13:54:41.950] iteration 21970: total_loss: 0.833757, loss_sup: 0.161173, loss_mps: 0.218721, loss_cps: 0.453863
[13:54:42.096] iteration 21971: total_loss: 0.380481, loss_sup: 0.048544, loss_mps: 0.124177, loss_cps: 0.207760
[13:54:42.242] iteration 21972: total_loss: 0.241006, loss_sup: 0.024926, loss_mps: 0.078674, loss_cps: 0.137406
[13:54:42.388] iteration 21973: total_loss: 0.606504, loss_sup: 0.149648, loss_mps: 0.150626, loss_cps: 0.306230
[13:54:42.536] iteration 21974: total_loss: 0.484163, loss_sup: 0.033517, loss_mps: 0.151817, loss_cps: 0.298830
[13:54:42.682] iteration 21975: total_loss: 0.648083, loss_sup: 0.086024, loss_mps: 0.185245, loss_cps: 0.376813
[13:54:42.830] iteration 21976: total_loss: 0.742831, loss_sup: 0.372166, loss_mps: 0.121643, loss_cps: 0.249021
[13:54:42.976] iteration 21977: total_loss: 0.588961, loss_sup: 0.033913, loss_mps: 0.177789, loss_cps: 0.377260
[13:54:43.122] iteration 21978: total_loss: 0.495575, loss_sup: 0.162745, loss_mps: 0.118913, loss_cps: 0.213917
[13:54:43.268] iteration 21979: total_loss: 0.293409, loss_sup: 0.026013, loss_mps: 0.096922, loss_cps: 0.170473
[13:54:43.414] iteration 21980: total_loss: 0.747825, loss_sup: 0.067484, loss_mps: 0.214628, loss_cps: 0.465713
[13:54:43.560] iteration 21981: total_loss: 0.454734, loss_sup: 0.009024, loss_mps: 0.145904, loss_cps: 0.299806
[13:54:43.706] iteration 21982: total_loss: 0.516394, loss_sup: 0.093290, loss_mps: 0.143411, loss_cps: 0.279693
[13:54:43.852] iteration 21983: total_loss: 0.499556, loss_sup: 0.007002, loss_mps: 0.163318, loss_cps: 0.329236
[13:54:43.998] iteration 21984: total_loss: 0.602979, loss_sup: 0.055531, loss_mps: 0.184857, loss_cps: 0.362591
[13:54:44.146] iteration 21985: total_loss: 0.519733, loss_sup: 0.018400, loss_mps: 0.162031, loss_cps: 0.339302
[13:54:44.292] iteration 21986: total_loss: 0.861258, loss_sup: 0.124958, loss_mps: 0.229428, loss_cps: 0.506872
[13:54:44.442] iteration 21987: total_loss: 0.736731, loss_sup: 0.354761, loss_mps: 0.138420, loss_cps: 0.243549
[13:54:44.588] iteration 21988: total_loss: 0.837768, loss_sup: 0.117681, loss_mps: 0.231865, loss_cps: 0.488222
[13:54:44.734] iteration 21989: total_loss: 0.452439, loss_sup: 0.034151, loss_mps: 0.141766, loss_cps: 0.276522
[13:54:44.883] iteration 21990: total_loss: 0.593352, loss_sup: 0.049194, loss_mps: 0.179670, loss_cps: 0.364489
[13:54:45.029] iteration 21991: total_loss: 0.686193, loss_sup: 0.051826, loss_mps: 0.213504, loss_cps: 0.420864
[13:54:45.175] iteration 21992: total_loss: 0.725590, loss_sup: 0.070321, loss_mps: 0.206729, loss_cps: 0.448539
[13:54:45.323] iteration 21993: total_loss: 0.663787, loss_sup: 0.048028, loss_mps: 0.197717, loss_cps: 0.418041
[13:54:45.471] iteration 21994: total_loss: 1.356456, loss_sup: 0.234430, loss_mps: 0.355236, loss_cps: 0.766790
[13:54:45.617] iteration 21995: total_loss: 0.346126, loss_sup: 0.095204, loss_mps: 0.090760, loss_cps: 0.160162
[13:54:45.764] iteration 21996: total_loss: 0.836959, loss_sup: 0.183700, loss_mps: 0.219349, loss_cps: 0.433910
[13:54:45.921] iteration 21997: total_loss: 0.668200, loss_sup: 0.012950, loss_mps: 0.219073, loss_cps: 0.436177
[13:54:46.068] iteration 21998: total_loss: 0.393619, loss_sup: 0.067681, loss_mps: 0.119746, loss_cps: 0.206192
[13:54:46.217] iteration 21999: total_loss: 0.577837, loss_sup: 0.057147, loss_mps: 0.176412, loss_cps: 0.344278
[13:54:46.364] iteration 22000: total_loss: 0.511846, loss_sup: 0.046693, loss_mps: 0.159382, loss_cps: 0.305771
[13:54:46.364] Evaluation Started ==>
[13:54:57.693] ==> valid iteration 22000: unet metrics: {'dc': 0.6460546155392433, 'jc': 0.5337413450353908, 'pre': 0.7651509865543943, 'hd': 5.461526527706647}, ynet metrics: {'dc': 0.590863366301303, 'jc': 0.4743030960801522, 'pre': 0.7939514612683278, 'hd': 5.399543017171763}.
[13:54:57.695] Evaluation Finished!⏹️
[13:54:57.845] iteration 22001: total_loss: 0.600277, loss_sup: 0.184057, loss_mps: 0.137072, loss_cps: 0.279149
[13:54:57.993] iteration 22002: total_loss: 0.776414, loss_sup: 0.151638, loss_mps: 0.212392, loss_cps: 0.412385
[13:54:58.140] iteration 22003: total_loss: 0.569264, loss_sup: 0.034819, loss_mps: 0.181835, loss_cps: 0.352610
[13:54:58.286] iteration 22004: total_loss: 0.294396, loss_sup: 0.013048, loss_mps: 0.100997, loss_cps: 0.180351
[13:54:58.431] iteration 22005: total_loss: 0.663841, loss_sup: 0.022966, loss_mps: 0.209638, loss_cps: 0.431237
[13:54:58.577] iteration 22006: total_loss: 0.425080, loss_sup: 0.058943, loss_mps: 0.135775, loss_cps: 0.230362
[13:54:58.722] iteration 22007: total_loss: 0.468375, loss_sup: 0.063356, loss_mps: 0.155229, loss_cps: 0.249789
[13:54:58.872] iteration 22008: total_loss: 0.750871, loss_sup: 0.236253, loss_mps: 0.183921, loss_cps: 0.330697
[13:54:59.018] iteration 22009: total_loss: 0.724180, loss_sup: 0.096451, loss_mps: 0.223692, loss_cps: 0.404038
[13:54:59.164] iteration 22010: total_loss: 0.536288, loss_sup: 0.109559, loss_mps: 0.145218, loss_cps: 0.281511
[13:54:59.310] iteration 22011: total_loss: 0.289855, loss_sup: 0.008380, loss_mps: 0.107446, loss_cps: 0.174029
[13:54:59.455] iteration 22012: total_loss: 0.469346, loss_sup: 0.074330, loss_mps: 0.129367, loss_cps: 0.265650
[13:54:59.601] iteration 22013: total_loss: 0.717114, loss_sup: 0.016660, loss_mps: 0.228610, loss_cps: 0.471844
[13:54:59.747] iteration 22014: total_loss: 0.522000, loss_sup: 0.171389, loss_mps: 0.122268, loss_cps: 0.228343
[13:54:59.895] iteration 22015: total_loss: 0.414643, loss_sup: 0.041483, loss_mps: 0.129331, loss_cps: 0.243829
[13:55:00.043] iteration 22016: total_loss: 0.555398, loss_sup: 0.037007, loss_mps: 0.175394, loss_cps: 0.342997
[13:55:00.188] iteration 22017: total_loss: 0.316559, loss_sup: 0.039466, loss_mps: 0.104533, loss_cps: 0.172560
[13:55:00.334] iteration 22018: total_loss: 0.841891, loss_sup: 0.091816, loss_mps: 0.245860, loss_cps: 0.504215
[13:55:00.480] iteration 22019: total_loss: 0.540094, loss_sup: 0.036027, loss_mps: 0.179862, loss_cps: 0.324204
[13:55:00.626] iteration 22020: total_loss: 1.171329, loss_sup: 0.193395, loss_mps: 0.325175, loss_cps: 0.652760
[13:55:00.772] iteration 22021: total_loss: 0.615692, loss_sup: 0.147525, loss_mps: 0.157630, loss_cps: 0.310537
[13:55:00.919] iteration 22022: total_loss: 1.130638, loss_sup: 0.465651, loss_mps: 0.241082, loss_cps: 0.423906
[13:55:01.065] iteration 22023: total_loss: 0.730485, loss_sup: 0.197502, loss_mps: 0.200204, loss_cps: 0.332779
[13:55:01.211] iteration 22024: total_loss: 0.535444, loss_sup: 0.076164, loss_mps: 0.164606, loss_cps: 0.294674
[13:55:01.357] iteration 22025: total_loss: 0.406077, loss_sup: 0.004323, loss_mps: 0.142365, loss_cps: 0.259388
[13:55:01.502] iteration 22026: total_loss: 0.574795, loss_sup: 0.018247, loss_mps: 0.181460, loss_cps: 0.375088
[13:55:01.648] iteration 22027: total_loss: 0.363104, loss_sup: 0.015792, loss_mps: 0.137303, loss_cps: 0.210009
[13:55:01.794] iteration 22028: total_loss: 0.389590, loss_sup: 0.065305, loss_mps: 0.120700, loss_cps: 0.203585
[13:55:01.940] iteration 22029: total_loss: 0.485154, loss_sup: 0.065413, loss_mps: 0.139830, loss_cps: 0.279911
[13:55:02.086] iteration 22030: total_loss: 0.526256, loss_sup: 0.059051, loss_mps: 0.163645, loss_cps: 0.303560
[13:55:02.232] iteration 22031: total_loss: 0.246256, loss_sup: 0.013449, loss_mps: 0.088673, loss_cps: 0.144134
[13:55:02.378] iteration 22032: total_loss: 0.354234, loss_sup: 0.048330, loss_mps: 0.112079, loss_cps: 0.193825
[13:55:02.524] iteration 22033: total_loss: 0.475745, loss_sup: 0.102239, loss_mps: 0.138632, loss_cps: 0.234874
[13:55:02.670] iteration 22034: total_loss: 0.822336, loss_sup: 0.473876, loss_mps: 0.128514, loss_cps: 0.219945
[13:55:02.816] iteration 22035: total_loss: 0.438455, loss_sup: 0.034919, loss_mps: 0.146653, loss_cps: 0.256883
[13:55:02.964] iteration 22036: total_loss: 0.349685, loss_sup: 0.060349, loss_mps: 0.103760, loss_cps: 0.185576
[13:55:03.110] iteration 22037: total_loss: 0.361931, loss_sup: 0.022436, loss_mps: 0.129793, loss_cps: 0.209702
[13:55:03.256] iteration 22038: total_loss: 0.759675, loss_sup: 0.079960, loss_mps: 0.225044, loss_cps: 0.454671
[13:55:03.402] iteration 22039: total_loss: 0.799034, loss_sup: 0.065963, loss_mps: 0.245120, loss_cps: 0.487951
[13:55:03.549] iteration 22040: total_loss: 0.403211, loss_sup: 0.014696, loss_mps: 0.136004, loss_cps: 0.252511
[13:55:03.695] iteration 22041: total_loss: 0.482328, loss_sup: 0.017376, loss_mps: 0.155016, loss_cps: 0.309936
[13:55:03.841] iteration 22042: total_loss: 0.656141, loss_sup: 0.141300, loss_mps: 0.172729, loss_cps: 0.342112
[13:55:03.991] iteration 22043: total_loss: 0.533668, loss_sup: 0.112550, loss_mps: 0.142858, loss_cps: 0.278260
[13:55:04.137] iteration 22044: total_loss: 0.625425, loss_sup: 0.039093, loss_mps: 0.186091, loss_cps: 0.400240
[13:55:04.283] iteration 22045: total_loss: 0.546982, loss_sup: 0.019549, loss_mps: 0.167561, loss_cps: 0.359872
[13:55:04.430] iteration 22046: total_loss: 0.438516, loss_sup: 0.118656, loss_mps: 0.114430, loss_cps: 0.205430
[13:55:04.577] iteration 22047: total_loss: 0.576598, loss_sup: 0.058443, loss_mps: 0.172142, loss_cps: 0.346013
[13:55:04.724] iteration 22048: total_loss: 0.426209, loss_sup: 0.055663, loss_mps: 0.131677, loss_cps: 0.238869
[13:55:04.870] iteration 22049: total_loss: 0.332218, loss_sup: 0.011456, loss_mps: 0.116111, loss_cps: 0.204651
[13:55:05.016] iteration 22050: total_loss: 0.576448, loss_sup: 0.056515, loss_mps: 0.187419, loss_cps: 0.332514
[13:55:05.162] iteration 22051: total_loss: 0.257457, loss_sup: 0.054795, loss_mps: 0.078952, loss_cps: 0.123710
[13:55:05.309] iteration 22052: total_loss: 0.337513, loss_sup: 0.008817, loss_mps: 0.117820, loss_cps: 0.210876
[13:55:05.455] iteration 22053: total_loss: 0.350387, loss_sup: 0.046883, loss_mps: 0.110856, loss_cps: 0.192648
[13:55:05.600] iteration 22054: total_loss: 0.959653, loss_sup: 0.036308, loss_mps: 0.275976, loss_cps: 0.647369
[13:55:05.747] iteration 22055: total_loss: 0.497859, loss_sup: 0.027968, loss_mps: 0.152334, loss_cps: 0.317556
[13:55:05.894] iteration 22056: total_loss: 0.453104, loss_sup: 0.056442, loss_mps: 0.138159, loss_cps: 0.258503
[13:55:06.040] iteration 22057: total_loss: 1.060425, loss_sup: 0.049786, loss_mps: 0.305241, loss_cps: 0.705398
[13:55:06.186] iteration 22058: total_loss: 0.639174, loss_sup: 0.061044, loss_mps: 0.188374, loss_cps: 0.389756
[13:55:06.333] iteration 22059: total_loss: 0.489917, loss_sup: 0.108389, loss_mps: 0.131955, loss_cps: 0.249573
[13:55:06.479] iteration 22060: total_loss: 0.441327, loss_sup: 0.144804, loss_mps: 0.101421, loss_cps: 0.195102
[13:55:06.625] iteration 22061: total_loss: 0.352824, loss_sup: 0.076931, loss_mps: 0.097025, loss_cps: 0.178868
[13:55:06.771] iteration 22062: total_loss: 0.847386, loss_sup: 0.077459, loss_mps: 0.232899, loss_cps: 0.537028
[13:55:06.918] iteration 22063: total_loss: 0.574751, loss_sup: 0.076847, loss_mps: 0.171917, loss_cps: 0.325988
[13:55:07.064] iteration 22064: total_loss: 0.451031, loss_sup: 0.022080, loss_mps: 0.145658, loss_cps: 0.283294
[13:55:07.211] iteration 22065: total_loss: 0.565346, loss_sup: 0.014646, loss_mps: 0.180756, loss_cps: 0.369943
[13:55:07.357] iteration 22066: total_loss: 0.344017, loss_sup: 0.018646, loss_mps: 0.117108, loss_cps: 0.208263
[13:55:07.504] iteration 22067: total_loss: 0.672248, loss_sup: 0.145882, loss_mps: 0.171664, loss_cps: 0.354702
[13:55:07.650] iteration 22068: total_loss: 1.200298, loss_sup: 0.251714, loss_mps: 0.285207, loss_cps: 0.663377
[13:55:07.796] iteration 22069: total_loss: 0.449786, loss_sup: 0.036537, loss_mps: 0.142595, loss_cps: 0.270655
[13:55:07.943] iteration 22070: total_loss: 0.319778, loss_sup: 0.016854, loss_mps: 0.109196, loss_cps: 0.193728
[13:55:08.090] iteration 22071: total_loss: 0.419256, loss_sup: 0.069620, loss_mps: 0.121822, loss_cps: 0.227814
[13:55:08.240] iteration 22072: total_loss: 0.660728, loss_sup: 0.049848, loss_mps: 0.201376, loss_cps: 0.409504
[13:55:08.386] iteration 22073: total_loss: 0.401415, loss_sup: 0.010076, loss_mps: 0.136740, loss_cps: 0.254599
[13:55:08.533] iteration 22074: total_loss: 0.549277, loss_sup: 0.128773, loss_mps: 0.143333, loss_cps: 0.277172
[13:55:08.678] iteration 22075: total_loss: 0.640812, loss_sup: 0.080213, loss_mps: 0.194287, loss_cps: 0.366312
[13:55:08.824] iteration 22076: total_loss: 1.123511, loss_sup: 0.074664, loss_mps: 0.330151, loss_cps: 0.718696
[13:55:08.970] iteration 22077: total_loss: 0.533792, loss_sup: 0.009314, loss_mps: 0.166978, loss_cps: 0.357500
[13:55:09.116] iteration 22078: total_loss: 0.582204, loss_sup: 0.045507, loss_mps: 0.174805, loss_cps: 0.361891
[13:55:09.268] iteration 22079: total_loss: 0.382265, loss_sup: 0.042766, loss_mps: 0.116008, loss_cps: 0.223491
[13:55:09.414] iteration 22080: total_loss: 0.411958, loss_sup: 0.047565, loss_mps: 0.134067, loss_cps: 0.230326
[13:55:09.561] iteration 22081: total_loss: 0.399814, loss_sup: 0.053683, loss_mps: 0.129282, loss_cps: 0.216849
[13:55:09.707] iteration 22082: total_loss: 1.137484, loss_sup: 0.299845, loss_mps: 0.257140, loss_cps: 0.580499
[13:55:09.853] iteration 22083: total_loss: 0.507380, loss_sup: 0.157856, loss_mps: 0.120732, loss_cps: 0.228792
[13:55:09.999] iteration 22084: total_loss: 0.528791, loss_sup: 0.031841, loss_mps: 0.176468, loss_cps: 0.320482
[13:55:10.145] iteration 22085: total_loss: 0.523016, loss_sup: 0.101100, loss_mps: 0.147613, loss_cps: 0.274303
[13:55:10.292] iteration 22086: total_loss: 0.573718, loss_sup: 0.083614, loss_mps: 0.167367, loss_cps: 0.322738
[13:55:10.440] iteration 22087: total_loss: 0.641922, loss_sup: 0.137714, loss_mps: 0.169451, loss_cps: 0.334757
[13:55:10.587] iteration 22088: total_loss: 0.444727, loss_sup: 0.040802, loss_mps: 0.141361, loss_cps: 0.262564
[13:55:10.734] iteration 22089: total_loss: 0.429446, loss_sup: 0.064939, loss_mps: 0.125614, loss_cps: 0.238893
[13:55:10.880] iteration 22090: total_loss: 0.568038, loss_sup: 0.026529, loss_mps: 0.182749, loss_cps: 0.358760
[13:55:11.027] iteration 22091: total_loss: 0.840628, loss_sup: 0.208637, loss_mps: 0.198589, loss_cps: 0.433402
[13:55:11.174] iteration 22092: total_loss: 0.549229, loss_sup: 0.026252, loss_mps: 0.176546, loss_cps: 0.346431
[13:55:11.320] iteration 22093: total_loss: 0.345944, loss_sup: 0.033284, loss_mps: 0.114421, loss_cps: 0.198239
[13:55:11.466] iteration 22094: total_loss: 0.562402, loss_sup: 0.138491, loss_mps: 0.152425, loss_cps: 0.271485
[13:55:11.613] iteration 22095: total_loss: 0.738607, loss_sup: 0.085789, loss_mps: 0.232282, loss_cps: 0.420536
[13:55:11.761] iteration 22096: total_loss: 0.452146, loss_sup: 0.029799, loss_mps: 0.151497, loss_cps: 0.270850
[13:55:11.907] iteration 22097: total_loss: 0.615802, loss_sup: 0.142433, loss_mps: 0.163287, loss_cps: 0.310082
[13:55:12.054] iteration 22098: total_loss: 0.539283, loss_sup: 0.041163, loss_mps: 0.164902, loss_cps: 0.333218
[13:55:12.201] iteration 22099: total_loss: 1.014689, loss_sup: 0.092413, loss_mps: 0.294370, loss_cps: 0.627905
[13:55:12.348] iteration 22100: total_loss: 0.602093, loss_sup: 0.143544, loss_mps: 0.151728, loss_cps: 0.306822
[13:55:12.348] Evaluation Started ==>
[13:55:23.688] ==> valid iteration 22100: unet metrics: {'dc': 0.6671530392738079, 'jc': 0.5484463156934318, 'pre': 0.7979871416841676, 'hd': 5.5286727110687774}, ynet metrics: {'dc': 0.6027586731068207, 'jc': 0.48724566927856866, 'pre': 0.7964790063334652, 'hd': 5.48130516353436}.
[13:55:23.691] Evaluation Finished!⏹️
[13:55:23.842] iteration 22101: total_loss: 0.390340, loss_sup: 0.030723, loss_mps: 0.129896, loss_cps: 0.229721
[13:55:23.989] iteration 22102: total_loss: 0.686880, loss_sup: 0.075095, loss_mps: 0.206153, loss_cps: 0.405632
[13:55:24.135] iteration 22103: total_loss: 0.463042, loss_sup: 0.085540, loss_mps: 0.133632, loss_cps: 0.243870
[13:55:24.292] iteration 22104: total_loss: 0.369071, loss_sup: 0.142131, loss_mps: 0.088016, loss_cps: 0.138925
[13:55:24.437] iteration 22105: total_loss: 0.372761, loss_sup: 0.063642, loss_mps: 0.106015, loss_cps: 0.203104
[13:55:24.589] iteration 22106: total_loss: 0.389381, loss_sup: 0.021653, loss_mps: 0.133372, loss_cps: 0.234356
[13:55:24.734] iteration 22107: total_loss: 0.432042, loss_sup: 0.052017, loss_mps: 0.141714, loss_cps: 0.238311
[13:55:24.879] iteration 22108: total_loss: 0.332145, loss_sup: 0.006022, loss_mps: 0.116222, loss_cps: 0.209900
[13:55:25.026] iteration 22109: total_loss: 0.540870, loss_sup: 0.031920, loss_mps: 0.166647, loss_cps: 0.342302
[13:55:25.172] iteration 22110: total_loss: 0.570223, loss_sup: 0.103253, loss_mps: 0.164457, loss_cps: 0.302514
[13:55:25.317] iteration 22111: total_loss: 0.453396, loss_sup: 0.025219, loss_mps: 0.153756, loss_cps: 0.274421
[13:55:25.466] iteration 22112: total_loss: 0.370185, loss_sup: 0.031343, loss_mps: 0.123541, loss_cps: 0.215301
[13:55:25.612] iteration 22113: total_loss: 0.460335, loss_sup: 0.039905, loss_mps: 0.153428, loss_cps: 0.267002
[13:55:25.757] iteration 22114: total_loss: 0.942343, loss_sup: 0.098504, loss_mps: 0.280070, loss_cps: 0.563770
[13:55:25.903] iteration 22115: total_loss: 0.425149, loss_sup: 0.080695, loss_mps: 0.122379, loss_cps: 0.222075
[13:55:26.048] iteration 22116: total_loss: 0.937155, loss_sup: 0.326105, loss_mps: 0.197792, loss_cps: 0.413258
[13:55:26.194] iteration 22117: total_loss: 0.513335, loss_sup: 0.031920, loss_mps: 0.159727, loss_cps: 0.321688
[13:55:26.340] iteration 22118: total_loss: 0.728118, loss_sup: 0.071716, loss_mps: 0.222463, loss_cps: 0.433939
[13:55:26.486] iteration 22119: total_loss: 0.497219, loss_sup: 0.025106, loss_mps: 0.160970, loss_cps: 0.311144
[13:55:26.632] iteration 22120: total_loss: 0.648781, loss_sup: 0.262053, loss_mps: 0.132546, loss_cps: 0.254182
[13:55:26.778] iteration 22121: total_loss: 0.640980, loss_sup: 0.035641, loss_mps: 0.200451, loss_cps: 0.404888
[13:55:26.924] iteration 22122: total_loss: 0.729922, loss_sup: 0.007240, loss_mps: 0.222961, loss_cps: 0.499721
[13:55:27.075] iteration 22123: total_loss: 0.483328, loss_sup: 0.258433, loss_mps: 0.082313, loss_cps: 0.142581
[13:55:27.222] iteration 22124: total_loss: 0.487868, loss_sup: 0.142074, loss_mps: 0.122839, loss_cps: 0.222955
[13:55:27.369] iteration 22125: total_loss: 0.394495, loss_sup: 0.042938, loss_mps: 0.120561, loss_cps: 0.230995
[13:55:27.515] iteration 22126: total_loss: 0.366523, loss_sup: 0.047664, loss_mps: 0.118941, loss_cps: 0.199918
[13:55:27.660] iteration 22127: total_loss: 0.891750, loss_sup: 0.073447, loss_mps: 0.246246, loss_cps: 0.572058
[13:55:27.807] iteration 22128: total_loss: 1.086451, loss_sup: 0.177761, loss_mps: 0.278114, loss_cps: 0.630576
[13:55:27.956] iteration 22129: total_loss: 0.463872, loss_sup: 0.137391, loss_mps: 0.119307, loss_cps: 0.207173
[13:55:28.102] iteration 22130: total_loss: 0.747854, loss_sup: 0.082261, loss_mps: 0.212097, loss_cps: 0.453497
[13:55:28.247] iteration 22131: total_loss: 0.776812, loss_sup: 0.053831, loss_mps: 0.210746, loss_cps: 0.512235
[13:55:28.393] iteration 22132: total_loss: 0.704311, loss_sup: 0.111148, loss_mps: 0.201773, loss_cps: 0.391389
[13:55:28.539] iteration 22133: total_loss: 0.423242, loss_sup: 0.085227, loss_mps: 0.116790, loss_cps: 0.221225
[13:55:28.685] iteration 22134: total_loss: 0.591287, loss_sup: 0.180797, loss_mps: 0.148447, loss_cps: 0.262043
[13:55:28.831] iteration 22135: total_loss: 0.479100, loss_sup: 0.053664, loss_mps: 0.143203, loss_cps: 0.282232
[13:55:28.978] iteration 22136: total_loss: 0.452234, loss_sup: 0.043821, loss_mps: 0.134985, loss_cps: 0.273428
[13:55:29.128] iteration 22137: total_loss: 0.495777, loss_sup: 0.089764, loss_mps: 0.138723, loss_cps: 0.267290
[13:55:29.274] iteration 22138: total_loss: 0.486045, loss_sup: 0.045210, loss_mps: 0.147194, loss_cps: 0.293641
[13:55:29.419] iteration 22139: total_loss: 0.694902, loss_sup: 0.091067, loss_mps: 0.200695, loss_cps: 0.403139
[13:55:29.565] iteration 22140: total_loss: 0.389636, loss_sup: 0.042344, loss_mps: 0.120897, loss_cps: 0.226395
[13:55:29.711] iteration 22141: total_loss: 0.511445, loss_sup: 0.074198, loss_mps: 0.159877, loss_cps: 0.277370
[13:55:29.856] iteration 22142: total_loss: 0.519391, loss_sup: 0.062249, loss_mps: 0.165082, loss_cps: 0.292060
[13:55:30.003] iteration 22143: total_loss: 0.559423, loss_sup: 0.192065, loss_mps: 0.134172, loss_cps: 0.233186
[13:55:30.148] iteration 22144: total_loss: 0.523670, loss_sup: 0.012524, loss_mps: 0.180965, loss_cps: 0.330182
[13:55:30.294] iteration 22145: total_loss: 0.652485, loss_sup: 0.107187, loss_mps: 0.188950, loss_cps: 0.356347
[13:55:30.440] iteration 22146: total_loss: 0.319491, loss_sup: 0.033906, loss_mps: 0.111992, loss_cps: 0.173593
[13:55:30.585] iteration 22147: total_loss: 1.202758, loss_sup: 0.110623, loss_mps: 0.331224, loss_cps: 0.760912
[13:55:30.731] iteration 22148: total_loss: 0.422625, loss_sup: 0.051819, loss_mps: 0.129893, loss_cps: 0.240913
[13:55:30.877] iteration 22149: total_loss: 0.354709, loss_sup: 0.073544, loss_mps: 0.106091, loss_cps: 0.175074
[13:55:31.022] iteration 22150: total_loss: 0.873210, loss_sup: 0.225183, loss_mps: 0.218128, loss_cps: 0.429899
[13:55:31.168] iteration 22151: total_loss: 0.825598, loss_sup: 0.155248, loss_mps: 0.223802, loss_cps: 0.446547
[13:55:31.314] iteration 22152: total_loss: 1.118206, loss_sup: 0.317925, loss_mps: 0.264656, loss_cps: 0.535626
[13:55:31.459] iteration 22153: total_loss: 0.523109, loss_sup: 0.012774, loss_mps: 0.176610, loss_cps: 0.333726
[13:55:31.527] iteration 22154: total_loss: 0.294270, loss_sup: 0.014667, loss_mps: 0.108990, loss_cps: 0.170612
[13:55:32.762] iteration 22155: total_loss: 0.521251, loss_sup: 0.008844, loss_mps: 0.173657, loss_cps: 0.338749
[13:55:32.910] iteration 22156: total_loss: 0.569466, loss_sup: 0.107901, loss_mps: 0.167305, loss_cps: 0.294260
[13:55:33.057] iteration 22157: total_loss: 0.857078, loss_sup: 0.041096, loss_mps: 0.253517, loss_cps: 0.562465
[13:55:33.203] iteration 22158: total_loss: 0.406477, loss_sup: 0.073606, loss_mps: 0.116094, loss_cps: 0.216777
[13:55:33.349] iteration 22159: total_loss: 0.452825, loss_sup: 0.062394, loss_mps: 0.137073, loss_cps: 0.253358
[13:55:33.497] iteration 22160: total_loss: 0.556186, loss_sup: 0.169410, loss_mps: 0.139207, loss_cps: 0.247568
[13:55:33.643] iteration 22161: total_loss: 0.408022, loss_sup: 0.027668, loss_mps: 0.145273, loss_cps: 0.235080
[13:55:33.796] iteration 22162: total_loss: 1.078088, loss_sup: 0.065785, loss_mps: 0.308875, loss_cps: 0.703428
[13:55:33.944] iteration 22163: total_loss: 0.661644, loss_sup: 0.147940, loss_mps: 0.166987, loss_cps: 0.346717
[13:55:34.091] iteration 22164: total_loss: 0.712322, loss_sup: 0.083766, loss_mps: 0.219271, loss_cps: 0.409286
[13:55:34.238] iteration 22165: total_loss: 0.857078, loss_sup: 0.124415, loss_mps: 0.248679, loss_cps: 0.483985
[13:55:34.387] iteration 22166: total_loss: 0.664484, loss_sup: 0.105789, loss_mps: 0.189444, loss_cps: 0.369251
[13:55:34.537] iteration 22167: total_loss: 0.657537, loss_sup: 0.073681, loss_mps: 0.193064, loss_cps: 0.390792
[13:55:34.683] iteration 22168: total_loss: 0.622361, loss_sup: 0.014270, loss_mps: 0.207086, loss_cps: 0.401004
[13:55:34.830] iteration 22169: total_loss: 0.518324, loss_sup: 0.086051, loss_mps: 0.154529, loss_cps: 0.277743
[13:55:34.978] iteration 22170: total_loss: 0.529049, loss_sup: 0.039281, loss_mps: 0.165028, loss_cps: 0.324740
[13:55:35.124] iteration 22171: total_loss: 0.474883, loss_sup: 0.078703, loss_mps: 0.139256, loss_cps: 0.256924
[13:55:35.272] iteration 22172: total_loss: 0.534506, loss_sup: 0.048125, loss_mps: 0.173198, loss_cps: 0.313183
[13:55:35.422] iteration 22173: total_loss: 0.435066, loss_sup: 0.164376, loss_mps: 0.097990, loss_cps: 0.172700
[13:55:35.569] iteration 22174: total_loss: 0.478954, loss_sup: 0.082216, loss_mps: 0.137940, loss_cps: 0.258797
[13:55:35.715] iteration 22175: total_loss: 0.664723, loss_sup: 0.129264, loss_mps: 0.184571, loss_cps: 0.350888
[13:55:35.863] iteration 22176: total_loss: 0.499897, loss_sup: 0.053447, loss_mps: 0.156127, loss_cps: 0.290323
[13:55:36.010] iteration 22177: total_loss: 0.725241, loss_sup: 0.098582, loss_mps: 0.211498, loss_cps: 0.415162
[13:55:36.156] iteration 22178: total_loss: 0.473727, loss_sup: 0.112167, loss_mps: 0.136041, loss_cps: 0.225519
[13:55:36.303] iteration 22179: total_loss: 0.473903, loss_sup: 0.067309, loss_mps: 0.150386, loss_cps: 0.256207
[13:55:36.448] iteration 22180: total_loss: 1.313573, loss_sup: 0.367639, loss_mps: 0.314258, loss_cps: 0.631676
[13:55:36.595] iteration 22181: total_loss: 0.361510, loss_sup: 0.015477, loss_mps: 0.128199, loss_cps: 0.217833
[13:55:36.741] iteration 22182: total_loss: 0.375725, loss_sup: 0.004918, loss_mps: 0.128739, loss_cps: 0.242068
[13:55:36.887] iteration 22183: total_loss: 0.562492, loss_sup: 0.061671, loss_mps: 0.162281, loss_cps: 0.338540
[13:55:37.032] iteration 22184: total_loss: 0.342915, loss_sup: 0.040812, loss_mps: 0.109328, loss_cps: 0.192775
[13:55:37.178] iteration 22185: total_loss: 0.583005, loss_sup: 0.235120, loss_mps: 0.121286, loss_cps: 0.226599
[13:55:37.323] iteration 22186: total_loss: 0.898098, loss_sup: 0.082996, loss_mps: 0.273447, loss_cps: 0.541655
[13:55:37.470] iteration 22187: total_loss: 0.553611, loss_sup: 0.016145, loss_mps: 0.172860, loss_cps: 0.364607
[13:55:37.616] iteration 22188: total_loss: 0.447637, loss_sup: 0.020569, loss_mps: 0.142574, loss_cps: 0.284494
[13:55:37.762] iteration 22189: total_loss: 0.424021, loss_sup: 0.146074, loss_mps: 0.103885, loss_cps: 0.174062
[13:55:37.908] iteration 22190: total_loss: 0.650447, loss_sup: 0.217911, loss_mps: 0.147397, loss_cps: 0.285139
[13:55:38.056] iteration 22191: total_loss: 0.472390, loss_sup: 0.113721, loss_mps: 0.133618, loss_cps: 0.225051
[13:55:38.202] iteration 22192: total_loss: 0.593734, loss_sup: 0.144676, loss_mps: 0.159401, loss_cps: 0.289656
[13:55:38.348] iteration 22193: total_loss: 0.604994, loss_sup: 0.177616, loss_mps: 0.134769, loss_cps: 0.292609
[13:55:38.494] iteration 22194: total_loss: 0.536800, loss_sup: 0.071288, loss_mps: 0.150859, loss_cps: 0.314653
[13:55:38.640] iteration 22195: total_loss: 0.404786, loss_sup: 0.015549, loss_mps: 0.134376, loss_cps: 0.254860
[13:55:38.787] iteration 22196: total_loss: 0.345303, loss_sup: 0.016345, loss_mps: 0.121587, loss_cps: 0.207371
[13:55:38.933] iteration 22197: total_loss: 0.661830, loss_sup: 0.073137, loss_mps: 0.189048, loss_cps: 0.399645
[13:55:39.079] iteration 22198: total_loss: 0.859306, loss_sup: 0.086177, loss_mps: 0.247679, loss_cps: 0.525450
[13:55:39.224] iteration 22199: total_loss: 0.495645, loss_sup: 0.012706, loss_mps: 0.162031, loss_cps: 0.320909
[13:55:39.370] iteration 22200: total_loss: 0.397978, loss_sup: 0.043495, loss_mps: 0.136306, loss_cps: 0.218177
[13:55:39.371] Evaluation Started ==>
[13:55:50.700] ==> valid iteration 22200: unet metrics: {'dc': 0.6408799463647665, 'jc': 0.5215328341115549, 'pre': 0.7999734542266052, 'hd': 5.593065026538469}, ynet metrics: {'dc': 0.5634067861590938, 'jc': 0.45107041363552125, 'pre': 0.7923053168276929, 'hd': 5.558563167020357}.
[13:55:50.703] Evaluation Finished!⏹️
[13:55:50.856] iteration 22201: total_loss: 0.316809, loss_sup: 0.046988, loss_mps: 0.098017, loss_cps: 0.171804
[13:55:51.003] iteration 22202: total_loss: 0.924270, loss_sup: 0.048384, loss_mps: 0.284316, loss_cps: 0.591571
[13:55:51.148] iteration 22203: total_loss: 0.412880, loss_sup: 0.050148, loss_mps: 0.126160, loss_cps: 0.236572
[13:55:51.293] iteration 22204: total_loss: 0.566631, loss_sup: 0.197915, loss_mps: 0.136773, loss_cps: 0.231943
[13:55:51.438] iteration 22205: total_loss: 0.474375, loss_sup: 0.035196, loss_mps: 0.150522, loss_cps: 0.288657
[13:55:51.586] iteration 22206: total_loss: 0.755553, loss_sup: 0.220641, loss_mps: 0.180454, loss_cps: 0.354458
[13:55:51.731] iteration 22207: total_loss: 0.529651, loss_sup: 0.006354, loss_mps: 0.182025, loss_cps: 0.341272
[13:55:51.880] iteration 22208: total_loss: 0.600645, loss_sup: 0.053563, loss_mps: 0.193198, loss_cps: 0.353883
[13:55:52.026] iteration 22209: total_loss: 0.437791, loss_sup: 0.016218, loss_mps: 0.142110, loss_cps: 0.279463
[13:55:52.174] iteration 22210: total_loss: 0.843297, loss_sup: 0.031660, loss_mps: 0.260151, loss_cps: 0.551485
[13:55:52.323] iteration 22211: total_loss: 0.432539, loss_sup: 0.085787, loss_mps: 0.120422, loss_cps: 0.226330
[13:55:52.468] iteration 22212: total_loss: 0.750549, loss_sup: 0.228977, loss_mps: 0.184504, loss_cps: 0.337067
[13:55:52.614] iteration 22213: total_loss: 0.679459, loss_sup: 0.041686, loss_mps: 0.216745, loss_cps: 0.421028
[13:55:52.762] iteration 22214: total_loss: 0.786665, loss_sup: 0.038653, loss_mps: 0.241434, loss_cps: 0.506578
[13:55:52.908] iteration 22215: total_loss: 0.413207, loss_sup: 0.077751, loss_mps: 0.119607, loss_cps: 0.215849
[13:55:53.054] iteration 22216: total_loss: 0.478034, loss_sup: 0.140826, loss_mps: 0.120857, loss_cps: 0.216351
[13:55:53.200] iteration 22217: total_loss: 0.377440, loss_sup: 0.031899, loss_mps: 0.124438, loss_cps: 0.221103
[13:55:53.345] iteration 22218: total_loss: 0.540403, loss_sup: 0.029178, loss_mps: 0.160928, loss_cps: 0.350297
[13:55:53.491] iteration 22219: total_loss: 0.390309, loss_sup: 0.004498, loss_mps: 0.136832, loss_cps: 0.248979
[13:55:53.638] iteration 22220: total_loss: 0.427821, loss_sup: 0.011360, loss_mps: 0.146621, loss_cps: 0.269840
[13:55:53.785] iteration 22221: total_loss: 0.357599, loss_sup: 0.030091, loss_mps: 0.110657, loss_cps: 0.216850
[13:55:53.932] iteration 22222: total_loss: 0.750717, loss_sup: 0.197848, loss_mps: 0.184630, loss_cps: 0.368239
[13:55:54.078] iteration 22223: total_loss: 0.551067, loss_sup: 0.186452, loss_mps: 0.128139, loss_cps: 0.236477
[13:55:54.224] iteration 22224: total_loss: 0.371271, loss_sup: 0.018431, loss_mps: 0.114314, loss_cps: 0.238526
[13:55:54.371] iteration 22225: total_loss: 0.872498, loss_sup: 0.190521, loss_mps: 0.213979, loss_cps: 0.467998
[13:55:54.517] iteration 22226: total_loss: 0.446411, loss_sup: 0.040757, loss_mps: 0.147661, loss_cps: 0.257993
[13:55:54.665] iteration 22227: total_loss: 0.439343, loss_sup: 0.116763, loss_mps: 0.118291, loss_cps: 0.204289
[13:55:54.811] iteration 22228: total_loss: 0.532837, loss_sup: 0.083368, loss_mps: 0.152159, loss_cps: 0.297310
[13:55:54.958] iteration 22229: total_loss: 0.805270, loss_sup: 0.034543, loss_mps: 0.251061, loss_cps: 0.519666
[13:55:55.104] iteration 22230: total_loss: 0.253961, loss_sup: 0.021870, loss_mps: 0.087582, loss_cps: 0.144509
[13:55:55.250] iteration 22231: total_loss: 0.397715, loss_sup: 0.047779, loss_mps: 0.124893, loss_cps: 0.225042
[13:55:55.395] iteration 22232: total_loss: 0.650585, loss_sup: 0.187147, loss_mps: 0.149433, loss_cps: 0.314004
[13:55:55.543] iteration 22233: total_loss: 0.492931, loss_sup: 0.053087, loss_mps: 0.147008, loss_cps: 0.292836
[13:55:55.690] iteration 22234: total_loss: 0.640492, loss_sup: 0.216500, loss_mps: 0.141400, loss_cps: 0.282593
[13:55:55.835] iteration 22235: total_loss: 0.485535, loss_sup: 0.131503, loss_mps: 0.126836, loss_cps: 0.227196
[13:55:55.981] iteration 22236: total_loss: 0.725759, loss_sup: 0.037492, loss_mps: 0.209241, loss_cps: 0.479025
[13:55:56.127] iteration 22237: total_loss: 0.350249, loss_sup: 0.012348, loss_mps: 0.114016, loss_cps: 0.223884
[13:55:56.272] iteration 22238: total_loss: 0.474652, loss_sup: 0.062624, loss_mps: 0.140730, loss_cps: 0.271298
[13:55:56.419] iteration 22239: total_loss: 0.258171, loss_sup: 0.030179, loss_mps: 0.090763, loss_cps: 0.137229
[13:55:56.564] iteration 22240: total_loss: 0.379180, loss_sup: 0.054669, loss_mps: 0.110768, loss_cps: 0.213743
[13:55:56.710] iteration 22241: total_loss: 0.418877, loss_sup: 0.029666, loss_mps: 0.142332, loss_cps: 0.246879
[13:55:56.857] iteration 22242: total_loss: 0.484671, loss_sup: 0.079940, loss_mps: 0.139269, loss_cps: 0.265462
[13:55:57.002] iteration 22243: total_loss: 0.394712, loss_sup: 0.015066, loss_mps: 0.131762, loss_cps: 0.247884
[13:55:57.148] iteration 22244: total_loss: 0.289014, loss_sup: 0.059166, loss_mps: 0.084559, loss_cps: 0.145289
[13:55:57.294] iteration 22245: total_loss: 0.463746, loss_sup: 0.020200, loss_mps: 0.150466, loss_cps: 0.293080
[13:55:57.441] iteration 22246: total_loss: 0.318607, loss_sup: 0.093772, loss_mps: 0.082802, loss_cps: 0.142033
[13:55:57.595] iteration 22247: total_loss: 0.503930, loss_sup: 0.129574, loss_mps: 0.135450, loss_cps: 0.238906
[13:55:57.742] iteration 22248: total_loss: 0.793577, loss_sup: 0.021481, loss_mps: 0.252141, loss_cps: 0.519955
[13:55:57.888] iteration 22249: total_loss: 0.392484, loss_sup: 0.037937, loss_mps: 0.128823, loss_cps: 0.225724
[13:55:58.034] iteration 22250: total_loss: 0.421937, loss_sup: 0.054023, loss_mps: 0.126865, loss_cps: 0.241049
[13:55:58.180] iteration 22251: total_loss: 0.337328, loss_sup: 0.044103, loss_mps: 0.102165, loss_cps: 0.191060
[13:55:58.327] iteration 22252: total_loss: 0.448340, loss_sup: 0.144504, loss_mps: 0.111525, loss_cps: 0.192311
[13:55:58.473] iteration 22253: total_loss: 0.577585, loss_sup: 0.016066, loss_mps: 0.192074, loss_cps: 0.369446
[13:55:58.619] iteration 22254: total_loss: 0.459260, loss_sup: 0.081223, loss_mps: 0.131430, loss_cps: 0.246608
[13:55:58.765] iteration 22255: total_loss: 0.555353, loss_sup: 0.057260, loss_mps: 0.164406, loss_cps: 0.333687
[13:55:58.911] iteration 22256: total_loss: 0.443239, loss_sup: 0.100101, loss_mps: 0.120049, loss_cps: 0.223089
[13:55:59.056] iteration 22257: total_loss: 0.523015, loss_sup: 0.021701, loss_mps: 0.171993, loss_cps: 0.329321
[13:55:59.203] iteration 22258: total_loss: 0.367020, loss_sup: 0.039901, loss_mps: 0.118312, loss_cps: 0.208807
[13:55:59.350] iteration 22259: total_loss: 1.056585, loss_sup: 0.302472, loss_mps: 0.239848, loss_cps: 0.514265
[13:55:59.496] iteration 22260: total_loss: 0.340561, loss_sup: 0.024480, loss_mps: 0.113140, loss_cps: 0.202940
[13:55:59.642] iteration 22261: total_loss: 0.470318, loss_sup: 0.022133, loss_mps: 0.146947, loss_cps: 0.301238
[13:55:59.788] iteration 22262: total_loss: 0.472930, loss_sup: 0.110838, loss_mps: 0.127944, loss_cps: 0.234147
[13:55:59.934] iteration 22263: total_loss: 0.337776, loss_sup: 0.057220, loss_mps: 0.103940, loss_cps: 0.176617
[13:56:00.079] iteration 22264: total_loss: 0.685319, loss_sup: 0.100576, loss_mps: 0.187297, loss_cps: 0.397446
[13:56:00.225] iteration 22265: total_loss: 0.499218, loss_sup: 0.014241, loss_mps: 0.168154, loss_cps: 0.316822
[13:56:00.372] iteration 22266: total_loss: 0.434779, loss_sup: 0.026564, loss_mps: 0.138572, loss_cps: 0.269643
[13:56:00.518] iteration 22267: total_loss: 0.279025, loss_sup: 0.003977, loss_mps: 0.097043, loss_cps: 0.178005
[13:56:00.664] iteration 22268: total_loss: 0.268786, loss_sup: 0.030883, loss_mps: 0.092062, loss_cps: 0.145840
[13:56:00.810] iteration 22269: total_loss: 0.349269, loss_sup: 0.016982, loss_mps: 0.121135, loss_cps: 0.211152
[13:56:00.958] iteration 22270: total_loss: 0.331417, loss_sup: 0.072533, loss_mps: 0.097946, loss_cps: 0.160939
[13:56:01.103] iteration 22271: total_loss: 0.488874, loss_sup: 0.027429, loss_mps: 0.159772, loss_cps: 0.301673
[13:56:01.250] iteration 22272: total_loss: 0.430731, loss_sup: 0.039609, loss_mps: 0.133126, loss_cps: 0.257996
[13:56:01.396] iteration 22273: total_loss: 0.851311, loss_sup: 0.078304, loss_mps: 0.243202, loss_cps: 0.529804
[13:56:01.541] iteration 22274: total_loss: 0.264723, loss_sup: 0.008254, loss_mps: 0.093153, loss_cps: 0.163316
[13:56:01.687] iteration 22275: total_loss: 0.439648, loss_sup: 0.045680, loss_mps: 0.136893, loss_cps: 0.257074
[13:56:01.833] iteration 22276: total_loss: 0.336210, loss_sup: 0.093681, loss_mps: 0.089467, loss_cps: 0.153062
[13:56:01.979] iteration 22277: total_loss: 0.637794, loss_sup: 0.105334, loss_mps: 0.170157, loss_cps: 0.362304
[13:56:02.124] iteration 22278: total_loss: 0.528802, loss_sup: 0.046049, loss_mps: 0.166641, loss_cps: 0.316111
[13:56:02.270] iteration 22279: total_loss: 0.634350, loss_sup: 0.034610, loss_mps: 0.186565, loss_cps: 0.413176
[13:56:02.417] iteration 22280: total_loss: 0.557189, loss_sup: 0.042008, loss_mps: 0.178859, loss_cps: 0.336322
[13:56:02.563] iteration 22281: total_loss: 0.828234, loss_sup: 0.024322, loss_mps: 0.254537, loss_cps: 0.549376
[13:56:02.709] iteration 22282: total_loss: 0.471483, loss_sup: 0.021054, loss_mps: 0.152029, loss_cps: 0.298400
[13:56:02.855] iteration 22283: total_loss: 0.582790, loss_sup: 0.116493, loss_mps: 0.160185, loss_cps: 0.306111
[13:56:03.002] iteration 22284: total_loss: 0.457245, loss_sup: 0.056428, loss_mps: 0.145340, loss_cps: 0.255477
[13:56:03.148] iteration 22285: total_loss: 1.087213, loss_sup: 0.266469, loss_mps: 0.245773, loss_cps: 0.574971
[13:56:03.296] iteration 22286: total_loss: 0.622886, loss_sup: 0.040315, loss_mps: 0.186212, loss_cps: 0.396359
[13:56:03.442] iteration 22287: total_loss: 0.939386, loss_sup: 0.176986, loss_mps: 0.244334, loss_cps: 0.518066
[13:56:03.588] iteration 22288: total_loss: 0.635261, loss_sup: 0.026917, loss_mps: 0.196885, loss_cps: 0.411458
[13:56:03.736] iteration 22289: total_loss: 0.290572, loss_sup: 0.034371, loss_mps: 0.092699, loss_cps: 0.163501
[13:56:03.882] iteration 22290: total_loss: 0.512471, loss_sup: 0.094058, loss_mps: 0.142240, loss_cps: 0.276174
[13:56:04.028] iteration 22291: total_loss: 0.237995, loss_sup: 0.001042, loss_mps: 0.088392, loss_cps: 0.148561
[13:56:04.176] iteration 22292: total_loss: 0.605607, loss_sup: 0.027059, loss_mps: 0.183315, loss_cps: 0.395234
[13:56:04.322] iteration 22293: total_loss: 0.410208, loss_sup: 0.036335, loss_mps: 0.131958, loss_cps: 0.241915
[13:56:04.468] iteration 22294: total_loss: 0.402501, loss_sup: 0.025200, loss_mps: 0.122136, loss_cps: 0.255165
[13:56:04.614] iteration 22295: total_loss: 0.459741, loss_sup: 0.047136, loss_mps: 0.148553, loss_cps: 0.264052
[13:56:04.765] iteration 22296: total_loss: 0.572056, loss_sup: 0.180653, loss_mps: 0.134484, loss_cps: 0.256920
[13:56:04.911] iteration 22297: total_loss: 0.245187, loss_sup: 0.002501, loss_mps: 0.086368, loss_cps: 0.156318
[13:56:05.057] iteration 22298: total_loss: 0.293061, loss_sup: 0.033093, loss_mps: 0.094378, loss_cps: 0.165591
[13:56:05.203] iteration 22299: total_loss: 0.349844, loss_sup: 0.009020, loss_mps: 0.121319, loss_cps: 0.219505
[13:56:05.350] iteration 22300: total_loss: 0.303469, loss_sup: 0.023098, loss_mps: 0.097186, loss_cps: 0.183185
[13:56:05.350] Evaluation Started ==>
[13:56:16.672] ==> valid iteration 22300: unet metrics: {'dc': 0.673451277372729, 'jc': 0.56042711869126, 'pre': 0.8211672041746864, 'hd': 5.329254321735848}, ynet metrics: {'dc': 0.6213146829467487, 'jc': 0.5070425779924826, 'pre': 0.80326910575003, 'hd': 5.457799374541304}.
[13:56:16.674] Evaluation Finished!⏹️
[13:56:16.825] iteration 22301: total_loss: 0.389710, loss_sup: 0.063972, loss_mps: 0.117221, loss_cps: 0.208517
[13:56:16.972] iteration 22302: total_loss: 0.542560, loss_sup: 0.139493, loss_mps: 0.144871, loss_cps: 0.258197
[13:56:17.118] iteration 22303: total_loss: 0.680109, loss_sup: 0.202354, loss_mps: 0.168176, loss_cps: 0.309579
[13:56:17.263] iteration 22304: total_loss: 0.705402, loss_sup: 0.161965, loss_mps: 0.184846, loss_cps: 0.358591
[13:56:17.409] iteration 22305: total_loss: 0.386702, loss_sup: 0.012861, loss_mps: 0.129364, loss_cps: 0.244477
[13:56:17.554] iteration 22306: total_loss: 0.411070, loss_sup: 0.012684, loss_mps: 0.135793, loss_cps: 0.262594
[13:56:17.701] iteration 22307: total_loss: 0.390756, loss_sup: 0.010443, loss_mps: 0.136288, loss_cps: 0.244025
[13:56:17.848] iteration 22308: total_loss: 1.023425, loss_sup: 0.398600, loss_mps: 0.216799, loss_cps: 0.408027
[13:56:17.996] iteration 22309: total_loss: 0.360040, loss_sup: 0.018068, loss_mps: 0.112988, loss_cps: 0.228984
[13:56:18.142] iteration 22310: total_loss: 0.307545, loss_sup: 0.048862, loss_mps: 0.092782, loss_cps: 0.165900
[13:56:18.289] iteration 22311: total_loss: 0.557718, loss_sup: 0.100713, loss_mps: 0.156934, loss_cps: 0.300072
[13:56:18.436] iteration 22312: total_loss: 0.647546, loss_sup: 0.112608, loss_mps: 0.188215, loss_cps: 0.346723
[13:56:18.582] iteration 22313: total_loss: 0.273059, loss_sup: 0.020975, loss_mps: 0.090240, loss_cps: 0.161844
[13:56:18.728] iteration 22314: total_loss: 0.287804, loss_sup: 0.046103, loss_mps: 0.084883, loss_cps: 0.156818
[13:56:18.873] iteration 22315: total_loss: 0.803623, loss_sup: 0.209780, loss_mps: 0.202170, loss_cps: 0.391673
[13:56:19.018] iteration 22316: total_loss: 0.458516, loss_sup: 0.200859, loss_mps: 0.094659, loss_cps: 0.162997
[13:56:19.163] iteration 22317: total_loss: 0.357940, loss_sup: 0.005866, loss_mps: 0.125481, loss_cps: 0.226593
[13:56:19.309] iteration 22318: total_loss: 0.501678, loss_sup: 0.139640, loss_mps: 0.130590, loss_cps: 0.231448
[13:56:19.455] iteration 22319: total_loss: 0.433898, loss_sup: 0.027467, loss_mps: 0.135560, loss_cps: 0.270870
[13:56:19.600] iteration 22320: total_loss: 0.499945, loss_sup: 0.085141, loss_mps: 0.142999, loss_cps: 0.271804
[13:56:19.745] iteration 22321: total_loss: 0.427421, loss_sup: 0.019689, loss_mps: 0.141252, loss_cps: 0.266480
[13:56:19.891] iteration 22322: total_loss: 0.350630, loss_sup: 0.059692, loss_mps: 0.110425, loss_cps: 0.180512
[13:56:20.038] iteration 22323: total_loss: 0.571005, loss_sup: 0.095166, loss_mps: 0.155146, loss_cps: 0.320693
[13:56:20.184] iteration 22324: total_loss: 0.271476, loss_sup: 0.009553, loss_mps: 0.099467, loss_cps: 0.162455
[13:56:20.330] iteration 22325: total_loss: 0.308142, loss_sup: 0.083878, loss_mps: 0.082453, loss_cps: 0.141811
[13:56:20.476] iteration 22326: total_loss: 0.338621, loss_sup: 0.011569, loss_mps: 0.112849, loss_cps: 0.214202
[13:56:20.621] iteration 22327: total_loss: 0.477945, loss_sup: 0.036296, loss_mps: 0.155385, loss_cps: 0.286264
[13:56:20.767] iteration 22328: total_loss: 0.429763, loss_sup: 0.091764, loss_mps: 0.118085, loss_cps: 0.219913
[13:56:20.912] iteration 22329: total_loss: 0.288643, loss_sup: 0.017877, loss_mps: 0.099690, loss_cps: 0.171076
[13:56:21.057] iteration 22330: total_loss: 0.550605, loss_sup: 0.155960, loss_mps: 0.148316, loss_cps: 0.246329
[13:56:21.202] iteration 22331: total_loss: 0.542975, loss_sup: 0.148393, loss_mps: 0.134125, loss_cps: 0.260458
[13:56:21.348] iteration 22332: total_loss: 0.414175, loss_sup: 0.152978, loss_mps: 0.093381, loss_cps: 0.167817
[13:56:21.493] iteration 22333: total_loss: 0.330662, loss_sup: 0.018753, loss_mps: 0.120299, loss_cps: 0.191609
[13:56:21.640] iteration 22334: total_loss: 0.362435, loss_sup: 0.039073, loss_mps: 0.118716, loss_cps: 0.204646
[13:56:21.786] iteration 22335: total_loss: 0.389786, loss_sup: 0.024968, loss_mps: 0.121390, loss_cps: 0.243429
[13:56:21.931] iteration 22336: total_loss: 0.374354, loss_sup: 0.022564, loss_mps: 0.126021, loss_cps: 0.225769
[13:56:22.079] iteration 22337: total_loss: 0.983242, loss_sup: 0.269327, loss_mps: 0.223957, loss_cps: 0.489958
[13:56:22.225] iteration 22338: total_loss: 0.519988, loss_sup: 0.085610, loss_mps: 0.150847, loss_cps: 0.283532
[13:56:22.374] iteration 22339: total_loss: 0.660872, loss_sup: 0.021517, loss_mps: 0.200980, loss_cps: 0.438374
[13:56:22.520] iteration 22340: total_loss: 0.342230, loss_sup: 0.016027, loss_mps: 0.116648, loss_cps: 0.209555
[13:56:22.667] iteration 22341: total_loss: 0.424654, loss_sup: 0.072188, loss_mps: 0.121092, loss_cps: 0.231374
[13:56:22.814] iteration 22342: total_loss: 0.331005, loss_sup: 0.014279, loss_mps: 0.112920, loss_cps: 0.203806
[13:56:22.959] iteration 22343: total_loss: 0.288521, loss_sup: 0.022101, loss_mps: 0.095384, loss_cps: 0.171036
[13:56:23.105] iteration 22344: total_loss: 0.297414, loss_sup: 0.073616, loss_mps: 0.080196, loss_cps: 0.143602
[13:56:23.252] iteration 22345: total_loss: 0.431189, loss_sup: 0.016057, loss_mps: 0.137109, loss_cps: 0.278023
[13:56:23.398] iteration 22346: total_loss: 0.275713, loss_sup: 0.038106, loss_mps: 0.083406, loss_cps: 0.154200
[13:56:23.544] iteration 22347: total_loss: 0.516627, loss_sup: 0.103430, loss_mps: 0.136155, loss_cps: 0.277042
[13:56:23.690] iteration 22348: total_loss: 0.419575, loss_sup: 0.048931, loss_mps: 0.131497, loss_cps: 0.239146
[13:56:23.838] iteration 22349: total_loss: 0.413717, loss_sup: 0.005553, loss_mps: 0.138703, loss_cps: 0.269460
[13:56:23.984] iteration 22350: total_loss: 0.363118, loss_sup: 0.097789, loss_mps: 0.093725, loss_cps: 0.171604
[13:56:24.130] iteration 22351: total_loss: 0.365640, loss_sup: 0.046097, loss_mps: 0.119928, loss_cps: 0.199615
[13:56:24.277] iteration 22352: total_loss: 0.342827, loss_sup: 0.011323, loss_mps: 0.108805, loss_cps: 0.222699
[13:56:24.423] iteration 22353: total_loss: 0.633960, loss_sup: 0.023443, loss_mps: 0.202183, loss_cps: 0.408334
[13:56:24.569] iteration 22354: total_loss: 0.411338, loss_sup: 0.096925, loss_mps: 0.113936, loss_cps: 0.200478
[13:56:24.717] iteration 22355: total_loss: 0.486165, loss_sup: 0.080800, loss_mps: 0.138361, loss_cps: 0.267004
[13:56:24.863] iteration 22356: total_loss: 0.517354, loss_sup: 0.131103, loss_mps: 0.127797, loss_cps: 0.258454
[13:56:25.010] iteration 22357: total_loss: 0.433027, loss_sup: 0.065534, loss_mps: 0.132849, loss_cps: 0.234645
[13:56:25.156] iteration 22358: total_loss: 0.544339, loss_sup: 0.097191, loss_mps: 0.153998, loss_cps: 0.293150
[13:56:25.303] iteration 22359: total_loss: 0.346421, loss_sup: 0.021358, loss_mps: 0.112266, loss_cps: 0.212797
[13:56:25.451] iteration 22360: total_loss: 0.522218, loss_sup: 0.126048, loss_mps: 0.135511, loss_cps: 0.260659
[13:56:25.598] iteration 22361: total_loss: 0.614797, loss_sup: 0.092244, loss_mps: 0.172983, loss_cps: 0.349571
[13:56:25.744] iteration 22362: total_loss: 0.676323, loss_sup: 0.053236, loss_mps: 0.211249, loss_cps: 0.411838
[13:56:25.890] iteration 22363: total_loss: 0.364344, loss_sup: 0.071132, loss_mps: 0.105550, loss_cps: 0.187661
[13:56:26.037] iteration 22364: total_loss: 0.376473, loss_sup: 0.026340, loss_mps: 0.119747, loss_cps: 0.230385
[13:56:26.184] iteration 22365: total_loss: 0.426578, loss_sup: 0.004433, loss_mps: 0.142404, loss_cps: 0.279741
[13:56:26.330] iteration 22366: total_loss: 0.327867, loss_sup: 0.002005, loss_mps: 0.109539, loss_cps: 0.216323
[13:56:26.476] iteration 22367: total_loss: 0.509833, loss_sup: 0.022151, loss_mps: 0.168573, loss_cps: 0.319109
[13:56:26.622] iteration 22368: total_loss: 0.450181, loss_sup: 0.031820, loss_mps: 0.134973, loss_cps: 0.283388
[13:56:26.768] iteration 22369: total_loss: 0.389622, loss_sup: 0.044879, loss_mps: 0.118926, loss_cps: 0.225816
[13:56:26.915] iteration 22370: total_loss: 0.265246, loss_sup: 0.048177, loss_mps: 0.078437, loss_cps: 0.138632
[13:56:27.062] iteration 22371: total_loss: 0.349426, loss_sup: 0.080165, loss_mps: 0.094715, loss_cps: 0.174547
[13:56:27.208] iteration 22372: total_loss: 0.482494, loss_sup: 0.140586, loss_mps: 0.117921, loss_cps: 0.223986
[13:56:27.354] iteration 22373: total_loss: 0.288103, loss_sup: 0.039298, loss_mps: 0.092254, loss_cps: 0.156551
[13:56:27.500] iteration 22374: total_loss: 0.336333, loss_sup: 0.050232, loss_mps: 0.105137, loss_cps: 0.180964
[13:56:27.646] iteration 22375: total_loss: 0.736873, loss_sup: 0.128821, loss_mps: 0.192397, loss_cps: 0.415655
[13:56:27.792] iteration 22376: total_loss: 0.313014, loss_sup: 0.027853, loss_mps: 0.103862, loss_cps: 0.181299
[13:56:27.940] iteration 22377: total_loss: 0.399897, loss_sup: 0.027207, loss_mps: 0.126702, loss_cps: 0.245987
[13:56:28.089] iteration 22378: total_loss: 0.529772, loss_sup: 0.046514, loss_mps: 0.170802, loss_cps: 0.312455
[13:56:28.235] iteration 22379: total_loss: 0.662429, loss_sup: 0.055966, loss_mps: 0.192095, loss_cps: 0.414368
[13:56:28.381] iteration 22380: total_loss: 0.515952, loss_sup: 0.065571, loss_mps: 0.152482, loss_cps: 0.297898
[13:56:28.527] iteration 22381: total_loss: 0.311529, loss_sup: 0.014544, loss_mps: 0.101744, loss_cps: 0.195240
[13:56:28.674] iteration 22382: total_loss: 0.301406, loss_sup: 0.002531, loss_mps: 0.108996, loss_cps: 0.189879
[13:56:28.821] iteration 22383: total_loss: 0.324205, loss_sup: 0.116657, loss_mps: 0.074618, loss_cps: 0.132930
[13:56:28.969] iteration 22384: total_loss: 0.384993, loss_sup: 0.032621, loss_mps: 0.121943, loss_cps: 0.230429
[13:56:29.117] iteration 22385: total_loss: 0.726394, loss_sup: 0.041308, loss_mps: 0.220352, loss_cps: 0.464734
[13:56:29.265] iteration 22386: total_loss: 0.407268, loss_sup: 0.083006, loss_mps: 0.117425, loss_cps: 0.206838
[13:56:29.412] iteration 22387: total_loss: 0.774134, loss_sup: 0.069247, loss_mps: 0.225012, loss_cps: 0.479875
[13:56:29.562] iteration 22388: total_loss: 0.680368, loss_sup: 0.051431, loss_mps: 0.199230, loss_cps: 0.429707
[13:56:29.709] iteration 22389: total_loss: 0.541309, loss_sup: 0.046493, loss_mps: 0.169515, loss_cps: 0.325301
[13:56:29.855] iteration 22390: total_loss: 0.243749, loss_sup: 0.000665, loss_mps: 0.089156, loss_cps: 0.153928
[13:56:30.002] iteration 22391: total_loss: 0.261785, loss_sup: 0.092469, loss_mps: 0.065511, loss_cps: 0.103805
[13:56:30.148] iteration 22392: total_loss: 0.433256, loss_sup: 0.027573, loss_mps: 0.141471, loss_cps: 0.264212
[13:56:30.296] iteration 22393: total_loss: 1.106572, loss_sup: 0.271173, loss_mps: 0.248211, loss_cps: 0.587189
[13:56:30.441] iteration 22394: total_loss: 1.302025, loss_sup: 0.141446, loss_mps: 0.344463, loss_cps: 0.816115
[13:56:30.587] iteration 22395: total_loss: 0.694047, loss_sup: 0.105669, loss_mps: 0.200447, loss_cps: 0.387931
[13:56:30.733] iteration 22396: total_loss: 0.430484, loss_sup: 0.055281, loss_mps: 0.126957, loss_cps: 0.248246
[13:56:30.879] iteration 22397: total_loss: 0.321754, loss_sup: 0.031101, loss_mps: 0.114411, loss_cps: 0.176242
[13:56:31.024] iteration 22398: total_loss: 0.586873, loss_sup: 0.041471, loss_mps: 0.169517, loss_cps: 0.375885
[13:56:31.170] iteration 22399: total_loss: 0.433102, loss_sup: 0.056095, loss_mps: 0.121582, loss_cps: 0.255425
[13:56:31.315] iteration 22400: total_loss: 1.405855, loss_sup: 0.106838, loss_mps: 0.392471, loss_cps: 0.906545
[13:56:31.316] Evaluation Started ==>
[13:56:42.643] ==> valid iteration 22400: unet metrics: {'dc': 0.6438751623590501, 'jc': 0.527190838428907, 'pre': 0.788443093062448, 'hd': 5.599081210072566}, ynet metrics: {'dc': 0.6101492827573559, 'jc': 0.49501785494742306, 'pre': 0.7895821532521192, 'hd': 5.592275724871429}.
[13:56:42.649] Evaluation Finished!⏹️
[13:56:42.800] iteration 22401: total_loss: 0.509740, loss_sup: 0.065212, loss_mps: 0.151818, loss_cps: 0.292711
[13:56:42.948] iteration 22402: total_loss: 0.588307, loss_sup: 0.057687, loss_mps: 0.170970, loss_cps: 0.359650
[13:56:43.093] iteration 22403: total_loss: 0.878180, loss_sup: 0.123761, loss_mps: 0.233068, loss_cps: 0.521352
[13:56:43.238] iteration 22404: total_loss: 0.520374, loss_sup: 0.133199, loss_mps: 0.127512, loss_cps: 0.259664
[13:56:43.383] iteration 22405: total_loss: 0.417687, loss_sup: 0.015151, loss_mps: 0.136729, loss_cps: 0.265806
[13:56:43.529] iteration 22406: total_loss: 0.548573, loss_sup: 0.156488, loss_mps: 0.127544, loss_cps: 0.264540
[13:56:43.675] iteration 22407: total_loss: 0.517438, loss_sup: 0.034109, loss_mps: 0.153637, loss_cps: 0.329692
[13:56:43.821] iteration 22408: total_loss: 0.519463, loss_sup: 0.015185, loss_mps: 0.171644, loss_cps: 0.332635
[13:56:43.966] iteration 22409: total_loss: 0.635624, loss_sup: 0.037502, loss_mps: 0.198462, loss_cps: 0.399660
[13:56:44.112] iteration 22410: total_loss: 0.840941, loss_sup: 0.145736, loss_mps: 0.234461, loss_cps: 0.460744
[13:56:44.258] iteration 22411: total_loss: 0.481271, loss_sup: 0.048863, loss_mps: 0.146796, loss_cps: 0.285611
[13:56:44.403] iteration 22412: total_loss: 0.576549, loss_sup: 0.104494, loss_mps: 0.162162, loss_cps: 0.309893
[13:56:44.549] iteration 22413: total_loss: 0.631547, loss_sup: 0.108045, loss_mps: 0.169268, loss_cps: 0.354235
[13:56:44.697] iteration 22414: total_loss: 0.639761, loss_sup: 0.114437, loss_mps: 0.187529, loss_cps: 0.337794
[13:56:44.842] iteration 22415: total_loss: 0.422042, loss_sup: 0.039557, loss_mps: 0.137807, loss_cps: 0.244678
[13:56:44.988] iteration 22416: total_loss: 0.353953, loss_sup: 0.090225, loss_mps: 0.096724, loss_cps: 0.167005
[13:56:45.134] iteration 22417: total_loss: 0.815812, loss_sup: 0.094498, loss_mps: 0.239174, loss_cps: 0.482140
[13:56:45.280] iteration 22418: total_loss: 0.463349, loss_sup: 0.028965, loss_mps: 0.143787, loss_cps: 0.290597
[13:56:45.425] iteration 22419: total_loss: 0.913269, loss_sup: 0.088435, loss_mps: 0.255311, loss_cps: 0.569523
[13:56:45.573] iteration 22420: total_loss: 0.530058, loss_sup: 0.033340, loss_mps: 0.165810, loss_cps: 0.330908
[13:56:45.719] iteration 22421: total_loss: 0.418707, loss_sup: 0.107741, loss_mps: 0.115103, loss_cps: 0.195863
[13:56:45.866] iteration 22422: total_loss: 0.713809, loss_sup: 0.190409, loss_mps: 0.184030, loss_cps: 0.339370
[13:56:46.012] iteration 22423: total_loss: 1.080022, loss_sup: 0.421358, loss_mps: 0.229502, loss_cps: 0.429162
[13:56:46.158] iteration 22424: total_loss: 0.922555, loss_sup: 0.181080, loss_mps: 0.249129, loss_cps: 0.492346
[13:56:46.304] iteration 22425: total_loss: 0.697484, loss_sup: 0.015453, loss_mps: 0.217067, loss_cps: 0.464963
[13:56:46.449] iteration 22426: total_loss: 0.365192, loss_sup: 0.104324, loss_mps: 0.098054, loss_cps: 0.162813
[13:56:46.595] iteration 22427: total_loss: 0.488873, loss_sup: 0.081768, loss_mps: 0.137429, loss_cps: 0.269676
[13:56:46.741] iteration 22428: total_loss: 0.493579, loss_sup: 0.048833, loss_mps: 0.148985, loss_cps: 0.295761
[13:56:46.888] iteration 22429: total_loss: 0.314728, loss_sup: 0.019285, loss_mps: 0.113282, loss_cps: 0.182161
[13:56:47.034] iteration 22430: total_loss: 0.670181, loss_sup: 0.019317, loss_mps: 0.205132, loss_cps: 0.445731
[13:56:47.181] iteration 22431: total_loss: 0.411160, loss_sup: 0.032591, loss_mps: 0.135202, loss_cps: 0.243367
[13:56:47.327] iteration 22432: total_loss: 1.021231, loss_sup: 0.080103, loss_mps: 0.283562, loss_cps: 0.657566
[13:56:47.473] iteration 22433: total_loss: 0.784244, loss_sup: 0.276261, loss_mps: 0.166403, loss_cps: 0.341581
[13:56:47.619] iteration 22434: total_loss: 0.576707, loss_sup: 0.060475, loss_mps: 0.185127, loss_cps: 0.331105
[13:56:47.765] iteration 22435: total_loss: 0.346120, loss_sup: 0.011032, loss_mps: 0.120540, loss_cps: 0.214548
[13:56:47.915] iteration 22436: total_loss: 0.326607, loss_sup: 0.071449, loss_mps: 0.096457, loss_cps: 0.158701
[13:56:48.063] iteration 22437: total_loss: 0.414963, loss_sup: 0.092457, loss_mps: 0.114098, loss_cps: 0.208409
[13:56:48.208] iteration 22438: total_loss: 0.471728, loss_sup: 0.021777, loss_mps: 0.166314, loss_cps: 0.283637
[13:56:48.357] iteration 22439: total_loss: 0.391532, loss_sup: 0.010162, loss_mps: 0.141791, loss_cps: 0.239579
[13:56:48.502] iteration 22440: total_loss: 0.309069, loss_sup: 0.019643, loss_mps: 0.107875, loss_cps: 0.181550
[13:56:48.649] iteration 22441: total_loss: 0.414171, loss_sup: 0.045305, loss_mps: 0.137776, loss_cps: 0.231090
[13:56:48.796] iteration 22442: total_loss: 0.552855, loss_sup: 0.046969, loss_mps: 0.174921, loss_cps: 0.330965
[13:56:48.944] iteration 22443: total_loss: 0.655421, loss_sup: 0.068419, loss_mps: 0.201988, loss_cps: 0.385014
[13:56:49.090] iteration 22444: total_loss: 1.035063, loss_sup: 0.134260, loss_mps: 0.288723, loss_cps: 0.612080
[13:56:49.237] iteration 22445: total_loss: 0.628150, loss_sup: 0.098841, loss_mps: 0.183840, loss_cps: 0.345469
[13:56:49.384] iteration 22446: total_loss: 0.742941, loss_sup: 0.118504, loss_mps: 0.202456, loss_cps: 0.421981
[13:56:49.530] iteration 22447: total_loss: 0.391890, loss_sup: 0.004465, loss_mps: 0.135745, loss_cps: 0.251680
[13:56:49.677] iteration 22448: total_loss: 0.441007, loss_sup: 0.014186, loss_mps: 0.144063, loss_cps: 0.282759
[13:56:49.824] iteration 22449: total_loss: 0.526659, loss_sup: 0.206538, loss_mps: 0.117822, loss_cps: 0.202299
[13:56:49.969] iteration 22450: total_loss: 0.368107, loss_sup: 0.023914, loss_mps: 0.127341, loss_cps: 0.216852
[13:56:50.114] iteration 22451: total_loss: 0.719906, loss_sup: 0.214226, loss_mps: 0.184877, loss_cps: 0.320803
[13:56:50.260] iteration 22452: total_loss: 0.394189, loss_sup: 0.003532, loss_mps: 0.143152, loss_cps: 0.247505
[13:56:50.406] iteration 22453: total_loss: 0.281388, loss_sup: 0.027672, loss_mps: 0.091488, loss_cps: 0.162228
[13:56:50.552] iteration 22454: total_loss: 0.919771, loss_sup: 0.087878, loss_mps: 0.264506, loss_cps: 0.567387
[13:56:50.699] iteration 22455: total_loss: 0.516350, loss_sup: 0.086430, loss_mps: 0.152819, loss_cps: 0.277101
[13:56:50.845] iteration 22456: total_loss: 0.635925, loss_sup: 0.045475, loss_mps: 0.185949, loss_cps: 0.404502
[13:56:50.990] iteration 22457: total_loss: 0.413169, loss_sup: 0.023297, loss_mps: 0.137633, loss_cps: 0.252239
[13:56:51.137] iteration 22458: total_loss: 0.294006, loss_sup: 0.036963, loss_mps: 0.094122, loss_cps: 0.162920
[13:56:51.283] iteration 22459: total_loss: 0.752803, loss_sup: 0.024904, loss_mps: 0.230568, loss_cps: 0.497331
[13:56:51.428] iteration 22460: total_loss: 0.538099, loss_sup: 0.134915, loss_mps: 0.139910, loss_cps: 0.263274
[13:56:51.573] iteration 22461: total_loss: 0.452604, loss_sup: 0.026905, loss_mps: 0.141009, loss_cps: 0.284690
[13:56:51.719] iteration 22462: total_loss: 0.609563, loss_sup: 0.060544, loss_mps: 0.180910, loss_cps: 0.368110
[13:56:51.865] iteration 22463: total_loss: 1.178803, loss_sup: 0.087505, loss_mps: 0.361263, loss_cps: 0.730036
[13:56:52.010] iteration 22464: total_loss: 0.408116, loss_sup: 0.048147, loss_mps: 0.125426, loss_cps: 0.234543
[13:56:52.157] iteration 22465: total_loss: 0.449881, loss_sup: 0.014724, loss_mps: 0.160901, loss_cps: 0.274256
[13:56:52.302] iteration 22466: total_loss: 0.718246, loss_sup: 0.180270, loss_mps: 0.185682, loss_cps: 0.352294
[13:56:52.448] iteration 22467: total_loss: 0.821801, loss_sup: 0.146991, loss_mps: 0.216419, loss_cps: 0.458392
[13:56:52.596] iteration 22468: total_loss: 0.610675, loss_sup: 0.008219, loss_mps: 0.198949, loss_cps: 0.403507
[13:56:52.742] iteration 22469: total_loss: 0.225046, loss_sup: 0.017928, loss_mps: 0.080642, loss_cps: 0.126476
[13:56:52.887] iteration 22470: total_loss: 1.119650, loss_sup: 0.054912, loss_mps: 0.334577, loss_cps: 0.730161
[13:56:53.040] iteration 22471: total_loss: 1.029738, loss_sup: 0.223256, loss_mps: 0.250742, loss_cps: 0.555740
[13:56:53.187] iteration 22472: total_loss: 0.335229, loss_sup: 0.048590, loss_mps: 0.101574, loss_cps: 0.185065
[13:56:53.333] iteration 22473: total_loss: 0.673908, loss_sup: 0.147399, loss_mps: 0.179010, loss_cps: 0.347498
[13:56:53.479] iteration 22474: total_loss: 0.801104, loss_sup: 0.122823, loss_mps: 0.216033, loss_cps: 0.462248
[13:56:53.624] iteration 22475: total_loss: 0.577912, loss_sup: 0.197958, loss_mps: 0.137454, loss_cps: 0.242500
[13:56:53.770] iteration 22476: total_loss: 0.812065, loss_sup: 0.108318, loss_mps: 0.222864, loss_cps: 0.480884
[13:56:53.916] iteration 22477: total_loss: 0.719374, loss_sup: 0.246887, loss_mps: 0.168547, loss_cps: 0.303940
[13:56:54.062] iteration 22478: total_loss: 0.543518, loss_sup: 0.053829, loss_mps: 0.165828, loss_cps: 0.323861
[13:56:54.208] iteration 22479: total_loss: 0.935330, loss_sup: 0.042362, loss_mps: 0.285094, loss_cps: 0.607874
[13:56:54.354] iteration 22480: total_loss: 0.530805, loss_sup: 0.171575, loss_mps: 0.132464, loss_cps: 0.226766
[13:56:54.500] iteration 22481: total_loss: 0.620617, loss_sup: 0.115153, loss_mps: 0.172472, loss_cps: 0.332992
[13:56:54.647] iteration 22482: total_loss: 0.576147, loss_sup: 0.091703, loss_mps: 0.171988, loss_cps: 0.312455
[13:56:54.794] iteration 22483: total_loss: 0.513237, loss_sup: 0.039252, loss_mps: 0.152322, loss_cps: 0.321663
[13:56:54.941] iteration 22484: total_loss: 0.662365, loss_sup: 0.048984, loss_mps: 0.217592, loss_cps: 0.395789
[13:56:55.086] iteration 22485: total_loss: 0.594949, loss_sup: 0.084622, loss_mps: 0.176478, loss_cps: 0.333849
[13:56:55.233] iteration 22486: total_loss: 0.611821, loss_sup: 0.027352, loss_mps: 0.183423, loss_cps: 0.401047
[13:56:55.379] iteration 22487: total_loss: 0.384649, loss_sup: 0.052160, loss_mps: 0.119505, loss_cps: 0.212983
[13:56:55.525] iteration 22488: total_loss: 0.301928, loss_sup: 0.057278, loss_mps: 0.093695, loss_cps: 0.150954
[13:56:55.671] iteration 22489: total_loss: 1.127616, loss_sup: 0.055117, loss_mps: 0.325395, loss_cps: 0.747105
[13:56:55.818] iteration 22490: total_loss: 0.533747, loss_sup: 0.062435, loss_mps: 0.164231, loss_cps: 0.307080
[13:56:55.965] iteration 22491: total_loss: 0.311256, loss_sup: 0.034820, loss_mps: 0.102807, loss_cps: 0.173628
[13:56:56.111] iteration 22492: total_loss: 0.638848, loss_sup: 0.076330, loss_mps: 0.185032, loss_cps: 0.377486
[13:56:56.257] iteration 22493: total_loss: 2.199249, loss_sup: 0.583696, loss_mps: 0.487366, loss_cps: 1.128187
[13:56:56.405] iteration 22494: total_loss: 1.407316, loss_sup: 0.126457, loss_mps: 0.396294, loss_cps: 0.884565
[13:56:56.551] iteration 22495: total_loss: 0.908668, loss_sup: 0.040245, loss_mps: 0.279165, loss_cps: 0.589257
[13:56:56.697] iteration 22496: total_loss: 0.526053, loss_sup: 0.094343, loss_mps: 0.145236, loss_cps: 0.286474
[13:56:56.843] iteration 22497: total_loss: 0.621910, loss_sup: 0.078250, loss_mps: 0.189038, loss_cps: 0.354622
[13:56:56.989] iteration 22498: total_loss: 0.461696, loss_sup: 0.099407, loss_mps: 0.127362, loss_cps: 0.234927
[13:56:57.136] iteration 22499: total_loss: 0.396466, loss_sup: 0.019831, loss_mps: 0.128995, loss_cps: 0.247640
[13:56:57.282] iteration 22500: total_loss: 0.460703, loss_sup: 0.046334, loss_mps: 0.147774, loss_cps: 0.266595
[13:56:57.282] Evaluation Started ==>
[13:57:08.656] ==> valid iteration 22500: unet metrics: {'dc': 0.5953143531743562, 'jc': 0.4832538901436804, 'pre': 0.8139085895056553, 'hd': 5.460697333066242}, ynet metrics: {'dc': 0.5422023820446501, 'jc': 0.43230481868944354, 'pre': 0.7634792898033594, 'hd': 5.5766315945722225}.
[13:57:08.657] Evaluation Finished!⏹️
[13:57:08.806] iteration 22501: total_loss: 0.520552, loss_sup: 0.131869, loss_mps: 0.134922, loss_cps: 0.253761
[13:57:08.954] iteration 22502: total_loss: 0.941653, loss_sup: 0.171564, loss_mps: 0.243218, loss_cps: 0.526871
[13:57:09.099] iteration 22503: total_loss: 0.726012, loss_sup: 0.036121, loss_mps: 0.220831, loss_cps: 0.469060
[13:57:09.248] iteration 22504: total_loss: 0.393328, loss_sup: 0.044188, loss_mps: 0.125664, loss_cps: 0.223477
[13:57:09.394] iteration 22505: total_loss: 0.469331, loss_sup: 0.067288, loss_mps: 0.139002, loss_cps: 0.263040
[13:57:09.539] iteration 22506: total_loss: 0.513592, loss_sup: 0.074064, loss_mps: 0.155399, loss_cps: 0.284129
[13:57:09.685] iteration 22507: total_loss: 0.710135, loss_sup: 0.085884, loss_mps: 0.219125, loss_cps: 0.405126
[13:57:09.830] iteration 22508: total_loss: 0.437419, loss_sup: 0.041200, loss_mps: 0.138798, loss_cps: 0.257420
[13:57:09.977] iteration 22509: total_loss: 1.077967, loss_sup: 0.071810, loss_mps: 0.319624, loss_cps: 0.686532
[13:57:10.122] iteration 22510: total_loss: 0.816872, loss_sup: 0.311482, loss_mps: 0.180707, loss_cps: 0.324682
[13:57:10.268] iteration 22511: total_loss: 0.414609, loss_sup: 0.046980, loss_mps: 0.129827, loss_cps: 0.237802
[13:57:10.414] iteration 22512: total_loss: 0.345497, loss_sup: 0.016440, loss_mps: 0.114513, loss_cps: 0.214544
[13:57:10.561] iteration 22513: total_loss: 0.717435, loss_sup: 0.088445, loss_mps: 0.206425, loss_cps: 0.422565
[13:57:10.707] iteration 22514: total_loss: 0.728798, loss_sup: 0.129868, loss_mps: 0.205954, loss_cps: 0.392975
[13:57:10.853] iteration 22515: total_loss: 0.684999, loss_sup: 0.109315, loss_mps: 0.192497, loss_cps: 0.383187
[13:57:10.998] iteration 22516: total_loss: 0.595242, loss_sup: 0.113436, loss_mps: 0.167873, loss_cps: 0.313933
[13:57:11.144] iteration 22517: total_loss: 0.579108, loss_sup: 0.203405, loss_mps: 0.138711, loss_cps: 0.236992
[13:57:11.289] iteration 22518: total_loss: 1.061759, loss_sup: 0.225449, loss_mps: 0.268269, loss_cps: 0.568040
[13:57:11.434] iteration 22519: total_loss: 0.212638, loss_sup: 0.013072, loss_mps: 0.075852, loss_cps: 0.123713
[13:57:11.581] iteration 22520: total_loss: 0.609037, loss_sup: 0.005331, loss_mps: 0.192983, loss_cps: 0.410723
[13:57:11.726] iteration 22521: total_loss: 0.675104, loss_sup: 0.130374, loss_mps: 0.200988, loss_cps: 0.343742
[13:57:11.873] iteration 22522: total_loss: 1.012544, loss_sup: 0.052541, loss_mps: 0.309290, loss_cps: 0.650713
[13:57:12.020] iteration 22523: total_loss: 0.306805, loss_sup: 0.048276, loss_mps: 0.105949, loss_cps: 0.152580
[13:57:12.165] iteration 22524: total_loss: 1.084446, loss_sup: 0.219026, loss_mps: 0.292883, loss_cps: 0.572537
[13:57:12.311] iteration 22525: total_loss: 0.331255, loss_sup: 0.008482, loss_mps: 0.116656, loss_cps: 0.206116
[13:57:12.456] iteration 22526: total_loss: 0.801367, loss_sup: 0.131749, loss_mps: 0.231057, loss_cps: 0.438561
[13:57:12.602] iteration 22527: total_loss: 0.456391, loss_sup: 0.144895, loss_mps: 0.111253, loss_cps: 0.200243
[13:57:12.749] iteration 22528: total_loss: 0.386896, loss_sup: 0.081046, loss_mps: 0.122988, loss_cps: 0.182863
[13:57:12.894] iteration 22529: total_loss: 0.496122, loss_sup: 0.033788, loss_mps: 0.166450, loss_cps: 0.295884
[13:57:13.040] iteration 22530: total_loss: 0.627450, loss_sup: 0.113510, loss_mps: 0.174792, loss_cps: 0.339148
[13:57:13.188] iteration 22531: total_loss: 0.476010, loss_sup: 0.090126, loss_mps: 0.135704, loss_cps: 0.250180
[13:57:13.334] iteration 22532: total_loss: 1.380303, loss_sup: 0.173180, loss_mps: 0.358911, loss_cps: 0.848212
[13:57:13.480] iteration 22533: total_loss: 0.320421, loss_sup: 0.034200, loss_mps: 0.105052, loss_cps: 0.181170
[13:57:13.632] iteration 22534: total_loss: 0.507141, loss_sup: 0.092701, loss_mps: 0.152719, loss_cps: 0.261722
[13:57:13.777] iteration 22535: total_loss: 0.700360, loss_sup: 0.029695, loss_mps: 0.229827, loss_cps: 0.440837
[13:57:13.923] iteration 22536: total_loss: 0.833339, loss_sup: 0.164951, loss_mps: 0.216003, loss_cps: 0.452385
[13:57:14.073] iteration 22537: total_loss: 0.365117, loss_sup: 0.055291, loss_mps: 0.114918, loss_cps: 0.194908
[13:57:14.220] iteration 22538: total_loss: 0.711537, loss_sup: 0.049409, loss_mps: 0.199709, loss_cps: 0.462419
[13:57:14.366] iteration 22539: total_loss: 1.089884, loss_sup: 0.123125, loss_mps: 0.313897, loss_cps: 0.652862
[13:57:14.514] iteration 22540: total_loss: 0.413640, loss_sup: 0.023470, loss_mps: 0.127927, loss_cps: 0.262242
[13:57:14.661] iteration 22541: total_loss: 0.922350, loss_sup: 0.064346, loss_mps: 0.269942, loss_cps: 0.588062
[13:57:14.807] iteration 22542: total_loss: 0.832473, loss_sup: 0.157758, loss_mps: 0.222030, loss_cps: 0.452684
[13:57:14.956] iteration 22543: total_loss: 0.691498, loss_sup: 0.045716, loss_mps: 0.221953, loss_cps: 0.423829
[13:57:15.102] iteration 22544: total_loss: 0.320496, loss_sup: 0.031645, loss_mps: 0.110183, loss_cps: 0.178668
[13:57:15.250] iteration 22545: total_loss: 0.602920, loss_sup: 0.167143, loss_mps: 0.151924, loss_cps: 0.283854
[13:57:15.396] iteration 22546: total_loss: 1.034462, loss_sup: 0.253581, loss_mps: 0.253407, loss_cps: 0.527475
[13:57:15.542] iteration 22547: total_loss: 0.462718, loss_sup: 0.071961, loss_mps: 0.140189, loss_cps: 0.250569
[13:57:15.688] iteration 22548: total_loss: 0.525494, loss_sup: 0.074736, loss_mps: 0.157964, loss_cps: 0.292794
[13:57:15.836] iteration 22549: total_loss: 0.838988, loss_sup: 0.244532, loss_mps: 0.198626, loss_cps: 0.395830
[13:57:15.982] iteration 22550: total_loss: 0.489102, loss_sup: 0.096405, loss_mps: 0.135295, loss_cps: 0.257402
[13:57:16.132] iteration 22551: total_loss: 0.697153, loss_sup: 0.082862, loss_mps: 0.193543, loss_cps: 0.420748
[13:57:16.278] iteration 22552: total_loss: 0.763778, loss_sup: 0.269048, loss_mps: 0.173546, loss_cps: 0.321184
[13:57:16.425] iteration 22553: total_loss: 0.486962, loss_sup: 0.028933, loss_mps: 0.164684, loss_cps: 0.293345
[13:57:16.571] iteration 22554: total_loss: 1.154065, loss_sup: 0.203236, loss_mps: 0.303071, loss_cps: 0.647759
[13:57:16.717] iteration 22555: total_loss: 0.762774, loss_sup: 0.234545, loss_mps: 0.188735, loss_cps: 0.339493
[13:57:16.863] iteration 22556: total_loss: 0.766194, loss_sup: 0.060164, loss_mps: 0.226737, loss_cps: 0.479293
[13:57:17.013] iteration 22557: total_loss: 1.091802, loss_sup: 0.145995, loss_mps: 0.301378, loss_cps: 0.644430
[13:57:17.160] iteration 22558: total_loss: 0.464697, loss_sup: 0.018914, loss_mps: 0.165304, loss_cps: 0.280479
[13:57:17.306] iteration 22559: total_loss: 0.484911, loss_sup: 0.061099, loss_mps: 0.153960, loss_cps: 0.269852
[13:57:17.452] iteration 22560: total_loss: 0.688418, loss_sup: 0.064321, loss_mps: 0.206930, loss_cps: 0.417167
[13:57:17.598] iteration 22561: total_loss: 0.290703, loss_sup: 0.017534, loss_mps: 0.106200, loss_cps: 0.166969
[13:57:17.743] iteration 22562: total_loss: 0.343629, loss_sup: 0.036800, loss_mps: 0.114888, loss_cps: 0.191941
[13:57:17.889] iteration 22563: total_loss: 0.620200, loss_sup: 0.056586, loss_mps: 0.190764, loss_cps: 0.372850
[13:57:18.035] iteration 22564: total_loss: 0.536134, loss_sup: 0.155584, loss_mps: 0.142284, loss_cps: 0.238266
[13:57:18.180] iteration 22565: total_loss: 0.619884, loss_sup: 0.009425, loss_mps: 0.214851, loss_cps: 0.395609
[13:57:18.326] iteration 22566: total_loss: 0.331510, loss_sup: 0.022778, loss_mps: 0.116412, loss_cps: 0.192319
[13:57:18.472] iteration 22567: total_loss: 0.420590, loss_sup: 0.038120, loss_mps: 0.134802, loss_cps: 0.247668
[13:57:18.617] iteration 22568: total_loss: 0.590887, loss_sup: 0.150240, loss_mps: 0.155196, loss_cps: 0.285450
[13:57:18.763] iteration 22569: total_loss: 0.682809, loss_sup: 0.077085, loss_mps: 0.193266, loss_cps: 0.412458
[13:57:18.909] iteration 22570: total_loss: 0.460883, loss_sup: 0.009640, loss_mps: 0.162938, loss_cps: 0.288305
[13:57:19.054] iteration 22571: total_loss: 0.971299, loss_sup: 0.065828, loss_mps: 0.281359, loss_cps: 0.624112
[13:57:19.118] iteration 22572: total_loss: 0.412932, loss_sup: 0.032772, loss_mps: 0.139888, loss_cps: 0.240272
[13:57:20.309] iteration 22573: total_loss: 0.635755, loss_sup: 0.117479, loss_mps: 0.184665, loss_cps: 0.333611
[13:57:20.458] iteration 22574: total_loss: 0.316028, loss_sup: 0.016249, loss_mps: 0.106256, loss_cps: 0.193523
[13:57:20.605] iteration 22575: total_loss: 0.404151, loss_sup: 0.070598, loss_mps: 0.114598, loss_cps: 0.218954
[13:57:20.751] iteration 22576: total_loss: 0.384863, loss_sup: 0.042395, loss_mps: 0.124584, loss_cps: 0.217884
[13:57:20.901] iteration 22577: total_loss: 0.859259, loss_sup: 0.146316, loss_mps: 0.231139, loss_cps: 0.481804
[13:57:21.047] iteration 22578: total_loss: 0.432710, loss_sup: 0.087569, loss_mps: 0.129404, loss_cps: 0.215736
[13:57:21.193] iteration 22579: total_loss: 0.561510, loss_sup: 0.137778, loss_mps: 0.149625, loss_cps: 0.274107
[13:57:21.345] iteration 22580: total_loss: 0.444301, loss_sup: 0.080277, loss_mps: 0.130045, loss_cps: 0.233979
[13:57:21.491] iteration 22581: total_loss: 0.373842, loss_sup: 0.009189, loss_mps: 0.130581, loss_cps: 0.234073
[13:57:21.638] iteration 22582: total_loss: 0.397632, loss_sup: 0.056725, loss_mps: 0.126444, loss_cps: 0.214464
[13:57:21.787] iteration 22583: total_loss: 0.601470, loss_sup: 0.112144, loss_mps: 0.163806, loss_cps: 0.325520
[13:57:21.933] iteration 22584: total_loss: 0.230575, loss_sup: 0.032797, loss_mps: 0.078242, loss_cps: 0.119536
[13:57:22.079] iteration 22585: total_loss: 0.939190, loss_sup: 0.094019, loss_mps: 0.278186, loss_cps: 0.566986
[13:57:22.225] iteration 22586: total_loss: 0.446458, loss_sup: 0.051860, loss_mps: 0.142023, loss_cps: 0.252575
[13:57:22.371] iteration 22587: total_loss: 0.507902, loss_sup: 0.052592, loss_mps: 0.168350, loss_cps: 0.286960
[13:57:22.516] iteration 22588: total_loss: 0.431562, loss_sup: 0.103272, loss_mps: 0.119208, loss_cps: 0.209082
[13:57:22.664] iteration 22589: total_loss: 0.581592, loss_sup: 0.043960, loss_mps: 0.187711, loss_cps: 0.349921
[13:57:22.810] iteration 22590: total_loss: 0.560132, loss_sup: 0.126090, loss_mps: 0.144396, loss_cps: 0.289646
[13:57:22.956] iteration 22591: total_loss: 0.617434, loss_sup: 0.171815, loss_mps: 0.163295, loss_cps: 0.282324
[13:57:23.106] iteration 22592: total_loss: 0.349029, loss_sup: 0.090834, loss_mps: 0.096773, loss_cps: 0.161422
[13:57:23.251] iteration 22593: total_loss: 0.644802, loss_sup: 0.127701, loss_mps: 0.170059, loss_cps: 0.347042
[13:57:23.397] iteration 22594: total_loss: 0.602041, loss_sup: 0.198156, loss_mps: 0.143603, loss_cps: 0.260282
[13:57:23.543] iteration 22595: total_loss: 0.571721, loss_sup: 0.083007, loss_mps: 0.159622, loss_cps: 0.329092
[13:57:23.689] iteration 22596: total_loss: 0.587522, loss_sup: 0.154793, loss_mps: 0.142225, loss_cps: 0.290504
[13:57:23.835] iteration 22597: total_loss: 0.648596, loss_sup: 0.090454, loss_mps: 0.188928, loss_cps: 0.369214
[13:57:23.981] iteration 22598: total_loss: 0.781428, loss_sup: 0.193607, loss_mps: 0.208509, loss_cps: 0.379312
[13:57:24.128] iteration 22599: total_loss: 0.344039, loss_sup: 0.010530, loss_mps: 0.119613, loss_cps: 0.213895
[13:57:24.277] iteration 22600: total_loss: 0.713812, loss_sup: 0.093860, loss_mps: 0.218328, loss_cps: 0.401624
[13:57:24.277] Evaluation Started ==>
[13:57:35.617] ==> valid iteration 22600: unet metrics: {'dc': 0.6404734522774074, 'jc': 0.5237741487679102, 'pre': 0.8048689869204922, 'hd': 5.4139998583410245}, ynet metrics: {'dc': 0.5705498092469246, 'jc': 0.4572168761725774, 'pre': 0.805747830352586, 'hd': 5.433115240636284}.
[13:57:35.619] Evaluation Finished!⏹️
[13:57:35.777] iteration 22601: total_loss: 0.497432, loss_sup: 0.034706, loss_mps: 0.145884, loss_cps: 0.316843
[13:57:35.925] iteration 22602: total_loss: 0.327751, loss_sup: 0.130277, loss_mps: 0.079367, loss_cps: 0.118107
[13:57:36.070] iteration 22603: total_loss: 0.426423, loss_sup: 0.030709, loss_mps: 0.134105, loss_cps: 0.261609
[13:57:36.216] iteration 22604: total_loss: 0.687543, loss_sup: 0.030875, loss_mps: 0.217541, loss_cps: 0.439127
[13:57:36.361] iteration 22605: total_loss: 0.722359, loss_sup: 0.141681, loss_mps: 0.200847, loss_cps: 0.379831
[13:57:36.506] iteration 22606: total_loss: 0.561184, loss_sup: 0.075552, loss_mps: 0.172158, loss_cps: 0.313473
[13:57:36.651] iteration 22607: total_loss: 0.295104, loss_sup: 0.022362, loss_mps: 0.100271, loss_cps: 0.172470
[13:57:36.799] iteration 22608: total_loss: 0.710603, loss_sup: 0.063211, loss_mps: 0.222519, loss_cps: 0.424874
[13:57:36.944] iteration 22609: total_loss: 0.538487, loss_sup: 0.149851, loss_mps: 0.138214, loss_cps: 0.250421
[13:57:37.091] iteration 22610: total_loss: 0.748834, loss_sup: 0.226581, loss_mps: 0.176504, loss_cps: 0.345749
[13:57:37.236] iteration 22611: total_loss: 0.430244, loss_sup: 0.061909, loss_mps: 0.128037, loss_cps: 0.240299
[13:57:37.381] iteration 22612: total_loss: 0.543497, loss_sup: 0.153621, loss_mps: 0.137438, loss_cps: 0.252438
[13:57:37.526] iteration 22613: total_loss: 0.511330, loss_sup: 0.102275, loss_mps: 0.142907, loss_cps: 0.266148
[13:57:37.671] iteration 22614: total_loss: 0.772201, loss_sup: 0.165863, loss_mps: 0.207962, loss_cps: 0.398376
[13:57:37.816] iteration 22615: total_loss: 0.367093, loss_sup: 0.066593, loss_mps: 0.115119, loss_cps: 0.185381
[13:57:37.963] iteration 22616: total_loss: 0.345587, loss_sup: 0.033622, loss_mps: 0.112931, loss_cps: 0.199034
[13:57:38.108] iteration 22617: total_loss: 0.368782, loss_sup: 0.018815, loss_mps: 0.126383, loss_cps: 0.223584
[13:57:38.255] iteration 22618: total_loss: 0.890624, loss_sup: 0.041275, loss_mps: 0.285531, loss_cps: 0.563818
[13:57:38.402] iteration 22619: total_loss: 0.297135, loss_sup: 0.021422, loss_mps: 0.100412, loss_cps: 0.175301
[13:57:38.548] iteration 22620: total_loss: 0.583841, loss_sup: 0.102174, loss_mps: 0.154565, loss_cps: 0.327102
[13:57:38.693] iteration 22621: total_loss: 0.368341, loss_sup: 0.025902, loss_mps: 0.125038, loss_cps: 0.217401
[13:57:38.838] iteration 22622: total_loss: 0.647197, loss_sup: 0.093026, loss_mps: 0.178925, loss_cps: 0.375245
[13:57:38.985] iteration 22623: total_loss: 0.526974, loss_sup: 0.008933, loss_mps: 0.170073, loss_cps: 0.347968
[13:57:39.130] iteration 22624: total_loss: 0.284074, loss_sup: 0.070442, loss_mps: 0.079620, loss_cps: 0.134012
[13:57:39.275] iteration 22625: total_loss: 0.753824, loss_sup: 0.064141, loss_mps: 0.230179, loss_cps: 0.459504
[13:57:39.421] iteration 22626: total_loss: 0.465950, loss_sup: 0.118548, loss_mps: 0.126395, loss_cps: 0.221007
[13:57:39.567] iteration 22627: total_loss: 0.477420, loss_sup: 0.047264, loss_mps: 0.146578, loss_cps: 0.283578
[13:57:39.712] iteration 22628: total_loss: 0.665577, loss_sup: 0.066578, loss_mps: 0.206227, loss_cps: 0.392772
[13:57:39.858] iteration 22629: total_loss: 0.573100, loss_sup: 0.167134, loss_mps: 0.148005, loss_cps: 0.257961
[13:57:40.004] iteration 22630: total_loss: 0.276520, loss_sup: 0.034590, loss_mps: 0.093191, loss_cps: 0.148738
[13:57:40.150] iteration 22631: total_loss: 0.492211, loss_sup: 0.048103, loss_mps: 0.151418, loss_cps: 0.292690
[13:57:40.296] iteration 22632: total_loss: 0.596702, loss_sup: 0.180724, loss_mps: 0.146962, loss_cps: 0.269015
[13:57:40.443] iteration 22633: total_loss: 0.418246, loss_sup: 0.032817, loss_mps: 0.133751, loss_cps: 0.251678
[13:57:40.589] iteration 22634: total_loss: 0.288616, loss_sup: 0.034685, loss_mps: 0.094599, loss_cps: 0.159332
[13:57:40.735] iteration 22635: total_loss: 0.488188, loss_sup: 0.030587, loss_mps: 0.159141, loss_cps: 0.298460
[13:57:40.883] iteration 22636: total_loss: 0.298410, loss_sup: 0.063016, loss_mps: 0.088686, loss_cps: 0.146708
[13:57:41.029] iteration 22637: total_loss: 0.566208, loss_sup: 0.125651, loss_mps: 0.157930, loss_cps: 0.282627
[13:57:41.174] iteration 22638: total_loss: 0.612116, loss_sup: 0.225153, loss_mps: 0.138110, loss_cps: 0.248853
[13:57:41.320] iteration 22639: total_loss: 0.641444, loss_sup: 0.023459, loss_mps: 0.217691, loss_cps: 0.400294
[13:57:41.469] iteration 22640: total_loss: 0.425367, loss_sup: 0.040473, loss_mps: 0.131838, loss_cps: 0.253056
[13:57:41.616] iteration 22641: total_loss: 0.489524, loss_sup: 0.032060, loss_mps: 0.159771, loss_cps: 0.297693
[13:57:41.763] iteration 22642: total_loss: 0.497734, loss_sup: 0.130914, loss_mps: 0.139736, loss_cps: 0.227085
[13:57:41.909] iteration 22643: total_loss: 0.496481, loss_sup: 0.070485, loss_mps: 0.142931, loss_cps: 0.283066
[13:57:42.057] iteration 22644: total_loss: 0.351755, loss_sup: 0.013956, loss_mps: 0.118949, loss_cps: 0.218850
[13:57:42.204] iteration 22645: total_loss: 0.280980, loss_sup: 0.018688, loss_mps: 0.094363, loss_cps: 0.167929
[13:57:42.351] iteration 22646: total_loss: 0.309005, loss_sup: 0.015077, loss_mps: 0.109229, loss_cps: 0.184699
[13:57:42.496] iteration 22647: total_loss: 0.490882, loss_sup: 0.167276, loss_mps: 0.111301, loss_cps: 0.212305
[13:57:42.649] iteration 22648: total_loss: 0.303568, loss_sup: 0.067785, loss_mps: 0.089030, loss_cps: 0.146752
[13:57:42.795] iteration 22649: total_loss: 0.321527, loss_sup: 0.019141, loss_mps: 0.110504, loss_cps: 0.191881
[13:57:42.942] iteration 22650: total_loss: 0.377852, loss_sup: 0.016236, loss_mps: 0.122564, loss_cps: 0.239052
[13:57:43.088] iteration 22651: total_loss: 0.475312, loss_sup: 0.056484, loss_mps: 0.145419, loss_cps: 0.273408
[13:57:43.236] iteration 22652: total_loss: 0.317719, loss_sup: 0.027704, loss_mps: 0.108508, loss_cps: 0.181507
[13:57:43.382] iteration 22653: total_loss: 0.252394, loss_sup: 0.009368, loss_mps: 0.089934, loss_cps: 0.153091
[13:57:43.529] iteration 22654: total_loss: 0.650310, loss_sup: 0.019173, loss_mps: 0.198507, loss_cps: 0.432630
[13:57:43.675] iteration 22655: total_loss: 0.745136, loss_sup: 0.133872, loss_mps: 0.193246, loss_cps: 0.418018
[13:57:43.822] iteration 22656: total_loss: 0.614192, loss_sup: 0.023808, loss_mps: 0.187305, loss_cps: 0.403079
[13:57:43.968] iteration 22657: total_loss: 0.364513, loss_sup: 0.155600, loss_mps: 0.072365, loss_cps: 0.136547
[13:57:44.113] iteration 22658: total_loss: 0.349821, loss_sup: 0.044704, loss_mps: 0.112564, loss_cps: 0.192553
[13:57:44.259] iteration 22659: total_loss: 0.320919, loss_sup: 0.014111, loss_mps: 0.107035, loss_cps: 0.199773
[13:57:44.404] iteration 22660: total_loss: 0.340515, loss_sup: 0.032630, loss_mps: 0.107387, loss_cps: 0.200498
[13:57:44.549] iteration 22661: total_loss: 0.356394, loss_sup: 0.090271, loss_mps: 0.100409, loss_cps: 0.165714
[13:57:44.695] iteration 22662: total_loss: 0.455775, loss_sup: 0.019095, loss_mps: 0.151616, loss_cps: 0.285064
[13:57:44.841] iteration 22663: total_loss: 0.517897, loss_sup: 0.062483, loss_mps: 0.153823, loss_cps: 0.301590
[13:57:44.987] iteration 22664: total_loss: 0.606944, loss_sup: 0.150936, loss_mps: 0.149857, loss_cps: 0.306151
[13:57:45.132] iteration 22665: total_loss: 0.431165, loss_sup: 0.092638, loss_mps: 0.119967, loss_cps: 0.218559
[13:57:45.282] iteration 22666: total_loss: 0.382401, loss_sup: 0.072651, loss_mps: 0.101497, loss_cps: 0.208253
[13:57:45.427] iteration 22667: total_loss: 0.413467, loss_sup: 0.076893, loss_mps: 0.115875, loss_cps: 0.220700
[13:57:45.576] iteration 22668: total_loss: 0.519042, loss_sup: 0.081054, loss_mps: 0.150153, loss_cps: 0.287835
[13:57:45.722] iteration 22669: total_loss: 0.461132, loss_sup: 0.019521, loss_mps: 0.145747, loss_cps: 0.295863
[13:57:45.868] iteration 22670: total_loss: 0.492653, loss_sup: 0.138478, loss_mps: 0.118474, loss_cps: 0.235701
[13:57:46.013] iteration 22671: total_loss: 0.238151, loss_sup: 0.012006, loss_mps: 0.085592, loss_cps: 0.140553
[13:57:46.160] iteration 22672: total_loss: 0.679234, loss_sup: 0.100490, loss_mps: 0.183357, loss_cps: 0.395387
[13:57:46.307] iteration 22673: total_loss: 1.541692, loss_sup: 0.299992, loss_mps: 0.362162, loss_cps: 0.879538
[13:57:46.453] iteration 22674: total_loss: 0.297357, loss_sup: 0.023832, loss_mps: 0.103614, loss_cps: 0.169910
[13:57:46.599] iteration 22675: total_loss: 0.629021, loss_sup: 0.151817, loss_mps: 0.156914, loss_cps: 0.320289
[13:57:46.744] iteration 22676: total_loss: 0.802408, loss_sup: 0.085476, loss_mps: 0.231199, loss_cps: 0.485733
[13:57:46.893] iteration 22677: total_loss: 0.673736, loss_sup: 0.053237, loss_mps: 0.195964, loss_cps: 0.424535
[13:57:47.038] iteration 22678: total_loss: 0.583694, loss_sup: 0.081586, loss_mps: 0.163306, loss_cps: 0.338802
[13:57:47.184] iteration 22679: total_loss: 0.381242, loss_sup: 0.015132, loss_mps: 0.126235, loss_cps: 0.239875
[13:57:47.330] iteration 22680: total_loss: 0.441729, loss_sup: 0.022197, loss_mps: 0.147374, loss_cps: 0.272158
[13:57:47.479] iteration 22681: total_loss: 0.816639, loss_sup: 0.265148, loss_mps: 0.178338, loss_cps: 0.373154
[13:57:47.628] iteration 22682: total_loss: 0.337378, loss_sup: 0.015230, loss_mps: 0.117523, loss_cps: 0.204626
[13:57:47.774] iteration 22683: total_loss: 0.814793, loss_sup: 0.040833, loss_mps: 0.239786, loss_cps: 0.534175
[13:57:47.919] iteration 22684: total_loss: 0.310868, loss_sup: 0.037814, loss_mps: 0.105301, loss_cps: 0.167752
[13:57:48.066] iteration 22685: total_loss: 0.505255, loss_sup: 0.026561, loss_mps: 0.152337, loss_cps: 0.326357
[13:57:48.212] iteration 22686: total_loss: 0.878851, loss_sup: 0.133140, loss_mps: 0.237556, loss_cps: 0.508156
[13:57:48.357] iteration 22687: total_loss: 0.316718, loss_sup: 0.020781, loss_mps: 0.103801, loss_cps: 0.192136
[13:57:48.503] iteration 22688: total_loss: 0.630085, loss_sup: 0.076110, loss_mps: 0.176739, loss_cps: 0.377237
[13:57:48.654] iteration 22689: total_loss: 0.576971, loss_sup: 0.063087, loss_mps: 0.170332, loss_cps: 0.343551
[13:57:48.800] iteration 22690: total_loss: 0.285017, loss_sup: 0.023083, loss_mps: 0.099214, loss_cps: 0.162719
[13:57:48.945] iteration 22691: total_loss: 0.591810, loss_sup: 0.060809, loss_mps: 0.182176, loss_cps: 0.348824
[13:57:49.091] iteration 22692: total_loss: 0.451196, loss_sup: 0.055606, loss_mps: 0.140806, loss_cps: 0.254784
[13:57:49.237] iteration 22693: total_loss: 0.448502, loss_sup: 0.015622, loss_mps: 0.154516, loss_cps: 0.278364
[13:57:49.383] iteration 22694: total_loss: 0.238937, loss_sup: 0.015866, loss_mps: 0.081702, loss_cps: 0.141369
[13:57:49.530] iteration 22695: total_loss: 0.401762, loss_sup: 0.032534, loss_mps: 0.124584, loss_cps: 0.244644
[13:57:49.676] iteration 22696: total_loss: 0.215661, loss_sup: 0.010078, loss_mps: 0.079002, loss_cps: 0.126580
[13:57:49.823] iteration 22697: total_loss: 0.301812, loss_sup: 0.010983, loss_mps: 0.099118, loss_cps: 0.191711
[13:57:49.971] iteration 22698: total_loss: 0.661170, loss_sup: 0.061175, loss_mps: 0.197166, loss_cps: 0.402830
[13:57:50.119] iteration 22699: total_loss: 0.315975, loss_sup: 0.025130, loss_mps: 0.103454, loss_cps: 0.187391
[13:57:50.265] iteration 22700: total_loss: 0.314549, loss_sup: 0.052612, loss_mps: 0.090817, loss_cps: 0.171120
[13:57:50.265] Evaluation Started ==>
[13:58:01.614] ==> valid iteration 22700: unet metrics: {'dc': 0.679832488870269, 'jc': 0.5621229011611449, 'pre': 0.8147195160779331, 'hd': 5.437772868141842}, ynet metrics: {'dc': 0.6032438570632406, 'jc': 0.4892735244830356, 'pre': 0.8141189397217937, 'hd': 5.384987081546471}.
[13:58:01.616] Evaluation Finished!⏹️
[13:58:01.768] iteration 22701: total_loss: 0.293693, loss_sup: 0.003581, loss_mps: 0.105479, loss_cps: 0.184633
[13:58:01.916] iteration 22702: total_loss: 0.333240, loss_sup: 0.030614, loss_mps: 0.105717, loss_cps: 0.196909
[13:58:02.062] iteration 22703: total_loss: 0.477551, loss_sup: 0.168456, loss_mps: 0.112680, loss_cps: 0.196415
[13:58:02.207] iteration 22704: total_loss: 0.715367, loss_sup: 0.067308, loss_mps: 0.215134, loss_cps: 0.432926
[13:58:02.353] iteration 22705: total_loss: 0.292392, loss_sup: 0.028468, loss_mps: 0.098611, loss_cps: 0.165313
[13:58:02.500] iteration 22706: total_loss: 0.627871, loss_sup: 0.067428, loss_mps: 0.186723, loss_cps: 0.373720
[13:58:02.646] iteration 22707: total_loss: 0.416737, loss_sup: 0.088210, loss_mps: 0.115454, loss_cps: 0.213073
[13:58:02.792] iteration 22708: total_loss: 0.286800, loss_sup: 0.081856, loss_mps: 0.079445, loss_cps: 0.125498
[13:58:02.937] iteration 22709: total_loss: 0.463355, loss_sup: 0.035957, loss_mps: 0.146738, loss_cps: 0.280660
[13:58:03.082] iteration 22710: total_loss: 0.362018, loss_sup: 0.021781, loss_mps: 0.113826, loss_cps: 0.226411
[13:58:03.229] iteration 22711: total_loss: 1.241697, loss_sup: 0.150862, loss_mps: 0.334759, loss_cps: 0.756075
[13:58:03.375] iteration 22712: total_loss: 0.601522, loss_sup: 0.012343, loss_mps: 0.186992, loss_cps: 0.402187
[13:58:03.520] iteration 22713: total_loss: 0.438603, loss_sup: 0.244559, loss_mps: 0.075255, loss_cps: 0.118789
[13:58:03.666] iteration 22714: total_loss: 0.482349, loss_sup: 0.033270, loss_mps: 0.152153, loss_cps: 0.296926
[13:58:03.815] iteration 22715: total_loss: 0.564280, loss_sup: 0.104529, loss_mps: 0.162031, loss_cps: 0.297720
[13:58:03.963] iteration 22716: total_loss: 0.586799, loss_sup: 0.017818, loss_mps: 0.187345, loss_cps: 0.381637
[13:58:04.109] iteration 22717: total_loss: 0.514197, loss_sup: 0.077471, loss_mps: 0.155875, loss_cps: 0.280850
[13:58:04.257] iteration 22718: total_loss: 0.467029, loss_sup: 0.015611, loss_mps: 0.145387, loss_cps: 0.306031
[13:58:04.404] iteration 22719: total_loss: 0.520039, loss_sup: 0.040566, loss_mps: 0.160572, loss_cps: 0.318901
[13:58:04.551] iteration 22720: total_loss: 0.459052, loss_sup: 0.027982, loss_mps: 0.145646, loss_cps: 0.285424
[13:58:04.697] iteration 22721: total_loss: 0.711349, loss_sup: 0.050950, loss_mps: 0.208944, loss_cps: 0.451455
[13:58:04.844] iteration 22722: total_loss: 0.569264, loss_sup: 0.098111, loss_mps: 0.153291, loss_cps: 0.317862
[13:58:04.990] iteration 22723: total_loss: 0.571922, loss_sup: 0.013408, loss_mps: 0.174419, loss_cps: 0.384095
[13:58:05.136] iteration 22724: total_loss: 0.377677, loss_sup: 0.044380, loss_mps: 0.114731, loss_cps: 0.218567
[13:58:05.282] iteration 22725: total_loss: 0.240407, loss_sup: 0.008981, loss_mps: 0.086667, loss_cps: 0.144759
[13:58:05.428] iteration 22726: total_loss: 0.541505, loss_sup: 0.045179, loss_mps: 0.170235, loss_cps: 0.326091
[13:58:05.577] iteration 22727: total_loss: 0.440093, loss_sup: 0.056362, loss_mps: 0.133290, loss_cps: 0.250441
[13:58:05.723] iteration 22728: total_loss: 0.523127, loss_sup: 0.065127, loss_mps: 0.156115, loss_cps: 0.301885
[13:58:05.869] iteration 22729: total_loss: 0.320305, loss_sup: 0.020210, loss_mps: 0.108889, loss_cps: 0.191207
[13:58:06.015] iteration 22730: total_loss: 0.803741, loss_sup: 0.176698, loss_mps: 0.198492, loss_cps: 0.428551
[13:58:06.161] iteration 22731: total_loss: 0.374425, loss_sup: 0.052139, loss_mps: 0.108532, loss_cps: 0.213753
[13:58:06.307] iteration 22732: total_loss: 1.184688, loss_sup: 0.377729, loss_mps: 0.252846, loss_cps: 0.554113
[13:58:06.453] iteration 22733: total_loss: 0.562849, loss_sup: 0.063517, loss_mps: 0.169264, loss_cps: 0.330069
[13:58:06.599] iteration 22734: total_loss: 0.464997, loss_sup: 0.148266, loss_mps: 0.113832, loss_cps: 0.202900
[13:58:06.746] iteration 22735: total_loss: 0.698142, loss_sup: 0.060484, loss_mps: 0.216431, loss_cps: 0.421227
[13:58:06.892] iteration 22736: total_loss: 0.452537, loss_sup: 0.060723, loss_mps: 0.147284, loss_cps: 0.244531
[13:58:07.038] iteration 22737: total_loss: 0.564660, loss_sup: 0.124365, loss_mps: 0.150747, loss_cps: 0.289548
[13:58:07.184] iteration 22738: total_loss: 0.569546, loss_sup: 0.071665, loss_mps: 0.169459, loss_cps: 0.328421
[13:58:07.331] iteration 22739: total_loss: 0.428198, loss_sup: 0.034320, loss_mps: 0.139664, loss_cps: 0.254214
[13:58:07.477] iteration 22740: total_loss: 0.402578, loss_sup: 0.013145, loss_mps: 0.136454, loss_cps: 0.252980
[13:58:07.623] iteration 22741: total_loss: 0.333693, loss_sup: 0.079593, loss_mps: 0.090976, loss_cps: 0.163124
[13:58:07.769] iteration 22742: total_loss: 0.319043, loss_sup: 0.005894, loss_mps: 0.113063, loss_cps: 0.200086
[13:58:07.915] iteration 22743: total_loss: 0.601949, loss_sup: 0.067867, loss_mps: 0.172613, loss_cps: 0.361468
[13:58:08.062] iteration 22744: total_loss: 0.371346, loss_sup: 0.046544, loss_mps: 0.113362, loss_cps: 0.211441
[13:58:08.208] iteration 22745: total_loss: 0.331207, loss_sup: 0.084440, loss_mps: 0.090353, loss_cps: 0.156414
[13:58:08.354] iteration 22746: total_loss: 0.732393, loss_sup: 0.244542, loss_mps: 0.177331, loss_cps: 0.310519
[13:58:08.500] iteration 22747: total_loss: 0.559139, loss_sup: 0.054986, loss_mps: 0.171726, loss_cps: 0.332426
[13:58:08.646] iteration 22748: total_loss: 0.374476, loss_sup: 0.040144, loss_mps: 0.116198, loss_cps: 0.218134
[13:58:08.792] iteration 22749: total_loss: 1.169484, loss_sup: 0.150815, loss_mps: 0.301244, loss_cps: 0.717425
[13:58:08.938] iteration 22750: total_loss: 0.400739, loss_sup: 0.053328, loss_mps: 0.122192, loss_cps: 0.225219
[13:58:09.084] iteration 22751: total_loss: 1.124869, loss_sup: 0.081118, loss_mps: 0.318570, loss_cps: 0.725182
[13:58:09.231] iteration 22752: total_loss: 0.787994, loss_sup: 0.159699, loss_mps: 0.201148, loss_cps: 0.427148
[13:58:09.379] iteration 22753: total_loss: 0.389592, loss_sup: 0.067305, loss_mps: 0.114929, loss_cps: 0.207358
[13:58:09.527] iteration 22754: total_loss: 0.762023, loss_sup: 0.251901, loss_mps: 0.170449, loss_cps: 0.339673
[13:58:09.673] iteration 22755: total_loss: 0.577796, loss_sup: 0.089791, loss_mps: 0.174338, loss_cps: 0.313667
[13:58:09.819] iteration 22756: total_loss: 0.415037, loss_sup: 0.018073, loss_mps: 0.135301, loss_cps: 0.261663
[13:58:09.965] iteration 22757: total_loss: 0.372881, loss_sup: 0.026683, loss_mps: 0.116386, loss_cps: 0.229812
[13:58:10.111] iteration 22758: total_loss: 1.093684, loss_sup: 0.094585, loss_mps: 0.294114, loss_cps: 0.704984
[13:58:10.260] iteration 22759: total_loss: 0.616610, loss_sup: 0.031858, loss_mps: 0.193460, loss_cps: 0.391293
[13:58:10.407] iteration 22760: total_loss: 0.797626, loss_sup: 0.092504, loss_mps: 0.214177, loss_cps: 0.490945
[13:58:10.553] iteration 22761: total_loss: 0.440074, loss_sup: 0.007086, loss_mps: 0.148787, loss_cps: 0.284201
[13:58:10.699] iteration 22762: total_loss: 0.427752, loss_sup: 0.025156, loss_mps: 0.135377, loss_cps: 0.267220
[13:58:10.849] iteration 22763: total_loss: 0.402876, loss_sup: 0.021810, loss_mps: 0.131118, loss_cps: 0.249948
[13:58:10.999] iteration 22764: total_loss: 0.597435, loss_sup: 0.142869, loss_mps: 0.144773, loss_cps: 0.309793
[13:58:11.145] iteration 22765: total_loss: 0.763022, loss_sup: 0.188820, loss_mps: 0.186612, loss_cps: 0.387589
[13:58:11.291] iteration 22766: total_loss: 0.780674, loss_sup: 0.075514, loss_mps: 0.216359, loss_cps: 0.488801
[13:58:11.437] iteration 22767: total_loss: 0.470367, loss_sup: 0.038267, loss_mps: 0.147886, loss_cps: 0.284214
[13:58:11.583] iteration 22768: total_loss: 0.731523, loss_sup: 0.088012, loss_mps: 0.209209, loss_cps: 0.434302
[13:58:11.729] iteration 22769: total_loss: 0.914755, loss_sup: 0.290541, loss_mps: 0.205423, loss_cps: 0.418791
[13:58:11.875] iteration 22770: total_loss: 0.519554, loss_sup: 0.088058, loss_mps: 0.148205, loss_cps: 0.283291
[13:58:12.021] iteration 22771: total_loss: 0.559475, loss_sup: 0.164617, loss_mps: 0.138436, loss_cps: 0.256423
[13:58:12.167] iteration 22772: total_loss: 1.034249, loss_sup: 0.229723, loss_mps: 0.246042, loss_cps: 0.558483
[13:58:12.313] iteration 22773: total_loss: 0.446971, loss_sup: 0.128716, loss_mps: 0.124583, loss_cps: 0.193672
[13:58:12.460] iteration 22774: total_loss: 0.332266, loss_sup: 0.006326, loss_mps: 0.111564, loss_cps: 0.214376
[13:58:12.607] iteration 22775: total_loss: 0.713613, loss_sup: 0.107060, loss_mps: 0.210191, loss_cps: 0.396363
[13:58:12.752] iteration 22776: total_loss: 0.465633, loss_sup: 0.032643, loss_mps: 0.156118, loss_cps: 0.276872
[13:58:12.899] iteration 22777: total_loss: 0.678069, loss_sup: 0.022394, loss_mps: 0.223724, loss_cps: 0.431951
[13:58:13.045] iteration 22778: total_loss: 0.454919, loss_sup: 0.067740, loss_mps: 0.130423, loss_cps: 0.256756
[13:58:13.191] iteration 22779: total_loss: 0.340400, loss_sup: 0.022980, loss_mps: 0.112501, loss_cps: 0.204919
[13:58:13.337] iteration 22780: total_loss: 0.603359, loss_sup: 0.070168, loss_mps: 0.199490, loss_cps: 0.333701
[13:58:13.483] iteration 22781: total_loss: 0.664496, loss_sup: 0.089035, loss_mps: 0.190057, loss_cps: 0.385404
[13:58:13.630] iteration 22782: total_loss: 0.854456, loss_sup: 0.034602, loss_mps: 0.264943, loss_cps: 0.554911
[13:58:13.777] iteration 22783: total_loss: 0.401715, loss_sup: 0.036821, loss_mps: 0.125557, loss_cps: 0.239336
[13:58:13.926] iteration 22784: total_loss: 0.575569, loss_sup: 0.058723, loss_mps: 0.186080, loss_cps: 0.330766
[13:58:14.073] iteration 22785: total_loss: 0.756038, loss_sup: 0.061557, loss_mps: 0.226847, loss_cps: 0.467634
[13:58:14.218] iteration 22786: total_loss: 0.524453, loss_sup: 0.121380, loss_mps: 0.147558, loss_cps: 0.255516
[13:58:14.364] iteration 22787: total_loss: 0.372770, loss_sup: 0.028866, loss_mps: 0.131461, loss_cps: 0.212443
[13:58:14.510] iteration 22788: total_loss: 0.650006, loss_sup: 0.075828, loss_mps: 0.194032, loss_cps: 0.380145
[13:58:14.658] iteration 22789: total_loss: 0.588306, loss_sup: 0.294661, loss_mps: 0.105723, loss_cps: 0.187922
[13:58:14.804] iteration 22790: total_loss: 0.567264, loss_sup: 0.081026, loss_mps: 0.171918, loss_cps: 0.314320
[13:58:14.951] iteration 22791: total_loss: 0.705478, loss_sup: 0.058346, loss_mps: 0.216446, loss_cps: 0.430686
[13:58:15.098] iteration 22792: total_loss: 0.719768, loss_sup: 0.044449, loss_mps: 0.219997, loss_cps: 0.455321
[13:58:15.244] iteration 22793: total_loss: 0.413934, loss_sup: 0.018024, loss_mps: 0.139666, loss_cps: 0.256244
[13:58:15.390] iteration 22794: total_loss: 0.401072, loss_sup: 0.008450, loss_mps: 0.136773, loss_cps: 0.255848
[13:58:15.538] iteration 22795: total_loss: 0.644777, loss_sup: 0.119714, loss_mps: 0.184131, loss_cps: 0.340932
[13:58:15.683] iteration 22796: total_loss: 0.446249, loss_sup: 0.050970, loss_mps: 0.139989, loss_cps: 0.255290
[13:58:15.832] iteration 22797: total_loss: 0.733694, loss_sup: 0.063629, loss_mps: 0.222181, loss_cps: 0.447884
[13:58:15.978] iteration 22798: total_loss: 0.549882, loss_sup: 0.052651, loss_mps: 0.168672, loss_cps: 0.328559
[13:58:16.125] iteration 22799: total_loss: 0.678633, loss_sup: 0.015816, loss_mps: 0.214630, loss_cps: 0.448186
[13:58:16.271] iteration 22800: total_loss: 0.314705, loss_sup: 0.011062, loss_mps: 0.108648, loss_cps: 0.194995
[13:58:16.271] Evaluation Started ==>
[13:58:27.607] ==> valid iteration 22800: unet metrics: {'dc': 0.6572724142131312, 'jc': 0.5409202748956503, 'pre': 0.787487055506328, 'hd': 5.456206613676526}, ynet metrics: {'dc': 0.5660933427127416, 'jc': 0.4510300120477601, 'pre': 0.8118268594008153, 'hd': 5.34685800366625}.
[13:58:27.609] Evaluation Finished!⏹️
[13:58:27.759] iteration 22801: total_loss: 0.738997, loss_sup: 0.130872, loss_mps: 0.201591, loss_cps: 0.406534
[13:58:27.907] iteration 22802: total_loss: 0.368637, loss_sup: 0.011649, loss_mps: 0.128566, loss_cps: 0.228422
[13:58:28.053] iteration 22803: total_loss: 0.386201, loss_sup: 0.030008, loss_mps: 0.118937, loss_cps: 0.237256
[13:58:28.198] iteration 22804: total_loss: 0.710007, loss_sup: 0.092529, loss_mps: 0.201105, loss_cps: 0.416373
[13:58:28.344] iteration 22805: total_loss: 0.324251, loss_sup: 0.005075, loss_mps: 0.112734, loss_cps: 0.206442
[13:58:28.491] iteration 22806: total_loss: 0.429226, loss_sup: 0.110804, loss_mps: 0.111553, loss_cps: 0.206869
[13:58:28.636] iteration 22807: total_loss: 0.399000, loss_sup: 0.045416, loss_mps: 0.130928, loss_cps: 0.222656
[13:58:28.782] iteration 22808: total_loss: 0.547622, loss_sup: 0.075035, loss_mps: 0.166437, loss_cps: 0.306150
[13:58:28.927] iteration 22809: total_loss: 0.504782, loss_sup: 0.074353, loss_mps: 0.155148, loss_cps: 0.275281
[13:58:29.074] iteration 22810: total_loss: 0.445266, loss_sup: 0.043420, loss_mps: 0.141131, loss_cps: 0.260715
[13:58:29.220] iteration 22811: total_loss: 0.776303, loss_sup: 0.074329, loss_mps: 0.239877, loss_cps: 0.462097
[13:58:29.366] iteration 22812: total_loss: 0.360386, loss_sup: 0.008728, loss_mps: 0.125167, loss_cps: 0.226490
[13:58:29.511] iteration 22813: total_loss: 0.333847, loss_sup: 0.016827, loss_mps: 0.117839, loss_cps: 0.199180
[13:58:29.657] iteration 22814: total_loss: 0.425654, loss_sup: 0.083802, loss_mps: 0.116354, loss_cps: 0.225498
[13:58:29.803] iteration 22815: total_loss: 0.446319, loss_sup: 0.029515, loss_mps: 0.139899, loss_cps: 0.276904
[13:58:29.950] iteration 22816: total_loss: 0.949046, loss_sup: 0.222995, loss_mps: 0.239709, loss_cps: 0.486341
[13:58:30.096] iteration 22817: total_loss: 0.321657, loss_sup: 0.034270, loss_mps: 0.104171, loss_cps: 0.183216
[13:58:30.243] iteration 22818: total_loss: 0.367822, loss_sup: 0.063878, loss_mps: 0.114663, loss_cps: 0.189280
[13:58:30.389] iteration 22819: total_loss: 0.735176, loss_sup: 0.081994, loss_mps: 0.209347, loss_cps: 0.443836
[13:58:30.535] iteration 22820: total_loss: 0.431736, loss_sup: 0.046911, loss_mps: 0.134528, loss_cps: 0.250297
[13:58:30.681] iteration 22821: total_loss: 0.275739, loss_sup: 0.007551, loss_mps: 0.097059, loss_cps: 0.171129
[13:58:30.829] iteration 22822: total_loss: 0.504596, loss_sup: 0.029700, loss_mps: 0.155401, loss_cps: 0.319496
[13:58:30.975] iteration 22823: total_loss: 0.495458, loss_sup: 0.140838, loss_mps: 0.119354, loss_cps: 0.235265
[13:58:31.122] iteration 22824: total_loss: 0.302731, loss_sup: 0.003510, loss_mps: 0.105811, loss_cps: 0.193410
[13:58:31.269] iteration 22825: total_loss: 0.981547, loss_sup: 0.325089, loss_mps: 0.222691, loss_cps: 0.433767
[13:58:31.415] iteration 22826: total_loss: 0.292607, loss_sup: 0.046367, loss_mps: 0.091899, loss_cps: 0.154341
[13:58:31.561] iteration 22827: total_loss: 0.456614, loss_sup: 0.015103, loss_mps: 0.156085, loss_cps: 0.285426
[13:58:31.707] iteration 22828: total_loss: 0.429445, loss_sup: 0.038633, loss_mps: 0.142704, loss_cps: 0.248108
[13:58:31.857] iteration 22829: total_loss: 0.485101, loss_sup: 0.015073, loss_mps: 0.165658, loss_cps: 0.304370
[13:58:32.004] iteration 22830: total_loss: 0.409109, loss_sup: 0.017532, loss_mps: 0.142706, loss_cps: 0.248871
[13:58:32.150] iteration 22831: total_loss: 0.360813, loss_sup: 0.030666, loss_mps: 0.115178, loss_cps: 0.214969
[13:58:32.297] iteration 22832: total_loss: 0.375375, loss_sup: 0.083758, loss_mps: 0.104206, loss_cps: 0.187411
[13:58:32.444] iteration 22833: total_loss: 0.289789, loss_sup: 0.041692, loss_mps: 0.097375, loss_cps: 0.150722
[13:58:32.590] iteration 22834: total_loss: 1.175341, loss_sup: 0.365958, loss_mps: 0.255191, loss_cps: 0.554192
[13:58:32.736] iteration 22835: total_loss: 0.280717, loss_sup: 0.008279, loss_mps: 0.100040, loss_cps: 0.172398
[13:58:32.882] iteration 22836: total_loss: 0.419092, loss_sup: 0.007310, loss_mps: 0.144855, loss_cps: 0.266927
[13:58:33.030] iteration 22837: total_loss: 0.449097, loss_sup: 0.029561, loss_mps: 0.146444, loss_cps: 0.273092
[13:58:33.177] iteration 22838: total_loss: 0.456804, loss_sup: 0.083306, loss_mps: 0.132678, loss_cps: 0.240821
[13:58:33.323] iteration 22839: total_loss: 0.717281, loss_sup: 0.263688, loss_mps: 0.162489, loss_cps: 0.291104
[13:58:33.469] iteration 22840: total_loss: 0.613029, loss_sup: 0.022380, loss_mps: 0.205085, loss_cps: 0.385564
[13:58:33.615] iteration 22841: total_loss: 0.687788, loss_sup: 0.130868, loss_mps: 0.192729, loss_cps: 0.364190
[13:58:33.762] iteration 22842: total_loss: 0.674009, loss_sup: 0.181169, loss_mps: 0.172244, loss_cps: 0.320597
[13:58:33.909] iteration 22843: total_loss: 0.325286, loss_sup: 0.001877, loss_mps: 0.123233, loss_cps: 0.200176
[13:58:34.061] iteration 22844: total_loss: 0.308000, loss_sup: 0.005524, loss_mps: 0.109349, loss_cps: 0.193127
[13:58:34.207] iteration 22845: total_loss: 0.477960, loss_sup: 0.049086, loss_mps: 0.149648, loss_cps: 0.279226
[13:58:34.354] iteration 22846: total_loss: 0.782742, loss_sup: 0.243176, loss_mps: 0.182313, loss_cps: 0.357253
[13:58:34.502] iteration 22847: total_loss: 0.715629, loss_sup: 0.003269, loss_mps: 0.235974, loss_cps: 0.476386
[13:58:34.648] iteration 22848: total_loss: 0.369017, loss_sup: 0.040978, loss_mps: 0.114338, loss_cps: 0.213702
[13:58:34.794] iteration 22849: total_loss: 0.449461, loss_sup: 0.027223, loss_mps: 0.143417, loss_cps: 0.278821
[13:58:34.940] iteration 22850: total_loss: 0.662012, loss_sup: 0.166689, loss_mps: 0.170449, loss_cps: 0.324875
[13:58:35.087] iteration 22851: total_loss: 0.624557, loss_sup: 0.156643, loss_mps: 0.148536, loss_cps: 0.319378
[13:58:35.235] iteration 22852: total_loss: 0.706077, loss_sup: 0.080154, loss_mps: 0.206341, loss_cps: 0.419582
[13:58:35.381] iteration 22853: total_loss: 0.564136, loss_sup: 0.126762, loss_mps: 0.158021, loss_cps: 0.279353
[13:58:35.528] iteration 22854: total_loss: 0.513258, loss_sup: 0.135841, loss_mps: 0.137093, loss_cps: 0.240323
[13:58:35.674] iteration 22855: total_loss: 0.374110, loss_sup: 0.014259, loss_mps: 0.120304, loss_cps: 0.239547
[13:58:35.820] iteration 22856: total_loss: 0.774567, loss_sup: 0.032588, loss_mps: 0.233770, loss_cps: 0.508209
[13:58:35.966] iteration 22857: total_loss: 0.403391, loss_sup: 0.019414, loss_mps: 0.133643, loss_cps: 0.250334
[13:58:36.113] iteration 22858: total_loss: 0.503500, loss_sup: 0.008364, loss_mps: 0.164754, loss_cps: 0.330383
[13:58:36.260] iteration 22859: total_loss: 0.595048, loss_sup: 0.189649, loss_mps: 0.152476, loss_cps: 0.252923
[13:58:36.406] iteration 22860: total_loss: 0.699301, loss_sup: 0.074759, loss_mps: 0.209345, loss_cps: 0.415197
[13:58:36.552] iteration 22861: total_loss: 0.370242, loss_sup: 0.043863, loss_mps: 0.116429, loss_cps: 0.209950
[13:58:36.699] iteration 22862: total_loss: 0.410033, loss_sup: 0.087916, loss_mps: 0.117301, loss_cps: 0.204817
[13:58:36.845] iteration 22863: total_loss: 0.671547, loss_sup: 0.162125, loss_mps: 0.175040, loss_cps: 0.334382
[13:58:36.991] iteration 22864: total_loss: 0.882715, loss_sup: 0.079132, loss_mps: 0.254653, loss_cps: 0.548930
[13:58:37.138] iteration 22865: total_loss: 0.584374, loss_sup: 0.187670, loss_mps: 0.145792, loss_cps: 0.250913
[13:58:37.287] iteration 22866: total_loss: 0.658130, loss_sup: 0.078127, loss_mps: 0.171829, loss_cps: 0.408175
[13:58:37.435] iteration 22867: total_loss: 0.408394, loss_sup: 0.089443, loss_mps: 0.112582, loss_cps: 0.206368
[13:58:37.581] iteration 22868: total_loss: 0.678341, loss_sup: 0.119653, loss_mps: 0.189777, loss_cps: 0.368911
[13:58:37.728] iteration 22869: total_loss: 0.418704, loss_sup: 0.008491, loss_mps: 0.137773, loss_cps: 0.272440
[13:58:37.873] iteration 22870: total_loss: 0.471748, loss_sup: 0.017226, loss_mps: 0.153886, loss_cps: 0.300636
[13:58:38.019] iteration 22871: total_loss: 0.446024, loss_sup: 0.039976, loss_mps: 0.140762, loss_cps: 0.265286
[13:58:38.166] iteration 22872: total_loss: 0.636078, loss_sup: 0.110159, loss_mps: 0.169098, loss_cps: 0.356821
[13:58:38.312] iteration 22873: total_loss: 0.424638, loss_sup: 0.048741, loss_mps: 0.133233, loss_cps: 0.242664
[13:58:38.461] iteration 22874: total_loss: 0.345189, loss_sup: 0.021467, loss_mps: 0.114240, loss_cps: 0.209482
[13:58:38.608] iteration 22875: total_loss: 0.362877, loss_sup: 0.009142, loss_mps: 0.129133, loss_cps: 0.224601
[13:58:38.754] iteration 22876: total_loss: 1.304863, loss_sup: 0.025695, loss_mps: 0.385061, loss_cps: 0.894108
[13:58:38.901] iteration 22877: total_loss: 0.521477, loss_sup: 0.043838, loss_mps: 0.158992, loss_cps: 0.318647
[13:58:39.048] iteration 22878: total_loss: 0.442158, loss_sup: 0.023371, loss_mps: 0.142435, loss_cps: 0.276353
[13:58:39.194] iteration 22879: total_loss: 0.438546, loss_sup: 0.063877, loss_mps: 0.131268, loss_cps: 0.243401
[13:58:39.341] iteration 22880: total_loss: 0.771845, loss_sup: 0.086856, loss_mps: 0.222614, loss_cps: 0.462375
[13:58:39.488] iteration 22881: total_loss: 0.565122, loss_sup: 0.067363, loss_mps: 0.165422, loss_cps: 0.332337
[13:58:39.636] iteration 22882: total_loss: 1.046459, loss_sup: 0.019016, loss_mps: 0.321537, loss_cps: 0.705905
[13:58:39.782] iteration 22883: total_loss: 0.946335, loss_sup: 0.167582, loss_mps: 0.231371, loss_cps: 0.547382
[13:58:39.933] iteration 22884: total_loss: 0.763787, loss_sup: 0.055762, loss_mps: 0.230894, loss_cps: 0.477130
[13:58:40.080] iteration 22885: total_loss: 0.742535, loss_sup: 0.151652, loss_mps: 0.186964, loss_cps: 0.403918
[13:58:40.227] iteration 22886: total_loss: 0.352397, loss_sup: 0.106813, loss_mps: 0.090652, loss_cps: 0.154933
[13:58:40.373] iteration 22887: total_loss: 0.532195, loss_sup: 0.093836, loss_mps: 0.156783, loss_cps: 0.281577
[13:58:40.520] iteration 22888: total_loss: 0.464868, loss_sup: 0.061429, loss_mps: 0.151568, loss_cps: 0.251870
[13:58:40.672] iteration 22889: total_loss: 0.318508, loss_sup: 0.015072, loss_mps: 0.115514, loss_cps: 0.187922
[13:58:40.821] iteration 22890: total_loss: 0.706203, loss_sup: 0.132219, loss_mps: 0.192940, loss_cps: 0.381044
[13:58:40.968] iteration 22891: total_loss: 0.536052, loss_sup: 0.083792, loss_mps: 0.159912, loss_cps: 0.292347
[13:58:41.118] iteration 22892: total_loss: 0.666446, loss_sup: 0.070113, loss_mps: 0.201589, loss_cps: 0.394744
[13:58:41.265] iteration 22893: total_loss: 0.479590, loss_sup: 0.111450, loss_mps: 0.134644, loss_cps: 0.233497
[13:58:41.411] iteration 22894: total_loss: 0.307031, loss_sup: 0.004602, loss_mps: 0.105579, loss_cps: 0.196851
[13:58:41.558] iteration 22895: total_loss: 0.416562, loss_sup: 0.014254, loss_mps: 0.144380, loss_cps: 0.257928
[13:58:41.705] iteration 22896: total_loss: 1.122641, loss_sup: 0.120084, loss_mps: 0.313607, loss_cps: 0.688950
[13:58:41.852] iteration 22897: total_loss: 0.304619, loss_sup: 0.028242, loss_mps: 0.097641, loss_cps: 0.178736
[13:58:41.998] iteration 22898: total_loss: 0.575152, loss_sup: 0.072708, loss_mps: 0.164234, loss_cps: 0.338210
[13:58:42.144] iteration 22899: total_loss: 0.527698, loss_sup: 0.106762, loss_mps: 0.146186, loss_cps: 0.274750
[13:58:42.293] iteration 22900: total_loss: 0.714045, loss_sup: 0.047818, loss_mps: 0.217904, loss_cps: 0.448323
[13:58:42.293] Evaluation Started ==>
[13:58:53.596] ==> valid iteration 22900: unet metrics: {'dc': 0.6366893144566876, 'jc': 0.5183496589484189, 'pre': 0.8212866957986259, 'hd': 5.379691297077088}, ynet metrics: {'dc': 0.5900769983896955, 'jc': 0.4762502153564533, 'pre': 0.7821799225992531, 'hd': 5.509020703787852}.
[13:58:53.598] Evaluation Finished!⏹️
[13:58:53.753] iteration 22901: total_loss: 0.399785, loss_sup: 0.020795, loss_mps: 0.135049, loss_cps: 0.243942
[13:58:53.900] iteration 22902: total_loss: 0.402905, loss_sup: 0.054467, loss_mps: 0.125408, loss_cps: 0.223030
[13:58:54.046] iteration 22903: total_loss: 0.612718, loss_sup: 0.024845, loss_mps: 0.183416, loss_cps: 0.404456
[13:58:54.191] iteration 22904: total_loss: 0.525918, loss_sup: 0.043743, loss_mps: 0.157139, loss_cps: 0.325037
[13:58:54.336] iteration 22905: total_loss: 0.656999, loss_sup: 0.073926, loss_mps: 0.201027, loss_cps: 0.382046
[13:58:54.481] iteration 22906: total_loss: 0.773632, loss_sup: 0.195751, loss_mps: 0.187128, loss_cps: 0.390754
[13:58:54.628] iteration 22907: total_loss: 0.622533, loss_sup: 0.077868, loss_mps: 0.185068, loss_cps: 0.359598
[13:58:54.773] iteration 22908: total_loss: 0.688294, loss_sup: 0.299742, loss_mps: 0.134898, loss_cps: 0.253654
[13:58:54.920] iteration 22909: total_loss: 0.692029, loss_sup: 0.066213, loss_mps: 0.203916, loss_cps: 0.421899
[13:58:55.069] iteration 22910: total_loss: 0.815657, loss_sup: 0.089662, loss_mps: 0.238561, loss_cps: 0.487434
[13:58:55.214] iteration 22911: total_loss: 0.892926, loss_sup: 0.236668, loss_mps: 0.217194, loss_cps: 0.439064
[13:58:55.360] iteration 22912: total_loss: 0.814927, loss_sup: 0.034990, loss_mps: 0.260684, loss_cps: 0.519253
[13:58:55.505] iteration 22913: total_loss: 0.660848, loss_sup: 0.159379, loss_mps: 0.176683, loss_cps: 0.324786
[13:58:55.651] iteration 22914: total_loss: 0.606938, loss_sup: 0.130086, loss_mps: 0.169464, loss_cps: 0.307387
[13:58:55.796] iteration 22915: total_loss: 0.775171, loss_sup: 0.024816, loss_mps: 0.232169, loss_cps: 0.518187
[13:58:55.943] iteration 22916: total_loss: 0.416732, loss_sup: 0.036084, loss_mps: 0.136323, loss_cps: 0.244325
[13:58:56.088] iteration 22917: total_loss: 0.527521, loss_sup: 0.023566, loss_mps: 0.164789, loss_cps: 0.339167
[13:58:56.235] iteration 22918: total_loss: 0.596593, loss_sup: 0.133868, loss_mps: 0.150316, loss_cps: 0.312409
[13:58:56.380] iteration 22919: total_loss: 0.405562, loss_sup: 0.007365, loss_mps: 0.147534, loss_cps: 0.250663
[13:58:56.528] iteration 22920: total_loss: 0.707757, loss_sup: 0.017216, loss_mps: 0.219929, loss_cps: 0.470613
[13:58:56.674] iteration 22921: total_loss: 0.362955, loss_sup: 0.035458, loss_mps: 0.115826, loss_cps: 0.211671
[13:58:56.821] iteration 22922: total_loss: 0.948002, loss_sup: 0.323299, loss_mps: 0.196636, loss_cps: 0.428067
[13:58:56.967] iteration 22923: total_loss: 0.295647, loss_sup: 0.033918, loss_mps: 0.100348, loss_cps: 0.161380
[13:58:57.113] iteration 22924: total_loss: 0.610259, loss_sup: 0.129522, loss_mps: 0.165895, loss_cps: 0.314842
[13:58:57.259] iteration 22925: total_loss: 0.570909, loss_sup: 0.078024, loss_mps: 0.179399, loss_cps: 0.313486
[13:58:57.405] iteration 22926: total_loss: 1.065135, loss_sup: 0.133602, loss_mps: 0.312249, loss_cps: 0.619284
[13:58:57.551] iteration 22927: total_loss: 0.444356, loss_sup: 0.039857, loss_mps: 0.145269, loss_cps: 0.259230
[13:58:57.696] iteration 22928: total_loss: 0.543707, loss_sup: 0.061856, loss_mps: 0.166308, loss_cps: 0.315544
[13:58:57.842] iteration 22929: total_loss: 0.444723, loss_sup: 0.054484, loss_mps: 0.138362, loss_cps: 0.251877
[13:58:57.990] iteration 22930: total_loss: 0.828620, loss_sup: 0.114592, loss_mps: 0.246194, loss_cps: 0.467835
[13:58:58.136] iteration 22931: total_loss: 0.631424, loss_sup: 0.056351, loss_mps: 0.196264, loss_cps: 0.378809
[13:58:58.281] iteration 22932: total_loss: 0.710711, loss_sup: 0.055195, loss_mps: 0.238757, loss_cps: 0.416759
[13:58:58.427] iteration 22933: total_loss: 0.492349, loss_sup: 0.021557, loss_mps: 0.171876, loss_cps: 0.298916
[13:58:58.573] iteration 22934: total_loss: 0.639461, loss_sup: 0.161623, loss_mps: 0.164207, loss_cps: 0.313631
[13:58:58.720] iteration 22935: total_loss: 0.653899, loss_sup: 0.031587, loss_mps: 0.204616, loss_cps: 0.417695
[13:58:58.867] iteration 22936: total_loss: 0.451651, loss_sup: 0.021449, loss_mps: 0.143451, loss_cps: 0.286750
[13:58:59.013] iteration 22937: total_loss: 0.959006, loss_sup: 0.178567, loss_mps: 0.237611, loss_cps: 0.542828
[13:58:59.159] iteration 22938: total_loss: 0.751723, loss_sup: 0.085166, loss_mps: 0.226163, loss_cps: 0.440394
[13:58:59.304] iteration 22939: total_loss: 0.502056, loss_sup: 0.027112, loss_mps: 0.161845, loss_cps: 0.313099
[13:58:59.454] iteration 22940: total_loss: 0.665670, loss_sup: 0.039550, loss_mps: 0.218605, loss_cps: 0.407514
[13:58:59.600] iteration 22941: total_loss: 0.737301, loss_sup: 0.082549, loss_mps: 0.217837, loss_cps: 0.436914
[13:58:59.747] iteration 22942: total_loss: 0.366067, loss_sup: 0.062939, loss_mps: 0.110486, loss_cps: 0.192642
[13:58:59.893] iteration 22943: total_loss: 0.522150, loss_sup: 0.044189, loss_mps: 0.160163, loss_cps: 0.317797
[13:59:00.039] iteration 22944: total_loss: 0.530323, loss_sup: 0.071701, loss_mps: 0.154413, loss_cps: 0.304209
[13:59:00.185] iteration 22945: total_loss: 0.502648, loss_sup: 0.019292, loss_mps: 0.160534, loss_cps: 0.322822
[13:59:00.332] iteration 22946: total_loss: 0.635050, loss_sup: 0.142781, loss_mps: 0.164157, loss_cps: 0.328112
[13:59:00.477] iteration 22947: total_loss: 0.302542, loss_sup: 0.049784, loss_mps: 0.094409, loss_cps: 0.158350
[13:59:00.623] iteration 22948: total_loss: 0.356615, loss_sup: 0.060464, loss_mps: 0.107247, loss_cps: 0.188904
[13:59:00.768] iteration 22949: total_loss: 0.611798, loss_sup: 0.148184, loss_mps: 0.155748, loss_cps: 0.307867
[13:59:00.914] iteration 22950: total_loss: 0.470457, loss_sup: 0.045262, loss_mps: 0.149096, loss_cps: 0.276099
[13:59:01.060] iteration 22951: total_loss: 0.950621, loss_sup: 0.140570, loss_mps: 0.246637, loss_cps: 0.563414
[13:59:01.207] iteration 22952: total_loss: 0.499611, loss_sup: 0.067766, loss_mps: 0.144741, loss_cps: 0.287104
[13:59:01.352] iteration 22953: total_loss: 0.828963, loss_sup: 0.174382, loss_mps: 0.217865, loss_cps: 0.436716
[13:59:01.498] iteration 22954: total_loss: 0.337900, loss_sup: 0.036703, loss_mps: 0.112415, loss_cps: 0.188782
[13:59:01.645] iteration 22955: total_loss: 0.498731, loss_sup: 0.159530, loss_mps: 0.118691, loss_cps: 0.220511
[13:59:01.792] iteration 22956: total_loss: 0.396024, loss_sup: 0.042510, loss_mps: 0.121057, loss_cps: 0.232456
[13:59:01.938] iteration 22957: total_loss: 0.417201, loss_sup: 0.062034, loss_mps: 0.127489, loss_cps: 0.227677
[13:59:02.083] iteration 22958: total_loss: 0.340663, loss_sup: 0.017910, loss_mps: 0.118097, loss_cps: 0.204656
[13:59:02.229] iteration 22959: total_loss: 0.851169, loss_sup: 0.089219, loss_mps: 0.236128, loss_cps: 0.525823
[13:59:02.377] iteration 22960: total_loss: 0.706055, loss_sup: 0.072005, loss_mps: 0.212792, loss_cps: 0.421258
[13:59:02.523] iteration 22961: total_loss: 0.334999, loss_sup: 0.022143, loss_mps: 0.115969, loss_cps: 0.196887
[13:59:02.670] iteration 22962: total_loss: 0.474619, loss_sup: 0.059295, loss_mps: 0.144409, loss_cps: 0.270916
[13:59:02.816] iteration 22963: total_loss: 0.417033, loss_sup: 0.121858, loss_mps: 0.109276, loss_cps: 0.185900
[13:59:02.961] iteration 22964: total_loss: 0.652883, loss_sup: 0.237844, loss_mps: 0.150301, loss_cps: 0.264738
[13:59:03.107] iteration 22965: total_loss: 0.363214, loss_sup: 0.028186, loss_mps: 0.123114, loss_cps: 0.211914
[13:59:03.253] iteration 22966: total_loss: 0.277710, loss_sup: 0.007751, loss_mps: 0.100616, loss_cps: 0.169342
[13:59:03.398] iteration 22967: total_loss: 0.326212, loss_sup: 0.005563, loss_mps: 0.115282, loss_cps: 0.205367
[13:59:03.544] iteration 22968: total_loss: 0.503853, loss_sup: 0.019471, loss_mps: 0.161682, loss_cps: 0.322700
[13:59:03.694] iteration 22969: total_loss: 0.374725, loss_sup: 0.041852, loss_mps: 0.124588, loss_cps: 0.208285
[13:59:03.840] iteration 22970: total_loss: 0.434673, loss_sup: 0.114823, loss_mps: 0.114791, loss_cps: 0.205059
[13:59:03.986] iteration 22971: total_loss: 0.376893, loss_sup: 0.014030, loss_mps: 0.127849, loss_cps: 0.235014
[13:59:04.133] iteration 22972: total_loss: 0.971419, loss_sup: 0.020835, loss_mps: 0.304453, loss_cps: 0.646131
[13:59:04.279] iteration 22973: total_loss: 0.318035, loss_sup: 0.010908, loss_mps: 0.116922, loss_cps: 0.190205
[13:59:04.424] iteration 22974: total_loss: 0.244232, loss_sup: 0.008512, loss_mps: 0.091229, loss_cps: 0.144491
[13:59:04.572] iteration 22975: total_loss: 0.508560, loss_sup: 0.085080, loss_mps: 0.145087, loss_cps: 0.278393
[13:59:04.719] iteration 22976: total_loss: 0.323753, loss_sup: 0.032106, loss_mps: 0.106446, loss_cps: 0.185200
[13:59:04.866] iteration 22977: total_loss: 0.466781, loss_sup: 0.052817, loss_mps: 0.144510, loss_cps: 0.269454
[13:59:05.012] iteration 22978: total_loss: 0.967725, loss_sup: 0.090478, loss_mps: 0.278538, loss_cps: 0.598708
[13:59:05.158] iteration 22979: total_loss: 0.838054, loss_sup: 0.097303, loss_mps: 0.228364, loss_cps: 0.512387
[13:59:05.304] iteration 22980: total_loss: 0.514106, loss_sup: 0.006071, loss_mps: 0.151460, loss_cps: 0.356575
[13:59:05.450] iteration 22981: total_loss: 0.484102, loss_sup: 0.065649, loss_mps: 0.143154, loss_cps: 0.275299
[13:59:05.596] iteration 22982: total_loss: 0.470466, loss_sup: 0.025025, loss_mps: 0.157171, loss_cps: 0.288270
[13:59:05.744] iteration 22983: total_loss: 0.308899, loss_sup: 0.024333, loss_mps: 0.105180, loss_cps: 0.179386
[13:59:05.890] iteration 22984: total_loss: 0.259316, loss_sup: 0.052805, loss_mps: 0.080465, loss_cps: 0.126047
[13:59:06.036] iteration 22985: total_loss: 0.628354, loss_sup: 0.010123, loss_mps: 0.203124, loss_cps: 0.415107
[13:59:06.181] iteration 22986: total_loss: 0.384178, loss_sup: 0.027011, loss_mps: 0.125731, loss_cps: 0.231436
[13:59:06.328] iteration 22987: total_loss: 0.303523, loss_sup: 0.011896, loss_mps: 0.106844, loss_cps: 0.184783
[13:59:06.474] iteration 22988: total_loss: 0.593083, loss_sup: 0.015618, loss_mps: 0.190779, loss_cps: 0.386687
[13:59:06.620] iteration 22989: total_loss: 0.553151, loss_sup: 0.148205, loss_mps: 0.139620, loss_cps: 0.265325
[13:59:06.683] iteration 22990: total_loss: 0.392334, loss_sup: 0.066089, loss_mps: 0.111037, loss_cps: 0.215208
[13:59:07.877] iteration 22991: total_loss: 0.514999, loss_sup: 0.071909, loss_mps: 0.164829, loss_cps: 0.278261
[13:59:08.025] iteration 22992: total_loss: 0.799373, loss_sup: 0.180883, loss_mps: 0.197703, loss_cps: 0.420787
[13:59:08.172] iteration 22993: total_loss: 0.460864, loss_sup: 0.052528, loss_mps: 0.136784, loss_cps: 0.271553
[13:59:08.318] iteration 22994: total_loss: 0.924583, loss_sup: 0.078677, loss_mps: 0.259203, loss_cps: 0.586703
[13:59:08.467] iteration 22995: total_loss: 0.433321, loss_sup: 0.012924, loss_mps: 0.142531, loss_cps: 0.277865
[13:59:08.613] iteration 22996: total_loss: 0.432937, loss_sup: 0.077446, loss_mps: 0.125429, loss_cps: 0.230062
[13:59:08.759] iteration 22997: total_loss: 0.455538, loss_sup: 0.036294, loss_mps: 0.138628, loss_cps: 0.280616
[13:59:08.906] iteration 22998: total_loss: 0.562753, loss_sup: 0.186023, loss_mps: 0.131551, loss_cps: 0.245179
[13:59:09.052] iteration 22999: total_loss: 0.456938, loss_sup: 0.095287, loss_mps: 0.126179, loss_cps: 0.235472
[13:59:09.201] iteration 23000: total_loss: 0.444576, loss_sup: 0.021565, loss_mps: 0.143269, loss_cps: 0.279742
[13:59:09.201] Evaluation Started ==>
[13:59:20.567] ==> valid iteration 23000: unet metrics: {'dc': 0.6271476430793803, 'jc': 0.5124329334286125, 'pre': 0.8009452179400447, 'hd': 5.3593988744828165}, ynet metrics: {'dc': 0.5665258390315814, 'jc': 0.45724205603144125, 'pre': 0.7908717232134721, 'hd': 5.341076505050462}.
[13:59:20.569] Evaluation Finished!⏹️
[13:59:20.718] iteration 23001: total_loss: 0.941289, loss_sup: 0.131097, loss_mps: 0.252971, loss_cps: 0.557221
[13:59:20.866] iteration 23002: total_loss: 0.588382, loss_sup: 0.043363, loss_mps: 0.176966, loss_cps: 0.368053
[13:59:21.011] iteration 23003: total_loss: 0.432242, loss_sup: 0.037334, loss_mps: 0.130730, loss_cps: 0.264179
[13:59:21.160] iteration 23004: total_loss: 0.705262, loss_sup: 0.130183, loss_mps: 0.196062, loss_cps: 0.379016
[13:59:21.308] iteration 23005: total_loss: 1.213282, loss_sup: 0.098458, loss_mps: 0.337808, loss_cps: 0.777016
[13:59:21.454] iteration 23006: total_loss: 0.414143, loss_sup: 0.030054, loss_mps: 0.126405, loss_cps: 0.257683
[13:59:21.600] iteration 23007: total_loss: 0.490572, loss_sup: 0.107507, loss_mps: 0.137829, loss_cps: 0.245236
[13:59:21.746] iteration 23008: total_loss: 0.382529, loss_sup: 0.032431, loss_mps: 0.122015, loss_cps: 0.228082
[13:59:21.891] iteration 23009: total_loss: 0.480799, loss_sup: 0.103475, loss_mps: 0.133236, loss_cps: 0.244088
[13:59:22.037] iteration 23010: total_loss: 0.727828, loss_sup: 0.097646, loss_mps: 0.198662, loss_cps: 0.431519
[13:59:22.183] iteration 23011: total_loss: 0.404490, loss_sup: 0.118401, loss_mps: 0.109859, loss_cps: 0.176230
[13:59:22.338] iteration 23012: total_loss: 0.560359, loss_sup: 0.154101, loss_mps: 0.139451, loss_cps: 0.266807
[13:59:22.489] iteration 23013: total_loss: 0.346780, loss_sup: 0.021255, loss_mps: 0.116798, loss_cps: 0.208726
[13:59:22.636] iteration 23014: total_loss: 0.432260, loss_sup: 0.014094, loss_mps: 0.138007, loss_cps: 0.280158
[13:59:22.782] iteration 23015: total_loss: 0.391735, loss_sup: 0.032249, loss_mps: 0.123574, loss_cps: 0.235911
[13:59:22.928] iteration 23016: total_loss: 0.826693, loss_sup: 0.134681, loss_mps: 0.217060, loss_cps: 0.474953
[13:59:23.074] iteration 23017: total_loss: 0.257688, loss_sup: 0.011807, loss_mps: 0.092001, loss_cps: 0.153879
[13:59:23.225] iteration 23018: total_loss: 0.431895, loss_sup: 0.085019, loss_mps: 0.128923, loss_cps: 0.217953
[13:59:23.371] iteration 23019: total_loss: 0.622511, loss_sup: 0.092813, loss_mps: 0.174322, loss_cps: 0.355377
[13:59:23.516] iteration 23020: total_loss: 0.713347, loss_sup: 0.067450, loss_mps: 0.203637, loss_cps: 0.442259
[13:59:23.663] iteration 23021: total_loss: 0.576324, loss_sup: 0.083945, loss_mps: 0.167547, loss_cps: 0.324832
[13:59:23.809] iteration 23022: total_loss: 0.572765, loss_sup: 0.252544, loss_mps: 0.107470, loss_cps: 0.212751
[13:59:23.957] iteration 23023: total_loss: 0.557746, loss_sup: 0.107902, loss_mps: 0.154464, loss_cps: 0.295379
[13:59:24.103] iteration 23024: total_loss: 0.497602, loss_sup: 0.032309, loss_mps: 0.169322, loss_cps: 0.295971
[13:59:24.251] iteration 23025: total_loss: 0.601226, loss_sup: 0.099183, loss_mps: 0.170399, loss_cps: 0.331645
[13:59:24.399] iteration 23026: total_loss: 0.499268, loss_sup: 0.072547, loss_mps: 0.145249, loss_cps: 0.281472
[13:59:24.545] iteration 23027: total_loss: 0.589334, loss_sup: 0.125576, loss_mps: 0.156960, loss_cps: 0.306798
[13:59:24.691] iteration 23028: total_loss: 0.747836, loss_sup: 0.214719, loss_mps: 0.171959, loss_cps: 0.361158
[13:59:24.837] iteration 23029: total_loss: 0.576105, loss_sup: 0.030984, loss_mps: 0.181160, loss_cps: 0.363961
[13:59:24.982] iteration 23030: total_loss: 0.624412, loss_sup: 0.126768, loss_mps: 0.177886, loss_cps: 0.319758
[13:59:25.129] iteration 23031: total_loss: 0.433796, loss_sup: 0.015932, loss_mps: 0.148060, loss_cps: 0.269803
[13:59:25.279] iteration 23032: total_loss: 0.250643, loss_sup: 0.059723, loss_mps: 0.076271, loss_cps: 0.114650
[13:59:25.426] iteration 23033: total_loss: 0.289213, loss_sup: 0.019630, loss_mps: 0.098347, loss_cps: 0.171235
[13:59:25.571] iteration 23034: total_loss: 0.542145, loss_sup: 0.051059, loss_mps: 0.172076, loss_cps: 0.319010
[13:59:25.717] iteration 23035: total_loss: 0.526980, loss_sup: 0.104764, loss_mps: 0.149466, loss_cps: 0.272750
[13:59:25.866] iteration 23036: total_loss: 0.367236, loss_sup: 0.041335, loss_mps: 0.121513, loss_cps: 0.204388
[13:59:26.013] iteration 23037: total_loss: 0.331713, loss_sup: 0.020420, loss_mps: 0.110643, loss_cps: 0.200650
[13:59:26.159] iteration 23038: total_loss: 0.610382, loss_sup: 0.193090, loss_mps: 0.143564, loss_cps: 0.273728
[13:59:26.305] iteration 23039: total_loss: 0.492234, loss_sup: 0.013869, loss_mps: 0.156613, loss_cps: 0.321753
[13:59:26.451] iteration 23040: total_loss: 0.864530, loss_sup: 0.480617, loss_mps: 0.138196, loss_cps: 0.245717
[13:59:26.597] iteration 23041: total_loss: 0.360916, loss_sup: 0.049788, loss_mps: 0.111653, loss_cps: 0.199476
[13:59:26.742] iteration 23042: total_loss: 0.381153, loss_sup: 0.053579, loss_mps: 0.116727, loss_cps: 0.210847
[13:59:26.888] iteration 23043: total_loss: 0.416578, loss_sup: 0.033568, loss_mps: 0.133418, loss_cps: 0.249592
[13:59:27.035] iteration 23044: total_loss: 0.561339, loss_sup: 0.129107, loss_mps: 0.154734, loss_cps: 0.277498
[13:59:27.181] iteration 23045: total_loss: 0.412392, loss_sup: 0.105189, loss_mps: 0.108897, loss_cps: 0.198307
[13:59:27.330] iteration 23046: total_loss: 0.303486, loss_sup: 0.042375, loss_mps: 0.096859, loss_cps: 0.164252
[13:59:27.476] iteration 23047: total_loss: 0.306193, loss_sup: 0.015632, loss_mps: 0.110918, loss_cps: 0.179642
[13:59:27.626] iteration 23048: total_loss: 0.644498, loss_sup: 0.010166, loss_mps: 0.210815, loss_cps: 0.423516
[13:59:27.772] iteration 23049: total_loss: 0.304210, loss_sup: 0.089206, loss_mps: 0.082805, loss_cps: 0.132198
[13:59:27.918] iteration 23050: total_loss: 0.356087, loss_sup: 0.032902, loss_mps: 0.117627, loss_cps: 0.205558
[13:59:28.064] iteration 23051: total_loss: 0.516539, loss_sup: 0.045496, loss_mps: 0.162366, loss_cps: 0.308677
[13:59:28.218] iteration 23052: total_loss: 0.339755, loss_sup: 0.081785, loss_mps: 0.094494, loss_cps: 0.163476
[13:59:28.364] iteration 23053: total_loss: 0.481521, loss_sup: 0.011787, loss_mps: 0.162717, loss_cps: 0.307017
[13:59:28.511] iteration 23054: total_loss: 0.289832, loss_sup: 0.023181, loss_mps: 0.098042, loss_cps: 0.168610
[13:59:28.659] iteration 23055: total_loss: 0.560322, loss_sup: 0.141361, loss_mps: 0.138828, loss_cps: 0.280133
[13:59:28.805] iteration 23056: total_loss: 0.747918, loss_sup: 0.094792, loss_mps: 0.214836, loss_cps: 0.438290
[13:59:28.951] iteration 23057: total_loss: 0.873843, loss_sup: 0.390513, loss_mps: 0.163526, loss_cps: 0.319804
[13:59:29.097] iteration 23058: total_loss: 0.758197, loss_sup: 0.053301, loss_mps: 0.235960, loss_cps: 0.468935
[13:59:29.244] iteration 23059: total_loss: 0.686209, loss_sup: 0.120077, loss_mps: 0.195530, loss_cps: 0.370602
[13:59:29.390] iteration 23060: total_loss: 0.642561, loss_sup: 0.189422, loss_mps: 0.149267, loss_cps: 0.303871
[13:59:29.536] iteration 23061: total_loss: 0.467670, loss_sup: 0.050365, loss_mps: 0.137172, loss_cps: 0.280133
[13:59:29.682] iteration 23062: total_loss: 0.700370, loss_sup: 0.298065, loss_mps: 0.146723, loss_cps: 0.255583
[13:59:29.829] iteration 23063: total_loss: 0.468106, loss_sup: 0.186265, loss_mps: 0.100795, loss_cps: 0.181046
[13:59:29.976] iteration 23064: total_loss: 0.341656, loss_sup: 0.020693, loss_mps: 0.106547, loss_cps: 0.214416
[13:59:30.122] iteration 23065: total_loss: 0.557453, loss_sup: 0.192115, loss_mps: 0.129759, loss_cps: 0.235578
[13:59:30.269] iteration 23066: total_loss: 0.427936, loss_sup: 0.027614, loss_mps: 0.140066, loss_cps: 0.260256
[13:59:30.415] iteration 23067: total_loss: 0.517370, loss_sup: 0.019932, loss_mps: 0.165874, loss_cps: 0.331565
[13:59:30.561] iteration 23068: total_loss: 0.344953, loss_sup: 0.007784, loss_mps: 0.122300, loss_cps: 0.214868
[13:59:30.706] iteration 23069: total_loss: 0.500786, loss_sup: 0.143728, loss_mps: 0.132836, loss_cps: 0.224222
[13:59:30.852] iteration 23070: total_loss: 0.475495, loss_sup: 0.080519, loss_mps: 0.142740, loss_cps: 0.252235
[13:59:30.998] iteration 23071: total_loss: 0.587299, loss_sup: 0.295072, loss_mps: 0.103507, loss_cps: 0.188719
[13:59:31.144] iteration 23072: total_loss: 0.672952, loss_sup: 0.040961, loss_mps: 0.202528, loss_cps: 0.429463
[13:59:31.290] iteration 23073: total_loss: 0.425810, loss_sup: 0.018300, loss_mps: 0.149286, loss_cps: 0.258225
[13:59:31.438] iteration 23074: total_loss: 0.474912, loss_sup: 0.031702, loss_mps: 0.148928, loss_cps: 0.294282
[13:59:31.586] iteration 23075: total_loss: 0.587133, loss_sup: 0.142113, loss_mps: 0.153650, loss_cps: 0.291370
[13:59:31.735] iteration 23076: total_loss: 0.365022, loss_sup: 0.023134, loss_mps: 0.118450, loss_cps: 0.223437
[13:59:31.882] iteration 23077: total_loss: 0.432323, loss_sup: 0.017727, loss_mps: 0.139474, loss_cps: 0.275123
[13:59:32.029] iteration 23078: total_loss: 0.391279, loss_sup: 0.059838, loss_mps: 0.111784, loss_cps: 0.219658
[13:59:32.175] iteration 23079: total_loss: 0.541126, loss_sup: 0.089119, loss_mps: 0.152337, loss_cps: 0.299670
[13:59:32.322] iteration 23080: total_loss: 0.329156, loss_sup: 0.022171, loss_mps: 0.110387, loss_cps: 0.196599
[13:59:32.470] iteration 23081: total_loss: 0.814176, loss_sup: 0.086941, loss_mps: 0.220967, loss_cps: 0.506267
[13:59:32.617] iteration 23082: total_loss: 0.280342, loss_sup: 0.049172, loss_mps: 0.086272, loss_cps: 0.144898
[13:59:32.766] iteration 23083: total_loss: 0.354993, loss_sup: 0.026403, loss_mps: 0.115024, loss_cps: 0.213566
[13:59:32.912] iteration 23084: total_loss: 0.373935, loss_sup: 0.070890, loss_mps: 0.110173, loss_cps: 0.192872
[13:59:33.062] iteration 23085: total_loss: 0.598351, loss_sup: 0.188879, loss_mps: 0.139440, loss_cps: 0.270031
[13:59:33.209] iteration 23086: total_loss: 0.378220, loss_sup: 0.027665, loss_mps: 0.123049, loss_cps: 0.227506
[13:59:33.355] iteration 23087: total_loss: 0.479666, loss_sup: 0.140536, loss_mps: 0.125874, loss_cps: 0.213256
[13:59:33.501] iteration 23088: total_loss: 0.488399, loss_sup: 0.104664, loss_mps: 0.135930, loss_cps: 0.247805
[13:59:33.647] iteration 23089: total_loss: 0.350550, loss_sup: 0.057980, loss_mps: 0.109389, loss_cps: 0.183181
[13:59:33.793] iteration 23090: total_loss: 0.709040, loss_sup: 0.115527, loss_mps: 0.191893, loss_cps: 0.401620
[13:59:33.939] iteration 23091: total_loss: 0.711295, loss_sup: 0.014731, loss_mps: 0.229592, loss_cps: 0.466972
[13:59:34.085] iteration 23092: total_loss: 0.267862, loss_sup: 0.012968, loss_mps: 0.095550, loss_cps: 0.159344
[13:59:34.231] iteration 23093: total_loss: 0.270857, loss_sup: 0.001927, loss_mps: 0.094200, loss_cps: 0.174731
[13:59:34.376] iteration 23094: total_loss: 0.449005, loss_sup: 0.042525, loss_mps: 0.140961, loss_cps: 0.265519
[13:59:34.522] iteration 23095: total_loss: 0.400497, loss_sup: 0.011421, loss_mps: 0.131734, loss_cps: 0.257342
[13:59:34.669] iteration 23096: total_loss: 0.367218, loss_sup: 0.030917, loss_mps: 0.121963, loss_cps: 0.214338
[13:59:34.815] iteration 23097: total_loss: 0.445888, loss_sup: 0.086124, loss_mps: 0.130052, loss_cps: 0.229712
[13:59:34.961] iteration 23098: total_loss: 0.469158, loss_sup: 0.065275, loss_mps: 0.134917, loss_cps: 0.268967
[13:59:35.109] iteration 23099: total_loss: 0.284020, loss_sup: 0.011370, loss_mps: 0.100963, loss_cps: 0.171687
[13:59:35.255] iteration 23100: total_loss: 0.596475, loss_sup: 0.107331, loss_mps: 0.167913, loss_cps: 0.321231
[13:59:35.255] Evaluation Started ==>
[13:59:46.633] ==> valid iteration 23100: unet metrics: {'dc': 0.6441010292045591, 'jc': 0.5280642307070226, 'pre': 0.8014718710897466, 'hd': 5.491028576035818}, ynet metrics: {'dc': 0.6070733073323971, 'jc': 0.49314635541062174, 'pre': 0.7985555280589837, 'hd': 5.482392240152535}.
[13:59:46.635] Evaluation Finished!⏹️
[13:59:46.792] iteration 23101: total_loss: 1.229014, loss_sup: 0.375839, loss_mps: 0.268501, loss_cps: 0.584674
[13:59:46.940] iteration 23102: total_loss: 0.933484, loss_sup: 0.190409, loss_mps: 0.243642, loss_cps: 0.499432
[13:59:47.085] iteration 23103: total_loss: 0.572805, loss_sup: 0.127234, loss_mps: 0.147928, loss_cps: 0.297643
[13:59:47.230] iteration 23104: total_loss: 0.426259, loss_sup: 0.060210, loss_mps: 0.131620, loss_cps: 0.234429
[13:59:47.378] iteration 23105: total_loss: 0.504198, loss_sup: 0.076329, loss_mps: 0.144171, loss_cps: 0.283699
[13:59:47.524] iteration 23106: total_loss: 0.379712, loss_sup: 0.021441, loss_mps: 0.121670, loss_cps: 0.236601
[13:59:47.669] iteration 23107: total_loss: 1.370918, loss_sup: 0.032993, loss_mps: 0.409492, loss_cps: 0.928433
[13:59:47.815] iteration 23108: total_loss: 0.233639, loss_sup: 0.020366, loss_mps: 0.080694, loss_cps: 0.132578
[13:59:47.961] iteration 23109: total_loss: 0.641382, loss_sup: 0.127987, loss_mps: 0.171306, loss_cps: 0.342088
[13:59:48.106] iteration 23110: total_loss: 0.541530, loss_sup: 0.054501, loss_mps: 0.166538, loss_cps: 0.320491
[13:59:48.255] iteration 23111: total_loss: 0.272224, loss_sup: 0.042288, loss_mps: 0.082654, loss_cps: 0.147282
[13:59:48.400] iteration 23112: total_loss: 0.445316, loss_sup: 0.054025, loss_mps: 0.134868, loss_cps: 0.256423
[13:59:48.546] iteration 23113: total_loss: 0.628209, loss_sup: 0.027360, loss_mps: 0.194485, loss_cps: 0.406364
[13:59:48.694] iteration 23114: total_loss: 0.347525, loss_sup: 0.007038, loss_mps: 0.126566, loss_cps: 0.213921
[13:59:48.840] iteration 23115: total_loss: 0.368921, loss_sup: 0.079147, loss_mps: 0.106267, loss_cps: 0.183507
[13:59:48.985] iteration 23116: total_loss: 0.502539, loss_sup: 0.115760, loss_mps: 0.140652, loss_cps: 0.246127
[13:59:49.131] iteration 23117: total_loss: 0.669714, loss_sup: 0.059786, loss_mps: 0.194377, loss_cps: 0.415551
[13:59:49.277] iteration 23118: total_loss: 0.521121, loss_sup: 0.029144, loss_mps: 0.168849, loss_cps: 0.323128
[13:59:49.423] iteration 23119: total_loss: 0.413491, loss_sup: 0.054379, loss_mps: 0.126266, loss_cps: 0.232846
[13:59:49.569] iteration 23120: total_loss: 0.566990, loss_sup: 0.051951, loss_mps: 0.170000, loss_cps: 0.345039
[13:59:49.716] iteration 23121: total_loss: 0.989829, loss_sup: 0.170381, loss_mps: 0.251223, loss_cps: 0.568225
[13:59:49.862] iteration 23122: total_loss: 0.236952, loss_sup: 0.011090, loss_mps: 0.087294, loss_cps: 0.138568
[13:59:50.007] iteration 23123: total_loss: 0.663482, loss_sup: 0.032646, loss_mps: 0.199809, loss_cps: 0.431027
[13:59:50.152] iteration 23124: total_loss: 0.288967, loss_sup: 0.016958, loss_mps: 0.103963, loss_cps: 0.168046
[13:59:50.300] iteration 23125: total_loss: 0.541038, loss_sup: 0.082473, loss_mps: 0.153144, loss_cps: 0.305421
[13:59:50.445] iteration 23126: total_loss: 0.923149, loss_sup: 0.088044, loss_mps: 0.274985, loss_cps: 0.560120
[13:59:50.591] iteration 23127: total_loss: 0.805977, loss_sup: 0.037316, loss_mps: 0.246067, loss_cps: 0.522594
[13:59:50.738] iteration 23128: total_loss: 0.525729, loss_sup: 0.036003, loss_mps: 0.171482, loss_cps: 0.318244
[13:59:50.884] iteration 23129: total_loss: 0.420369, loss_sup: 0.069105, loss_mps: 0.124305, loss_cps: 0.226960
[13:59:51.029] iteration 23130: total_loss: 1.054047, loss_sup: 0.104705, loss_mps: 0.303684, loss_cps: 0.645658
[13:59:51.175] iteration 23131: total_loss: 0.757776, loss_sup: 0.010894, loss_mps: 0.237147, loss_cps: 0.509735
[13:59:51.320] iteration 23132: total_loss: 0.769929, loss_sup: 0.019558, loss_mps: 0.232418, loss_cps: 0.517952
[13:59:51.466] iteration 23133: total_loss: 0.516790, loss_sup: 0.057866, loss_mps: 0.168801, loss_cps: 0.290123
[13:59:51.611] iteration 23134: total_loss: 0.764409, loss_sup: 0.015665, loss_mps: 0.242350, loss_cps: 0.506394
[13:59:51.758] iteration 23135: total_loss: 0.437525, loss_sup: 0.048043, loss_mps: 0.136786, loss_cps: 0.252696
[13:59:51.905] iteration 23136: total_loss: 0.441543, loss_sup: 0.016389, loss_mps: 0.142884, loss_cps: 0.282270
[13:59:52.050] iteration 23137: total_loss: 0.471770, loss_sup: 0.044168, loss_mps: 0.149977, loss_cps: 0.277625
[13:59:52.196] iteration 23138: total_loss: 0.321239, loss_sup: 0.031234, loss_mps: 0.109883, loss_cps: 0.180122
[13:59:52.342] iteration 23139: total_loss: 0.433468, loss_sup: 0.024026, loss_mps: 0.134664, loss_cps: 0.274779
[13:59:52.488] iteration 23140: total_loss: 0.376317, loss_sup: 0.035503, loss_mps: 0.125945, loss_cps: 0.214868
[13:59:52.633] iteration 23141: total_loss: 0.805457, loss_sup: 0.157698, loss_mps: 0.204091, loss_cps: 0.443668
[13:59:52.779] iteration 23142: total_loss: 0.424078, loss_sup: 0.058316, loss_mps: 0.126673, loss_cps: 0.239089
[13:59:52.925] iteration 23143: total_loss: 0.584855, loss_sup: 0.074281, loss_mps: 0.168140, loss_cps: 0.342435
[13:59:53.071] iteration 23144: total_loss: 0.596888, loss_sup: 0.140293, loss_mps: 0.157600, loss_cps: 0.298994
[13:59:53.217] iteration 23145: total_loss: 0.581801, loss_sup: 0.057846, loss_mps: 0.172185, loss_cps: 0.351769
[13:59:53.363] iteration 23146: total_loss: 0.260652, loss_sup: 0.034577, loss_mps: 0.085531, loss_cps: 0.140544
[13:59:53.510] iteration 23147: total_loss: 0.268953, loss_sup: 0.004224, loss_mps: 0.096836, loss_cps: 0.167892
[13:59:53.656] iteration 23148: total_loss: 0.572907, loss_sup: 0.026280, loss_mps: 0.186451, loss_cps: 0.360177
[13:59:53.806] iteration 23149: total_loss: 0.893067, loss_sup: 0.137005, loss_mps: 0.234815, loss_cps: 0.521247
[13:59:53.952] iteration 23150: total_loss: 0.358215, loss_sup: 0.015308, loss_mps: 0.121928, loss_cps: 0.220979
[13:59:54.099] iteration 23151: total_loss: 0.388233, loss_sup: 0.017343, loss_mps: 0.132373, loss_cps: 0.238517
[13:59:54.249] iteration 23152: total_loss: 0.357298, loss_sup: 0.074976, loss_mps: 0.099037, loss_cps: 0.183285
[13:59:54.395] iteration 23153: total_loss: 0.362973, loss_sup: 0.023258, loss_mps: 0.112415, loss_cps: 0.227300
[13:59:54.544] iteration 23154: total_loss: 0.527622, loss_sup: 0.031114, loss_mps: 0.173482, loss_cps: 0.323025
[13:59:54.693] iteration 23155: total_loss: 1.041981, loss_sup: 0.160923, loss_mps: 0.279809, loss_cps: 0.601249
[13:59:54.839] iteration 23156: total_loss: 0.702214, loss_sup: 0.196767, loss_mps: 0.179947, loss_cps: 0.325499
[13:59:54.988] iteration 23157: total_loss: 0.480822, loss_sup: 0.089314, loss_mps: 0.134059, loss_cps: 0.257449
[13:59:55.144] iteration 23158: total_loss: 0.348195, loss_sup: 0.026344, loss_mps: 0.114511, loss_cps: 0.207340
[13:59:55.292] iteration 23159: total_loss: 0.610424, loss_sup: 0.116745, loss_mps: 0.165477, loss_cps: 0.328202
[13:59:55.438] iteration 23160: total_loss: 0.579885, loss_sup: 0.035144, loss_mps: 0.181643, loss_cps: 0.363099
[13:59:55.584] iteration 23161: total_loss: 0.382439, loss_sup: 0.014913, loss_mps: 0.124925, loss_cps: 0.242602
[13:59:55.730] iteration 23162: total_loss: 0.832514, loss_sup: 0.348075, loss_mps: 0.161927, loss_cps: 0.322511
[13:59:55.877] iteration 23163: total_loss: 0.550478, loss_sup: 0.017113, loss_mps: 0.184327, loss_cps: 0.349038
[13:59:56.023] iteration 23164: total_loss: 0.327996, loss_sup: 0.029374, loss_mps: 0.105282, loss_cps: 0.193340
[13:59:56.169] iteration 23165: total_loss: 0.665157, loss_sup: 0.044071, loss_mps: 0.190847, loss_cps: 0.430239
[13:59:56.317] iteration 23166: total_loss: 0.692051, loss_sup: 0.046513, loss_mps: 0.199340, loss_cps: 0.446198
[13:59:56.467] iteration 23167: total_loss: 1.157813, loss_sup: 0.517882, loss_mps: 0.204443, loss_cps: 0.435487
[13:59:56.616] iteration 23168: total_loss: 0.302936, loss_sup: 0.060460, loss_mps: 0.090693, loss_cps: 0.151783
[13:59:56.762] iteration 23169: total_loss: 0.628328, loss_sup: 0.086287, loss_mps: 0.173091, loss_cps: 0.368949
[13:59:56.908] iteration 23170: total_loss: 0.419875, loss_sup: 0.149644, loss_mps: 0.105217, loss_cps: 0.165015
[13:59:57.055] iteration 23171: total_loss: 0.417396, loss_sup: 0.042350, loss_mps: 0.134815, loss_cps: 0.240232
[13:59:57.201] iteration 23172: total_loss: 0.720482, loss_sup: 0.144426, loss_mps: 0.200004, loss_cps: 0.376052
[13:59:57.347] iteration 23173: total_loss: 0.397750, loss_sup: 0.073121, loss_mps: 0.121208, loss_cps: 0.203421
[13:59:57.493] iteration 23174: total_loss: 0.683898, loss_sup: 0.085166, loss_mps: 0.200917, loss_cps: 0.397815
[13:59:57.640] iteration 23175: total_loss: 0.268918, loss_sup: 0.042204, loss_mps: 0.083945, loss_cps: 0.142769
[13:59:57.787] iteration 23176: total_loss: 0.608120, loss_sup: 0.073220, loss_mps: 0.175534, loss_cps: 0.359365
[13:59:57.933] iteration 23177: total_loss: 0.512916, loss_sup: 0.031746, loss_mps: 0.168399, loss_cps: 0.312771
[13:59:58.079] iteration 23178: total_loss: 0.983525, loss_sup: 0.114461, loss_mps: 0.267226, loss_cps: 0.601838
[13:59:58.225] iteration 23179: total_loss: 0.364219, loss_sup: 0.026042, loss_mps: 0.118457, loss_cps: 0.219720
[13:59:58.371] iteration 23180: total_loss: 0.557333, loss_sup: 0.103541, loss_mps: 0.149829, loss_cps: 0.303964
[13:59:58.518] iteration 23181: total_loss: 0.296643, loss_sup: 0.009547, loss_mps: 0.110283, loss_cps: 0.176813
[13:59:58.665] iteration 23182: total_loss: 0.698847, loss_sup: 0.128125, loss_mps: 0.189261, loss_cps: 0.381461
[13:59:58.810] iteration 23183: total_loss: 0.642313, loss_sup: 0.064291, loss_mps: 0.182079, loss_cps: 0.395943
[13:59:58.957] iteration 23184: total_loss: 0.512692, loss_sup: 0.058403, loss_mps: 0.165694, loss_cps: 0.288595
[13:59:59.104] iteration 23185: total_loss: 0.441227, loss_sup: 0.065058, loss_mps: 0.131060, loss_cps: 0.245109
[13:59:59.251] iteration 23186: total_loss: 0.460955, loss_sup: 0.053938, loss_mps: 0.137625, loss_cps: 0.269392
[13:59:59.400] iteration 23187: total_loss: 0.389777, loss_sup: 0.015840, loss_mps: 0.133696, loss_cps: 0.240241
[13:59:59.549] iteration 23188: total_loss: 0.446085, loss_sup: 0.079280, loss_mps: 0.126440, loss_cps: 0.240364
[13:59:59.696] iteration 23189: total_loss: 0.260930, loss_sup: 0.011511, loss_mps: 0.095893, loss_cps: 0.153526
[13:59:59.843] iteration 23190: total_loss: 0.564658, loss_sup: 0.031048, loss_mps: 0.176487, loss_cps: 0.357124
[13:59:59.989] iteration 23191: total_loss: 0.255029, loss_sup: 0.007617, loss_mps: 0.097389, loss_cps: 0.150023
[14:00:00.137] iteration 23192: total_loss: 0.357069, loss_sup: 0.011253, loss_mps: 0.127994, loss_cps: 0.217823
[14:00:00.283] iteration 23193: total_loss: 0.418337, loss_sup: 0.069306, loss_mps: 0.124303, loss_cps: 0.224728
[14:00:00.429] iteration 23194: total_loss: 0.280606, loss_sup: 0.005940, loss_mps: 0.098087, loss_cps: 0.176580
[14:00:00.575] iteration 23195: total_loss: 0.812992, loss_sup: 0.174602, loss_mps: 0.216551, loss_cps: 0.421839
[14:00:00.721] iteration 23196: total_loss: 0.912719, loss_sup: 0.118488, loss_mps: 0.243328, loss_cps: 0.550903
[14:00:00.868] iteration 23197: total_loss: 0.394959, loss_sup: 0.032363, loss_mps: 0.126156, loss_cps: 0.236440
[14:00:01.014] iteration 23198: total_loss: 0.491307, loss_sup: 0.039951, loss_mps: 0.145273, loss_cps: 0.306083
[14:00:01.160] iteration 23199: total_loss: 0.429421, loss_sup: 0.050013, loss_mps: 0.123106, loss_cps: 0.256303
[14:00:01.306] iteration 23200: total_loss: 0.403947, loss_sup: 0.092172, loss_mps: 0.114137, loss_cps: 0.197637
[14:00:01.306] Evaluation Started ==>
[14:00:12.678] ==> valid iteration 23200: unet metrics: {'dc': 0.6444973511953915, 'jc': 0.5303309812271302, 'pre': 0.7996393254358833, 'hd': 5.27973024311276}, ynet metrics: {'dc': 0.5851766144747675, 'jc': 0.4700009012232128, 'pre': 0.8124621729899244, 'hd': 5.483895007268765}.
[14:00:12.680] Evaluation Finished!⏹️
[14:00:12.833] iteration 23201: total_loss: 0.384039, loss_sup: 0.013586, loss_mps: 0.126629, loss_cps: 0.243824
[14:00:12.982] iteration 23202: total_loss: 0.477088, loss_sup: 0.109366, loss_mps: 0.126398, loss_cps: 0.241323
[14:00:13.128] iteration 23203: total_loss: 0.303001, loss_sup: 0.043485, loss_mps: 0.092784, loss_cps: 0.166732
[14:00:13.273] iteration 23204: total_loss: 0.505878, loss_sup: 0.211491, loss_mps: 0.102946, loss_cps: 0.191441
[14:00:13.418] iteration 23205: total_loss: 0.692594, loss_sup: 0.166404, loss_mps: 0.184266, loss_cps: 0.341924
[14:00:13.564] iteration 23206: total_loss: 0.421628, loss_sup: 0.033964, loss_mps: 0.129389, loss_cps: 0.258276
[14:00:13.710] iteration 23207: total_loss: 0.582586, loss_sup: 0.087446, loss_mps: 0.161817, loss_cps: 0.333323
[14:00:13.860] iteration 23208: total_loss: 0.338277, loss_sup: 0.010449, loss_mps: 0.117814, loss_cps: 0.210015
[14:00:14.006] iteration 23209: total_loss: 0.350695, loss_sup: 0.078645, loss_mps: 0.095846, loss_cps: 0.176205
[14:00:14.151] iteration 23210: total_loss: 0.475215, loss_sup: 0.038717, loss_mps: 0.150449, loss_cps: 0.286049
[14:00:14.296] iteration 23211: total_loss: 0.266637, loss_sup: 0.007295, loss_mps: 0.096958, loss_cps: 0.162383
[14:00:14.443] iteration 23212: total_loss: 0.326703, loss_sup: 0.003696, loss_mps: 0.108553, loss_cps: 0.214453
[14:00:14.589] iteration 23213: total_loss: 0.743013, loss_sup: 0.148035, loss_mps: 0.191304, loss_cps: 0.403674
[14:00:14.735] iteration 23214: total_loss: 0.439306, loss_sup: 0.016673, loss_mps: 0.136748, loss_cps: 0.285886
[14:00:14.880] iteration 23215: total_loss: 0.635233, loss_sup: 0.070001, loss_mps: 0.183577, loss_cps: 0.381655
[14:00:15.028] iteration 23216: total_loss: 0.568998, loss_sup: 0.069369, loss_mps: 0.170478, loss_cps: 0.329150
[14:00:15.174] iteration 23217: total_loss: 0.667511, loss_sup: 0.048930, loss_mps: 0.201991, loss_cps: 0.416590
[14:00:15.319] iteration 23218: total_loss: 0.338017, loss_sup: 0.005886, loss_mps: 0.117903, loss_cps: 0.214227
[14:00:15.467] iteration 23219: total_loss: 0.276284, loss_sup: 0.013124, loss_mps: 0.097766, loss_cps: 0.165395
[14:00:15.613] iteration 23220: total_loss: 0.359125, loss_sup: 0.031271, loss_mps: 0.117102, loss_cps: 0.210753
[14:00:15.758] iteration 23221: total_loss: 0.735168, loss_sup: 0.074017, loss_mps: 0.200048, loss_cps: 0.461102
[14:00:15.904] iteration 23222: total_loss: 0.512728, loss_sup: 0.061982, loss_mps: 0.156110, loss_cps: 0.294636
[14:00:16.051] iteration 23223: total_loss: 0.448250, loss_sup: 0.060162, loss_mps: 0.133813, loss_cps: 0.254274
[14:00:16.197] iteration 23224: total_loss: 0.672676, loss_sup: 0.062732, loss_mps: 0.205778, loss_cps: 0.404166
[14:00:16.343] iteration 23225: total_loss: 0.549297, loss_sup: 0.107732, loss_mps: 0.141481, loss_cps: 0.300084
[14:00:16.489] iteration 23226: total_loss: 0.368615, loss_sup: 0.009433, loss_mps: 0.131167, loss_cps: 0.228015
[14:00:16.635] iteration 23227: total_loss: 0.405210, loss_sup: 0.036747, loss_mps: 0.139659, loss_cps: 0.228804
[14:00:16.783] iteration 23228: total_loss: 0.742516, loss_sup: 0.127732, loss_mps: 0.210194, loss_cps: 0.404589
[14:00:16.929] iteration 23229: total_loss: 0.287435, loss_sup: 0.027051, loss_mps: 0.090601, loss_cps: 0.169783
[14:00:17.080] iteration 23230: total_loss: 0.332660, loss_sup: 0.039122, loss_mps: 0.110500, loss_cps: 0.183038
[14:00:17.227] iteration 23231: total_loss: 0.421145, loss_sup: 0.131207, loss_mps: 0.105811, loss_cps: 0.184127
[14:00:17.378] iteration 23232: total_loss: 0.466399, loss_sup: 0.038078, loss_mps: 0.141602, loss_cps: 0.286719
[14:00:17.524] iteration 23233: total_loss: 0.606291, loss_sup: 0.042862, loss_mps: 0.182383, loss_cps: 0.381047
[14:00:17.669] iteration 23234: total_loss: 0.644433, loss_sup: 0.151892, loss_mps: 0.165617, loss_cps: 0.326923
[14:00:17.817] iteration 23235: total_loss: 0.708139, loss_sup: 0.183086, loss_mps: 0.173771, loss_cps: 0.351282
[14:00:17.963] iteration 23236: total_loss: 0.414835, loss_sup: 0.017865, loss_mps: 0.129884, loss_cps: 0.267085
[14:00:18.109] iteration 23237: total_loss: 0.531749, loss_sup: 0.075459, loss_mps: 0.155999, loss_cps: 0.300291
[14:00:18.257] iteration 23238: total_loss: 1.119951, loss_sup: 0.081194, loss_mps: 0.333865, loss_cps: 0.704892
[14:00:18.403] iteration 23239: total_loss: 0.260516, loss_sup: 0.023811, loss_mps: 0.089222, loss_cps: 0.147484
[14:00:18.549] iteration 23240: total_loss: 0.357429, loss_sup: 0.054093, loss_mps: 0.110374, loss_cps: 0.192962
[14:00:18.695] iteration 23241: total_loss: 0.979742, loss_sup: 0.154524, loss_mps: 0.248890, loss_cps: 0.576328
[14:00:18.841] iteration 23242: total_loss: 0.282814, loss_sup: 0.004858, loss_mps: 0.100954, loss_cps: 0.177002
[14:00:18.987] iteration 23243: total_loss: 0.441893, loss_sup: 0.024781, loss_mps: 0.149143, loss_cps: 0.267969
[14:00:19.133] iteration 23244: total_loss: 0.816096, loss_sup: 0.103759, loss_mps: 0.225899, loss_cps: 0.486439
[14:00:19.278] iteration 23245: total_loss: 0.266587, loss_sup: 0.012417, loss_mps: 0.093726, loss_cps: 0.160444
[14:00:19.423] iteration 23246: total_loss: 0.786991, loss_sup: 0.058091, loss_mps: 0.229052, loss_cps: 0.499849
[14:00:19.569] iteration 23247: total_loss: 0.765304, loss_sup: 0.056704, loss_mps: 0.233691, loss_cps: 0.474910
[14:00:19.715] iteration 23248: total_loss: 0.377617, loss_sup: 0.013960, loss_mps: 0.124504, loss_cps: 0.239153
[14:00:19.861] iteration 23249: total_loss: 0.650320, loss_sup: 0.014967, loss_mps: 0.210687, loss_cps: 0.424666
[14:00:20.007] iteration 23250: total_loss: 0.938515, loss_sup: 0.093967, loss_mps: 0.266266, loss_cps: 0.578283
[14:00:20.153] iteration 23251: total_loss: 0.874860, loss_sup: 0.128561, loss_mps: 0.235232, loss_cps: 0.511068
[14:00:20.298] iteration 23252: total_loss: 0.549309, loss_sup: 0.064215, loss_mps: 0.162314, loss_cps: 0.322780
[14:00:20.444] iteration 23253: total_loss: 0.759962, loss_sup: 0.027199, loss_mps: 0.228657, loss_cps: 0.504107
[14:00:20.590] iteration 23254: total_loss: 0.389089, loss_sup: 0.009336, loss_mps: 0.130368, loss_cps: 0.249384
[14:00:20.735] iteration 23255: total_loss: 0.233079, loss_sup: 0.012727, loss_mps: 0.090134, loss_cps: 0.130219
[14:00:20.881] iteration 23256: total_loss: 0.736564, loss_sup: 0.052860, loss_mps: 0.215823, loss_cps: 0.467881
[14:00:21.029] iteration 23257: total_loss: 0.389506, loss_sup: 0.124792, loss_mps: 0.093783, loss_cps: 0.170932
[14:00:21.176] iteration 23258: total_loss: 0.797140, loss_sup: 0.404603, loss_mps: 0.130807, loss_cps: 0.261731
[14:00:21.322] iteration 23259: total_loss: 0.776680, loss_sup: 0.074832, loss_mps: 0.230502, loss_cps: 0.471346
[14:00:21.468] iteration 23260: total_loss: 0.502407, loss_sup: 0.052173, loss_mps: 0.153762, loss_cps: 0.296472
[14:00:21.618] iteration 23261: total_loss: 1.230138, loss_sup: 0.047857, loss_mps: 0.372770, loss_cps: 0.809510
[14:00:21.765] iteration 23262: total_loss: 0.618286, loss_sup: 0.102764, loss_mps: 0.165398, loss_cps: 0.350124
[14:00:21.915] iteration 23263: total_loss: 0.601585, loss_sup: 0.173164, loss_mps: 0.152722, loss_cps: 0.275699
[14:00:22.060] iteration 23264: total_loss: 0.418301, loss_sup: 0.043812, loss_mps: 0.124431, loss_cps: 0.250058
[14:00:22.207] iteration 23265: total_loss: 0.378378, loss_sup: 0.022293, loss_mps: 0.125830, loss_cps: 0.230256
[14:00:22.353] iteration 23266: total_loss: 0.399041, loss_sup: 0.067916, loss_mps: 0.119791, loss_cps: 0.211334
[14:00:22.498] iteration 23267: total_loss: 0.501730, loss_sup: 0.012786, loss_mps: 0.154567, loss_cps: 0.334377
[14:00:22.644] iteration 23268: total_loss: 0.688968, loss_sup: 0.151231, loss_mps: 0.192319, loss_cps: 0.345418
[14:00:22.790] iteration 23269: total_loss: 0.654766, loss_sup: 0.088000, loss_mps: 0.184305, loss_cps: 0.382462
[14:00:22.936] iteration 23270: total_loss: 0.683811, loss_sup: 0.155958, loss_mps: 0.173839, loss_cps: 0.354013
[14:00:23.081] iteration 23271: total_loss: 0.474716, loss_sup: 0.027097, loss_mps: 0.147177, loss_cps: 0.300442
[14:00:23.230] iteration 23272: total_loss: 0.636068, loss_sup: 0.141108, loss_mps: 0.176315, loss_cps: 0.318645
[14:00:23.375] iteration 23273: total_loss: 0.558420, loss_sup: 0.026446, loss_mps: 0.181077, loss_cps: 0.350897
[14:00:23.521] iteration 23274: total_loss: 0.281980, loss_sup: 0.020881, loss_mps: 0.097565, loss_cps: 0.163535
[14:00:23.668] iteration 23275: total_loss: 1.072590, loss_sup: 0.299705, loss_mps: 0.263285, loss_cps: 0.509600
[14:00:23.814] iteration 23276: total_loss: 0.618927, loss_sup: 0.051498, loss_mps: 0.187338, loss_cps: 0.380092
[14:00:23.960] iteration 23277: total_loss: 0.377359, loss_sup: 0.011750, loss_mps: 0.126533, loss_cps: 0.239076
[14:00:24.105] iteration 23278: total_loss: 0.740222, loss_sup: 0.098044, loss_mps: 0.215591, loss_cps: 0.426586
[14:00:24.252] iteration 23279: total_loss: 0.665080, loss_sup: 0.064550, loss_mps: 0.189439, loss_cps: 0.411092
[14:00:24.397] iteration 23280: total_loss: 0.466099, loss_sup: 0.017510, loss_mps: 0.146083, loss_cps: 0.302506
[14:00:24.544] iteration 23281: total_loss: 0.759775, loss_sup: 0.106720, loss_mps: 0.217140, loss_cps: 0.435914
[14:00:24.690] iteration 23282: total_loss: 0.473457, loss_sup: 0.008358, loss_mps: 0.155567, loss_cps: 0.309532
[14:00:24.836] iteration 23283: total_loss: 0.484467, loss_sup: 0.034425, loss_mps: 0.149481, loss_cps: 0.300561
[14:00:24.982] iteration 23284: total_loss: 0.523428, loss_sup: 0.150162, loss_mps: 0.132330, loss_cps: 0.240935
[14:00:25.128] iteration 23285: total_loss: 0.738583, loss_sup: 0.052330, loss_mps: 0.200991, loss_cps: 0.485263
[14:00:25.274] iteration 23286: total_loss: 0.633660, loss_sup: 0.109686, loss_mps: 0.186908, loss_cps: 0.337066
[14:00:25.420] iteration 23287: total_loss: 0.247229, loss_sup: 0.063368, loss_mps: 0.067168, loss_cps: 0.116694
[14:00:25.566] iteration 23288: total_loss: 0.453716, loss_sup: 0.083308, loss_mps: 0.134012, loss_cps: 0.236395
[14:00:25.716] iteration 23289: total_loss: 0.511743, loss_sup: 0.165423, loss_mps: 0.119029, loss_cps: 0.227291
[14:00:25.864] iteration 23290: total_loss: 0.674437, loss_sup: 0.035760, loss_mps: 0.207003, loss_cps: 0.431674
[14:00:26.010] iteration 23291: total_loss: 0.343436, loss_sup: 0.018926, loss_mps: 0.108632, loss_cps: 0.215877
[14:00:26.155] iteration 23292: total_loss: 0.641445, loss_sup: 0.066344, loss_mps: 0.186643, loss_cps: 0.388458
[14:00:26.301] iteration 23293: total_loss: 0.440725, loss_sup: 0.018066, loss_mps: 0.140424, loss_cps: 0.282235
[14:00:26.447] iteration 23294: total_loss: 0.437285, loss_sup: 0.047606, loss_mps: 0.140655, loss_cps: 0.249024
[14:00:26.595] iteration 23295: total_loss: 0.590307, loss_sup: 0.017906, loss_mps: 0.188926, loss_cps: 0.383475
[14:00:26.741] iteration 23296: total_loss: 0.434836, loss_sup: 0.063339, loss_mps: 0.132060, loss_cps: 0.239437
[14:00:26.887] iteration 23297: total_loss: 0.834313, loss_sup: 0.228784, loss_mps: 0.207921, loss_cps: 0.397608
[14:00:27.034] iteration 23298: total_loss: 1.074782, loss_sup: 0.326660, loss_mps: 0.258446, loss_cps: 0.489676
[14:00:27.180] iteration 23299: total_loss: 0.478187, loss_sup: 0.161614, loss_mps: 0.114446, loss_cps: 0.202127
[14:00:27.325] iteration 23300: total_loss: 0.352912, loss_sup: 0.004473, loss_mps: 0.124908, loss_cps: 0.223531
[14:00:27.325] Evaluation Started ==>
[14:00:38.693] ==> valid iteration 23300: unet metrics: {'dc': 0.6297234316144436, 'jc': 0.5157298462745268, 'pre': 0.7750186822870283, 'hd': 5.409735283393166}, ynet metrics: {'dc': 0.5968157416781386, 'jc': 0.48428501961072146, 'pre': 0.8085603739574728, 'hd': 5.553406337748594}.
[14:00:38.695] Evaluation Finished!⏹️
[14:00:38.849] iteration 23301: total_loss: 0.274920, loss_sup: 0.024681, loss_mps: 0.094817, loss_cps: 0.155422
[14:00:38.999] iteration 23302: total_loss: 0.368576, loss_sup: 0.022205, loss_mps: 0.120364, loss_cps: 0.226006
[14:00:39.144] iteration 23303: total_loss: 0.819444, loss_sup: 0.253840, loss_mps: 0.195909, loss_cps: 0.369695
[14:00:39.289] iteration 23304: total_loss: 0.824156, loss_sup: 0.084746, loss_mps: 0.229779, loss_cps: 0.509630
[14:00:39.435] iteration 23305: total_loss: 0.416903, loss_sup: 0.045097, loss_mps: 0.128966, loss_cps: 0.242839
[14:00:39.581] iteration 23306: total_loss: 0.377892, loss_sup: 0.071891, loss_mps: 0.110795, loss_cps: 0.195206
[14:00:39.729] iteration 23307: total_loss: 0.831120, loss_sup: 0.174646, loss_mps: 0.224841, loss_cps: 0.431632
[14:00:39.875] iteration 23308: total_loss: 0.479050, loss_sup: 0.110958, loss_mps: 0.131130, loss_cps: 0.236962
[14:00:40.022] iteration 23309: total_loss: 0.476395, loss_sup: 0.031582, loss_mps: 0.166294, loss_cps: 0.278519
[14:00:40.169] iteration 23310: total_loss: 0.652017, loss_sup: 0.018638, loss_mps: 0.209934, loss_cps: 0.423446
[14:00:40.314] iteration 23311: total_loss: 0.442780, loss_sup: 0.042778, loss_mps: 0.145041, loss_cps: 0.254961
[14:00:40.460] iteration 23312: total_loss: 0.403043, loss_sup: 0.043049, loss_mps: 0.137154, loss_cps: 0.222840
[14:00:40.606] iteration 23313: total_loss: 0.697956, loss_sup: 0.227387, loss_mps: 0.165992, loss_cps: 0.304577
[14:00:40.752] iteration 23314: total_loss: 0.729793, loss_sup: 0.012215, loss_mps: 0.219967, loss_cps: 0.497612
[14:00:40.898] iteration 23315: total_loss: 0.532420, loss_sup: 0.060283, loss_mps: 0.164605, loss_cps: 0.307532
[14:00:41.046] iteration 23316: total_loss: 0.427838, loss_sup: 0.082578, loss_mps: 0.120503, loss_cps: 0.224758
[14:00:41.192] iteration 23317: total_loss: 0.566181, loss_sup: 0.067952, loss_mps: 0.162209, loss_cps: 0.336020
[14:00:41.340] iteration 23318: total_loss: 0.462382, loss_sup: 0.081617, loss_mps: 0.135918, loss_cps: 0.244846
[14:00:41.485] iteration 23319: total_loss: 0.609992, loss_sup: 0.103124, loss_mps: 0.172483, loss_cps: 0.334384
[14:00:41.631] iteration 23320: total_loss: 0.542097, loss_sup: 0.045287, loss_mps: 0.168283, loss_cps: 0.328527
[14:00:41.776] iteration 23321: total_loss: 0.430474, loss_sup: 0.025859, loss_mps: 0.128446, loss_cps: 0.276169
[14:00:41.922] iteration 23322: total_loss: 0.325843, loss_sup: 0.024867, loss_mps: 0.104577, loss_cps: 0.196398
[14:00:42.068] iteration 23323: total_loss: 0.450495, loss_sup: 0.039019, loss_mps: 0.152463, loss_cps: 0.259013
[14:00:42.213] iteration 23324: total_loss: 0.844694, loss_sup: 0.167023, loss_mps: 0.225257, loss_cps: 0.452415
[14:00:42.360] iteration 23325: total_loss: 0.647672, loss_sup: 0.030113, loss_mps: 0.206572, loss_cps: 0.410987
[14:00:42.507] iteration 23326: total_loss: 0.921203, loss_sup: 0.050798, loss_mps: 0.276393, loss_cps: 0.594012
[14:00:42.653] iteration 23327: total_loss: 0.311157, loss_sup: 0.070009, loss_mps: 0.092492, loss_cps: 0.148656
[14:00:42.801] iteration 23328: total_loss: 0.213043, loss_sup: 0.009183, loss_mps: 0.077071, loss_cps: 0.126789
[14:00:42.946] iteration 23329: total_loss: 0.741780, loss_sup: 0.204234, loss_mps: 0.184012, loss_cps: 0.353534
[14:00:43.093] iteration 23330: total_loss: 0.336956, loss_sup: 0.010366, loss_mps: 0.120333, loss_cps: 0.206257
[14:00:43.239] iteration 23331: total_loss: 0.862320, loss_sup: 0.141823, loss_mps: 0.237470, loss_cps: 0.483027
[14:00:43.387] iteration 23332: total_loss: 0.374435, loss_sup: 0.030084, loss_mps: 0.125410, loss_cps: 0.218942
[14:00:43.535] iteration 23333: total_loss: 0.584621, loss_sup: 0.059568, loss_mps: 0.178905, loss_cps: 0.346149
[14:00:43.681] iteration 23334: total_loss: 0.577356, loss_sup: 0.040081, loss_mps: 0.182229, loss_cps: 0.355046
[14:00:43.828] iteration 23335: total_loss: 0.860037, loss_sup: 0.272376, loss_mps: 0.193369, loss_cps: 0.394292
[14:00:43.973] iteration 23336: total_loss: 0.360156, loss_sup: 0.158271, loss_mps: 0.073448, loss_cps: 0.128437
[14:00:44.122] iteration 23337: total_loss: 0.897605, loss_sup: 0.081633, loss_mps: 0.277144, loss_cps: 0.538829
[14:00:44.267] iteration 23338: total_loss: 0.470333, loss_sup: 0.013788, loss_mps: 0.152026, loss_cps: 0.304519
[14:00:44.413] iteration 23339: total_loss: 0.249605, loss_sup: 0.015516, loss_mps: 0.089226, loss_cps: 0.144863
[14:00:44.559] iteration 23340: total_loss: 0.769875, loss_sup: 0.002239, loss_mps: 0.246261, loss_cps: 0.521375
[14:00:44.709] iteration 23341: total_loss: 0.684299, loss_sup: 0.041526, loss_mps: 0.219886, loss_cps: 0.422886
[14:00:44.855] iteration 23342: total_loss: 0.488513, loss_sup: 0.058573, loss_mps: 0.151519, loss_cps: 0.278421
[14:00:45.001] iteration 23343: total_loss: 0.460698, loss_sup: 0.043635, loss_mps: 0.141308, loss_cps: 0.275755
[14:00:45.149] iteration 23344: total_loss: 0.698915, loss_sup: 0.070603, loss_mps: 0.211841, loss_cps: 0.416470
[14:00:45.299] iteration 23345: total_loss: 0.394160, loss_sup: 0.033824, loss_mps: 0.123001, loss_cps: 0.237335
[14:00:45.446] iteration 23346: total_loss: 0.455436, loss_sup: 0.029390, loss_mps: 0.149199, loss_cps: 0.276847
[14:00:45.592] iteration 23347: total_loss: 0.341001, loss_sup: 0.020565, loss_mps: 0.112101, loss_cps: 0.208335
[14:00:45.739] iteration 23348: total_loss: 0.674051, loss_sup: 0.134379, loss_mps: 0.176145, loss_cps: 0.363527
[14:00:45.889] iteration 23349: total_loss: 0.450912, loss_sup: 0.049382, loss_mps: 0.136617, loss_cps: 0.264912
[14:00:46.037] iteration 23350: total_loss: 0.208682, loss_sup: 0.002283, loss_mps: 0.076084, loss_cps: 0.130316
[14:00:46.184] iteration 23351: total_loss: 0.424882, loss_sup: 0.015401, loss_mps: 0.143628, loss_cps: 0.265853
[14:00:46.330] iteration 23352: total_loss: 0.631411, loss_sup: 0.027653, loss_mps: 0.189766, loss_cps: 0.413992
[14:00:46.475] iteration 23353: total_loss: 0.259803, loss_sup: 0.011479, loss_mps: 0.092692, loss_cps: 0.155632
[14:00:46.622] iteration 23354: total_loss: 0.316563, loss_sup: 0.032275, loss_mps: 0.101612, loss_cps: 0.182676
[14:00:46.769] iteration 23355: total_loss: 0.739305, loss_sup: 0.062628, loss_mps: 0.214994, loss_cps: 0.461684
[14:00:46.916] iteration 23356: total_loss: 0.830882, loss_sup: 0.083355, loss_mps: 0.234969, loss_cps: 0.512558
[14:00:47.065] iteration 23357: total_loss: 0.494756, loss_sup: 0.024860, loss_mps: 0.156843, loss_cps: 0.313054
[14:00:47.211] iteration 23358: total_loss: 0.293059, loss_sup: 0.012762, loss_mps: 0.104725, loss_cps: 0.175572
[14:00:47.358] iteration 23359: total_loss: 0.572296, loss_sup: 0.068127, loss_mps: 0.173442, loss_cps: 0.330727
[14:00:47.504] iteration 23360: total_loss: 0.698878, loss_sup: 0.096232, loss_mps: 0.203130, loss_cps: 0.399515
[14:00:47.652] iteration 23361: total_loss: 0.341978, loss_sup: 0.005604, loss_mps: 0.123459, loss_cps: 0.212915
[14:00:47.799] iteration 23362: total_loss: 0.709328, loss_sup: 0.075646, loss_mps: 0.208758, loss_cps: 0.424925
[14:00:47.945] iteration 23363: total_loss: 0.803158, loss_sup: 0.034490, loss_mps: 0.237250, loss_cps: 0.531417
[14:00:48.092] iteration 23364: total_loss: 0.319698, loss_sup: 0.030381, loss_mps: 0.112658, loss_cps: 0.176658
[14:00:48.239] iteration 23365: total_loss: 0.481003, loss_sup: 0.088198, loss_mps: 0.131591, loss_cps: 0.261214
[14:00:48.387] iteration 23366: total_loss: 0.405075, loss_sup: 0.047788, loss_mps: 0.130868, loss_cps: 0.226419
[14:00:48.534] iteration 23367: total_loss: 0.386991, loss_sup: 0.026658, loss_mps: 0.127717, loss_cps: 0.232616
[14:00:48.682] iteration 23368: total_loss: 0.923115, loss_sup: 0.248484, loss_mps: 0.202844, loss_cps: 0.471787
[14:00:48.831] iteration 23369: total_loss: 0.436517, loss_sup: 0.043858, loss_mps: 0.140261, loss_cps: 0.252398
[14:00:48.977] iteration 23370: total_loss: 0.471108, loss_sup: 0.021471, loss_mps: 0.155028, loss_cps: 0.294609
[14:00:49.124] iteration 23371: total_loss: 0.322226, loss_sup: 0.037939, loss_mps: 0.106448, loss_cps: 0.177840
[14:00:49.271] iteration 23372: total_loss: 0.514843, loss_sup: 0.070626, loss_mps: 0.148235, loss_cps: 0.295982
[14:00:49.417] iteration 23373: total_loss: 0.556994, loss_sup: 0.060689, loss_mps: 0.157272, loss_cps: 0.339033
[14:00:49.563] iteration 23374: total_loss: 0.342448, loss_sup: 0.152656, loss_mps: 0.068688, loss_cps: 0.121105
[14:00:49.710] iteration 23375: total_loss: 0.244209, loss_sup: 0.031161, loss_mps: 0.083605, loss_cps: 0.129443
[14:00:49.861] iteration 23376: total_loss: 0.301919, loss_sup: 0.024666, loss_mps: 0.103505, loss_cps: 0.173747
[14:00:50.007] iteration 23377: total_loss: 0.397561, loss_sup: 0.131975, loss_mps: 0.094950, loss_cps: 0.170637
[14:00:50.154] iteration 23378: total_loss: 0.455830, loss_sup: 0.041942, loss_mps: 0.138806, loss_cps: 0.275082
[14:00:50.300] iteration 23379: total_loss: 0.825540, loss_sup: 0.099332, loss_mps: 0.242335, loss_cps: 0.483873
[14:00:50.446] iteration 23380: total_loss: 0.631828, loss_sup: 0.090686, loss_mps: 0.178118, loss_cps: 0.363023
[14:00:50.592] iteration 23381: total_loss: 0.311360, loss_sup: 0.016209, loss_mps: 0.112664, loss_cps: 0.182487
[14:00:50.739] iteration 23382: total_loss: 0.427569, loss_sup: 0.005329, loss_mps: 0.147629, loss_cps: 0.274610
[14:00:50.886] iteration 23383: total_loss: 0.322955, loss_sup: 0.055798, loss_mps: 0.094246, loss_cps: 0.172912
[14:00:51.032] iteration 23384: total_loss: 0.328067, loss_sup: 0.011233, loss_mps: 0.109958, loss_cps: 0.206877
[14:00:51.180] iteration 23385: total_loss: 0.446119, loss_sup: 0.007418, loss_mps: 0.148454, loss_cps: 0.290248
[14:00:51.327] iteration 23386: total_loss: 0.843954, loss_sup: 0.108582, loss_mps: 0.223910, loss_cps: 0.511463
[14:00:51.473] iteration 23387: total_loss: 0.582900, loss_sup: 0.027884, loss_mps: 0.180005, loss_cps: 0.375011
[14:00:51.620] iteration 23388: total_loss: 1.385520, loss_sup: 0.177705, loss_mps: 0.368169, loss_cps: 0.839646
[14:00:51.767] iteration 23389: total_loss: 0.885821, loss_sup: 0.113574, loss_mps: 0.238893, loss_cps: 0.533353
[14:00:51.913] iteration 23390: total_loss: 0.317589, loss_sup: 0.067755, loss_mps: 0.091243, loss_cps: 0.158591
[14:00:52.060] iteration 23391: total_loss: 0.501323, loss_sup: 0.144409, loss_mps: 0.121913, loss_cps: 0.235001
[14:00:52.207] iteration 23392: total_loss: 0.579032, loss_sup: 0.358447, loss_mps: 0.080747, loss_cps: 0.139838
[14:00:52.354] iteration 23393: total_loss: 0.508008, loss_sup: 0.286937, loss_mps: 0.077927, loss_cps: 0.143145
[14:00:52.500] iteration 23394: total_loss: 0.498697, loss_sup: 0.124187, loss_mps: 0.130495, loss_cps: 0.244015
[14:00:52.646] iteration 23395: total_loss: 0.577497, loss_sup: 0.036486, loss_mps: 0.170649, loss_cps: 0.370361
[14:00:52.792] iteration 23396: total_loss: 0.484352, loss_sup: 0.028924, loss_mps: 0.155233, loss_cps: 0.300195
[14:00:52.938] iteration 23397: total_loss: 0.360986, loss_sup: 0.082843, loss_mps: 0.109971, loss_cps: 0.168173
[14:00:53.084] iteration 23398: total_loss: 0.486029, loss_sup: 0.022352, loss_mps: 0.152337, loss_cps: 0.311340
[14:00:53.232] iteration 23399: total_loss: 0.947524, loss_sup: 0.171807, loss_mps: 0.241511, loss_cps: 0.534206
[14:00:53.379] iteration 23400: total_loss: 0.235423, loss_sup: 0.010555, loss_mps: 0.081045, loss_cps: 0.143823
[14:00:53.379] Evaluation Started ==>
[14:01:04.686] ==> valid iteration 23400: unet metrics: {'dc': 0.6233382049712269, 'jc': 0.5105865011942239, 'pre': 0.8015541355328063, 'hd': 5.323193620081544}, ynet metrics: {'dc': 0.5882560368345785, 'jc': 0.47263655739076255, 'pre': 0.8078092766924592, 'hd': 5.563754327972455}.
[14:01:04.688] Evaluation Finished!⏹️
[14:01:04.840] iteration 23401: total_loss: 0.560607, loss_sup: 0.040744, loss_mps: 0.167544, loss_cps: 0.352319
[14:01:04.989] iteration 23402: total_loss: 0.735040, loss_sup: 0.027047, loss_mps: 0.222037, loss_cps: 0.485957
[14:01:05.135] iteration 23403: total_loss: 0.496029, loss_sup: 0.075106, loss_mps: 0.144839, loss_cps: 0.276084
[14:01:05.280] iteration 23404: total_loss: 0.707417, loss_sup: 0.121452, loss_mps: 0.186637, loss_cps: 0.399327
[14:01:05.425] iteration 23405: total_loss: 0.407053, loss_sup: 0.048551, loss_mps: 0.120462, loss_cps: 0.238040
[14:01:05.570] iteration 23406: total_loss: 0.571625, loss_sup: 0.077317, loss_mps: 0.166651, loss_cps: 0.327657
[14:01:05.717] iteration 23407: total_loss: 0.407550, loss_sup: 0.086337, loss_mps: 0.114564, loss_cps: 0.206650
[14:01:05.780] iteration 23408: total_loss: 0.617386, loss_sup: 0.001459, loss_mps: 0.188539, loss_cps: 0.427388
[14:01:07.007] iteration 23409: total_loss: 0.399707, loss_sup: 0.114505, loss_mps: 0.105037, loss_cps: 0.180164
[14:01:07.155] iteration 23410: total_loss: 0.537155, loss_sup: 0.214498, loss_mps: 0.113995, loss_cps: 0.208662
[14:01:07.301] iteration 23411: total_loss: 0.572636, loss_sup: 0.103297, loss_mps: 0.158866, loss_cps: 0.310473
[14:01:07.446] iteration 23412: total_loss: 0.431498, loss_sup: 0.003752, loss_mps: 0.147645, loss_cps: 0.280101
[14:01:07.592] iteration 23413: total_loss: 0.346971, loss_sup: 0.020784, loss_mps: 0.115561, loss_cps: 0.210626
[14:01:07.738] iteration 23414: total_loss: 0.855917, loss_sup: 0.087208, loss_mps: 0.238403, loss_cps: 0.530306
[14:01:07.884] iteration 23415: total_loss: 0.537055, loss_sup: 0.075091, loss_mps: 0.159229, loss_cps: 0.302734
[14:01:08.032] iteration 23416: total_loss: 0.492696, loss_sup: 0.067198, loss_mps: 0.151901, loss_cps: 0.273596
[14:01:08.178] iteration 23417: total_loss: 0.428439, loss_sup: 0.155632, loss_mps: 0.101730, loss_cps: 0.171076
[14:01:08.323] iteration 23418: total_loss: 0.617284, loss_sup: 0.104759, loss_mps: 0.163331, loss_cps: 0.349194
[14:01:08.468] iteration 23419: total_loss: 0.457059, loss_sup: 0.011811, loss_mps: 0.152035, loss_cps: 0.293212
[14:01:08.615] iteration 23420: total_loss: 0.863748, loss_sup: 0.226060, loss_mps: 0.206783, loss_cps: 0.430905
[14:01:08.761] iteration 23421: total_loss: 0.538333, loss_sup: 0.145586, loss_mps: 0.136669, loss_cps: 0.256079
[14:01:08.906] iteration 23422: total_loss: 0.541018, loss_sup: 0.067919, loss_mps: 0.166112, loss_cps: 0.306987
[14:01:09.052] iteration 23423: total_loss: 0.455502, loss_sup: 0.062578, loss_mps: 0.142664, loss_cps: 0.250260
[14:01:09.198] iteration 23424: total_loss: 0.422325, loss_sup: 0.024798, loss_mps: 0.139773, loss_cps: 0.257754
[14:01:09.343] iteration 23425: total_loss: 0.320286, loss_sup: 0.077525, loss_mps: 0.087071, loss_cps: 0.155690
[14:01:09.488] iteration 23426: total_loss: 0.751044, loss_sup: 0.148371, loss_mps: 0.207043, loss_cps: 0.395631
[14:01:09.634] iteration 23427: total_loss: 0.306661, loss_sup: 0.002760, loss_mps: 0.114161, loss_cps: 0.189740
[14:01:09.780] iteration 23428: total_loss: 0.469718, loss_sup: 0.036869, loss_mps: 0.150204, loss_cps: 0.282645
[14:01:09.926] iteration 23429: total_loss: 0.587027, loss_sup: 0.115601, loss_mps: 0.168006, loss_cps: 0.303420
[14:01:10.072] iteration 23430: total_loss: 0.547126, loss_sup: 0.058065, loss_mps: 0.169490, loss_cps: 0.319571
[14:01:10.221] iteration 23431: total_loss: 0.529109, loss_sup: 0.046490, loss_mps: 0.153275, loss_cps: 0.329345
[14:01:10.367] iteration 23432: total_loss: 0.215739, loss_sup: 0.027309, loss_mps: 0.077787, loss_cps: 0.110643
[14:01:10.514] iteration 23433: total_loss: 0.349951, loss_sup: 0.060142, loss_mps: 0.107541, loss_cps: 0.182268
[14:01:10.660] iteration 23434: total_loss: 0.601493, loss_sup: 0.024448, loss_mps: 0.195378, loss_cps: 0.381667
[14:01:10.807] iteration 23435: total_loss: 1.070439, loss_sup: 0.104388, loss_mps: 0.327360, loss_cps: 0.638690
[14:01:10.958] iteration 23436: total_loss: 0.471498, loss_sup: 0.015080, loss_mps: 0.148013, loss_cps: 0.308404
[14:01:11.105] iteration 23437: total_loss: 0.391359, loss_sup: 0.037568, loss_mps: 0.131908, loss_cps: 0.221884
[14:01:11.251] iteration 23438: total_loss: 0.361180, loss_sup: 0.065574, loss_mps: 0.101843, loss_cps: 0.193762
[14:01:11.398] iteration 23439: total_loss: 0.322271, loss_sup: 0.027408, loss_mps: 0.114344, loss_cps: 0.180520
[14:01:11.546] iteration 23440: total_loss: 0.196636, loss_sup: 0.001225, loss_mps: 0.076109, loss_cps: 0.119302
[14:01:11.692] iteration 23441: total_loss: 0.702208, loss_sup: 0.091259, loss_mps: 0.198210, loss_cps: 0.412739
[14:01:11.839] iteration 23442: total_loss: 0.607381, loss_sup: 0.011751, loss_mps: 0.190221, loss_cps: 0.405408
[14:01:11.985] iteration 23443: total_loss: 0.284275, loss_sup: 0.020363, loss_mps: 0.094916, loss_cps: 0.168997
[14:01:12.130] iteration 23444: total_loss: 0.485699, loss_sup: 0.006689, loss_mps: 0.165456, loss_cps: 0.313554
[14:01:12.277] iteration 23445: total_loss: 0.468461, loss_sup: 0.021716, loss_mps: 0.157170, loss_cps: 0.289574
[14:01:12.423] iteration 23446: total_loss: 0.320647, loss_sup: 0.017333, loss_mps: 0.108976, loss_cps: 0.194338
[14:01:12.569] iteration 23447: total_loss: 0.405604, loss_sup: 0.149953, loss_mps: 0.096275, loss_cps: 0.159376
[14:01:12.716] iteration 23448: total_loss: 0.644999, loss_sup: 0.170881, loss_mps: 0.153756, loss_cps: 0.320363
[14:01:12.863] iteration 23449: total_loss: 0.325834, loss_sup: 0.069078, loss_mps: 0.093560, loss_cps: 0.163196
[14:01:13.009] iteration 23450: total_loss: 0.379796, loss_sup: 0.048003, loss_mps: 0.113689, loss_cps: 0.218105
[14:01:13.154] iteration 23451: total_loss: 0.342988, loss_sup: 0.069099, loss_mps: 0.098462, loss_cps: 0.175427
[14:01:13.299] iteration 23452: total_loss: 0.434239, loss_sup: 0.138216, loss_mps: 0.106672, loss_cps: 0.189351
[14:01:13.445] iteration 23453: total_loss: 0.642966, loss_sup: 0.142989, loss_mps: 0.167028, loss_cps: 0.332949
[14:01:13.593] iteration 23454: total_loss: 0.425450, loss_sup: 0.052618, loss_mps: 0.123373, loss_cps: 0.249459
[14:01:13.739] iteration 23455: total_loss: 0.360660, loss_sup: 0.071005, loss_mps: 0.102171, loss_cps: 0.187483
[14:01:13.885] iteration 23456: total_loss: 1.491955, loss_sup: 0.352055, loss_mps: 0.340779, loss_cps: 0.799121
[14:01:14.031] iteration 23457: total_loss: 0.451510, loss_sup: 0.057521, loss_mps: 0.126340, loss_cps: 0.267648
[14:01:14.177] iteration 23458: total_loss: 0.279443, loss_sup: 0.016120, loss_mps: 0.094082, loss_cps: 0.169240
[14:01:14.323] iteration 23459: total_loss: 0.482722, loss_sup: 0.085572, loss_mps: 0.138946, loss_cps: 0.258204
[14:01:14.469] iteration 23460: total_loss: 0.565847, loss_sup: 0.013352, loss_mps: 0.175985, loss_cps: 0.376509
[14:01:14.615] iteration 23461: total_loss: 0.373389, loss_sup: 0.084469, loss_mps: 0.099355, loss_cps: 0.189565
[14:01:14.761] iteration 23462: total_loss: 0.347821, loss_sup: 0.012527, loss_mps: 0.121370, loss_cps: 0.213924
[14:01:14.907] iteration 23463: total_loss: 0.555932, loss_sup: 0.124685, loss_mps: 0.147474, loss_cps: 0.283773
[14:01:15.052] iteration 23464: total_loss: 0.400225, loss_sup: 0.053974, loss_mps: 0.126426, loss_cps: 0.219826
[14:01:15.198] iteration 23465: total_loss: 0.280527, loss_sup: 0.022835, loss_mps: 0.091552, loss_cps: 0.166140
[14:01:15.344] iteration 23466: total_loss: 0.225190, loss_sup: 0.007403, loss_mps: 0.081047, loss_cps: 0.136739
[14:01:15.491] iteration 23467: total_loss: 0.475782, loss_sup: 0.081152, loss_mps: 0.131339, loss_cps: 0.263291
[14:01:15.638] iteration 23468: total_loss: 0.553621, loss_sup: 0.032812, loss_mps: 0.170017, loss_cps: 0.350793
[14:01:15.785] iteration 23469: total_loss: 0.480412, loss_sup: 0.125392, loss_mps: 0.118993, loss_cps: 0.236026
[14:01:15.933] iteration 23470: total_loss: 0.869520, loss_sup: 0.088595, loss_mps: 0.252239, loss_cps: 0.528685
[14:01:16.079] iteration 23471: total_loss: 0.440492, loss_sup: 0.017931, loss_mps: 0.154008, loss_cps: 0.268554
[14:01:16.225] iteration 23472: total_loss: 0.354630, loss_sup: 0.093249, loss_mps: 0.096063, loss_cps: 0.165318
[14:01:16.370] iteration 23473: total_loss: 0.638183, loss_sup: 0.044617, loss_mps: 0.201233, loss_cps: 0.392333
[14:01:16.517] iteration 23474: total_loss: 0.273429, loss_sup: 0.004345, loss_mps: 0.097552, loss_cps: 0.171532
[14:01:16.663] iteration 23475: total_loss: 1.112955, loss_sup: 0.195303, loss_mps: 0.283052, loss_cps: 0.634599
[14:01:16.810] iteration 23476: total_loss: 0.576821, loss_sup: 0.092170, loss_mps: 0.173370, loss_cps: 0.311281
[14:01:16.956] iteration 23477: total_loss: 0.674603, loss_sup: 0.139573, loss_mps: 0.180351, loss_cps: 0.354679
[14:01:17.102] iteration 23478: total_loss: 0.451786, loss_sup: 0.087190, loss_mps: 0.125066, loss_cps: 0.239530
[14:01:17.248] iteration 23479: total_loss: 0.466945, loss_sup: 0.048804, loss_mps: 0.142635, loss_cps: 0.275506
[14:01:17.394] iteration 23480: total_loss: 0.655197, loss_sup: 0.074931, loss_mps: 0.190803, loss_cps: 0.389463
[14:01:17.539] iteration 23481: total_loss: 0.243979, loss_sup: 0.040770, loss_mps: 0.074526, loss_cps: 0.128684
[14:01:17.685] iteration 23482: total_loss: 0.897355, loss_sup: 0.025997, loss_mps: 0.259294, loss_cps: 0.612065
[14:01:17.831] iteration 23483: total_loss: 0.445108, loss_sup: 0.053455, loss_mps: 0.143210, loss_cps: 0.248443
[14:01:17.978] iteration 23484: total_loss: 0.437191, loss_sup: 0.042466, loss_mps: 0.138378, loss_cps: 0.256347
[14:01:18.123] iteration 23485: total_loss: 0.275121, loss_sup: 0.007509, loss_mps: 0.096737, loss_cps: 0.170876
[14:01:18.269] iteration 23486: total_loss: 0.324273, loss_sup: 0.068576, loss_mps: 0.095790, loss_cps: 0.159906
[14:01:18.415] iteration 23487: total_loss: 0.344480, loss_sup: 0.030038, loss_mps: 0.112905, loss_cps: 0.201538
[14:01:18.561] iteration 23488: total_loss: 0.420231, loss_sup: 0.180016, loss_mps: 0.089388, loss_cps: 0.150827
[14:01:18.706] iteration 23489: total_loss: 0.391268, loss_sup: 0.004657, loss_mps: 0.137728, loss_cps: 0.248883
[14:01:18.852] iteration 23490: total_loss: 0.524671, loss_sup: 0.074244, loss_mps: 0.156758, loss_cps: 0.293670
[14:01:18.998] iteration 23491: total_loss: 0.302460, loss_sup: 0.023475, loss_mps: 0.103326, loss_cps: 0.175659
[14:01:19.144] iteration 23492: total_loss: 0.648384, loss_sup: 0.084811, loss_mps: 0.172902, loss_cps: 0.390672
[14:01:19.290] iteration 23493: total_loss: 0.466666, loss_sup: 0.039536, loss_mps: 0.156915, loss_cps: 0.270215
[14:01:19.436] iteration 23494: total_loss: 0.274313, loss_sup: 0.055754, loss_mps: 0.076858, loss_cps: 0.141701
[14:01:19.583] iteration 23495: total_loss: 0.202972, loss_sup: 0.012429, loss_mps: 0.071920, loss_cps: 0.118623
[14:01:19.728] iteration 23496: total_loss: 0.454467, loss_sup: 0.038219, loss_mps: 0.138923, loss_cps: 0.277325
[14:01:19.874] iteration 23497: total_loss: 0.482194, loss_sup: 0.009565, loss_mps: 0.164662, loss_cps: 0.307967
[14:01:20.022] iteration 23498: total_loss: 0.628168, loss_sup: 0.081171, loss_mps: 0.181159, loss_cps: 0.365838
[14:01:20.168] iteration 23499: total_loss: 0.361642, loss_sup: 0.034833, loss_mps: 0.115076, loss_cps: 0.211732
[14:01:20.313] iteration 23500: total_loss: 0.479080, loss_sup: 0.051600, loss_mps: 0.140773, loss_cps: 0.286707
[14:01:20.314] Evaluation Started ==>
[14:01:31.638] ==> valid iteration 23500: unet metrics: {'dc': 0.6699210573490303, 'jc': 0.5552091354172748, 'pre': 0.79367004553875, 'hd': 5.489444800211743}, ynet metrics: {'dc': 0.615061200262374, 'jc': 0.49852564466116905, 'pre': 0.8033312859085264, 'hd': 5.543821488897775}.
[14:01:31.640] Evaluation Finished!⏹️
[14:01:31.796] iteration 23501: total_loss: 0.337188, loss_sup: 0.019089, loss_mps: 0.113527, loss_cps: 0.204572
[14:01:31.945] iteration 23502: total_loss: 0.677240, loss_sup: 0.208415, loss_mps: 0.165253, loss_cps: 0.303571
[14:01:32.091] iteration 23503: total_loss: 0.440755, loss_sup: 0.060955, loss_mps: 0.133069, loss_cps: 0.246731
[14:01:32.236] iteration 23504: total_loss: 0.513950, loss_sup: 0.064517, loss_mps: 0.153542, loss_cps: 0.295891
[14:01:32.381] iteration 23505: total_loss: 0.454711, loss_sup: 0.075958, loss_mps: 0.124933, loss_cps: 0.253820
[14:01:32.526] iteration 23506: total_loss: 0.812000, loss_sup: 0.035695, loss_mps: 0.243114, loss_cps: 0.533191
[14:01:32.671] iteration 23507: total_loss: 0.363231, loss_sup: 0.146686, loss_mps: 0.081826, loss_cps: 0.134719
[14:01:32.819] iteration 23508: total_loss: 0.488316, loss_sup: 0.057989, loss_mps: 0.151356, loss_cps: 0.278971
[14:01:32.965] iteration 23509: total_loss: 0.373186, loss_sup: 0.017726, loss_mps: 0.126450, loss_cps: 0.229010
[14:01:33.112] iteration 23510: total_loss: 0.484711, loss_sup: 0.035363, loss_mps: 0.141421, loss_cps: 0.307926
[14:01:33.259] iteration 23511: total_loss: 0.327990, loss_sup: 0.073495, loss_mps: 0.098863, loss_cps: 0.155633
[14:01:33.412] iteration 23512: total_loss: 0.635212, loss_sup: 0.072485, loss_mps: 0.185663, loss_cps: 0.377064
[14:01:33.560] iteration 23513: total_loss: 0.362320, loss_sup: 0.007416, loss_mps: 0.123546, loss_cps: 0.231358
[14:01:33.707] iteration 23514: total_loss: 0.385221, loss_sup: 0.113008, loss_mps: 0.098481, loss_cps: 0.173733
[14:01:33.853] iteration 23515: total_loss: 0.296080, loss_sup: 0.026301, loss_mps: 0.099583, loss_cps: 0.170197
[14:01:33.999] iteration 23516: total_loss: 0.438845, loss_sup: 0.049321, loss_mps: 0.135847, loss_cps: 0.253677
[14:01:34.147] iteration 23517: total_loss: 0.343371, loss_sup: 0.062004, loss_mps: 0.104061, loss_cps: 0.177306
[14:01:34.296] iteration 23518: total_loss: 0.210961, loss_sup: 0.021542, loss_mps: 0.072309, loss_cps: 0.117109
[14:01:34.442] iteration 23519: total_loss: 0.810767, loss_sup: 0.098699, loss_mps: 0.226016, loss_cps: 0.486053
[14:01:34.588] iteration 23520: total_loss: 0.461086, loss_sup: 0.049392, loss_mps: 0.140266, loss_cps: 0.271428
[14:01:34.733] iteration 23521: total_loss: 0.515971, loss_sup: 0.041232, loss_mps: 0.156658, loss_cps: 0.318082
[14:01:34.879] iteration 23522: total_loss: 0.344662, loss_sup: 0.073062, loss_mps: 0.093675, loss_cps: 0.177924
[14:01:35.027] iteration 23523: total_loss: 0.332639, loss_sup: 0.022732, loss_mps: 0.100422, loss_cps: 0.209485
[14:01:35.174] iteration 23524: total_loss: 0.422374, loss_sup: 0.018774, loss_mps: 0.142410, loss_cps: 0.261190
[14:01:35.319] iteration 23525: total_loss: 0.729763, loss_sup: 0.259587, loss_mps: 0.168677, loss_cps: 0.301498
[14:01:35.469] iteration 23526: total_loss: 0.401119, loss_sup: 0.048449, loss_mps: 0.124721, loss_cps: 0.227948
[14:01:35.615] iteration 23527: total_loss: 0.246141, loss_sup: 0.033823, loss_mps: 0.076055, loss_cps: 0.136263
[14:01:35.760] iteration 23528: total_loss: 0.377292, loss_sup: 0.014320, loss_mps: 0.125337, loss_cps: 0.237635
[14:01:35.905] iteration 23529: total_loss: 0.631020, loss_sup: 0.047825, loss_mps: 0.184064, loss_cps: 0.399130
[14:01:36.051] iteration 23530: total_loss: 0.373552, loss_sup: 0.070007, loss_mps: 0.105822, loss_cps: 0.197723
[14:01:36.196] iteration 23531: total_loss: 0.582407, loss_sup: 0.051513, loss_mps: 0.180939, loss_cps: 0.349955
[14:01:36.341] iteration 23532: total_loss: 0.607910, loss_sup: 0.109777, loss_mps: 0.166774, loss_cps: 0.331359
[14:01:36.487] iteration 23533: total_loss: 0.632944, loss_sup: 0.051432, loss_mps: 0.204586, loss_cps: 0.376926
[14:01:36.633] iteration 23534: total_loss: 0.223214, loss_sup: 0.018588, loss_mps: 0.076201, loss_cps: 0.128425
[14:01:36.778] iteration 23535: total_loss: 1.248150, loss_sup: 0.111554, loss_mps: 0.349787, loss_cps: 0.786810
[14:01:36.924] iteration 23536: total_loss: 0.377854, loss_sup: 0.017564, loss_mps: 0.121673, loss_cps: 0.238618
[14:01:37.070] iteration 23537: total_loss: 0.469332, loss_sup: 0.122742, loss_mps: 0.123510, loss_cps: 0.223079
[14:01:37.215] iteration 23538: total_loss: 0.414724, loss_sup: 0.015927, loss_mps: 0.138982, loss_cps: 0.259816
[14:01:37.363] iteration 23539: total_loss: 0.525364, loss_sup: 0.087358, loss_mps: 0.147869, loss_cps: 0.290138
[14:01:37.509] iteration 23540: total_loss: 0.312091, loss_sup: 0.015043, loss_mps: 0.110008, loss_cps: 0.187039
[14:01:37.654] iteration 23541: total_loss: 0.692179, loss_sup: 0.173479, loss_mps: 0.176802, loss_cps: 0.341899
[14:01:37.799] iteration 23542: total_loss: 0.597484, loss_sup: 0.058929, loss_mps: 0.171546, loss_cps: 0.367008
[14:01:37.946] iteration 23543: total_loss: 0.354307, loss_sup: 0.011131, loss_mps: 0.125673, loss_cps: 0.217502
[14:01:38.093] iteration 23544: total_loss: 0.529324, loss_sup: 0.089193, loss_mps: 0.142488, loss_cps: 0.297644
[14:01:38.238] iteration 23545: total_loss: 0.796726, loss_sup: 0.128417, loss_mps: 0.214584, loss_cps: 0.453725
[14:01:38.384] iteration 23546: total_loss: 0.708775, loss_sup: 0.001620, loss_mps: 0.233105, loss_cps: 0.474050
[14:01:38.529] iteration 23547: total_loss: 0.458603, loss_sup: 0.017782, loss_mps: 0.151521, loss_cps: 0.289300
[14:01:38.674] iteration 23548: total_loss: 0.881561, loss_sup: 0.179012, loss_mps: 0.225454, loss_cps: 0.477094
[14:01:38.820] iteration 23549: total_loss: 0.393517, loss_sup: 0.012691, loss_mps: 0.137155, loss_cps: 0.243671
[14:01:38.966] iteration 23550: total_loss: 0.787074, loss_sup: 0.148357, loss_mps: 0.204421, loss_cps: 0.434296
[14:01:39.111] iteration 23551: total_loss: 0.303766, loss_sup: 0.029239, loss_mps: 0.094565, loss_cps: 0.179962
[14:01:39.257] iteration 23552: total_loss: 0.618935, loss_sup: 0.122178, loss_mps: 0.158326, loss_cps: 0.338431
[14:01:39.402] iteration 23553: total_loss: 0.385387, loss_sup: 0.080356, loss_mps: 0.108127, loss_cps: 0.196903
[14:01:39.548] iteration 23554: total_loss: 0.331602, loss_sup: 0.029611, loss_mps: 0.104184, loss_cps: 0.197807
[14:01:39.694] iteration 23555: total_loss: 0.600843, loss_sup: 0.002796, loss_mps: 0.185153, loss_cps: 0.412894
[14:01:39.840] iteration 23556: total_loss: 0.462529, loss_sup: 0.075076, loss_mps: 0.136641, loss_cps: 0.250811
[14:01:39.985] iteration 23557: total_loss: 0.540058, loss_sup: 0.145191, loss_mps: 0.137429, loss_cps: 0.257438
[14:01:40.131] iteration 23558: total_loss: 0.604384, loss_sup: 0.187377, loss_mps: 0.144270, loss_cps: 0.272737
[14:01:40.277] iteration 23559: total_loss: 0.325568, loss_sup: 0.040690, loss_mps: 0.100341, loss_cps: 0.184538
[14:01:40.422] iteration 23560: total_loss: 0.447135, loss_sup: 0.154384, loss_mps: 0.111207, loss_cps: 0.181544
[14:01:40.568] iteration 23561: total_loss: 0.444463, loss_sup: 0.057101, loss_mps: 0.127237, loss_cps: 0.260125
[14:01:40.714] iteration 23562: total_loss: 0.399582, loss_sup: 0.007329, loss_mps: 0.131678, loss_cps: 0.260574
[14:01:40.861] iteration 23563: total_loss: 0.373054, loss_sup: 0.094896, loss_mps: 0.099191, loss_cps: 0.178968
[14:01:41.008] iteration 23564: total_loss: 0.392349, loss_sup: 0.074774, loss_mps: 0.119435, loss_cps: 0.198139
[14:01:41.156] iteration 23565: total_loss: 0.912828, loss_sup: 0.131514, loss_mps: 0.250534, loss_cps: 0.530780
[14:01:41.302] iteration 23566: total_loss: 0.774761, loss_sup: 0.057338, loss_mps: 0.219822, loss_cps: 0.497601
[14:01:41.448] iteration 23567: total_loss: 0.371743, loss_sup: 0.098082, loss_mps: 0.095843, loss_cps: 0.177818
[14:01:41.595] iteration 23568: total_loss: 0.322589, loss_sup: 0.088659, loss_mps: 0.088175, loss_cps: 0.145754
[14:01:41.743] iteration 23569: total_loss: 0.504829, loss_sup: 0.086361, loss_mps: 0.141875, loss_cps: 0.276592
[14:01:41.894] iteration 23570: total_loss: 0.519846, loss_sup: 0.122226, loss_mps: 0.139853, loss_cps: 0.257767
[14:01:42.042] iteration 23571: total_loss: 0.693595, loss_sup: 0.254249, loss_mps: 0.145879, loss_cps: 0.293468
[14:01:42.188] iteration 23572: total_loss: 0.568638, loss_sup: 0.175655, loss_mps: 0.141524, loss_cps: 0.251460
[14:01:42.334] iteration 23573: total_loss: 0.503975, loss_sup: 0.107072, loss_mps: 0.138491, loss_cps: 0.258412
[14:01:42.479] iteration 23574: total_loss: 0.753950, loss_sup: 0.259503, loss_mps: 0.167991, loss_cps: 0.326457
[14:01:42.625] iteration 23575: total_loss: 0.374656, loss_sup: 0.038784, loss_mps: 0.114350, loss_cps: 0.221522
[14:01:42.771] iteration 23576: total_loss: 0.340325, loss_sup: 0.066160, loss_mps: 0.097291, loss_cps: 0.176874
[14:01:42.916] iteration 23577: total_loss: 0.357691, loss_sup: 0.039167, loss_mps: 0.110809, loss_cps: 0.207715
[14:01:43.062] iteration 23578: total_loss: 0.518842, loss_sup: 0.198346, loss_mps: 0.111003, loss_cps: 0.209493
[14:01:43.207] iteration 23579: total_loss: 0.307086, loss_sup: 0.067704, loss_mps: 0.091281, loss_cps: 0.148101
[14:01:43.353] iteration 23580: total_loss: 0.627058, loss_sup: 0.157591, loss_mps: 0.168226, loss_cps: 0.301241
[14:01:43.498] iteration 23581: total_loss: 0.488429, loss_sup: 0.123729, loss_mps: 0.136656, loss_cps: 0.228044
[14:01:43.644] iteration 23582: total_loss: 0.239216, loss_sup: 0.024908, loss_mps: 0.082197, loss_cps: 0.132111
[14:01:43.790] iteration 23583: total_loss: 0.280484, loss_sup: 0.039059, loss_mps: 0.095244, loss_cps: 0.146180
[14:01:43.936] iteration 23584: total_loss: 0.383326, loss_sup: 0.065357, loss_mps: 0.120887, loss_cps: 0.197081
[14:01:44.081] iteration 23585: total_loss: 0.377944, loss_sup: 0.018365, loss_mps: 0.126709, loss_cps: 0.232870
[14:01:44.227] iteration 23586: total_loss: 0.532987, loss_sup: 0.088936, loss_mps: 0.148217, loss_cps: 0.295834
[14:01:44.373] iteration 23587: total_loss: 0.630290, loss_sup: 0.234280, loss_mps: 0.151290, loss_cps: 0.244721
[14:01:44.518] iteration 23588: total_loss: 0.648116, loss_sup: 0.063001, loss_mps: 0.210917, loss_cps: 0.374198
[14:01:44.664] iteration 23589: total_loss: 0.608966, loss_sup: 0.130060, loss_mps: 0.161301, loss_cps: 0.317604
[14:01:44.810] iteration 23590: total_loss: 0.517432, loss_sup: 0.019743, loss_mps: 0.166952, loss_cps: 0.330737
[14:01:44.956] iteration 23591: total_loss: 0.561685, loss_sup: 0.016792, loss_mps: 0.186242, loss_cps: 0.358651
[14:01:45.102] iteration 23592: total_loss: 0.539293, loss_sup: 0.022558, loss_mps: 0.173199, loss_cps: 0.343536
[14:01:45.248] iteration 23593: total_loss: 0.612952, loss_sup: 0.027903, loss_mps: 0.183374, loss_cps: 0.401675
[14:01:45.395] iteration 23594: total_loss: 0.273104, loss_sup: 0.036793, loss_mps: 0.092733, loss_cps: 0.143578
[14:01:45.542] iteration 23595: total_loss: 0.478265, loss_sup: 0.071164, loss_mps: 0.144161, loss_cps: 0.262940
[14:01:45.688] iteration 23596: total_loss: 0.605264, loss_sup: 0.178286, loss_mps: 0.154156, loss_cps: 0.272822
[14:01:45.834] iteration 23597: total_loss: 0.514626, loss_sup: 0.053904, loss_mps: 0.145091, loss_cps: 0.315631
[14:01:45.979] iteration 23598: total_loss: 0.667302, loss_sup: 0.087316, loss_mps: 0.193160, loss_cps: 0.386827
[14:01:46.126] iteration 23599: total_loss: 0.532280, loss_sup: 0.077396, loss_mps: 0.155639, loss_cps: 0.299245
[14:01:46.273] iteration 23600: total_loss: 0.234067, loss_sup: 0.024813, loss_mps: 0.078395, loss_cps: 0.130859
[14:01:46.273] Evaluation Started ==>
[14:01:57.706] ==> valid iteration 23600: unet metrics: {'dc': 0.6254822682033684, 'jc': 0.5131952010685853, 'pre': 0.7774774907173033, 'hd': 5.433558352443506}, ynet metrics: {'dc': 0.5814798617403141, 'jc': 0.47030371335994614, 'pre': 0.8007941235032353, 'hd': 5.4532654163478185}.
[14:01:57.708] Evaluation Finished!⏹️
[14:01:57.858] iteration 23601: total_loss: 1.064059, loss_sup: 0.233917, loss_mps: 0.268086, loss_cps: 0.562057
[14:01:58.005] iteration 23602: total_loss: 0.335526, loss_sup: 0.105371, loss_mps: 0.080763, loss_cps: 0.149392
[14:01:58.152] iteration 23603: total_loss: 0.318810, loss_sup: 0.012916, loss_mps: 0.104509, loss_cps: 0.201385
[14:01:58.300] iteration 23604: total_loss: 0.607078, loss_sup: 0.056663, loss_mps: 0.185468, loss_cps: 0.364948
[14:01:58.446] iteration 23605: total_loss: 0.445092, loss_sup: 0.031142, loss_mps: 0.141651, loss_cps: 0.272300
[14:01:58.592] iteration 23606: total_loss: 0.383043, loss_sup: 0.043838, loss_mps: 0.121443, loss_cps: 0.217761
[14:01:58.738] iteration 23607: total_loss: 1.560391, loss_sup: 0.098702, loss_mps: 0.426311, loss_cps: 1.035379
[14:01:58.884] iteration 23608: total_loss: 0.762121, loss_sup: 0.046120, loss_mps: 0.223426, loss_cps: 0.492575
[14:01:59.029] iteration 23609: total_loss: 0.377041, loss_sup: 0.056628, loss_mps: 0.116256, loss_cps: 0.204157
[14:01:59.175] iteration 23610: total_loss: 0.410648, loss_sup: 0.093418, loss_mps: 0.115490, loss_cps: 0.201740
[14:01:59.321] iteration 23611: total_loss: 0.427432, loss_sup: 0.119722, loss_mps: 0.107219, loss_cps: 0.200491
[14:01:59.470] iteration 23612: total_loss: 0.372476, loss_sup: 0.003950, loss_mps: 0.128421, loss_cps: 0.240106
[14:01:59.619] iteration 23613: total_loss: 0.535553, loss_sup: 0.082826, loss_mps: 0.149022, loss_cps: 0.303704
[14:01:59.765] iteration 23614: total_loss: 0.485544, loss_sup: 0.007985, loss_mps: 0.156710, loss_cps: 0.320848
[14:01:59.911] iteration 23615: total_loss: 0.245613, loss_sup: 0.017358, loss_mps: 0.087737, loss_cps: 0.140518
[14:02:00.057] iteration 23616: total_loss: 0.507582, loss_sup: 0.015623, loss_mps: 0.166327, loss_cps: 0.325632
[14:02:00.204] iteration 23617: total_loss: 0.533693, loss_sup: 0.211221, loss_mps: 0.111395, loss_cps: 0.211076
[14:02:00.350] iteration 23618: total_loss: 1.041065, loss_sup: 0.573966, loss_mps: 0.158738, loss_cps: 0.308361
[14:02:00.496] iteration 23619: total_loss: 0.314010, loss_sup: 0.015735, loss_mps: 0.105657, loss_cps: 0.192618
[14:02:00.642] iteration 23620: total_loss: 0.718582, loss_sup: 0.207344, loss_mps: 0.177777, loss_cps: 0.333461
[14:02:00.788] iteration 23621: total_loss: 0.635562, loss_sup: 0.062353, loss_mps: 0.189237, loss_cps: 0.383972
[14:02:00.934] iteration 23622: total_loss: 0.846018, loss_sup: 0.119199, loss_mps: 0.224818, loss_cps: 0.502001
[14:02:01.081] iteration 23623: total_loss: 0.315651, loss_sup: 0.027065, loss_mps: 0.106766, loss_cps: 0.181820
[14:02:01.227] iteration 23624: total_loss: 0.644528, loss_sup: 0.033612, loss_mps: 0.198903, loss_cps: 0.412013
[14:02:01.376] iteration 23625: total_loss: 0.666568, loss_sup: 0.055014, loss_mps: 0.210164, loss_cps: 0.401390
[14:02:01.522] iteration 23626: total_loss: 0.340127, loss_sup: 0.003752, loss_mps: 0.115264, loss_cps: 0.221111
[14:02:01.667] iteration 23627: total_loss: 0.437548, loss_sup: 0.113064, loss_mps: 0.122948, loss_cps: 0.201535
[14:02:01.813] iteration 23628: total_loss: 0.648540, loss_sup: 0.128764, loss_mps: 0.185848, loss_cps: 0.333928
[14:02:01.959] iteration 23629: total_loss: 0.653434, loss_sup: 0.068645, loss_mps: 0.198347, loss_cps: 0.386442
[14:02:02.106] iteration 23630: total_loss: 0.804886, loss_sup: 0.068758, loss_mps: 0.242327, loss_cps: 0.493801
[14:02:02.253] iteration 23631: total_loss: 0.961474, loss_sup: 0.260556, loss_mps: 0.237270, loss_cps: 0.463648
[14:02:02.399] iteration 23632: total_loss: 0.593802, loss_sup: 0.156725, loss_mps: 0.145578, loss_cps: 0.291499
[14:02:02.544] iteration 23633: total_loss: 0.796017, loss_sup: 0.251506, loss_mps: 0.180659, loss_cps: 0.363853
[14:02:02.690] iteration 23634: total_loss: 0.847251, loss_sup: 0.389004, loss_mps: 0.150734, loss_cps: 0.307513
[14:02:02.836] iteration 23635: total_loss: 0.252039, loss_sup: 0.024073, loss_mps: 0.082083, loss_cps: 0.145883
[14:02:02.984] iteration 23636: total_loss: 0.649787, loss_sup: 0.062455, loss_mps: 0.198363, loss_cps: 0.388969
[14:02:03.131] iteration 23637: total_loss: 0.607125, loss_sup: 0.067316, loss_mps: 0.189841, loss_cps: 0.349969
[14:02:03.279] iteration 23638: total_loss: 0.531019, loss_sup: 0.098588, loss_mps: 0.158359, loss_cps: 0.274072
[14:02:03.426] iteration 23639: total_loss: 0.507149, loss_sup: 0.033194, loss_mps: 0.164069, loss_cps: 0.309885
[14:02:03.572] iteration 23640: total_loss: 0.330969, loss_sup: 0.028143, loss_mps: 0.105853, loss_cps: 0.196974
[14:02:03.718] iteration 23641: total_loss: 0.460355, loss_sup: 0.066961, loss_mps: 0.158506, loss_cps: 0.234888
[14:02:03.866] iteration 23642: total_loss: 0.289725, loss_sup: 0.049240, loss_mps: 0.094738, loss_cps: 0.145747
[14:02:04.012] iteration 23643: total_loss: 0.507614, loss_sup: 0.029849, loss_mps: 0.170641, loss_cps: 0.307125
[14:02:04.159] iteration 23644: total_loss: 0.449183, loss_sup: 0.041848, loss_mps: 0.142384, loss_cps: 0.264950
[14:02:04.306] iteration 23645: total_loss: 0.287913, loss_sup: 0.089661, loss_mps: 0.074473, loss_cps: 0.123779
[14:02:04.452] iteration 23646: total_loss: 0.720753, loss_sup: 0.080195, loss_mps: 0.221702, loss_cps: 0.418857
[14:02:04.600] iteration 23647: total_loss: 0.749253, loss_sup: 0.064956, loss_mps: 0.226408, loss_cps: 0.457889
[14:02:04.749] iteration 23648: total_loss: 0.585015, loss_sup: 0.083300, loss_mps: 0.176005, loss_cps: 0.325711
[14:02:04.895] iteration 23649: total_loss: 0.409549, loss_sup: 0.009828, loss_mps: 0.130558, loss_cps: 0.269163
[14:02:05.042] iteration 23650: total_loss: 0.351904, loss_sup: 0.021722, loss_mps: 0.114593, loss_cps: 0.215589
[14:02:05.188] iteration 23651: total_loss: 0.478992, loss_sup: 0.152662, loss_mps: 0.115446, loss_cps: 0.210885
[14:02:05.335] iteration 23652: total_loss: 0.472799, loss_sup: 0.094699, loss_mps: 0.132449, loss_cps: 0.245651
[14:02:05.482] iteration 23653: total_loss: 0.889868, loss_sup: 0.287508, loss_mps: 0.202649, loss_cps: 0.399711
[14:02:05.628] iteration 23654: total_loss: 0.311411, loss_sup: 0.010108, loss_mps: 0.106056, loss_cps: 0.195247
[14:02:05.776] iteration 23655: total_loss: 0.307693, loss_sup: 0.034262, loss_mps: 0.105033, loss_cps: 0.168399
[14:02:05.924] iteration 23656: total_loss: 0.490895, loss_sup: 0.078359, loss_mps: 0.139185, loss_cps: 0.273350
[14:02:06.070] iteration 23657: total_loss: 0.662752, loss_sup: 0.115651, loss_mps: 0.189283, loss_cps: 0.357817
[14:02:06.217] iteration 23658: total_loss: 0.854970, loss_sup: 0.001973, loss_mps: 0.267351, loss_cps: 0.585647
[14:02:06.363] iteration 23659: total_loss: 0.366190, loss_sup: 0.017640, loss_mps: 0.120296, loss_cps: 0.228254
[14:02:06.510] iteration 23660: total_loss: 0.395526, loss_sup: 0.034719, loss_mps: 0.124654, loss_cps: 0.236154
[14:02:06.657] iteration 23661: total_loss: 0.424339, loss_sup: 0.047587, loss_mps: 0.130320, loss_cps: 0.246431
[14:02:06.804] iteration 23662: total_loss: 0.386714, loss_sup: 0.029232, loss_mps: 0.122091, loss_cps: 0.235391
[14:02:06.951] iteration 23663: total_loss: 0.277533, loss_sup: 0.029692, loss_mps: 0.087124, loss_cps: 0.160718
[14:02:07.098] iteration 23664: total_loss: 0.395554, loss_sup: 0.047889, loss_mps: 0.135093, loss_cps: 0.212571
[14:02:07.244] iteration 23665: total_loss: 0.467803, loss_sup: 0.050381, loss_mps: 0.146966, loss_cps: 0.270455
[14:02:07.390] iteration 23666: total_loss: 0.572938, loss_sup: 0.093534, loss_mps: 0.166975, loss_cps: 0.312429
[14:02:07.538] iteration 23667: total_loss: 0.660807, loss_sup: 0.039220, loss_mps: 0.212777, loss_cps: 0.408810
[14:02:07.687] iteration 23668: total_loss: 0.312107, loss_sup: 0.068670, loss_mps: 0.093730, loss_cps: 0.149707
[14:02:07.835] iteration 23669: total_loss: 0.502412, loss_sup: 0.012310, loss_mps: 0.171611, loss_cps: 0.318490
[14:02:07.983] iteration 23670: total_loss: 0.434933, loss_sup: 0.045872, loss_mps: 0.135860, loss_cps: 0.253201
[14:02:08.131] iteration 23671: total_loss: 0.366307, loss_sup: 0.071666, loss_mps: 0.109990, loss_cps: 0.184651
[14:02:08.277] iteration 23672: total_loss: 0.478123, loss_sup: 0.053621, loss_mps: 0.145209, loss_cps: 0.279293
[14:02:08.423] iteration 23673: total_loss: 0.417620, loss_sup: 0.036244, loss_mps: 0.136547, loss_cps: 0.244829
[14:02:08.570] iteration 23674: total_loss: 0.525812, loss_sup: 0.006594, loss_mps: 0.176178, loss_cps: 0.343040
[14:02:08.716] iteration 23675: total_loss: 0.342965, loss_sup: 0.049506, loss_mps: 0.110045, loss_cps: 0.183414
[14:02:08.862] iteration 23676: total_loss: 0.354578, loss_sup: 0.046207, loss_mps: 0.118540, loss_cps: 0.189831
[14:02:09.009] iteration 23677: total_loss: 0.356623, loss_sup: 0.060641, loss_mps: 0.108481, loss_cps: 0.187501
[14:02:09.155] iteration 23678: total_loss: 0.483643, loss_sup: 0.089881, loss_mps: 0.138241, loss_cps: 0.255522
[14:02:09.301] iteration 23679: total_loss: 0.389374, loss_sup: 0.072576, loss_mps: 0.114129, loss_cps: 0.202669
[14:02:09.447] iteration 23680: total_loss: 0.413021, loss_sup: 0.011645, loss_mps: 0.139841, loss_cps: 0.261535
[14:02:09.594] iteration 23681: total_loss: 0.551100, loss_sup: 0.097484, loss_mps: 0.159093, loss_cps: 0.294523
[14:02:09.742] iteration 23682: total_loss: 0.274310, loss_sup: 0.019436, loss_mps: 0.093373, loss_cps: 0.161501
[14:02:09.888] iteration 23683: total_loss: 0.265838, loss_sup: 0.023397, loss_mps: 0.094417, loss_cps: 0.148024
[14:02:10.034] iteration 23684: total_loss: 0.404854, loss_sup: 0.020378, loss_mps: 0.139120, loss_cps: 0.245355
[14:02:10.180] iteration 23685: total_loss: 0.402277, loss_sup: 0.109610, loss_mps: 0.105672, loss_cps: 0.186996
[14:02:10.327] iteration 23686: total_loss: 0.546682, loss_sup: 0.048683, loss_mps: 0.150931, loss_cps: 0.347068
[14:02:10.473] iteration 23687: total_loss: 0.335002, loss_sup: 0.069875, loss_mps: 0.097249, loss_cps: 0.167879
[14:02:10.620] iteration 23688: total_loss: 0.458125, loss_sup: 0.027446, loss_mps: 0.143541, loss_cps: 0.287137
[14:02:10.766] iteration 23689: total_loss: 0.396999, loss_sup: 0.080963, loss_mps: 0.112665, loss_cps: 0.203371
[14:02:10.916] iteration 23690: total_loss: 0.504558, loss_sup: 0.072993, loss_mps: 0.144371, loss_cps: 0.287194
[14:02:11.062] iteration 23691: total_loss: 0.432710, loss_sup: 0.029258, loss_mps: 0.151209, loss_cps: 0.252243
[14:02:11.209] iteration 23692: total_loss: 0.317066, loss_sup: 0.005287, loss_mps: 0.106325, loss_cps: 0.205454
[14:02:11.356] iteration 23693: total_loss: 0.280936, loss_sup: 0.012985, loss_mps: 0.090712, loss_cps: 0.177240
[14:02:11.503] iteration 23694: total_loss: 0.278180, loss_sup: 0.031227, loss_mps: 0.085449, loss_cps: 0.161503
[14:02:11.652] iteration 23695: total_loss: 0.312462, loss_sup: 0.046671, loss_mps: 0.094237, loss_cps: 0.171555
[14:02:11.802] iteration 23696: total_loss: 0.340141, loss_sup: 0.016884, loss_mps: 0.108591, loss_cps: 0.214665
[14:02:11.949] iteration 23697: total_loss: 0.399847, loss_sup: 0.143438, loss_mps: 0.090962, loss_cps: 0.165447
[14:02:12.095] iteration 23698: total_loss: 0.287780, loss_sup: 0.019820, loss_mps: 0.094809, loss_cps: 0.173151
[14:02:12.242] iteration 23699: total_loss: 0.368030, loss_sup: 0.061619, loss_mps: 0.110939, loss_cps: 0.195472
[14:02:12.388] iteration 23700: total_loss: 1.040815, loss_sup: 0.004143, loss_mps: 0.307165, loss_cps: 0.729506
[14:02:12.389] Evaluation Started ==>
[14:02:23.765] ==> valid iteration 23700: unet metrics: {'dc': 0.6399783190215674, 'jc': 0.5251094966699708, 'pre': 0.7946540279756222, 'hd': 5.440152880389255}, ynet metrics: {'dc': 0.6192928297968846, 'jc': 0.503662817103888, 'pre': 0.8016193061353609, 'hd': 5.397752892349613}.
[14:02:23.768] Evaluation Finished!⏹️
[14:02:23.920] iteration 23701: total_loss: 0.340046, loss_sup: 0.035787, loss_mps: 0.106491, loss_cps: 0.197769
[14:02:24.068] iteration 23702: total_loss: 0.565049, loss_sup: 0.026148, loss_mps: 0.182954, loss_cps: 0.355948
[14:02:24.214] iteration 23703: total_loss: 0.250927, loss_sup: 0.005698, loss_mps: 0.089379, loss_cps: 0.155850
[14:02:24.358] iteration 23704: total_loss: 0.712834, loss_sup: 0.104432, loss_mps: 0.193971, loss_cps: 0.414431
[14:02:24.503] iteration 23705: total_loss: 0.509649, loss_sup: 0.110659, loss_mps: 0.133811, loss_cps: 0.265179
[14:02:24.648] iteration 23706: total_loss: 0.555030, loss_sup: 0.049881, loss_mps: 0.157544, loss_cps: 0.347605
[14:02:24.794] iteration 23707: total_loss: 0.326817, loss_sup: 0.087271, loss_mps: 0.082501, loss_cps: 0.157046
[14:02:24.941] iteration 23708: total_loss: 0.273934, loss_sup: 0.009341, loss_mps: 0.094040, loss_cps: 0.170553
[14:02:25.087] iteration 23709: total_loss: 0.554003, loss_sup: 0.029435, loss_mps: 0.165981, loss_cps: 0.358588
[14:02:25.232] iteration 23710: total_loss: 0.474091, loss_sup: 0.152453, loss_mps: 0.109787, loss_cps: 0.211850
[14:02:25.377] iteration 23711: total_loss: 0.394115, loss_sup: 0.021849, loss_mps: 0.130799, loss_cps: 0.241467
[14:02:25.523] iteration 23712: total_loss: 0.324645, loss_sup: 0.077680, loss_mps: 0.086441, loss_cps: 0.160524
[14:02:25.669] iteration 23713: total_loss: 0.613856, loss_sup: 0.032470, loss_mps: 0.186525, loss_cps: 0.394861
[14:02:25.815] iteration 23714: total_loss: 0.695667, loss_sup: 0.075901, loss_mps: 0.211791, loss_cps: 0.407975
[14:02:25.961] iteration 23715: total_loss: 0.777509, loss_sup: 0.099343, loss_mps: 0.214375, loss_cps: 0.463791
[14:02:26.109] iteration 23716: total_loss: 0.365103, loss_sup: 0.045931, loss_mps: 0.115258, loss_cps: 0.203915
[14:02:26.255] iteration 23717: total_loss: 0.643408, loss_sup: 0.128481, loss_mps: 0.167917, loss_cps: 0.347010
[14:02:26.401] iteration 23718: total_loss: 1.351731, loss_sup: 0.469960, loss_mps: 0.274847, loss_cps: 0.606924
[14:02:26.546] iteration 23719: total_loss: 0.755113, loss_sup: 0.076084, loss_mps: 0.214663, loss_cps: 0.464366
[14:02:26.692] iteration 23720: total_loss: 0.441193, loss_sup: 0.024171, loss_mps: 0.134651, loss_cps: 0.282371
[14:02:26.838] iteration 23721: total_loss: 0.383555, loss_sup: 0.073825, loss_mps: 0.108205, loss_cps: 0.201524
[14:02:26.983] iteration 23722: total_loss: 0.273477, loss_sup: 0.009053, loss_mps: 0.098949, loss_cps: 0.165475
[14:02:27.129] iteration 23723: total_loss: 0.369006, loss_sup: 0.022470, loss_mps: 0.117393, loss_cps: 0.229143
[14:02:27.274] iteration 23724: total_loss: 0.485634, loss_sup: 0.010095, loss_mps: 0.160731, loss_cps: 0.314808
[14:02:27.420] iteration 23725: total_loss: 0.370497, loss_sup: 0.015565, loss_mps: 0.128582, loss_cps: 0.226350
[14:02:27.565] iteration 23726: total_loss: 0.686540, loss_sup: 0.045934, loss_mps: 0.193475, loss_cps: 0.447131
[14:02:27.710] iteration 23727: total_loss: 1.006790, loss_sup: 0.178935, loss_mps: 0.259323, loss_cps: 0.568532
[14:02:27.858] iteration 23728: total_loss: 0.557310, loss_sup: 0.041211, loss_mps: 0.164903, loss_cps: 0.351196
[14:02:28.006] iteration 23729: total_loss: 0.683954, loss_sup: 0.016830, loss_mps: 0.212294, loss_cps: 0.454830
[14:02:28.154] iteration 23730: total_loss: 0.677140, loss_sup: 0.177731, loss_mps: 0.180139, loss_cps: 0.319270
[14:02:28.301] iteration 23731: total_loss: 0.654933, loss_sup: 0.126073, loss_mps: 0.183582, loss_cps: 0.345278
[14:02:28.447] iteration 23732: total_loss: 0.467808, loss_sup: 0.068292, loss_mps: 0.143495, loss_cps: 0.256021
[14:02:28.593] iteration 23733: total_loss: 0.439174, loss_sup: 0.056769, loss_mps: 0.137211, loss_cps: 0.245195
[14:02:28.739] iteration 23734: total_loss: 0.624741, loss_sup: 0.025568, loss_mps: 0.188617, loss_cps: 0.410557
[14:02:28.884] iteration 23735: total_loss: 0.305754, loss_sup: 0.014761, loss_mps: 0.103262, loss_cps: 0.187731
[14:02:29.029] iteration 23736: total_loss: 0.581773, loss_sup: 0.044293, loss_mps: 0.185721, loss_cps: 0.351759
[14:02:29.175] iteration 23737: total_loss: 0.735930, loss_sup: 0.146465, loss_mps: 0.188136, loss_cps: 0.401330
[14:02:29.320] iteration 23738: total_loss: 1.021067, loss_sup: 0.125918, loss_mps: 0.289802, loss_cps: 0.605347
[14:02:29.466] iteration 23739: total_loss: 0.378798, loss_sup: 0.023335, loss_mps: 0.125887, loss_cps: 0.229575
[14:02:29.612] iteration 23740: total_loss: 0.265801, loss_sup: 0.013470, loss_mps: 0.089962, loss_cps: 0.162369
[14:02:29.758] iteration 23741: total_loss: 0.509469, loss_sup: 0.015646, loss_mps: 0.174708, loss_cps: 0.319114
[14:02:29.903] iteration 23742: total_loss: 0.335392, loss_sup: 0.041438, loss_mps: 0.103900, loss_cps: 0.190053
[14:02:30.051] iteration 23743: total_loss: 0.508388, loss_sup: 0.021155, loss_mps: 0.159480, loss_cps: 0.327753
[14:02:30.196] iteration 23744: total_loss: 0.450293, loss_sup: 0.018660, loss_mps: 0.145374, loss_cps: 0.286260
[14:02:30.342] iteration 23745: total_loss: 1.828651, loss_sup: 0.452916, loss_mps: 0.415134, loss_cps: 0.960601
[14:02:30.489] iteration 23746: total_loss: 0.488019, loss_sup: 0.056467, loss_mps: 0.148377, loss_cps: 0.283174
[14:02:30.634] iteration 23747: total_loss: 0.270678, loss_sup: 0.002700, loss_mps: 0.097884, loss_cps: 0.170094
[14:02:30.780] iteration 23748: total_loss: 0.292864, loss_sup: 0.020661, loss_mps: 0.097959, loss_cps: 0.174245
[14:02:30.927] iteration 23749: total_loss: 0.613319, loss_sup: 0.040639, loss_mps: 0.201732, loss_cps: 0.370948
[14:02:31.073] iteration 23750: total_loss: 0.442528, loss_sup: 0.076874, loss_mps: 0.129229, loss_cps: 0.236425
[14:02:31.219] iteration 23751: total_loss: 0.362386, loss_sup: 0.016502, loss_mps: 0.123041, loss_cps: 0.222843
[14:02:31.366] iteration 23752: total_loss: 0.684358, loss_sup: 0.090250, loss_mps: 0.193997, loss_cps: 0.400111
[14:02:31.512] iteration 23753: total_loss: 0.683361, loss_sup: 0.255144, loss_mps: 0.151941, loss_cps: 0.276277
[14:02:31.658] iteration 23754: total_loss: 0.564757, loss_sup: 0.126489, loss_mps: 0.155223, loss_cps: 0.283044
[14:02:31.803] iteration 23755: total_loss: 0.665562, loss_sup: 0.143687, loss_mps: 0.174401, loss_cps: 0.347474
[14:02:31.949] iteration 23756: total_loss: 0.405653, loss_sup: 0.141151, loss_mps: 0.103153, loss_cps: 0.161349
[14:02:32.095] iteration 23757: total_loss: 1.091656, loss_sup: 0.300144, loss_mps: 0.255928, loss_cps: 0.535585
[14:02:32.241] iteration 23758: total_loss: 0.468958, loss_sup: 0.009874, loss_mps: 0.156188, loss_cps: 0.302896
[14:02:32.386] iteration 23759: total_loss: 0.240350, loss_sup: 0.012027, loss_mps: 0.086659, loss_cps: 0.141664
[14:02:32.532] iteration 23760: total_loss: 0.592133, loss_sup: 0.041729, loss_mps: 0.182724, loss_cps: 0.367680
[14:02:32.678] iteration 23761: total_loss: 0.370007, loss_sup: 0.033548, loss_mps: 0.124151, loss_cps: 0.212308
[14:02:32.824] iteration 23762: total_loss: 0.836084, loss_sup: 0.027736, loss_mps: 0.250261, loss_cps: 0.558086
[14:02:32.969] iteration 23763: total_loss: 0.871109, loss_sup: 0.247389, loss_mps: 0.205022, loss_cps: 0.418699
[14:02:33.115] iteration 23764: total_loss: 0.562060, loss_sup: 0.075209, loss_mps: 0.168381, loss_cps: 0.318471
[14:02:33.260] iteration 23765: total_loss: 0.444095, loss_sup: 0.016561, loss_mps: 0.143707, loss_cps: 0.283827
[14:02:33.406] iteration 23766: total_loss: 0.352144, loss_sup: 0.018877, loss_mps: 0.118087, loss_cps: 0.215180
[14:02:33.551] iteration 23767: total_loss: 0.423725, loss_sup: 0.021172, loss_mps: 0.134186, loss_cps: 0.268367
[14:02:33.699] iteration 23768: total_loss: 0.495862, loss_sup: 0.007374, loss_mps: 0.168488, loss_cps: 0.320000
[14:02:33.845] iteration 23769: total_loss: 0.862065, loss_sup: 0.131382, loss_mps: 0.241208, loss_cps: 0.489475
[14:02:33.991] iteration 23770: total_loss: 0.721327, loss_sup: 0.145553, loss_mps: 0.198131, loss_cps: 0.377643
[14:02:34.138] iteration 23771: total_loss: 0.626759, loss_sup: 0.098155, loss_mps: 0.174316, loss_cps: 0.354288
[14:02:34.285] iteration 23772: total_loss: 0.454186, loss_sup: 0.132913, loss_mps: 0.120715, loss_cps: 0.200558
[14:02:34.432] iteration 23773: total_loss: 0.582754, loss_sup: 0.044244, loss_mps: 0.185213, loss_cps: 0.353297
[14:02:34.578] iteration 23774: total_loss: 0.393375, loss_sup: 0.041538, loss_mps: 0.132986, loss_cps: 0.218851
[14:02:34.724] iteration 23775: total_loss: 0.302838, loss_sup: 0.033934, loss_mps: 0.100191, loss_cps: 0.168713
[14:02:34.874] iteration 23776: total_loss: 0.509167, loss_sup: 0.158290, loss_mps: 0.122817, loss_cps: 0.228059
[14:02:35.020] iteration 23777: total_loss: 0.629524, loss_sup: 0.089116, loss_mps: 0.182868, loss_cps: 0.357540
[14:02:35.167] iteration 23778: total_loss: 0.400843, loss_sup: 0.018861, loss_mps: 0.133754, loss_cps: 0.248227
[14:02:35.312] iteration 23779: total_loss: 0.422374, loss_sup: 0.053144, loss_mps: 0.124159, loss_cps: 0.245071
[14:02:35.460] iteration 23780: total_loss: 0.224037, loss_sup: 0.010977, loss_mps: 0.078844, loss_cps: 0.134216
[14:02:35.605] iteration 23781: total_loss: 0.304941, loss_sup: 0.026988, loss_mps: 0.111600, loss_cps: 0.166354
[14:02:35.751] iteration 23782: total_loss: 0.945646, loss_sup: 0.114956, loss_mps: 0.263061, loss_cps: 0.567628
[14:02:35.896] iteration 23783: total_loss: 0.697518, loss_sup: 0.103906, loss_mps: 0.207272, loss_cps: 0.386341
[14:02:36.043] iteration 23784: total_loss: 0.329791, loss_sup: 0.056120, loss_mps: 0.098253, loss_cps: 0.175419
[14:02:36.189] iteration 23785: total_loss: 0.486631, loss_sup: 0.062558, loss_mps: 0.146566, loss_cps: 0.277507
[14:02:36.334] iteration 23786: total_loss: 0.511497, loss_sup: 0.003165, loss_mps: 0.174496, loss_cps: 0.333835
[14:02:36.479] iteration 23787: total_loss: 0.533702, loss_sup: 0.071449, loss_mps: 0.161503, loss_cps: 0.300750
[14:02:36.627] iteration 23788: total_loss: 0.430528, loss_sup: 0.005655, loss_mps: 0.144241, loss_cps: 0.280632
[14:02:36.772] iteration 23789: total_loss: 0.270559, loss_sup: 0.006084, loss_mps: 0.097411, loss_cps: 0.167065
[14:02:36.918] iteration 23790: total_loss: 0.743288, loss_sup: 0.019602, loss_mps: 0.235608, loss_cps: 0.488079
[14:02:37.064] iteration 23791: total_loss: 0.577587, loss_sup: 0.015527, loss_mps: 0.178994, loss_cps: 0.383066
[14:02:37.211] iteration 23792: total_loss: 0.998363, loss_sup: 0.479959, loss_mps: 0.171435, loss_cps: 0.346969
[14:02:37.358] iteration 23793: total_loss: 0.380853, loss_sup: 0.019831, loss_mps: 0.128849, loss_cps: 0.232173
[14:02:37.504] iteration 23794: total_loss: 0.431163, loss_sup: 0.038910, loss_mps: 0.139732, loss_cps: 0.252521
[14:02:37.650] iteration 23795: total_loss: 0.491500, loss_sup: 0.061367, loss_mps: 0.147323, loss_cps: 0.282810
[14:02:37.796] iteration 23796: total_loss: 0.495411, loss_sup: 0.059227, loss_mps: 0.150543, loss_cps: 0.285641
[14:02:37.942] iteration 23797: total_loss: 0.628078, loss_sup: 0.018943, loss_mps: 0.203076, loss_cps: 0.406059
[14:02:38.088] iteration 23798: total_loss: 0.555501, loss_sup: 0.081273, loss_mps: 0.152399, loss_cps: 0.321829
[14:02:38.234] iteration 23799: total_loss: 0.317384, loss_sup: 0.014063, loss_mps: 0.114068, loss_cps: 0.189253
[14:02:38.379] iteration 23800: total_loss: 0.601949, loss_sup: 0.128051, loss_mps: 0.162393, loss_cps: 0.311505
[14:02:38.379] Evaluation Started ==>
[14:02:49.707] ==> valid iteration 23800: unet metrics: {'dc': 0.6483435339508148, 'jc': 0.5321835352608203, 'pre': 0.7841319124780538, 'hd': 5.508969188240304}, ynet metrics: {'dc': 0.6020117172993918, 'jc': 0.48828961257410325, 'pre': 0.7980810716808164, 'hd': 5.367313976368062}.
[14:02:49.709] Evaluation Finished!⏹️
[14:02:49.860] iteration 23801: total_loss: 0.360632, loss_sup: 0.046216, loss_mps: 0.109794, loss_cps: 0.204622
[14:02:50.006] iteration 23802: total_loss: 0.231473, loss_sup: 0.017279, loss_mps: 0.085131, loss_cps: 0.129064
[14:02:50.152] iteration 23803: total_loss: 0.385067, loss_sup: 0.023363, loss_mps: 0.128045, loss_cps: 0.233659
[14:02:50.297] iteration 23804: total_loss: 0.332140, loss_sup: 0.032736, loss_mps: 0.111813, loss_cps: 0.187590
[14:02:50.444] iteration 23805: total_loss: 0.316620, loss_sup: 0.109948, loss_mps: 0.078370, loss_cps: 0.128302
[14:02:50.590] iteration 23806: total_loss: 0.750342, loss_sup: 0.379287, loss_mps: 0.131458, loss_cps: 0.239597
[14:02:50.735] iteration 23807: total_loss: 0.787651, loss_sup: 0.355906, loss_mps: 0.157294, loss_cps: 0.274452
[14:02:50.880] iteration 23808: total_loss: 0.883470, loss_sup: 0.100348, loss_mps: 0.235887, loss_cps: 0.547235
[14:02:51.029] iteration 23809: total_loss: 0.424412, loss_sup: 0.064216, loss_mps: 0.131211, loss_cps: 0.228985
[14:02:51.180] iteration 23810: total_loss: 0.515868, loss_sup: 0.108433, loss_mps: 0.130268, loss_cps: 0.277167
[14:02:51.326] iteration 23811: total_loss: 0.302636, loss_sup: 0.031940, loss_mps: 0.103102, loss_cps: 0.167593
[14:02:51.472] iteration 23812: total_loss: 0.468590, loss_sup: 0.040491, loss_mps: 0.148448, loss_cps: 0.279651
[14:02:51.617] iteration 23813: total_loss: 0.528358, loss_sup: 0.095763, loss_mps: 0.145079, loss_cps: 0.287515
[14:02:51.763] iteration 23814: total_loss: 0.504970, loss_sup: 0.055975, loss_mps: 0.157305, loss_cps: 0.291691
[14:02:51.908] iteration 23815: total_loss: 0.392671, loss_sup: 0.058766, loss_mps: 0.117239, loss_cps: 0.216665
[14:02:52.053] iteration 23816: total_loss: 0.525111, loss_sup: 0.061717, loss_mps: 0.160468, loss_cps: 0.302925
[14:02:52.199] iteration 23817: total_loss: 0.524431, loss_sup: 0.086636, loss_mps: 0.163076, loss_cps: 0.274719
[14:02:52.344] iteration 23818: total_loss: 0.352216, loss_sup: 0.012461, loss_mps: 0.124953, loss_cps: 0.214802
[14:02:52.491] iteration 23819: total_loss: 0.396909, loss_sup: 0.062088, loss_mps: 0.122370, loss_cps: 0.212450
[14:02:52.637] iteration 23820: total_loss: 0.338932, loss_sup: 0.010674, loss_mps: 0.120892, loss_cps: 0.207365
[14:02:52.782] iteration 23821: total_loss: 0.344867, loss_sup: 0.019556, loss_mps: 0.114706, loss_cps: 0.210605
[14:02:52.928] iteration 23822: total_loss: 0.330949, loss_sup: 0.040005, loss_mps: 0.106070, loss_cps: 0.184874
[14:02:53.073] iteration 23823: total_loss: 0.623528, loss_sup: 0.113317, loss_mps: 0.167891, loss_cps: 0.342319
[14:02:53.218] iteration 23824: total_loss: 0.600084, loss_sup: 0.023717, loss_mps: 0.183705, loss_cps: 0.392662
[14:02:53.364] iteration 23825: total_loss: 0.386193, loss_sup: 0.065576, loss_mps: 0.113676, loss_cps: 0.206941
[14:02:53.427] iteration 23826: total_loss: 0.161817, loss_sup: 0.002702, loss_mps: 0.067820, loss_cps: 0.091295
[14:02:54.635] iteration 23827: total_loss: 0.416552, loss_sup: 0.025179, loss_mps: 0.130899, loss_cps: 0.260475
[14:02:54.785] iteration 23828: total_loss: 0.673123, loss_sup: 0.045400, loss_mps: 0.210015, loss_cps: 0.417709
[14:02:54.933] iteration 23829: total_loss: 0.400834, loss_sup: 0.042577, loss_mps: 0.122646, loss_cps: 0.235611
[14:02:55.082] iteration 23830: total_loss: 0.524286, loss_sup: 0.074454, loss_mps: 0.158495, loss_cps: 0.291338
[14:02:55.228] iteration 23831: total_loss: 0.267861, loss_sup: 0.070198, loss_mps: 0.068484, loss_cps: 0.129178
[14:02:55.375] iteration 23832: total_loss: 0.832707, loss_sup: 0.069793, loss_mps: 0.240856, loss_cps: 0.522058
[14:02:55.523] iteration 23833: total_loss: 0.948575, loss_sup: 0.054186, loss_mps: 0.284907, loss_cps: 0.609481
[14:02:55.670] iteration 23834: total_loss: 0.251688, loss_sup: 0.001923, loss_mps: 0.089049, loss_cps: 0.160716
[14:02:55.816] iteration 23835: total_loss: 0.222301, loss_sup: 0.024784, loss_mps: 0.072257, loss_cps: 0.125259
[14:02:55.963] iteration 23836: total_loss: 0.417111, loss_sup: 0.142376, loss_mps: 0.099036, loss_cps: 0.175698
[14:02:56.111] iteration 23837: total_loss: 0.295388, loss_sup: 0.003714, loss_mps: 0.102274, loss_cps: 0.189401
[14:02:56.257] iteration 23838: total_loss: 0.637985, loss_sup: 0.162610, loss_mps: 0.153329, loss_cps: 0.322046
[14:02:56.404] iteration 23839: total_loss: 0.424939, loss_sup: 0.077783, loss_mps: 0.123936, loss_cps: 0.223219
[14:02:56.551] iteration 23840: total_loss: 0.773157, loss_sup: 0.076836, loss_mps: 0.216984, loss_cps: 0.479337
[14:02:56.698] iteration 23841: total_loss: 0.408357, loss_sup: 0.011861, loss_mps: 0.134023, loss_cps: 0.262472
[14:02:56.844] iteration 23842: total_loss: 0.496304, loss_sup: 0.150598, loss_mps: 0.115631, loss_cps: 0.230075
[14:02:56.991] iteration 23843: total_loss: 0.563848, loss_sup: 0.084694, loss_mps: 0.162015, loss_cps: 0.317139
[14:02:57.139] iteration 23844: total_loss: 1.158666, loss_sup: 0.102356, loss_mps: 0.325123, loss_cps: 0.731188
[14:02:57.286] iteration 23845: total_loss: 0.307730, loss_sup: 0.072849, loss_mps: 0.084281, loss_cps: 0.150600
[14:02:57.432] iteration 23846: total_loss: 0.249710, loss_sup: 0.030344, loss_mps: 0.078115, loss_cps: 0.141251
[14:02:57.579] iteration 23847: total_loss: 0.468526, loss_sup: 0.048974, loss_mps: 0.138910, loss_cps: 0.280642
[14:02:57.725] iteration 23848: total_loss: 0.912116, loss_sup: 0.124410, loss_mps: 0.236221, loss_cps: 0.551485
[14:02:57.872] iteration 23849: total_loss: 0.432506, loss_sup: 0.047862, loss_mps: 0.129013, loss_cps: 0.255631
[14:02:58.022] iteration 23850: total_loss: 0.394161, loss_sup: 0.082894, loss_mps: 0.112272, loss_cps: 0.198996
[14:02:58.168] iteration 23851: total_loss: 0.601620, loss_sup: 0.038400, loss_mps: 0.183607, loss_cps: 0.379614
[14:02:58.315] iteration 23852: total_loss: 1.086604, loss_sup: 0.113871, loss_mps: 0.292788, loss_cps: 0.679945
[14:02:58.465] iteration 23853: total_loss: 0.727456, loss_sup: 0.112200, loss_mps: 0.204484, loss_cps: 0.410772
[14:02:58.611] iteration 23854: total_loss: 0.356109, loss_sup: 0.044349, loss_mps: 0.112134, loss_cps: 0.199626
[14:02:58.758] iteration 23855: total_loss: 0.743103, loss_sup: 0.106076, loss_mps: 0.198303, loss_cps: 0.438724
[14:02:58.904] iteration 23856: total_loss: 0.238291, loss_sup: 0.010276, loss_mps: 0.082800, loss_cps: 0.145216
[14:02:59.053] iteration 23857: total_loss: 0.437246, loss_sup: 0.016311, loss_mps: 0.142135, loss_cps: 0.278799
[14:02:59.200] iteration 23858: total_loss: 0.652841, loss_sup: 0.194087, loss_mps: 0.161291, loss_cps: 0.297463
[14:02:59.347] iteration 23859: total_loss: 0.415838, loss_sup: 0.057833, loss_mps: 0.123046, loss_cps: 0.234959
[14:02:59.493] iteration 23860: total_loss: 0.832038, loss_sup: 0.265920, loss_mps: 0.199621, loss_cps: 0.366497
[14:02:59.640] iteration 23861: total_loss: 0.434652, loss_sup: 0.075938, loss_mps: 0.125761, loss_cps: 0.232953
[14:02:59.787] iteration 23862: total_loss: 0.409410, loss_sup: 0.015899, loss_mps: 0.141102, loss_cps: 0.252408
[14:02:59.933] iteration 23863: total_loss: 0.548675, loss_sup: 0.082666, loss_mps: 0.157881, loss_cps: 0.308127
[14:03:00.079] iteration 23864: total_loss: 0.630771, loss_sup: 0.088449, loss_mps: 0.188345, loss_cps: 0.353977
[14:03:00.226] iteration 23865: total_loss: 0.487811, loss_sup: 0.005357, loss_mps: 0.156380, loss_cps: 0.326074
[14:03:00.372] iteration 23866: total_loss: 0.391911, loss_sup: 0.019879, loss_mps: 0.128233, loss_cps: 0.243799
[14:03:00.519] iteration 23867: total_loss: 0.579246, loss_sup: 0.018009, loss_mps: 0.195646, loss_cps: 0.365591
[14:03:00.666] iteration 23868: total_loss: 0.353571, loss_sup: 0.026145, loss_mps: 0.122777, loss_cps: 0.204649
[14:03:00.813] iteration 23869: total_loss: 0.385659, loss_sup: 0.037670, loss_mps: 0.122027, loss_cps: 0.225963
[14:03:00.960] iteration 23870: total_loss: 0.379203, loss_sup: 0.024428, loss_mps: 0.127727, loss_cps: 0.227049
[14:03:01.106] iteration 23871: total_loss: 0.321057, loss_sup: 0.038189, loss_mps: 0.106998, loss_cps: 0.175870
[14:03:01.252] iteration 23872: total_loss: 0.823953, loss_sup: 0.062070, loss_mps: 0.247671, loss_cps: 0.514212
[14:03:01.398] iteration 23873: total_loss: 0.452386, loss_sup: 0.042139, loss_mps: 0.147194, loss_cps: 0.263053
[14:03:01.544] iteration 23874: total_loss: 0.396420, loss_sup: 0.095275, loss_mps: 0.103983, loss_cps: 0.197162
[14:03:01.691] iteration 23875: total_loss: 0.374475, loss_sup: 0.028789, loss_mps: 0.131459, loss_cps: 0.214227
[14:03:01.837] iteration 23876: total_loss: 0.421973, loss_sup: 0.067807, loss_mps: 0.137110, loss_cps: 0.217056
[14:03:01.984] iteration 23877: total_loss: 0.494271, loss_sup: 0.236359, loss_mps: 0.091201, loss_cps: 0.166711
[14:03:02.130] iteration 23878: total_loss: 0.625297, loss_sup: 0.110305, loss_mps: 0.166360, loss_cps: 0.348633
[14:03:02.277] iteration 23879: total_loss: 0.429512, loss_sup: 0.069795, loss_mps: 0.128555, loss_cps: 0.231161
[14:03:02.423] iteration 23880: total_loss: 0.958605, loss_sup: 0.109444, loss_mps: 0.272124, loss_cps: 0.577037
[14:03:02.569] iteration 23881: total_loss: 0.929974, loss_sup: 0.097292, loss_mps: 0.260415, loss_cps: 0.572268
[14:03:02.716] iteration 23882: total_loss: 0.346661, loss_sup: 0.040648, loss_mps: 0.121928, loss_cps: 0.184085
[14:03:02.862] iteration 23883: total_loss: 0.430650, loss_sup: 0.070776, loss_mps: 0.126776, loss_cps: 0.233098
[14:03:03.008] iteration 23884: total_loss: 0.713408, loss_sup: 0.059415, loss_mps: 0.220321, loss_cps: 0.433672
[14:03:03.155] iteration 23885: total_loss: 0.300889, loss_sup: 0.038482, loss_mps: 0.091936, loss_cps: 0.170471
[14:03:03.301] iteration 23886: total_loss: 0.744897, loss_sup: 0.291225, loss_mps: 0.152471, loss_cps: 0.301200
[14:03:03.447] iteration 23887: total_loss: 0.363371, loss_sup: 0.060112, loss_mps: 0.104468, loss_cps: 0.198791
[14:03:03.594] iteration 23888: total_loss: 0.655189, loss_sup: 0.221641, loss_mps: 0.141057, loss_cps: 0.292491
[14:03:03.740] iteration 23889: total_loss: 0.289280, loss_sup: 0.056270, loss_mps: 0.091018, loss_cps: 0.141991
[14:03:03.889] iteration 23890: total_loss: 0.357649, loss_sup: 0.010661, loss_mps: 0.128165, loss_cps: 0.218823
[14:03:04.036] iteration 23891: total_loss: 0.318152, loss_sup: 0.015149, loss_mps: 0.110883, loss_cps: 0.192121
[14:03:04.182] iteration 23892: total_loss: 0.319596, loss_sup: 0.008032, loss_mps: 0.111542, loss_cps: 0.200021
[14:03:04.329] iteration 23893: total_loss: 0.407023, loss_sup: 0.007094, loss_mps: 0.139664, loss_cps: 0.260265
[14:03:04.476] iteration 23894: total_loss: 0.371578, loss_sup: 0.152108, loss_mps: 0.082583, loss_cps: 0.136887
[14:03:04.624] iteration 23895: total_loss: 0.411897, loss_sup: 0.009018, loss_mps: 0.135965, loss_cps: 0.266914
[14:03:04.770] iteration 23896: total_loss: 0.731665, loss_sup: 0.093391, loss_mps: 0.213459, loss_cps: 0.424815
[14:03:04.917] iteration 23897: total_loss: 0.503009, loss_sup: 0.032808, loss_mps: 0.148304, loss_cps: 0.321897
[14:03:05.064] iteration 23898: total_loss: 0.520401, loss_sup: 0.015550, loss_mps: 0.169888, loss_cps: 0.334963
[14:03:05.210] iteration 23899: total_loss: 0.443480, loss_sup: 0.066413, loss_mps: 0.133125, loss_cps: 0.243941
[14:03:05.356] iteration 23900: total_loss: 0.296238, loss_sup: 0.046720, loss_mps: 0.094115, loss_cps: 0.155403
[14:03:05.357] Evaluation Started ==>
[14:03:16.701] ==> valid iteration 23900: unet metrics: {'dc': 0.6647228839017898, 'jc': 0.5503332666958414, 'pre': 0.8014794412521176, 'hd': 5.40868466071654}, ynet metrics: {'dc': 0.6242740103147053, 'jc': 0.5103077981116052, 'pre': 0.8137977441145142, 'hd': 5.3680793761852055}.
[14:03:16.703] Evaluation Finished!⏹️
[14:03:16.858] iteration 23901: total_loss: 0.248323, loss_sup: 0.015766, loss_mps: 0.086776, loss_cps: 0.145781
[14:03:17.005] iteration 23902: total_loss: 0.328174, loss_sup: 0.077000, loss_mps: 0.093246, loss_cps: 0.157928
[14:03:17.152] iteration 23903: total_loss: 0.479321, loss_sup: 0.016424, loss_mps: 0.155427, loss_cps: 0.307470
[14:03:17.298] iteration 23904: total_loss: 0.605095, loss_sup: 0.088644, loss_mps: 0.169030, loss_cps: 0.347420
[14:03:17.443] iteration 23905: total_loss: 0.555074, loss_sup: 0.094301, loss_mps: 0.149592, loss_cps: 0.311181
[14:03:17.591] iteration 23906: total_loss: 0.450207, loss_sup: 0.188005, loss_mps: 0.094887, loss_cps: 0.167315
[14:03:17.737] iteration 23907: total_loss: 0.696657, loss_sup: 0.178670, loss_mps: 0.179608, loss_cps: 0.338379
[14:03:17.884] iteration 23908: total_loss: 0.724198, loss_sup: 0.044582, loss_mps: 0.242023, loss_cps: 0.437593
[14:03:18.029] iteration 23909: total_loss: 0.539598, loss_sup: 0.029258, loss_mps: 0.167482, loss_cps: 0.342859
[14:03:18.175] iteration 23910: total_loss: 0.760785, loss_sup: 0.113632, loss_mps: 0.223480, loss_cps: 0.423673
[14:03:18.321] iteration 23911: total_loss: 0.840207, loss_sup: 0.372671, loss_mps: 0.167886, loss_cps: 0.299651
[14:03:18.466] iteration 23912: total_loss: 0.220493, loss_sup: 0.011252, loss_mps: 0.080933, loss_cps: 0.128308
[14:03:18.611] iteration 23913: total_loss: 0.630002, loss_sup: 0.013081, loss_mps: 0.182970, loss_cps: 0.433951
[14:03:18.758] iteration 23914: total_loss: 0.287135, loss_sup: 0.036826, loss_mps: 0.090698, loss_cps: 0.159610
[14:03:18.905] iteration 23915: total_loss: 0.469328, loss_sup: 0.121393, loss_mps: 0.128838, loss_cps: 0.219097
[14:03:19.051] iteration 23916: total_loss: 0.867500, loss_sup: 0.210742, loss_mps: 0.208782, loss_cps: 0.447976
[14:03:19.199] iteration 23917: total_loss: 0.327057, loss_sup: 0.068593, loss_mps: 0.094406, loss_cps: 0.164058
[14:03:19.347] iteration 23918: total_loss: 0.286603, loss_sup: 0.003077, loss_mps: 0.100048, loss_cps: 0.183478
[14:03:19.496] iteration 23919: total_loss: 0.415486, loss_sup: 0.019821, loss_mps: 0.132859, loss_cps: 0.262805
[14:03:19.641] iteration 23920: total_loss: 0.377317, loss_sup: 0.064285, loss_mps: 0.111768, loss_cps: 0.201264
[14:03:19.787] iteration 23921: total_loss: 0.566315, loss_sup: 0.085085, loss_mps: 0.160441, loss_cps: 0.320789
[14:03:19.934] iteration 23922: total_loss: 0.385864, loss_sup: 0.039701, loss_mps: 0.120807, loss_cps: 0.225356
[14:03:20.081] iteration 23923: total_loss: 0.185253, loss_sup: 0.009183, loss_mps: 0.071195, loss_cps: 0.104875
[14:03:20.227] iteration 23924: total_loss: 0.416871, loss_sup: 0.023482, loss_mps: 0.147604, loss_cps: 0.245784
[14:03:20.373] iteration 23925: total_loss: 0.551000, loss_sup: 0.069564, loss_mps: 0.178819, loss_cps: 0.302618
[14:03:20.519] iteration 23926: total_loss: 0.510207, loss_sup: 0.179722, loss_mps: 0.116840, loss_cps: 0.213646
[14:03:20.665] iteration 23927: total_loss: 0.610995, loss_sup: 0.067605, loss_mps: 0.197216, loss_cps: 0.346174
[14:03:20.811] iteration 23928: total_loss: 0.473632, loss_sup: 0.041844, loss_mps: 0.145406, loss_cps: 0.286382
[14:03:20.958] iteration 23929: total_loss: 0.399026, loss_sup: 0.015987, loss_mps: 0.133754, loss_cps: 0.249284
[14:03:21.108] iteration 23930: total_loss: 0.513142, loss_sup: 0.027117, loss_mps: 0.160515, loss_cps: 0.325510
[14:03:21.256] iteration 23931: total_loss: 0.394691, loss_sup: 0.045314, loss_mps: 0.116095, loss_cps: 0.233281
[14:03:21.403] iteration 23932: total_loss: 0.280480, loss_sup: 0.028161, loss_mps: 0.095260, loss_cps: 0.157059
[14:03:21.549] iteration 23933: total_loss: 0.609501, loss_sup: 0.275175, loss_mps: 0.121102, loss_cps: 0.213224
[14:03:21.696] iteration 23934: total_loss: 0.485504, loss_sup: 0.096034, loss_mps: 0.134018, loss_cps: 0.255452
[14:03:21.843] iteration 23935: total_loss: 0.609719, loss_sup: 0.120292, loss_mps: 0.153852, loss_cps: 0.335574
[14:03:21.990] iteration 23936: total_loss: 0.626955, loss_sup: 0.035600, loss_mps: 0.184393, loss_cps: 0.406962
[14:03:22.136] iteration 23937: total_loss: 0.774842, loss_sup: 0.078546, loss_mps: 0.221065, loss_cps: 0.475230
[14:03:22.282] iteration 23938: total_loss: 0.576844, loss_sup: 0.042440, loss_mps: 0.181326, loss_cps: 0.353077
[14:03:22.428] iteration 23939: total_loss: 0.608319, loss_sup: 0.052400, loss_mps: 0.174743, loss_cps: 0.381176
[14:03:22.574] iteration 23940: total_loss: 0.423296, loss_sup: 0.029553, loss_mps: 0.134386, loss_cps: 0.259357
[14:03:22.720] iteration 23941: total_loss: 0.291204, loss_sup: 0.023071, loss_mps: 0.099000, loss_cps: 0.169132
[14:03:22.866] iteration 23942: total_loss: 0.728581, loss_sup: 0.023181, loss_mps: 0.230666, loss_cps: 0.474734
[14:03:23.014] iteration 23943: total_loss: 0.481570, loss_sup: 0.024687, loss_mps: 0.157854, loss_cps: 0.299029
[14:03:23.160] iteration 23944: total_loss: 0.900699, loss_sup: 0.243823, loss_mps: 0.218600, loss_cps: 0.438276
[14:03:23.305] iteration 23945: total_loss: 0.299349, loss_sup: 0.060576, loss_mps: 0.090518, loss_cps: 0.148256
[14:03:23.451] iteration 23946: total_loss: 0.475440, loss_sup: 0.026872, loss_mps: 0.153851, loss_cps: 0.294717
[14:03:23.597] iteration 23947: total_loss: 0.434706, loss_sup: 0.020215, loss_mps: 0.137642, loss_cps: 0.276849
[14:03:23.746] iteration 23948: total_loss: 0.239680, loss_sup: 0.001005, loss_mps: 0.086648, loss_cps: 0.152027
[14:03:23.892] iteration 23949: total_loss: 1.121664, loss_sup: 0.088256, loss_mps: 0.315200, loss_cps: 0.718207
[14:03:24.039] iteration 23950: total_loss: 0.274225, loss_sup: 0.031208, loss_mps: 0.091305, loss_cps: 0.151712
[14:03:24.184] iteration 23951: total_loss: 0.663007, loss_sup: 0.106911, loss_mps: 0.178850, loss_cps: 0.377247
[14:03:24.330] iteration 23952: total_loss: 0.780177, loss_sup: 0.131209, loss_mps: 0.208676, loss_cps: 0.440292
[14:03:24.477] iteration 23953: total_loss: 0.567306, loss_sup: 0.116428, loss_mps: 0.145357, loss_cps: 0.305520
[14:03:24.624] iteration 23954: total_loss: 0.435604, loss_sup: 0.028198, loss_mps: 0.140238, loss_cps: 0.267168
[14:03:24.770] iteration 23955: total_loss: 1.071796, loss_sup: 0.173430, loss_mps: 0.278913, loss_cps: 0.619453
[14:03:24.916] iteration 23956: total_loss: 0.257079, loss_sup: 0.012976, loss_mps: 0.088601, loss_cps: 0.155501
[14:03:25.062] iteration 23957: total_loss: 0.315512, loss_sup: 0.009510, loss_mps: 0.114708, loss_cps: 0.191293
[14:03:25.209] iteration 23958: total_loss: 0.502504, loss_sup: 0.053824, loss_mps: 0.152464, loss_cps: 0.296216
[14:03:25.355] iteration 23959: total_loss: 0.507418, loss_sup: 0.121968, loss_mps: 0.135262, loss_cps: 0.250188
[14:03:25.502] iteration 23960: total_loss: 0.434131, loss_sup: 0.054586, loss_mps: 0.121165, loss_cps: 0.258379
[14:03:25.648] iteration 23961: total_loss: 0.699339, loss_sup: 0.124722, loss_mps: 0.183858, loss_cps: 0.390759
[14:03:25.795] iteration 23962: total_loss: 0.317553, loss_sup: 0.056400, loss_mps: 0.092689, loss_cps: 0.168464
[14:03:25.941] iteration 23963: total_loss: 0.383957, loss_sup: 0.090583, loss_mps: 0.110554, loss_cps: 0.182820
[14:03:26.087] iteration 23964: total_loss: 0.870873, loss_sup: 0.034440, loss_mps: 0.265780, loss_cps: 0.570652
[14:03:26.233] iteration 23965: total_loss: 0.538911, loss_sup: 0.054639, loss_mps: 0.162982, loss_cps: 0.321290
[14:03:26.379] iteration 23966: total_loss: 0.371523, loss_sup: 0.003264, loss_mps: 0.121478, loss_cps: 0.246781
[14:03:26.524] iteration 23967: total_loss: 0.423111, loss_sup: 0.036375, loss_mps: 0.135449, loss_cps: 0.251288
[14:03:26.671] iteration 23968: total_loss: 0.330628, loss_sup: 0.018047, loss_mps: 0.108566, loss_cps: 0.204015
[14:03:26.817] iteration 23969: total_loss: 0.536519, loss_sup: 0.092281, loss_mps: 0.152953, loss_cps: 0.291285
[14:03:26.963] iteration 23970: total_loss: 0.719288, loss_sup: 0.064599, loss_mps: 0.204716, loss_cps: 0.449972
[14:03:27.114] iteration 23971: total_loss: 0.303136, loss_sup: 0.074184, loss_mps: 0.086295, loss_cps: 0.142656
[14:03:27.262] iteration 23972: total_loss: 0.717547, loss_sup: 0.095831, loss_mps: 0.209627, loss_cps: 0.412089
[14:03:27.409] iteration 23973: total_loss: 0.452587, loss_sup: 0.013878, loss_mps: 0.144506, loss_cps: 0.294202
[14:03:27.555] iteration 23974: total_loss: 1.063733, loss_sup: 0.415853, loss_mps: 0.226905, loss_cps: 0.420976
[14:03:27.702] iteration 23975: total_loss: 0.416650, loss_sup: 0.005816, loss_mps: 0.134069, loss_cps: 0.276764
[14:03:27.849] iteration 23976: total_loss: 0.582591, loss_sup: 0.113692, loss_mps: 0.154759, loss_cps: 0.314140
[14:03:27.995] iteration 23977: total_loss: 0.619965, loss_sup: 0.066398, loss_mps: 0.181344, loss_cps: 0.372223
[14:03:28.142] iteration 23978: total_loss: 0.226499, loss_sup: 0.012822, loss_mps: 0.078638, loss_cps: 0.135039
[14:03:28.288] iteration 23979: total_loss: 0.610853, loss_sup: 0.063314, loss_mps: 0.191537, loss_cps: 0.356003
[14:03:28.434] iteration 23980: total_loss: 0.554531, loss_sup: 0.159911, loss_mps: 0.131990, loss_cps: 0.262630
[14:03:28.582] iteration 23981: total_loss: 0.435658, loss_sup: 0.015820, loss_mps: 0.147571, loss_cps: 0.272266
[14:03:28.728] iteration 23982: total_loss: 0.289595, loss_sup: 0.033274, loss_mps: 0.098117, loss_cps: 0.158204
[14:03:28.874] iteration 23983: total_loss: 0.384741, loss_sup: 0.051005, loss_mps: 0.116592, loss_cps: 0.217143
[14:03:29.025] iteration 23984: total_loss: 0.330764, loss_sup: 0.129309, loss_mps: 0.073567, loss_cps: 0.127888
[14:03:29.172] iteration 23985: total_loss: 0.571926, loss_sup: 0.076943, loss_mps: 0.172534, loss_cps: 0.322449
[14:03:29.319] iteration 23986: total_loss: 0.470555, loss_sup: 0.036208, loss_mps: 0.146983, loss_cps: 0.287364
[14:03:29.465] iteration 23987: total_loss: 0.336625, loss_sup: 0.003139, loss_mps: 0.110031, loss_cps: 0.223455
[14:03:29.611] iteration 23988: total_loss: 0.417896, loss_sup: 0.097264, loss_mps: 0.120735, loss_cps: 0.199897
[14:03:29.757] iteration 23989: total_loss: 0.392717, loss_sup: 0.044539, loss_mps: 0.123381, loss_cps: 0.224796
[14:03:29.905] iteration 23990: total_loss: 0.771102, loss_sup: 0.183569, loss_mps: 0.199269, loss_cps: 0.388263
[14:03:30.051] iteration 23991: total_loss: 0.494486, loss_sup: 0.003172, loss_mps: 0.157400, loss_cps: 0.333914
[14:03:30.198] iteration 23992: total_loss: 0.582609, loss_sup: 0.041186, loss_mps: 0.175819, loss_cps: 0.365604
[14:03:30.344] iteration 23993: total_loss: 0.430777, loss_sup: 0.032382, loss_mps: 0.128904, loss_cps: 0.269491
[14:03:30.491] iteration 23994: total_loss: 0.391794, loss_sup: 0.013835, loss_mps: 0.140333, loss_cps: 0.237626
[14:03:30.638] iteration 23995: total_loss: 0.479885, loss_sup: 0.015382, loss_mps: 0.158178, loss_cps: 0.306325
[14:03:30.785] iteration 23996: total_loss: 0.441137, loss_sup: 0.023825, loss_mps: 0.133330, loss_cps: 0.283983
[14:03:30.931] iteration 23997: total_loss: 0.265662, loss_sup: 0.083795, loss_mps: 0.072838, loss_cps: 0.109029
[14:03:31.077] iteration 23998: total_loss: 0.338315, loss_sup: 0.058216, loss_mps: 0.102894, loss_cps: 0.177205
[14:03:31.223] iteration 23999: total_loss: 0.350337, loss_sup: 0.008154, loss_mps: 0.117658, loss_cps: 0.224525
[14:03:31.373] iteration 24000: total_loss: 0.865714, loss_sup: 0.103560, loss_mps: 0.245342, loss_cps: 0.516813
[14:03:31.373] Evaluation Started ==>
[14:03:42.679] ==> valid iteration 24000: unet metrics: {'dc': 0.6388508055300973, 'jc': 0.521871711348756, 'pre': 0.8023645772582302, 'hd': 5.452535732206767}, ynet metrics: {'dc': 0.6080148016872755, 'jc': 0.49454320723271805, 'pre': 0.7943443287942549, 'hd': 5.530601865027733}.
[14:03:42.681] Evaluation Finished!⏹️
[14:03:42.831] iteration 24001: total_loss: 0.393815, loss_sup: 0.090164, loss_mps: 0.108478, loss_cps: 0.195173
[14:03:42.979] iteration 24002: total_loss: 0.439546, loss_sup: 0.039890, loss_mps: 0.137744, loss_cps: 0.261912
[14:03:43.125] iteration 24003: total_loss: 0.287177, loss_sup: 0.014395, loss_mps: 0.098280, loss_cps: 0.174503
[14:03:43.271] iteration 24004: total_loss: 0.294006, loss_sup: 0.029015, loss_mps: 0.095007, loss_cps: 0.169984
[14:03:43.416] iteration 24005: total_loss: 0.372601, loss_sup: 0.021470, loss_mps: 0.128304, loss_cps: 0.222827
[14:03:43.562] iteration 24006: total_loss: 0.350308, loss_sup: 0.043920, loss_mps: 0.115659, loss_cps: 0.190729
[14:03:43.707] iteration 24007: total_loss: 0.728714, loss_sup: 0.106699, loss_mps: 0.203111, loss_cps: 0.418904
[14:03:43.854] iteration 24008: total_loss: 0.471238, loss_sup: 0.081519, loss_mps: 0.135988, loss_cps: 0.253731
[14:03:44.000] iteration 24009: total_loss: 0.559137, loss_sup: 0.047448, loss_mps: 0.187031, loss_cps: 0.324658
[14:03:44.146] iteration 24010: total_loss: 0.394450, loss_sup: 0.008515, loss_mps: 0.130566, loss_cps: 0.255369
[14:03:44.291] iteration 24011: total_loss: 0.513219, loss_sup: 0.083914, loss_mps: 0.142202, loss_cps: 0.287103
[14:03:44.438] iteration 24012: total_loss: 0.263672, loss_sup: 0.003643, loss_mps: 0.092955, loss_cps: 0.167073
[14:03:44.587] iteration 24013: total_loss: 0.441965, loss_sup: 0.032389, loss_mps: 0.145689, loss_cps: 0.263888
[14:03:44.733] iteration 24014: total_loss: 0.467824, loss_sup: 0.166430, loss_mps: 0.117312, loss_cps: 0.184083
[14:03:44.879] iteration 24015: total_loss: 0.453581, loss_sup: 0.015343, loss_mps: 0.143647, loss_cps: 0.294591
[14:03:45.024] iteration 24016: total_loss: 0.430935, loss_sup: 0.124086, loss_mps: 0.112683, loss_cps: 0.194166
[14:03:45.170] iteration 24017: total_loss: 0.321093, loss_sup: 0.063956, loss_mps: 0.090231, loss_cps: 0.166906
[14:03:45.316] iteration 24018: total_loss: 0.636712, loss_sup: 0.102602, loss_mps: 0.200601, loss_cps: 0.333509
[14:03:45.461] iteration 24019: total_loss: 1.065104, loss_sup: 0.090522, loss_mps: 0.309728, loss_cps: 0.664854
[14:03:45.606] iteration 24020: total_loss: 0.424598, loss_sup: 0.016919, loss_mps: 0.149899, loss_cps: 0.257781
[14:03:45.751] iteration 24021: total_loss: 0.542670, loss_sup: 0.047039, loss_mps: 0.176341, loss_cps: 0.319290
[14:03:45.899] iteration 24022: total_loss: 0.460423, loss_sup: 0.091242, loss_mps: 0.125350, loss_cps: 0.243831
[14:03:46.048] iteration 24023: total_loss: 0.285144, loss_sup: 0.019009, loss_mps: 0.095434, loss_cps: 0.170701
[14:03:46.196] iteration 24024: total_loss: 0.445788, loss_sup: 0.052684, loss_mps: 0.154096, loss_cps: 0.239007
[14:03:46.343] iteration 24025: total_loss: 0.512359, loss_sup: 0.026437, loss_mps: 0.157031, loss_cps: 0.328891
[14:03:46.488] iteration 24026: total_loss: 0.770341, loss_sup: 0.030847, loss_mps: 0.247323, loss_cps: 0.492171
[14:03:46.634] iteration 24027: total_loss: 0.308743, loss_sup: 0.026470, loss_mps: 0.099614, loss_cps: 0.182658
[14:03:46.782] iteration 24028: total_loss: 0.589765, loss_sup: 0.133145, loss_mps: 0.178041, loss_cps: 0.278580
[14:03:46.929] iteration 24029: total_loss: 0.289918, loss_sup: 0.022490, loss_mps: 0.095819, loss_cps: 0.171609
[14:03:47.075] iteration 24030: total_loss: 0.552515, loss_sup: 0.078936, loss_mps: 0.164663, loss_cps: 0.308916
[14:03:47.220] iteration 24031: total_loss: 0.490789, loss_sup: 0.025257, loss_mps: 0.150051, loss_cps: 0.315481
[14:03:47.366] iteration 24032: total_loss: 0.220296, loss_sup: 0.004274, loss_mps: 0.081333, loss_cps: 0.134689
[14:03:47.512] iteration 24033: total_loss: 0.536102, loss_sup: 0.082814, loss_mps: 0.153838, loss_cps: 0.299450
[14:03:47.658] iteration 24034: total_loss: 0.628539, loss_sup: 0.078530, loss_mps: 0.172969, loss_cps: 0.377041
[14:03:47.804] iteration 24035: total_loss: 0.318535, loss_sup: 0.034649, loss_mps: 0.100034, loss_cps: 0.183852
[14:03:47.951] iteration 24036: total_loss: 0.385266, loss_sup: 0.098244, loss_mps: 0.105450, loss_cps: 0.181571
[14:03:48.096] iteration 24037: total_loss: 0.391645, loss_sup: 0.042007, loss_mps: 0.121104, loss_cps: 0.228533
[14:03:48.242] iteration 24038: total_loss: 0.451552, loss_sup: 0.279271, loss_mps: 0.067648, loss_cps: 0.104633
[14:03:48.388] iteration 24039: total_loss: 0.367540, loss_sup: 0.034906, loss_mps: 0.117980, loss_cps: 0.214655
[14:03:48.534] iteration 24040: total_loss: 0.446866, loss_sup: 0.090151, loss_mps: 0.127020, loss_cps: 0.229696
[14:03:48.679] iteration 24041: total_loss: 0.723021, loss_sup: 0.028998, loss_mps: 0.224278, loss_cps: 0.469745
[14:03:48.825] iteration 24042: total_loss: 0.294713, loss_sup: 0.014318, loss_mps: 0.101545, loss_cps: 0.178851
[14:03:48.971] iteration 24043: total_loss: 0.300154, loss_sup: 0.055363, loss_mps: 0.088828, loss_cps: 0.155963
[14:03:49.120] iteration 24044: total_loss: 0.339510, loss_sup: 0.043843, loss_mps: 0.105460, loss_cps: 0.190207
[14:03:49.265] iteration 24045: total_loss: 0.813922, loss_sup: 0.159907, loss_mps: 0.220595, loss_cps: 0.433420
[14:03:49.410] iteration 24046: total_loss: 0.317279, loss_sup: 0.088341, loss_mps: 0.089091, loss_cps: 0.139847
[14:03:49.556] iteration 24047: total_loss: 0.875875, loss_sup: 0.063084, loss_mps: 0.255665, loss_cps: 0.557126
[14:03:49.703] iteration 24048: total_loss: 0.508229, loss_sup: 0.035025, loss_mps: 0.156341, loss_cps: 0.316863
[14:03:49.849] iteration 24049: total_loss: 0.374301, loss_sup: 0.084002, loss_mps: 0.103869, loss_cps: 0.186431
[14:03:49.995] iteration 24050: total_loss: 0.235872, loss_sup: 0.008329, loss_mps: 0.082698, loss_cps: 0.144845
[14:03:50.141] iteration 24051: total_loss: 0.831699, loss_sup: 0.265661, loss_mps: 0.179590, loss_cps: 0.386447
[14:03:50.286] iteration 24052: total_loss: 0.435647, loss_sup: 0.110378, loss_mps: 0.118014, loss_cps: 0.207255
[14:03:50.432] iteration 24053: total_loss: 0.452518, loss_sup: 0.096123, loss_mps: 0.119508, loss_cps: 0.236886
[14:03:50.577] iteration 24054: total_loss: 0.526701, loss_sup: 0.020690, loss_mps: 0.160881, loss_cps: 0.345130
[14:03:50.724] iteration 24055: total_loss: 0.268349, loss_sup: 0.020231, loss_mps: 0.093323, loss_cps: 0.154794
[14:03:50.873] iteration 24056: total_loss: 0.437424, loss_sup: 0.060492, loss_mps: 0.126386, loss_cps: 0.250547
[14:03:51.020] iteration 24057: total_loss: 0.385661, loss_sup: 0.082858, loss_mps: 0.106902, loss_cps: 0.195901
[14:03:51.165] iteration 24058: total_loss: 0.852676, loss_sup: 0.101234, loss_mps: 0.234694, loss_cps: 0.516748
[14:03:51.311] iteration 24059: total_loss: 0.396720, loss_sup: 0.022719, loss_mps: 0.129269, loss_cps: 0.244732
[14:03:51.458] iteration 24060: total_loss: 0.497299, loss_sup: 0.047393, loss_mps: 0.151861, loss_cps: 0.298045
[14:03:51.605] iteration 24061: total_loss: 0.358442, loss_sup: 0.034390, loss_mps: 0.111959, loss_cps: 0.212093
[14:03:51.752] iteration 24062: total_loss: 0.415294, loss_sup: 0.050587, loss_mps: 0.124383, loss_cps: 0.240324
[14:03:51.897] iteration 24063: total_loss: 1.084128, loss_sup: 0.107207, loss_mps: 0.305213, loss_cps: 0.671708
[14:03:52.044] iteration 24064: total_loss: 0.531230, loss_sup: 0.148659, loss_mps: 0.121071, loss_cps: 0.261500
[14:03:52.190] iteration 24065: total_loss: 0.319529, loss_sup: 0.075370, loss_mps: 0.087152, loss_cps: 0.157006
[14:03:52.336] iteration 24066: total_loss: 0.320443, loss_sup: 0.015612, loss_mps: 0.102397, loss_cps: 0.202434
[14:03:52.482] iteration 24067: total_loss: 0.523304, loss_sup: 0.151656, loss_mps: 0.131048, loss_cps: 0.240599
[14:03:52.628] iteration 24068: total_loss: 0.340176, loss_sup: 0.012279, loss_mps: 0.113009, loss_cps: 0.214888
[14:03:52.775] iteration 24069: total_loss: 0.222602, loss_sup: 0.005901, loss_mps: 0.080504, loss_cps: 0.136197
[14:03:52.921] iteration 24070: total_loss: 0.316997, loss_sup: 0.036365, loss_mps: 0.101248, loss_cps: 0.179384
[14:03:53.068] iteration 24071: total_loss: 1.236229, loss_sup: 0.021942, loss_mps: 0.389118, loss_cps: 0.825169
[14:03:53.213] iteration 24072: total_loss: 0.461888, loss_sup: 0.136603, loss_mps: 0.113304, loss_cps: 0.211982
[14:03:53.359] iteration 24073: total_loss: 0.243226, loss_sup: 0.012324, loss_mps: 0.084101, loss_cps: 0.146801
[14:03:53.506] iteration 24074: total_loss: 0.898607, loss_sup: 0.066624, loss_mps: 0.241584, loss_cps: 0.590399
[14:03:53.652] iteration 24075: total_loss: 0.417303, loss_sup: 0.025853, loss_mps: 0.131907, loss_cps: 0.259543
[14:03:53.799] iteration 24076: total_loss: 0.354821, loss_sup: 0.026540, loss_mps: 0.111568, loss_cps: 0.216713
[14:03:53.945] iteration 24077: total_loss: 0.512648, loss_sup: 0.118398, loss_mps: 0.135293, loss_cps: 0.258957
[14:03:54.091] iteration 24078: total_loss: 0.322304, loss_sup: 0.060755, loss_mps: 0.098141, loss_cps: 0.163409
[14:03:54.237] iteration 24079: total_loss: 0.326133, loss_sup: 0.033651, loss_mps: 0.103532, loss_cps: 0.188950
[14:03:54.383] iteration 24080: total_loss: 0.827265, loss_sup: 0.085447, loss_mps: 0.228470, loss_cps: 0.513348
[14:03:54.532] iteration 24081: total_loss: 0.408385, loss_sup: 0.041174, loss_mps: 0.127390, loss_cps: 0.239820
[14:03:54.678] iteration 24082: total_loss: 0.428718, loss_sup: 0.014860, loss_mps: 0.134583, loss_cps: 0.279275
[14:03:54.824] iteration 24083: total_loss: 0.427834, loss_sup: 0.014934, loss_mps: 0.138379, loss_cps: 0.274521
[14:03:54.971] iteration 24084: total_loss: 0.699239, loss_sup: 0.068824, loss_mps: 0.209198, loss_cps: 0.421217
[14:03:55.120] iteration 24085: total_loss: 0.597842, loss_sup: 0.088567, loss_mps: 0.175493, loss_cps: 0.333782
[14:03:55.266] iteration 24086: total_loss: 0.365012, loss_sup: 0.025541, loss_mps: 0.118732, loss_cps: 0.220739
[14:03:55.412] iteration 24087: total_loss: 0.252428, loss_sup: 0.007101, loss_mps: 0.094805, loss_cps: 0.150522
[14:03:55.558] iteration 24088: total_loss: 0.270609, loss_sup: 0.033330, loss_mps: 0.083541, loss_cps: 0.153738
[14:03:55.704] iteration 24089: total_loss: 0.306594, loss_sup: 0.021413, loss_mps: 0.104702, loss_cps: 0.180478
[14:03:55.850] iteration 24090: total_loss: 0.485646, loss_sup: 0.045941, loss_mps: 0.144683, loss_cps: 0.295023
[14:03:55.995] iteration 24091: total_loss: 0.615008, loss_sup: 0.061289, loss_mps: 0.181060, loss_cps: 0.372659
[14:03:56.141] iteration 24092: total_loss: 0.513832, loss_sup: 0.048264, loss_mps: 0.154460, loss_cps: 0.311108
[14:03:56.289] iteration 24093: total_loss: 0.424056, loss_sup: 0.163129, loss_mps: 0.101217, loss_cps: 0.159710
[14:03:56.435] iteration 24094: total_loss: 0.389062, loss_sup: 0.068265, loss_mps: 0.110589, loss_cps: 0.210207
[14:03:56.581] iteration 24095: total_loss: 0.470567, loss_sup: 0.073587, loss_mps: 0.134879, loss_cps: 0.262101
[14:03:56.727] iteration 24096: total_loss: 0.626843, loss_sup: 0.192494, loss_mps: 0.148331, loss_cps: 0.286017
[14:03:56.876] iteration 24097: total_loss: 0.464917, loss_sup: 0.026883, loss_mps: 0.150427, loss_cps: 0.287607
[14:03:57.021] iteration 24098: total_loss: 0.909252, loss_sup: 0.230114, loss_mps: 0.212828, loss_cps: 0.466311
[14:03:57.167] iteration 24099: total_loss: 0.368349, loss_sup: 0.006630, loss_mps: 0.125619, loss_cps: 0.236101
[14:03:57.313] iteration 24100: total_loss: 0.675017, loss_sup: 0.040766, loss_mps: 0.202554, loss_cps: 0.431697
[14:03:57.313] Evaluation Started ==>
[14:04:08.631] ==> valid iteration 24100: unet metrics: {'dc': 0.6163626048443656, 'jc': 0.49973341662642884, 'pre': 0.7841516511237182, 'hd': 5.468505671370571}, ynet metrics: {'dc': 0.6105880993401736, 'jc': 0.49910957099857584, 'pre': 0.7881441552406176, 'hd': 5.521868901787442}.
[14:04:08.632] Evaluation Finished!⏹️
[14:04:08.782] iteration 24101: total_loss: 0.357936, loss_sup: 0.034744, loss_mps: 0.121523, loss_cps: 0.201670
[14:04:08.928] iteration 24102: total_loss: 0.570498, loss_sup: 0.032581, loss_mps: 0.181294, loss_cps: 0.356623
[14:04:09.075] iteration 24103: total_loss: 0.497268, loss_sup: 0.024985, loss_mps: 0.145207, loss_cps: 0.327077
[14:04:09.222] iteration 24104: total_loss: 0.275940, loss_sup: 0.050542, loss_mps: 0.081561, loss_cps: 0.143838
[14:04:09.368] iteration 24105: total_loss: 0.416304, loss_sup: 0.027975, loss_mps: 0.129544, loss_cps: 0.258785
[14:04:09.513] iteration 24106: total_loss: 0.474557, loss_sup: 0.067965, loss_mps: 0.136851, loss_cps: 0.269741
[14:04:09.661] iteration 24107: total_loss: 0.333413, loss_sup: 0.129986, loss_mps: 0.075124, loss_cps: 0.128303
[14:04:09.807] iteration 24108: total_loss: 0.348311, loss_sup: 0.004809, loss_mps: 0.121928, loss_cps: 0.221573
[14:04:09.953] iteration 24109: total_loss: 0.593136, loss_sup: 0.161121, loss_mps: 0.136039, loss_cps: 0.295976
[14:04:10.098] iteration 24110: total_loss: 0.559691, loss_sup: 0.026606, loss_mps: 0.177205, loss_cps: 0.355880
[14:04:10.244] iteration 24111: total_loss: 0.484681, loss_sup: 0.004575, loss_mps: 0.156442, loss_cps: 0.323664
[14:04:10.390] iteration 24112: total_loss: 0.442653, loss_sup: 0.114827, loss_mps: 0.115810, loss_cps: 0.212016
[14:04:10.535] iteration 24113: total_loss: 0.636558, loss_sup: 0.028387, loss_mps: 0.200633, loss_cps: 0.407538
[14:04:10.681] iteration 24114: total_loss: 0.371797, loss_sup: 0.029789, loss_mps: 0.118388, loss_cps: 0.223620
[14:04:10.826] iteration 24115: total_loss: 0.435837, loss_sup: 0.005608, loss_mps: 0.147433, loss_cps: 0.282797
[14:04:10.971] iteration 24116: total_loss: 0.275809, loss_sup: 0.074622, loss_mps: 0.075213, loss_cps: 0.125974
[14:04:11.117] iteration 24117: total_loss: 0.362606, loss_sup: 0.042962, loss_mps: 0.113065, loss_cps: 0.206578
[14:04:11.262] iteration 24118: total_loss: 0.388250, loss_sup: 0.078239, loss_mps: 0.106687, loss_cps: 0.203323
[14:04:11.408] iteration 24119: total_loss: 0.496732, loss_sup: 0.044187, loss_mps: 0.146183, loss_cps: 0.306362
[14:04:11.554] iteration 24120: total_loss: 0.330177, loss_sup: 0.039753, loss_mps: 0.107551, loss_cps: 0.182873
[14:04:11.700] iteration 24121: total_loss: 0.867177, loss_sup: 0.184210, loss_mps: 0.224802, loss_cps: 0.458165
[14:04:11.845] iteration 24122: total_loss: 0.392083, loss_sup: 0.010714, loss_mps: 0.130938, loss_cps: 0.250431
[14:04:11.991] iteration 24123: total_loss: 0.231524, loss_sup: 0.007815, loss_mps: 0.083024, loss_cps: 0.140685
[14:04:12.136] iteration 24124: total_loss: 0.362645, loss_sup: 0.031819, loss_mps: 0.119810, loss_cps: 0.211016
[14:04:12.281] iteration 24125: total_loss: 0.453392, loss_sup: 0.081870, loss_mps: 0.140385, loss_cps: 0.231138
[14:04:12.427] iteration 24126: total_loss: 0.304531, loss_sup: 0.017896, loss_mps: 0.111925, loss_cps: 0.174710
[14:04:12.572] iteration 24127: total_loss: 0.624689, loss_sup: 0.029652, loss_mps: 0.203344, loss_cps: 0.391692
[14:04:12.720] iteration 24128: total_loss: 0.550767, loss_sup: 0.064587, loss_mps: 0.164482, loss_cps: 0.321698
[14:04:12.865] iteration 24129: total_loss: 0.353222, loss_sup: 0.111692, loss_mps: 0.088033, loss_cps: 0.153497
[14:04:13.011] iteration 24130: total_loss: 0.444230, loss_sup: 0.053713, loss_mps: 0.133011, loss_cps: 0.257507
[14:04:13.156] iteration 24131: total_loss: 0.293600, loss_sup: 0.061297, loss_mps: 0.083944, loss_cps: 0.148359
[14:04:13.302] iteration 24132: total_loss: 0.490453, loss_sup: 0.073835, loss_mps: 0.148464, loss_cps: 0.268155
[14:04:13.448] iteration 24133: total_loss: 0.258500, loss_sup: 0.045277, loss_mps: 0.078635, loss_cps: 0.134588
[14:04:13.593] iteration 24134: total_loss: 0.626848, loss_sup: 0.129306, loss_mps: 0.165404, loss_cps: 0.332138
[14:04:13.739] iteration 24135: total_loss: 0.346412, loss_sup: 0.041515, loss_mps: 0.109712, loss_cps: 0.195184
[14:04:13.884] iteration 24136: total_loss: 0.191838, loss_sup: 0.027864, loss_mps: 0.064926, loss_cps: 0.099048
[14:04:14.030] iteration 24137: total_loss: 0.833691, loss_sup: 0.156921, loss_mps: 0.226735, loss_cps: 0.450036
[14:04:14.176] iteration 24138: total_loss: 0.562663, loss_sup: 0.069361, loss_mps: 0.165641, loss_cps: 0.327661
[14:04:14.322] iteration 24139: total_loss: 0.450112, loss_sup: 0.048788, loss_mps: 0.136324, loss_cps: 0.265001
[14:04:14.468] iteration 24140: total_loss: 0.395952, loss_sup: 0.090657, loss_mps: 0.109048, loss_cps: 0.196247
[14:04:14.618] iteration 24141: total_loss: 1.055018, loss_sup: 0.057146, loss_mps: 0.299928, loss_cps: 0.697944
[14:04:14.764] iteration 24142: total_loss: 0.859764, loss_sup: 0.480142, loss_mps: 0.131583, loss_cps: 0.248040
[14:04:14.910] iteration 24143: total_loss: 0.384650, loss_sup: 0.007335, loss_mps: 0.133876, loss_cps: 0.243438
[14:04:15.057] iteration 24144: total_loss: 0.229210, loss_sup: 0.012495, loss_mps: 0.078425, loss_cps: 0.138290
[14:04:15.205] iteration 24145: total_loss: 0.513020, loss_sup: 0.094662, loss_mps: 0.149842, loss_cps: 0.268516
[14:04:15.352] iteration 24146: total_loss: 0.474450, loss_sup: 0.037461, loss_mps: 0.144899, loss_cps: 0.292090
[14:04:15.498] iteration 24147: total_loss: 0.381146, loss_sup: 0.040499, loss_mps: 0.110745, loss_cps: 0.229902
[14:04:15.643] iteration 24148: total_loss: 0.562034, loss_sup: 0.078995, loss_mps: 0.168644, loss_cps: 0.314395
[14:04:15.789] iteration 24149: total_loss: 0.348191, loss_sup: 0.064607, loss_mps: 0.096615, loss_cps: 0.186968
[14:04:15.934] iteration 24150: total_loss: 0.559081, loss_sup: 0.158946, loss_mps: 0.142832, loss_cps: 0.257304
[14:04:16.085] iteration 24151: total_loss: 0.549008, loss_sup: 0.057230, loss_mps: 0.162169, loss_cps: 0.329609
[14:04:16.233] iteration 24152: total_loss: 0.476324, loss_sup: 0.044411, loss_mps: 0.148299, loss_cps: 0.283614
[14:04:16.380] iteration 24153: total_loss: 0.271924, loss_sup: 0.003919, loss_mps: 0.100574, loss_cps: 0.167431
[14:04:16.526] iteration 24154: total_loss: 0.552408, loss_sup: 0.063214, loss_mps: 0.165006, loss_cps: 0.324188
[14:04:16.675] iteration 24155: total_loss: 0.277124, loss_sup: 0.035391, loss_mps: 0.092470, loss_cps: 0.149264
[14:04:16.822] iteration 24156: total_loss: 0.302128, loss_sup: 0.007684, loss_mps: 0.106199, loss_cps: 0.188245
[14:04:16.968] iteration 24157: total_loss: 0.527214, loss_sup: 0.049507, loss_mps: 0.163662, loss_cps: 0.314045
[14:04:17.113] iteration 24158: total_loss: 0.342169, loss_sup: 0.039426, loss_mps: 0.107290, loss_cps: 0.195453
[14:04:17.263] iteration 24159: total_loss: 0.410612, loss_sup: 0.161011, loss_mps: 0.093038, loss_cps: 0.156564
[14:04:17.408] iteration 24160: total_loss: 0.388747, loss_sup: 0.021521, loss_mps: 0.128121, loss_cps: 0.239105
[14:04:17.554] iteration 24161: total_loss: 0.342791, loss_sup: 0.007252, loss_mps: 0.119209, loss_cps: 0.216330
[14:04:17.700] iteration 24162: total_loss: 0.266600, loss_sup: 0.007006, loss_mps: 0.098412, loss_cps: 0.161182
[14:04:17.846] iteration 24163: total_loss: 0.528585, loss_sup: 0.017861, loss_mps: 0.163832, loss_cps: 0.346892
[14:04:17.992] iteration 24164: total_loss: 0.305585, loss_sup: 0.020257, loss_mps: 0.101055, loss_cps: 0.184273
[14:04:18.138] iteration 24165: total_loss: 0.370904, loss_sup: 0.038142, loss_mps: 0.119629, loss_cps: 0.213132
[14:04:18.283] iteration 24166: total_loss: 0.296485, loss_sup: 0.056223, loss_mps: 0.092919, loss_cps: 0.147343
[14:04:18.429] iteration 24167: total_loss: 0.426344, loss_sup: 0.014364, loss_mps: 0.135336, loss_cps: 0.276644
[14:04:18.575] iteration 24168: total_loss: 0.517496, loss_sup: 0.038101, loss_mps: 0.155194, loss_cps: 0.324202
[14:04:18.723] iteration 24169: total_loss: 0.237247, loss_sup: 0.011034, loss_mps: 0.094229, loss_cps: 0.131984
[14:04:18.869] iteration 24170: total_loss: 0.588519, loss_sup: 0.006259, loss_mps: 0.181801, loss_cps: 0.400458
[14:04:19.015] iteration 24171: total_loss: 0.547383, loss_sup: 0.017283, loss_mps: 0.171086, loss_cps: 0.359014
[14:04:19.163] iteration 24172: total_loss: 0.668184, loss_sup: 0.372789, loss_mps: 0.102186, loss_cps: 0.193209
[14:04:19.309] iteration 24173: total_loss: 0.518163, loss_sup: 0.130179, loss_mps: 0.130255, loss_cps: 0.257729
[14:04:19.454] iteration 24174: total_loss: 0.721161, loss_sup: 0.138278, loss_mps: 0.184658, loss_cps: 0.398225
[14:04:19.600] iteration 24175: total_loss: 0.316551, loss_sup: 0.000919, loss_mps: 0.110785, loss_cps: 0.204848
[14:04:19.746] iteration 24176: total_loss: 0.730182, loss_sup: 0.271496, loss_mps: 0.153306, loss_cps: 0.305379
[14:04:19.896] iteration 24177: total_loss: 0.567623, loss_sup: 0.135113, loss_mps: 0.146465, loss_cps: 0.286045
[14:04:20.042] iteration 24178: total_loss: 0.607810, loss_sup: 0.104792, loss_mps: 0.158540, loss_cps: 0.344478
[14:04:20.187] iteration 24179: total_loss: 0.257121, loss_sup: 0.025327, loss_mps: 0.084451, loss_cps: 0.147342
[14:04:20.334] iteration 24180: total_loss: 0.481815, loss_sup: 0.010189, loss_mps: 0.154485, loss_cps: 0.317140
[14:04:20.481] iteration 24181: total_loss: 0.447093, loss_sup: 0.061144, loss_mps: 0.134084, loss_cps: 0.251865
[14:04:20.627] iteration 24182: total_loss: 0.943572, loss_sup: 0.284160, loss_mps: 0.210988, loss_cps: 0.448423
[14:04:20.773] iteration 24183: total_loss: 0.571058, loss_sup: 0.077244, loss_mps: 0.167251, loss_cps: 0.326564
[14:04:20.919] iteration 24184: total_loss: 0.440171, loss_sup: 0.102509, loss_mps: 0.123344, loss_cps: 0.214318
[14:04:21.065] iteration 24185: total_loss: 0.304993, loss_sup: 0.006058, loss_mps: 0.107289, loss_cps: 0.191647
[14:04:21.213] iteration 24186: total_loss: 0.382276, loss_sup: 0.029988, loss_mps: 0.120191, loss_cps: 0.232097
[14:04:21.360] iteration 24187: total_loss: 0.437718, loss_sup: 0.044830, loss_mps: 0.133970, loss_cps: 0.258918
[14:04:21.505] iteration 24188: total_loss: 0.532573, loss_sup: 0.080518, loss_mps: 0.158049, loss_cps: 0.294006
[14:04:21.652] iteration 24189: total_loss: 0.463358, loss_sup: 0.051582, loss_mps: 0.139260, loss_cps: 0.272516
[14:04:21.800] iteration 24190: total_loss: 0.752948, loss_sup: 0.014543, loss_mps: 0.222608, loss_cps: 0.515797
[14:04:21.946] iteration 24191: total_loss: 0.516424, loss_sup: 0.079421, loss_mps: 0.145972, loss_cps: 0.291030
[14:04:22.092] iteration 24192: total_loss: 0.867745, loss_sup: 0.192724, loss_mps: 0.208084, loss_cps: 0.466936
[14:04:22.238] iteration 24193: total_loss: 0.327623, loss_sup: 0.020847, loss_mps: 0.106011, loss_cps: 0.200765
[14:04:22.385] iteration 24194: total_loss: 0.679202, loss_sup: 0.276892, loss_mps: 0.133354, loss_cps: 0.268957
[14:04:22.531] iteration 24195: total_loss: 0.543740, loss_sup: 0.253625, loss_mps: 0.105368, loss_cps: 0.184748
[14:04:22.680] iteration 24196: total_loss: 0.561273, loss_sup: 0.149112, loss_mps: 0.153877, loss_cps: 0.258283
[14:04:22.828] iteration 24197: total_loss: 1.163564, loss_sup: 0.192443, loss_mps: 0.293463, loss_cps: 0.677657
[14:04:22.974] iteration 24198: total_loss: 0.380201, loss_sup: 0.044808, loss_mps: 0.129868, loss_cps: 0.205525
[14:04:23.120] iteration 24199: total_loss: 0.373688, loss_sup: 0.015889, loss_mps: 0.124053, loss_cps: 0.233746
[14:04:23.266] iteration 24200: total_loss: 0.478536, loss_sup: 0.037687, loss_mps: 0.146339, loss_cps: 0.294510
[14:04:23.266] Evaluation Started ==>
[14:04:34.647] ==> valid iteration 24200: unet metrics: {'dc': 0.6369308862658054, 'jc': 0.5238532073893015, 'pre': 0.7895233066670273, 'hd': 5.402379865563973}, ynet metrics: {'dc': 0.5897305711180579, 'jc': 0.4774449796851698, 'pre': 0.7994224039870252, 'hd': 5.318440397021457}.
[14:04:34.648] Evaluation Finished!⏹️
[14:04:34.798] iteration 24201: total_loss: 0.291450, loss_sup: 0.069271, loss_mps: 0.083499, loss_cps: 0.138680
[14:04:34.946] iteration 24202: total_loss: 0.997241, loss_sup: 0.065673, loss_mps: 0.289229, loss_cps: 0.642339
[14:04:35.093] iteration 24203: total_loss: 0.476153, loss_sup: 0.062644, loss_mps: 0.158935, loss_cps: 0.254574
[14:04:35.239] iteration 24204: total_loss: 1.212144, loss_sup: 0.171161, loss_mps: 0.316067, loss_cps: 0.724917
[14:04:35.385] iteration 24205: total_loss: 0.307610, loss_sup: 0.076711, loss_mps: 0.085018, loss_cps: 0.145881
[14:04:35.532] iteration 24206: total_loss: 0.383261, loss_sup: 0.123003, loss_mps: 0.091198, loss_cps: 0.169060
[14:04:35.677] iteration 24207: total_loss: 0.301138, loss_sup: 0.013327, loss_mps: 0.102291, loss_cps: 0.185520
[14:04:35.823] iteration 24208: total_loss: 0.413037, loss_sup: 0.004245, loss_mps: 0.139945, loss_cps: 0.268846
[14:04:35.969] iteration 24209: total_loss: 0.586506, loss_sup: 0.060445, loss_mps: 0.168671, loss_cps: 0.357391
[14:04:36.115] iteration 24210: total_loss: 0.457651, loss_sup: 0.046577, loss_mps: 0.145497, loss_cps: 0.265577
[14:04:36.261] iteration 24211: total_loss: 0.563040, loss_sup: 0.022319, loss_mps: 0.179642, loss_cps: 0.361079
[14:04:36.407] iteration 24212: total_loss: 0.533769, loss_sup: 0.034235, loss_mps: 0.165157, loss_cps: 0.334377
[14:04:36.554] iteration 24213: total_loss: 0.620968, loss_sup: 0.067554, loss_mps: 0.176086, loss_cps: 0.377328
[14:04:36.700] iteration 24214: total_loss: 0.319878, loss_sup: 0.008679, loss_mps: 0.117927, loss_cps: 0.193272
[14:04:36.846] iteration 24215: total_loss: 0.830406, loss_sup: 0.075712, loss_mps: 0.242477, loss_cps: 0.512217
[14:04:36.993] iteration 24216: total_loss: 0.421534, loss_sup: 0.015554, loss_mps: 0.137900, loss_cps: 0.268081
[14:04:37.138] iteration 24217: total_loss: 0.403728, loss_sup: 0.017229, loss_mps: 0.128562, loss_cps: 0.257937
[14:04:37.285] iteration 24218: total_loss: 0.374461, loss_sup: 0.062481, loss_mps: 0.117761, loss_cps: 0.194220
[14:04:37.430] iteration 24219: total_loss: 0.375219, loss_sup: 0.042961, loss_mps: 0.116031, loss_cps: 0.216227
[14:04:37.576] iteration 24220: total_loss: 0.327021, loss_sup: 0.002001, loss_mps: 0.118366, loss_cps: 0.206654
[14:04:37.721] iteration 24221: total_loss: 0.340240, loss_sup: 0.011686, loss_mps: 0.117411, loss_cps: 0.211143
[14:04:37.868] iteration 24222: total_loss: 0.410444, loss_sup: 0.133613, loss_mps: 0.101043, loss_cps: 0.175789
[14:04:38.014] iteration 24223: total_loss: 0.490389, loss_sup: 0.013844, loss_mps: 0.156126, loss_cps: 0.320418
[14:04:38.159] iteration 24224: total_loss: 0.620181, loss_sup: 0.180050, loss_mps: 0.151164, loss_cps: 0.288968
[14:04:38.305] iteration 24225: total_loss: 0.525348, loss_sup: 0.038636, loss_mps: 0.161241, loss_cps: 0.325470
[14:04:38.451] iteration 24226: total_loss: 0.615514, loss_sup: 0.188256, loss_mps: 0.141950, loss_cps: 0.285308
[14:04:38.597] iteration 24227: total_loss: 0.390481, loss_sup: 0.031760, loss_mps: 0.129304, loss_cps: 0.229417
[14:04:38.743] iteration 24228: total_loss: 0.372667, loss_sup: 0.067907, loss_mps: 0.106944, loss_cps: 0.197816
[14:04:38.892] iteration 24229: total_loss: 0.743403, loss_sup: 0.061434, loss_mps: 0.227907, loss_cps: 0.454062
[14:04:39.038] iteration 24230: total_loss: 0.343777, loss_sup: 0.013951, loss_mps: 0.119762, loss_cps: 0.210063
[14:04:39.183] iteration 24231: total_loss: 0.778584, loss_sup: 0.029814, loss_mps: 0.235070, loss_cps: 0.513699
[14:04:39.329] iteration 24232: total_loss: 0.512820, loss_sup: 0.083825, loss_mps: 0.157070, loss_cps: 0.271924
[14:04:39.475] iteration 24233: total_loss: 0.498457, loss_sup: 0.075589, loss_mps: 0.140765, loss_cps: 0.282103
[14:04:39.620] iteration 24234: total_loss: 0.640543, loss_sup: 0.230841, loss_mps: 0.143209, loss_cps: 0.266493
[14:04:39.766] iteration 24235: total_loss: 0.344728, loss_sup: 0.098797, loss_mps: 0.095509, loss_cps: 0.150422
[14:04:39.912] iteration 24236: total_loss: 0.437489, loss_sup: 0.085748, loss_mps: 0.122353, loss_cps: 0.229388
[14:04:40.058] iteration 24237: total_loss: 0.334588, loss_sup: 0.009106, loss_mps: 0.114975, loss_cps: 0.210507
[14:04:40.204] iteration 24238: total_loss: 0.527867, loss_sup: 0.181917, loss_mps: 0.121496, loss_cps: 0.224455
[14:04:40.350] iteration 24239: total_loss: 0.879109, loss_sup: 0.149631, loss_mps: 0.233658, loss_cps: 0.495820
[14:04:40.495] iteration 24240: total_loss: 0.317911, loss_sup: 0.000448, loss_mps: 0.112655, loss_cps: 0.204809
[14:04:40.642] iteration 24241: total_loss: 0.450246, loss_sup: 0.033990, loss_mps: 0.139788, loss_cps: 0.276468
[14:04:40.788] iteration 24242: total_loss: 0.821777, loss_sup: 0.036629, loss_mps: 0.248550, loss_cps: 0.536598
[14:04:40.933] iteration 24243: total_loss: 0.199949, loss_sup: 0.004999, loss_mps: 0.073686, loss_cps: 0.121264
[14:04:40.997] iteration 24244: total_loss: 0.938537, loss_sup: 0.043895, loss_mps: 0.271753, loss_cps: 0.622889
[14:04:42.207] iteration 24245: total_loss: 0.668964, loss_sup: 0.085354, loss_mps: 0.189383, loss_cps: 0.394227
[14:04:42.356] iteration 24246: total_loss: 0.912574, loss_sup: 0.052848, loss_mps: 0.279901, loss_cps: 0.579825
[14:04:42.502] iteration 24247: total_loss: 0.311531, loss_sup: 0.032633, loss_mps: 0.102891, loss_cps: 0.176007
[14:04:42.649] iteration 24248: total_loss: 0.507466, loss_sup: 0.155337, loss_mps: 0.125088, loss_cps: 0.227041
[14:04:42.796] iteration 24249: total_loss: 0.635777, loss_sup: 0.191567, loss_mps: 0.146343, loss_cps: 0.297866
[14:04:42.943] iteration 24250: total_loss: 0.436254, loss_sup: 0.092639, loss_mps: 0.119564, loss_cps: 0.224052
[14:04:43.089] iteration 24251: total_loss: 0.467943, loss_sup: 0.036960, loss_mps: 0.146886, loss_cps: 0.284097
[14:04:43.238] iteration 24252: total_loss: 0.638098, loss_sup: 0.096230, loss_mps: 0.176620, loss_cps: 0.365248
[14:04:43.384] iteration 24253: total_loss: 0.837741, loss_sup: 0.111020, loss_mps: 0.230126, loss_cps: 0.496596
[14:04:43.530] iteration 24254: total_loss: 0.355116, loss_sup: 0.034525, loss_mps: 0.110221, loss_cps: 0.210370
[14:04:43.676] iteration 24255: total_loss: 0.891894, loss_sup: 0.039326, loss_mps: 0.273919, loss_cps: 0.578649
[14:04:43.825] iteration 24256: total_loss: 0.999480, loss_sup: 0.061151, loss_mps: 0.278923, loss_cps: 0.659407
[14:04:43.972] iteration 24257: total_loss: 0.656111, loss_sup: 0.014320, loss_mps: 0.209605, loss_cps: 0.432186
[14:04:44.119] iteration 24258: total_loss: 0.679367, loss_sup: 0.155359, loss_mps: 0.171146, loss_cps: 0.352861
[14:04:44.267] iteration 24259: total_loss: 0.455038, loss_sup: 0.071121, loss_mps: 0.129726, loss_cps: 0.254191
[14:04:44.413] iteration 24260: total_loss: 0.674994, loss_sup: 0.030855, loss_mps: 0.217970, loss_cps: 0.426168
[14:04:44.560] iteration 24261: total_loss: 0.232786, loss_sup: 0.001931, loss_mps: 0.083716, loss_cps: 0.147139
[14:04:44.707] iteration 24262: total_loss: 0.522565, loss_sup: 0.030931, loss_mps: 0.158326, loss_cps: 0.333308
[14:04:44.854] iteration 24263: total_loss: 0.627958, loss_sup: 0.019242, loss_mps: 0.200082, loss_cps: 0.408634
[14:04:45.002] iteration 24264: total_loss: 0.665179, loss_sup: 0.194046, loss_mps: 0.168531, loss_cps: 0.302602
[14:04:45.149] iteration 24265: total_loss: 0.501006, loss_sup: 0.018598, loss_mps: 0.155744, loss_cps: 0.326664
[14:04:45.296] iteration 24266: total_loss: 0.427748, loss_sup: 0.061946, loss_mps: 0.130026, loss_cps: 0.235776
[14:04:45.443] iteration 24267: total_loss: 0.560856, loss_sup: 0.031045, loss_mps: 0.174884, loss_cps: 0.354927
[14:04:45.590] iteration 24268: total_loss: 0.486558, loss_sup: 0.055391, loss_mps: 0.143719, loss_cps: 0.287448
[14:04:45.737] iteration 24269: total_loss: 0.437548, loss_sup: 0.064539, loss_mps: 0.130707, loss_cps: 0.242302
[14:04:45.884] iteration 24270: total_loss: 0.350073, loss_sup: 0.030041, loss_mps: 0.111591, loss_cps: 0.208441
[14:04:46.031] iteration 24271: total_loss: 0.245795, loss_sup: 0.002484, loss_mps: 0.090244, loss_cps: 0.153067
[14:04:46.178] iteration 24272: total_loss: 0.645749, loss_sup: 0.099189, loss_mps: 0.182470, loss_cps: 0.364090
[14:04:46.325] iteration 24273: total_loss: 0.562627, loss_sup: 0.071031, loss_mps: 0.164989, loss_cps: 0.326606
[14:04:46.472] iteration 24274: total_loss: 0.334942, loss_sup: 0.030447, loss_mps: 0.109273, loss_cps: 0.195222
[14:04:46.618] iteration 24275: total_loss: 0.462776, loss_sup: 0.032683, loss_mps: 0.139607, loss_cps: 0.290487
[14:04:46.764] iteration 24276: total_loss: 0.590753, loss_sup: 0.110959, loss_mps: 0.170288, loss_cps: 0.309505
[14:04:46.912] iteration 24277: total_loss: 0.645376, loss_sup: 0.144324, loss_mps: 0.158323, loss_cps: 0.342729
[14:04:47.059] iteration 24278: total_loss: 0.449724, loss_sup: 0.128187, loss_mps: 0.108022, loss_cps: 0.213515
[14:04:47.206] iteration 24279: total_loss: 0.422825, loss_sup: 0.054690, loss_mps: 0.134409, loss_cps: 0.233726
[14:04:47.352] iteration 24280: total_loss: 0.738310, loss_sup: 0.187867, loss_mps: 0.182918, loss_cps: 0.367525
[14:04:47.499] iteration 24281: total_loss: 0.564907, loss_sup: 0.117789, loss_mps: 0.164189, loss_cps: 0.282929
[14:04:47.645] iteration 24282: total_loss: 0.501097, loss_sup: 0.025988, loss_mps: 0.159107, loss_cps: 0.316002
[14:04:47.792] iteration 24283: total_loss: 0.282267, loss_sup: 0.018640, loss_mps: 0.097231, loss_cps: 0.166396
[14:04:47.938] iteration 24284: total_loss: 0.330258, loss_sup: 0.022600, loss_mps: 0.121425, loss_cps: 0.186233
[14:04:48.086] iteration 24285: total_loss: 0.531241, loss_sup: 0.149985, loss_mps: 0.123770, loss_cps: 0.257486
[14:04:48.232] iteration 24286: total_loss: 0.425568, loss_sup: 0.027238, loss_mps: 0.140578, loss_cps: 0.257753
[14:04:48.378] iteration 24287: total_loss: 0.903858, loss_sup: 0.160779, loss_mps: 0.241917, loss_cps: 0.501162
[14:04:48.525] iteration 24288: total_loss: 0.493688, loss_sup: 0.091261, loss_mps: 0.135195, loss_cps: 0.267232
[14:04:48.671] iteration 24289: total_loss: 0.664206, loss_sup: 0.133789, loss_mps: 0.181020, loss_cps: 0.349397
[14:04:48.817] iteration 24290: total_loss: 0.447340, loss_sup: 0.030017, loss_mps: 0.142605, loss_cps: 0.274718
[14:04:48.963] iteration 24291: total_loss: 0.515095, loss_sup: 0.099504, loss_mps: 0.142006, loss_cps: 0.273585
[14:04:49.110] iteration 24292: total_loss: 0.309559, loss_sup: 0.015382, loss_mps: 0.109589, loss_cps: 0.184588
[14:04:49.256] iteration 24293: total_loss: 0.668983, loss_sup: 0.250808, loss_mps: 0.147858, loss_cps: 0.270317
[14:04:49.403] iteration 24294: total_loss: 0.578927, loss_sup: 0.296085, loss_mps: 0.106330, loss_cps: 0.176512
[14:04:49.551] iteration 24295: total_loss: 0.593150, loss_sup: 0.020962, loss_mps: 0.200885, loss_cps: 0.371304
[14:04:49.697] iteration 24296: total_loss: 0.414385, loss_sup: 0.013895, loss_mps: 0.144795, loss_cps: 0.255695
[14:04:49.843] iteration 24297: total_loss: 0.291371, loss_sup: 0.030046, loss_mps: 0.093489, loss_cps: 0.167837
[14:04:49.991] iteration 24298: total_loss: 0.797471, loss_sup: 0.173885, loss_mps: 0.211285, loss_cps: 0.412300
[14:04:50.137] iteration 24299: total_loss: 0.520880, loss_sup: 0.059160, loss_mps: 0.159839, loss_cps: 0.301882
[14:04:50.284] iteration 24300: total_loss: 0.441418, loss_sup: 0.153179, loss_mps: 0.109024, loss_cps: 0.179216
[14:04:50.284] Evaluation Started ==>
[14:05:01.603] ==> valid iteration 24300: unet metrics: {'dc': 0.6509275281977185, 'jc': 0.5362012554302364, 'pre': 0.7937511541832307, 'hd': 5.405051649892993}, ynet metrics: {'dc': 0.5984943913383781, 'jc': 0.4856669952489345, 'pre': 0.7975339728362019, 'hd': 5.491870686029578}.
[14:05:01.605] Evaluation Finished!⏹️
[14:05:01.757] iteration 24301: total_loss: 0.499439, loss_sup: 0.062683, loss_mps: 0.157399, loss_cps: 0.279357
[14:05:01.906] iteration 24302: total_loss: 0.540511, loss_sup: 0.084328, loss_mps: 0.149219, loss_cps: 0.306964
[14:05:02.052] iteration 24303: total_loss: 0.558515, loss_sup: 0.073676, loss_mps: 0.161120, loss_cps: 0.323719
[14:05:02.197] iteration 24304: total_loss: 0.272590, loss_sup: 0.044297, loss_mps: 0.082696, loss_cps: 0.145598
[14:05:02.342] iteration 24305: total_loss: 0.453602, loss_sup: 0.026915, loss_mps: 0.152011, loss_cps: 0.274677
[14:05:02.487] iteration 24306: total_loss: 0.735070, loss_sup: 0.077545, loss_mps: 0.222163, loss_cps: 0.435362
[14:05:02.632] iteration 24307: total_loss: 0.495215, loss_sup: 0.037832, loss_mps: 0.144738, loss_cps: 0.312645
[14:05:02.777] iteration 24308: total_loss: 0.243435, loss_sup: 0.007642, loss_mps: 0.091901, loss_cps: 0.143892
[14:05:02.922] iteration 24309: total_loss: 0.899145, loss_sup: 0.067804, loss_mps: 0.247855, loss_cps: 0.583486
[14:05:03.067] iteration 24310: total_loss: 0.783162, loss_sup: 0.064817, loss_mps: 0.232108, loss_cps: 0.486237
[14:05:03.214] iteration 24311: total_loss: 0.365780, loss_sup: 0.049397, loss_mps: 0.121796, loss_cps: 0.194586
[14:05:03.359] iteration 24312: total_loss: 0.370295, loss_sup: 0.019923, loss_mps: 0.122455, loss_cps: 0.227917
[14:05:03.504] iteration 24313: total_loss: 0.612386, loss_sup: 0.029513, loss_mps: 0.186148, loss_cps: 0.396726
[14:05:03.649] iteration 24314: total_loss: 0.615896, loss_sup: 0.165105, loss_mps: 0.154786, loss_cps: 0.296005
[14:05:03.794] iteration 24315: total_loss: 0.434787, loss_sup: 0.032172, loss_mps: 0.136257, loss_cps: 0.266359
[14:05:03.940] iteration 24316: total_loss: 0.400452, loss_sup: 0.019548, loss_mps: 0.128899, loss_cps: 0.252005
[14:05:04.085] iteration 24317: total_loss: 0.314245, loss_sup: 0.018132, loss_mps: 0.106120, loss_cps: 0.189993
[14:05:04.230] iteration 24318: total_loss: 0.734559, loss_sup: 0.181624, loss_mps: 0.193992, loss_cps: 0.358943
[14:05:04.375] iteration 24319: total_loss: 0.360725, loss_sup: 0.019551, loss_mps: 0.119141, loss_cps: 0.222033
[14:05:04.521] iteration 24320: total_loss: 0.445246, loss_sup: 0.037889, loss_mps: 0.145318, loss_cps: 0.262039
[14:05:04.666] iteration 24321: total_loss: 0.724341, loss_sup: 0.184811, loss_mps: 0.174796, loss_cps: 0.364733
[14:05:04.811] iteration 24322: total_loss: 0.587946, loss_sup: 0.127751, loss_mps: 0.148544, loss_cps: 0.311651
[14:05:04.958] iteration 24323: total_loss: 0.477882, loss_sup: 0.032427, loss_mps: 0.163712, loss_cps: 0.281744
[14:05:05.103] iteration 24324: total_loss: 0.833141, loss_sup: 0.099938, loss_mps: 0.234569, loss_cps: 0.498634
[14:05:05.249] iteration 24325: total_loss: 0.472957, loss_sup: 0.080907, loss_mps: 0.131461, loss_cps: 0.260589
[14:05:05.394] iteration 24326: total_loss: 0.417162, loss_sup: 0.033119, loss_mps: 0.135140, loss_cps: 0.248902
[14:05:05.540] iteration 24327: total_loss: 0.248795, loss_sup: 0.020076, loss_mps: 0.082649, loss_cps: 0.146070
[14:05:05.688] iteration 24328: total_loss: 0.753705, loss_sup: 0.092786, loss_mps: 0.214446, loss_cps: 0.446474
[14:05:05.832] iteration 24329: total_loss: 0.486290, loss_sup: 0.033463, loss_mps: 0.153587, loss_cps: 0.299240
[14:05:05.978] iteration 24330: total_loss: 0.536500, loss_sup: 0.022536, loss_mps: 0.167947, loss_cps: 0.346018
[14:05:06.123] iteration 24331: total_loss: 0.553058, loss_sup: 0.119556, loss_mps: 0.148029, loss_cps: 0.285473
[14:05:06.268] iteration 24332: total_loss: 0.738434, loss_sup: 0.075777, loss_mps: 0.212388, loss_cps: 0.450269
[14:05:06.414] iteration 24333: total_loss: 0.602527, loss_sup: 0.076794, loss_mps: 0.173738, loss_cps: 0.351995
[14:05:06.559] iteration 24334: total_loss: 0.338387, loss_sup: 0.045866, loss_mps: 0.109261, loss_cps: 0.183260
[14:05:06.704] iteration 24335: total_loss: 0.458351, loss_sup: 0.030871, loss_mps: 0.155033, loss_cps: 0.272447
[14:05:06.850] iteration 24336: total_loss: 0.348006, loss_sup: 0.055914, loss_mps: 0.110488, loss_cps: 0.181605
[14:05:06.995] iteration 24337: total_loss: 0.515167, loss_sup: 0.050557, loss_mps: 0.157987, loss_cps: 0.306624
[14:05:07.140] iteration 24338: total_loss: 0.514839, loss_sup: 0.100427, loss_mps: 0.146737, loss_cps: 0.267674
[14:05:07.287] iteration 24339: total_loss: 0.952924, loss_sup: 0.044417, loss_mps: 0.295673, loss_cps: 0.612834
[14:05:07.432] iteration 24340: total_loss: 0.632492, loss_sup: 0.259935, loss_mps: 0.140052, loss_cps: 0.232505
[14:05:07.579] iteration 24341: total_loss: 0.520908, loss_sup: 0.067761, loss_mps: 0.156255, loss_cps: 0.296891
[14:05:07.725] iteration 24342: total_loss: 0.343751, loss_sup: 0.018886, loss_mps: 0.118835, loss_cps: 0.206030
[14:05:07.871] iteration 24343: total_loss: 0.728942, loss_sup: 0.248998, loss_mps: 0.166881, loss_cps: 0.313063
[14:05:08.016] iteration 24344: total_loss: 0.309340, loss_sup: 0.012865, loss_mps: 0.107071, loss_cps: 0.189404
[14:05:08.162] iteration 24345: total_loss: 0.595712, loss_sup: 0.107842, loss_mps: 0.167525, loss_cps: 0.320345
[14:05:08.307] iteration 24346: total_loss: 0.757177, loss_sup: 0.114038, loss_mps: 0.205166, loss_cps: 0.437974
[14:05:08.452] iteration 24347: total_loss: 0.187910, loss_sup: 0.001454, loss_mps: 0.070193, loss_cps: 0.116262
[14:05:08.598] iteration 24348: total_loss: 0.356678, loss_sup: 0.013352, loss_mps: 0.118743, loss_cps: 0.224582
[14:05:08.746] iteration 24349: total_loss: 0.231087, loss_sup: 0.022520, loss_mps: 0.078788, loss_cps: 0.129779
[14:05:08.892] iteration 24350: total_loss: 0.365448, loss_sup: 0.101074, loss_mps: 0.097953, loss_cps: 0.166422
[14:05:09.037] iteration 24351: total_loss: 0.689662, loss_sup: 0.113200, loss_mps: 0.185020, loss_cps: 0.391443
[14:05:09.183] iteration 24352: total_loss: 0.374796, loss_sup: 0.006744, loss_mps: 0.122677, loss_cps: 0.245375
[14:05:09.328] iteration 24353: total_loss: 0.400270, loss_sup: 0.030044, loss_mps: 0.125567, loss_cps: 0.244659
[14:05:09.474] iteration 24354: total_loss: 0.411793, loss_sup: 0.049556, loss_mps: 0.135007, loss_cps: 0.227230
[14:05:09.620] iteration 24355: total_loss: 0.462041, loss_sup: 0.039034, loss_mps: 0.147696, loss_cps: 0.275311
[14:05:09.766] iteration 24356: total_loss: 0.426687, loss_sup: 0.009219, loss_mps: 0.147945, loss_cps: 0.269523
[14:05:09.911] iteration 24357: total_loss: 0.466475, loss_sup: 0.018504, loss_mps: 0.151151, loss_cps: 0.296820
[14:05:10.056] iteration 24358: total_loss: 0.409171, loss_sup: 0.117645, loss_mps: 0.105862, loss_cps: 0.185664
[14:05:10.203] iteration 24359: total_loss: 0.447495, loss_sup: 0.032899, loss_mps: 0.137596, loss_cps: 0.276999
[14:05:10.348] iteration 24360: total_loss: 0.683002, loss_sup: 0.017588, loss_mps: 0.210048, loss_cps: 0.455365
[14:05:10.494] iteration 24361: total_loss: 0.269190, loss_sup: 0.013408, loss_mps: 0.089800, loss_cps: 0.165982
[14:05:10.641] iteration 24362: total_loss: 0.285997, loss_sup: 0.012009, loss_mps: 0.094985, loss_cps: 0.179004
[14:05:10.786] iteration 24363: total_loss: 0.508496, loss_sup: 0.023476, loss_mps: 0.166902, loss_cps: 0.318117
[14:05:10.931] iteration 24364: total_loss: 0.321467, loss_sup: 0.098630, loss_mps: 0.085953, loss_cps: 0.136885
[14:05:11.077] iteration 24365: total_loss: 0.520371, loss_sup: 0.055900, loss_mps: 0.155604, loss_cps: 0.308867
[14:05:11.224] iteration 24366: total_loss: 0.720261, loss_sup: 0.121320, loss_mps: 0.185741, loss_cps: 0.413200
[14:05:11.370] iteration 24367: total_loss: 0.424120, loss_sup: 0.089818, loss_mps: 0.114908, loss_cps: 0.219395
[14:05:11.515] iteration 24368: total_loss: 0.506238, loss_sup: 0.041553, loss_mps: 0.159726, loss_cps: 0.304959
[14:05:11.661] iteration 24369: total_loss: 0.457984, loss_sup: 0.048419, loss_mps: 0.136467, loss_cps: 0.273098
[14:05:11.806] iteration 24370: total_loss: 0.862737, loss_sup: 0.066271, loss_mps: 0.260803, loss_cps: 0.535663
[14:05:11.951] iteration 24371: total_loss: 0.445058, loss_sup: 0.048567, loss_mps: 0.139476, loss_cps: 0.257015
[14:05:12.097] iteration 24372: total_loss: 1.249295, loss_sup: 0.342366, loss_mps: 0.272757, loss_cps: 0.634172
[14:05:12.243] iteration 24373: total_loss: 0.718586, loss_sup: 0.098849, loss_mps: 0.205200, loss_cps: 0.414536
[14:05:12.389] iteration 24374: total_loss: 0.433882, loss_sup: 0.007539, loss_mps: 0.140412, loss_cps: 0.285931
[14:05:12.535] iteration 24375: total_loss: 0.242955, loss_sup: 0.005046, loss_mps: 0.085585, loss_cps: 0.152324
[14:05:12.681] iteration 24376: total_loss: 0.826293, loss_sup: 0.213287, loss_mps: 0.185535, loss_cps: 0.427471
[14:05:12.826] iteration 24377: total_loss: 0.417934, loss_sup: 0.041313, loss_mps: 0.130184, loss_cps: 0.246437
[14:05:12.972] iteration 24378: total_loss: 0.528959, loss_sup: 0.064120, loss_mps: 0.154312, loss_cps: 0.310527
[14:05:13.118] iteration 24379: total_loss: 0.436358, loss_sup: 0.036628, loss_mps: 0.133625, loss_cps: 0.266105
[14:05:13.264] iteration 24380: total_loss: 0.562197, loss_sup: 0.096786, loss_mps: 0.159545, loss_cps: 0.305866
[14:05:13.409] iteration 24381: total_loss: 0.673977, loss_sup: 0.051470, loss_mps: 0.201541, loss_cps: 0.420967
[14:05:13.555] iteration 24382: total_loss: 0.418364, loss_sup: 0.039000, loss_mps: 0.137093, loss_cps: 0.242272
[14:05:13.702] iteration 24383: total_loss: 0.937605, loss_sup: 0.104585, loss_mps: 0.254943, loss_cps: 0.578078
[14:05:13.847] iteration 24384: total_loss: 0.244922, loss_sup: 0.011331, loss_mps: 0.080710, loss_cps: 0.152882
[14:05:13.993] iteration 24385: total_loss: 0.580649, loss_sup: 0.086007, loss_mps: 0.175561, loss_cps: 0.319081
[14:05:14.140] iteration 24386: total_loss: 0.486360, loss_sup: 0.076282, loss_mps: 0.138386, loss_cps: 0.271693
[14:05:14.285] iteration 24387: total_loss: 0.391481, loss_sup: 0.030665, loss_mps: 0.125867, loss_cps: 0.234948
[14:05:14.431] iteration 24388: total_loss: 0.347719, loss_sup: 0.030464, loss_mps: 0.112122, loss_cps: 0.205133
[14:05:14.577] iteration 24389: total_loss: 0.402940, loss_sup: 0.005735, loss_mps: 0.145405, loss_cps: 0.251800
[14:05:14.724] iteration 24390: total_loss: 0.370736, loss_sup: 0.003016, loss_mps: 0.119365, loss_cps: 0.248355
[14:05:14.870] iteration 24391: total_loss: 0.312002, loss_sup: 0.043903, loss_mps: 0.097286, loss_cps: 0.170812
[14:05:15.016] iteration 24392: total_loss: 0.228029, loss_sup: 0.009421, loss_mps: 0.080513, loss_cps: 0.138095
[14:05:15.162] iteration 24393: total_loss: 0.303293, loss_sup: 0.002871, loss_mps: 0.109524, loss_cps: 0.190898
[14:05:15.309] iteration 24394: total_loss: 0.288068, loss_sup: 0.022447, loss_mps: 0.098367, loss_cps: 0.167255
[14:05:15.455] iteration 24395: total_loss: 0.384542, loss_sup: 0.072161, loss_mps: 0.113676, loss_cps: 0.198705
[14:05:15.601] iteration 24396: total_loss: 0.558633, loss_sup: 0.004843, loss_mps: 0.183862, loss_cps: 0.369928
[14:05:15.748] iteration 24397: total_loss: 0.256857, loss_sup: 0.015957, loss_mps: 0.090740, loss_cps: 0.150160
[14:05:15.895] iteration 24398: total_loss: 0.368009, loss_sup: 0.025863, loss_mps: 0.120467, loss_cps: 0.221678
[14:05:16.041] iteration 24399: total_loss: 0.530991, loss_sup: 0.020685, loss_mps: 0.176444, loss_cps: 0.333863
[14:05:16.189] iteration 24400: total_loss: 0.512815, loss_sup: 0.013676, loss_mps: 0.163988, loss_cps: 0.335150
[14:05:16.189] Evaluation Started ==>
[14:05:27.510] ==> valid iteration 24400: unet metrics: {'dc': 0.6627043456982892, 'jc': 0.5478492710479366, 'pre': 0.8031675134244792, 'hd': 5.408089327851449}, ynet metrics: {'dc': 0.6247659338889267, 'jc': 0.5095947362701471, 'pre': 0.8022503100162007, 'hd': 5.466252556749849}.
[14:05:27.512] Evaluation Finished!⏹️
[14:05:27.664] iteration 24401: total_loss: 0.789047, loss_sup: 0.066774, loss_mps: 0.227688, loss_cps: 0.494585
[14:05:27.811] iteration 24402: total_loss: 0.560661, loss_sup: 0.012890, loss_mps: 0.177969, loss_cps: 0.369803
[14:05:27.956] iteration 24403: total_loss: 0.485518, loss_sup: 0.061134, loss_mps: 0.151001, loss_cps: 0.273383
[14:05:28.102] iteration 24404: total_loss: 0.373474, loss_sup: 0.037873, loss_mps: 0.122145, loss_cps: 0.213455
[14:05:28.248] iteration 24405: total_loss: 0.411636, loss_sup: 0.015895, loss_mps: 0.135148, loss_cps: 0.260593
[14:05:28.393] iteration 24406: total_loss: 0.368211, loss_sup: 0.086987, loss_mps: 0.104608, loss_cps: 0.176616
[14:05:28.540] iteration 24407: total_loss: 0.388465, loss_sup: 0.027384, loss_mps: 0.122810, loss_cps: 0.238271
[14:05:28.685] iteration 24408: total_loss: 0.575477, loss_sup: 0.004350, loss_mps: 0.183366, loss_cps: 0.387760
[14:05:28.831] iteration 24409: total_loss: 0.546233, loss_sup: 0.025868, loss_mps: 0.163909, loss_cps: 0.356455
[14:05:28.979] iteration 24410: total_loss: 0.654953, loss_sup: 0.096022, loss_mps: 0.194095, loss_cps: 0.364836
[14:05:29.124] iteration 24411: total_loss: 0.362410, loss_sup: 0.030512, loss_mps: 0.112032, loss_cps: 0.219867
[14:05:29.270] iteration 24412: total_loss: 0.375769, loss_sup: 0.036792, loss_mps: 0.113859, loss_cps: 0.225119
[14:05:29.418] iteration 24413: total_loss: 0.338241, loss_sup: 0.023234, loss_mps: 0.116717, loss_cps: 0.198289
[14:05:29.566] iteration 24414: total_loss: 0.425310, loss_sup: 0.083233, loss_mps: 0.114961, loss_cps: 0.227117
[14:05:29.714] iteration 24415: total_loss: 0.310983, loss_sup: 0.038797, loss_mps: 0.101299, loss_cps: 0.170888
[14:05:29.859] iteration 24416: total_loss: 0.270416, loss_sup: 0.077426, loss_mps: 0.070673, loss_cps: 0.122316
[14:05:30.004] iteration 24417: total_loss: 0.763716, loss_sup: 0.079508, loss_mps: 0.208741, loss_cps: 0.475467
[14:05:30.151] iteration 24418: total_loss: 0.222486, loss_sup: 0.027034, loss_mps: 0.073905, loss_cps: 0.121548
[14:05:30.297] iteration 24419: total_loss: 0.360448, loss_sup: 0.006526, loss_mps: 0.124510, loss_cps: 0.229412
[14:05:30.442] iteration 24420: total_loss: 0.309966, loss_sup: 0.054654, loss_mps: 0.096810, loss_cps: 0.158502
[14:05:30.588] iteration 24421: total_loss: 0.475300, loss_sup: 0.182091, loss_mps: 0.098399, loss_cps: 0.194810
[14:05:30.733] iteration 24422: total_loss: 0.417198, loss_sup: 0.057757, loss_mps: 0.131493, loss_cps: 0.227948
[14:05:30.878] iteration 24423: total_loss: 0.445534, loss_sup: 0.060274, loss_mps: 0.133053, loss_cps: 0.252207
[14:05:31.023] iteration 24424: total_loss: 0.534413, loss_sup: 0.026727, loss_mps: 0.178669, loss_cps: 0.329017
[14:05:31.169] iteration 24425: total_loss: 0.623637, loss_sup: 0.122693, loss_mps: 0.169413, loss_cps: 0.331531
[14:05:31.314] iteration 24426: total_loss: 0.440555, loss_sup: 0.047314, loss_mps: 0.137808, loss_cps: 0.255434
[14:05:31.460] iteration 24427: total_loss: 0.373819, loss_sup: 0.061452, loss_mps: 0.111443, loss_cps: 0.200924
[14:05:31.605] iteration 24428: total_loss: 1.102969, loss_sup: 0.243147, loss_mps: 0.272840, loss_cps: 0.586982
[14:05:31.750] iteration 24429: total_loss: 0.266337, loss_sup: 0.010490, loss_mps: 0.088257, loss_cps: 0.167590
[14:05:31.896] iteration 24430: total_loss: 1.089811, loss_sup: 0.181386, loss_mps: 0.274614, loss_cps: 0.633811
[14:05:32.043] iteration 24431: total_loss: 0.727671, loss_sup: 0.020302, loss_mps: 0.226626, loss_cps: 0.480744
[14:05:32.188] iteration 24432: total_loss: 0.546765, loss_sup: 0.016316, loss_mps: 0.170746, loss_cps: 0.359702
[14:05:32.335] iteration 24433: total_loss: 0.931164, loss_sup: 0.168840, loss_mps: 0.244988, loss_cps: 0.517336
[14:05:32.481] iteration 24434: total_loss: 0.546549, loss_sup: 0.081065, loss_mps: 0.157372, loss_cps: 0.308112
[14:05:32.628] iteration 24435: total_loss: 0.388672, loss_sup: 0.057987, loss_mps: 0.113630, loss_cps: 0.217055
[14:05:32.774] iteration 24436: total_loss: 0.606000, loss_sup: 0.042139, loss_mps: 0.192094, loss_cps: 0.371767
[14:05:32.919] iteration 24437: total_loss: 0.357676, loss_sup: 0.023503, loss_mps: 0.114272, loss_cps: 0.219901
[14:05:33.065] iteration 24438: total_loss: 0.286428, loss_sup: 0.006721, loss_mps: 0.100226, loss_cps: 0.179481
[14:05:33.212] iteration 24439: total_loss: 1.023877, loss_sup: 0.112272, loss_mps: 0.294039, loss_cps: 0.617567
[14:05:33.358] iteration 24440: total_loss: 0.501316, loss_sup: 0.145809, loss_mps: 0.125325, loss_cps: 0.230183
[14:05:33.504] iteration 24441: total_loss: 0.439018, loss_sup: 0.025033, loss_mps: 0.136228, loss_cps: 0.277757
[14:05:33.650] iteration 24442: total_loss: 0.279496, loss_sup: 0.003787, loss_mps: 0.097483, loss_cps: 0.178226
[14:05:33.797] iteration 24443: total_loss: 0.316887, loss_sup: 0.028230, loss_mps: 0.106670, loss_cps: 0.181988
[14:05:33.943] iteration 24444: total_loss: 0.660403, loss_sup: 0.072096, loss_mps: 0.205582, loss_cps: 0.382725
[14:05:34.089] iteration 24445: total_loss: 0.401197, loss_sup: 0.049827, loss_mps: 0.119924, loss_cps: 0.231446
[14:05:34.234] iteration 24446: total_loss: 0.570413, loss_sup: 0.153457, loss_mps: 0.149204, loss_cps: 0.267753
[14:05:34.380] iteration 24447: total_loss: 0.467709, loss_sup: 0.050139, loss_mps: 0.152927, loss_cps: 0.264643
[14:05:34.526] iteration 24448: total_loss: 0.285615, loss_sup: 0.029727, loss_mps: 0.094742, loss_cps: 0.161145
[14:05:34.671] iteration 24449: total_loss: 0.348610, loss_sup: 0.021745, loss_mps: 0.115572, loss_cps: 0.211293
[14:05:34.817] iteration 24450: total_loss: 0.330780, loss_sup: 0.045089, loss_mps: 0.104599, loss_cps: 0.181092
[14:05:34.963] iteration 24451: total_loss: 0.387559, loss_sup: 0.095575, loss_mps: 0.104032, loss_cps: 0.187952
[14:05:35.112] iteration 24452: total_loss: 0.584449, loss_sup: 0.286853, loss_mps: 0.101674, loss_cps: 0.195922
[14:05:35.258] iteration 24453: total_loss: 0.593882, loss_sup: 0.016713, loss_mps: 0.190904, loss_cps: 0.386265
[14:05:35.403] iteration 24454: total_loss: 0.640170, loss_sup: 0.091452, loss_mps: 0.183197, loss_cps: 0.365521
[14:05:35.549] iteration 24455: total_loss: 0.638561, loss_sup: 0.079760, loss_mps: 0.185821, loss_cps: 0.372980
[14:05:35.694] iteration 24456: total_loss: 0.282576, loss_sup: 0.024176, loss_mps: 0.091453, loss_cps: 0.166947
[14:05:35.840] iteration 24457: total_loss: 0.537480, loss_sup: 0.047028, loss_mps: 0.142876, loss_cps: 0.347577
[14:05:35.985] iteration 24458: total_loss: 0.303416, loss_sup: 0.022323, loss_mps: 0.097653, loss_cps: 0.183440
[14:05:36.131] iteration 24459: total_loss: 0.433029, loss_sup: 0.064141, loss_mps: 0.134054, loss_cps: 0.234834
[14:05:36.279] iteration 24460: total_loss: 0.433879, loss_sup: 0.107855, loss_mps: 0.118179, loss_cps: 0.207845
[14:05:36.425] iteration 24461: total_loss: 0.514481, loss_sup: 0.108737, loss_mps: 0.147854, loss_cps: 0.257890
[14:05:36.574] iteration 24462: total_loss: 0.270735, loss_sup: 0.005494, loss_mps: 0.094526, loss_cps: 0.170716
[14:05:36.720] iteration 24463: total_loss: 0.374388, loss_sup: 0.020811, loss_mps: 0.130034, loss_cps: 0.223544
[14:05:36.866] iteration 24464: total_loss: 0.374090, loss_sup: 0.027701, loss_mps: 0.120639, loss_cps: 0.225750
[14:05:37.012] iteration 24465: total_loss: 0.247945, loss_sup: 0.000565, loss_mps: 0.089321, loss_cps: 0.158058
[14:05:37.161] iteration 24466: total_loss: 0.583245, loss_sup: 0.218690, loss_mps: 0.132510, loss_cps: 0.232045
[14:05:37.307] iteration 24467: total_loss: 0.385384, loss_sup: 0.007814, loss_mps: 0.130079, loss_cps: 0.247490
[14:05:37.452] iteration 24468: total_loss: 0.553554, loss_sup: 0.055560, loss_mps: 0.169443, loss_cps: 0.328551
[14:05:37.601] iteration 24469: total_loss: 0.699799, loss_sup: 0.050709, loss_mps: 0.216913, loss_cps: 0.432177
[14:05:37.747] iteration 24470: total_loss: 0.280887, loss_sup: 0.072063, loss_mps: 0.077108, loss_cps: 0.131716
[14:05:37.893] iteration 24471: total_loss: 0.311503, loss_sup: 0.018027, loss_mps: 0.107115, loss_cps: 0.186361
[14:05:38.039] iteration 24472: total_loss: 0.400872, loss_sup: 0.074016, loss_mps: 0.119078, loss_cps: 0.207778
[14:05:38.185] iteration 24473: total_loss: 0.525681, loss_sup: 0.137175, loss_mps: 0.137021, loss_cps: 0.251485
[14:05:38.331] iteration 24474: total_loss: 0.475761, loss_sup: 0.060578, loss_mps: 0.140843, loss_cps: 0.274340
[14:05:38.477] iteration 24475: total_loss: 0.386421, loss_sup: 0.037671, loss_mps: 0.130297, loss_cps: 0.218453
[14:05:38.622] iteration 24476: total_loss: 0.537519, loss_sup: 0.077703, loss_mps: 0.156507, loss_cps: 0.303308
[14:05:38.768] iteration 24477: total_loss: 0.523207, loss_sup: 0.008652, loss_mps: 0.174775, loss_cps: 0.339780
[14:05:38.914] iteration 24478: total_loss: 0.319773, loss_sup: 0.009558, loss_mps: 0.109826, loss_cps: 0.200388
[14:05:39.062] iteration 24479: total_loss: 0.759812, loss_sup: 0.113077, loss_mps: 0.201486, loss_cps: 0.445250
[14:05:39.207] iteration 24480: total_loss: 0.662127, loss_sup: 0.018867, loss_mps: 0.204766, loss_cps: 0.438495
[14:05:39.353] iteration 24481: total_loss: 0.248495, loss_sup: 0.014281, loss_mps: 0.087283, loss_cps: 0.146932
[14:05:39.501] iteration 24482: total_loss: 1.358528, loss_sup: 0.312411, loss_mps: 0.327418, loss_cps: 0.718699
[14:05:39.646] iteration 24483: total_loss: 0.607358, loss_sup: 0.077039, loss_mps: 0.179991, loss_cps: 0.350328
[14:05:39.793] iteration 24484: total_loss: 0.404300, loss_sup: 0.023587, loss_mps: 0.134308, loss_cps: 0.246406
[14:05:39.942] iteration 24485: total_loss: 0.417896, loss_sup: 0.033515, loss_mps: 0.136666, loss_cps: 0.247716
[14:05:40.088] iteration 24486: total_loss: 0.417279, loss_sup: 0.107017, loss_mps: 0.107095, loss_cps: 0.203167
[14:05:40.235] iteration 24487: total_loss: 0.551574, loss_sup: 0.050317, loss_mps: 0.160124, loss_cps: 0.341133
[14:05:40.381] iteration 24488: total_loss: 0.650090, loss_sup: 0.110096, loss_mps: 0.178128, loss_cps: 0.361866
[14:05:40.527] iteration 24489: total_loss: 0.614819, loss_sup: 0.050183, loss_mps: 0.190827, loss_cps: 0.373810
[14:05:40.673] iteration 24490: total_loss: 0.614804, loss_sup: 0.171622, loss_mps: 0.157875, loss_cps: 0.285307
[14:05:40.819] iteration 24491: total_loss: 0.359682, loss_sup: 0.118801, loss_mps: 0.087418, loss_cps: 0.153462
[14:05:40.965] iteration 24492: total_loss: 0.330730, loss_sup: 0.006503, loss_mps: 0.116337, loss_cps: 0.207890
[14:05:41.115] iteration 24493: total_loss: 0.632598, loss_sup: 0.012817, loss_mps: 0.198597, loss_cps: 0.421185
[14:05:41.261] iteration 24494: total_loss: 0.944468, loss_sup: 0.047091, loss_mps: 0.280601, loss_cps: 0.616776
[14:05:41.409] iteration 24495: total_loss: 2.334899, loss_sup: 0.302946, loss_mps: 0.589961, loss_cps: 1.441991
[14:05:41.555] iteration 24496: total_loss: 0.686462, loss_sup: 0.157993, loss_mps: 0.173519, loss_cps: 0.354950
[14:05:41.701] iteration 24497: total_loss: 0.339775, loss_sup: 0.006062, loss_mps: 0.110644, loss_cps: 0.223069
[14:05:41.848] iteration 24498: total_loss: 0.646666, loss_sup: 0.035635, loss_mps: 0.197477, loss_cps: 0.413554
[14:05:41.994] iteration 24499: total_loss: 0.453960, loss_sup: 0.007695, loss_mps: 0.144659, loss_cps: 0.301605
[14:05:42.140] iteration 24500: total_loss: 0.520438, loss_sup: 0.045975, loss_mps: 0.164835, loss_cps: 0.309629
[14:05:42.140] Evaluation Started ==>
[14:05:53.462] ==> valid iteration 24500: unet metrics: {'dc': 0.6606055403082147, 'jc': 0.5470984711414381, 'pre': 0.7873627589838001, 'hd': 5.289488862947052}, ynet metrics: {'dc': 0.6321301143231063, 'jc': 0.5178874510630188, 'pre': 0.824556719849192, 'hd': 5.3499316480209735}.
[14:05:53.464] Evaluation Finished!⏹️
[14:05:53.614] iteration 24501: total_loss: 0.587150, loss_sup: 0.061362, loss_mps: 0.170237, loss_cps: 0.355552
[14:05:53.762] iteration 24502: total_loss: 0.768469, loss_sup: 0.068418, loss_mps: 0.205612, loss_cps: 0.494439
[14:05:53.907] iteration 24503: total_loss: 0.287770, loss_sup: 0.021073, loss_mps: 0.094994, loss_cps: 0.171702
[14:05:54.053] iteration 24504: total_loss: 1.166297, loss_sup: 0.308197, loss_mps: 0.254528, loss_cps: 0.603572
[14:05:54.199] iteration 24505: total_loss: 0.880806, loss_sup: 0.067482, loss_mps: 0.251240, loss_cps: 0.562083
[14:05:54.345] iteration 24506: total_loss: 0.705240, loss_sup: 0.076088, loss_mps: 0.207446, loss_cps: 0.421707
[14:05:54.491] iteration 24507: total_loss: 0.443232, loss_sup: 0.045279, loss_mps: 0.145572, loss_cps: 0.252380
[14:05:54.636] iteration 24508: total_loss: 0.924295, loss_sup: 0.148494, loss_mps: 0.261455, loss_cps: 0.514345
[14:05:54.786] iteration 24509: total_loss: 0.869126, loss_sup: 0.091661, loss_mps: 0.246565, loss_cps: 0.530900
[14:05:54.932] iteration 24510: total_loss: 0.455261, loss_sup: 0.007769, loss_mps: 0.156556, loss_cps: 0.290936
[14:05:55.077] iteration 24511: total_loss: 0.499084, loss_sup: 0.081165, loss_mps: 0.148437, loss_cps: 0.269482
[14:05:55.226] iteration 24512: total_loss: 0.436319, loss_sup: 0.056298, loss_mps: 0.144548, loss_cps: 0.235472
[14:05:55.372] iteration 24513: total_loss: 0.612540, loss_sup: 0.053468, loss_mps: 0.197208, loss_cps: 0.361864
[14:05:55.517] iteration 24514: total_loss: 0.289411, loss_sup: 0.002610, loss_mps: 0.104021, loss_cps: 0.182779
[14:05:55.663] iteration 24515: total_loss: 0.328164, loss_sup: 0.015742, loss_mps: 0.108629, loss_cps: 0.203793
[14:05:55.809] iteration 24516: total_loss: 0.407953, loss_sup: 0.004527, loss_mps: 0.145849, loss_cps: 0.257577
[14:05:55.957] iteration 24517: total_loss: 0.384478, loss_sup: 0.056163, loss_mps: 0.115457, loss_cps: 0.212858
[14:05:56.103] iteration 24518: total_loss: 0.323786, loss_sup: 0.033402, loss_mps: 0.111295, loss_cps: 0.179089
[14:05:56.249] iteration 24519: total_loss: 0.863849, loss_sup: 0.057524, loss_mps: 0.254325, loss_cps: 0.552000
[14:05:56.395] iteration 24520: total_loss: 0.281611, loss_sup: 0.022881, loss_mps: 0.096119, loss_cps: 0.162610
[14:05:56.541] iteration 24521: total_loss: 0.268638, loss_sup: 0.036854, loss_mps: 0.091012, loss_cps: 0.140772
[14:05:56.686] iteration 24522: total_loss: 0.456527, loss_sup: 0.027762, loss_mps: 0.148424, loss_cps: 0.280342
[14:05:56.832] iteration 24523: total_loss: 0.438801, loss_sup: 0.195388, loss_mps: 0.089561, loss_cps: 0.153852
[14:05:56.977] iteration 24524: total_loss: 0.277335, loss_sup: 0.037918, loss_mps: 0.084119, loss_cps: 0.155298
[14:05:57.124] iteration 24525: total_loss: 0.424865, loss_sup: 0.034997, loss_mps: 0.139033, loss_cps: 0.250835
[14:05:57.270] iteration 24526: total_loss: 0.847703, loss_sup: 0.074544, loss_mps: 0.254466, loss_cps: 0.518693
[14:05:57.416] iteration 24527: total_loss: 0.566790, loss_sup: 0.196304, loss_mps: 0.127832, loss_cps: 0.242654
[14:05:57.566] iteration 24528: total_loss: 0.294005, loss_sup: 0.022274, loss_mps: 0.101483, loss_cps: 0.170248
[14:05:57.712] iteration 24529: total_loss: 0.298983, loss_sup: 0.067679, loss_mps: 0.084716, loss_cps: 0.146588
[14:05:57.858] iteration 24530: total_loss: 0.470653, loss_sup: 0.059275, loss_mps: 0.141126, loss_cps: 0.270253
[14:05:58.007] iteration 24531: total_loss: 0.481840, loss_sup: 0.039260, loss_mps: 0.155493, loss_cps: 0.287087
[14:05:58.154] iteration 24532: total_loss: 0.251856, loss_sup: 0.039242, loss_mps: 0.083894, loss_cps: 0.128719
[14:05:58.301] iteration 24533: total_loss: 0.438438, loss_sup: 0.100347, loss_mps: 0.125389, loss_cps: 0.212701
[14:05:58.447] iteration 24534: total_loss: 1.196564, loss_sup: 0.165261, loss_mps: 0.318978, loss_cps: 0.712326
[14:05:58.593] iteration 24535: total_loss: 0.370350, loss_sup: 0.008884, loss_mps: 0.124748, loss_cps: 0.236719
[14:05:58.740] iteration 24536: total_loss: 0.420160, loss_sup: 0.116631, loss_mps: 0.116976, loss_cps: 0.186553
[14:05:58.888] iteration 24537: total_loss: 0.389337, loss_sup: 0.140078, loss_mps: 0.094570, loss_cps: 0.154689
[14:05:59.035] iteration 24538: total_loss: 0.331163, loss_sup: 0.074071, loss_mps: 0.096396, loss_cps: 0.160696
[14:05:59.182] iteration 24539: total_loss: 0.473450, loss_sup: 0.070638, loss_mps: 0.133872, loss_cps: 0.268940
[14:05:59.328] iteration 24540: total_loss: 0.727796, loss_sup: 0.066375, loss_mps: 0.209927, loss_cps: 0.451494
[14:05:59.475] iteration 24541: total_loss: 0.677096, loss_sup: 0.139584, loss_mps: 0.174901, loss_cps: 0.362611
[14:05:59.621] iteration 24542: total_loss: 0.347966, loss_sup: 0.058266, loss_mps: 0.100806, loss_cps: 0.188894
[14:05:59.767] iteration 24543: total_loss: 0.262465, loss_sup: 0.017658, loss_mps: 0.092891, loss_cps: 0.151916
[14:05:59.913] iteration 24544: total_loss: 0.380460, loss_sup: 0.085776, loss_mps: 0.111788, loss_cps: 0.182896
[14:06:00.060] iteration 24545: total_loss: 0.335907, loss_sup: 0.024245, loss_mps: 0.111743, loss_cps: 0.199918
[14:06:00.206] iteration 24546: total_loss: 0.328132, loss_sup: 0.017898, loss_mps: 0.110752, loss_cps: 0.199481
[14:06:00.352] iteration 24547: total_loss: 0.440430, loss_sup: 0.128547, loss_mps: 0.107535, loss_cps: 0.204348
[14:06:00.500] iteration 24548: total_loss: 0.589191, loss_sup: 0.095878, loss_mps: 0.167473, loss_cps: 0.325840
[14:06:00.646] iteration 24549: total_loss: 0.644510, loss_sup: 0.240033, loss_mps: 0.143208, loss_cps: 0.261269
[14:06:00.791] iteration 24550: total_loss: 0.320470, loss_sup: 0.105211, loss_mps: 0.082852, loss_cps: 0.132407
[14:06:00.938] iteration 24551: total_loss: 0.356685, loss_sup: 0.026031, loss_mps: 0.115721, loss_cps: 0.214933
[14:06:01.084] iteration 24552: total_loss: 0.773276, loss_sup: 0.063816, loss_mps: 0.232344, loss_cps: 0.477115
[14:06:01.231] iteration 24553: total_loss: 0.555300, loss_sup: 0.095840, loss_mps: 0.158095, loss_cps: 0.301364
[14:06:01.380] iteration 24554: total_loss: 0.462987, loss_sup: 0.040326, loss_mps: 0.142758, loss_cps: 0.279902
[14:06:01.527] iteration 24555: total_loss: 0.705155, loss_sup: 0.065499, loss_mps: 0.212244, loss_cps: 0.427411
[14:06:01.675] iteration 24556: total_loss: 0.397174, loss_sup: 0.005445, loss_mps: 0.132228, loss_cps: 0.259500
[14:06:01.824] iteration 24557: total_loss: 0.504011, loss_sup: 0.043066, loss_mps: 0.157738, loss_cps: 0.303206
[14:06:01.970] iteration 24558: total_loss: 0.492099, loss_sup: 0.036140, loss_mps: 0.162669, loss_cps: 0.293290
[14:06:02.116] iteration 24559: total_loss: 0.388037, loss_sup: 0.069795, loss_mps: 0.119188, loss_cps: 0.199055
[14:06:02.263] iteration 24560: total_loss: 0.378905, loss_sup: 0.002620, loss_mps: 0.124756, loss_cps: 0.251529
[14:06:02.410] iteration 24561: total_loss: 0.422268, loss_sup: 0.019087, loss_mps: 0.139323, loss_cps: 0.263859
[14:06:02.557] iteration 24562: total_loss: 0.219898, loss_sup: 0.016428, loss_mps: 0.076164, loss_cps: 0.127306
[14:06:02.703] iteration 24563: total_loss: 0.511390, loss_sup: 0.007832, loss_mps: 0.167077, loss_cps: 0.336481
[14:06:02.850] iteration 24564: total_loss: 0.616301, loss_sup: 0.012626, loss_mps: 0.198848, loss_cps: 0.404828
[14:06:02.996] iteration 24565: total_loss: 0.326813, loss_sup: 0.044688, loss_mps: 0.099189, loss_cps: 0.182936
[14:06:03.143] iteration 24566: total_loss: 0.214874, loss_sup: 0.003702, loss_mps: 0.078327, loss_cps: 0.132845
[14:06:03.293] iteration 24567: total_loss: 0.260012, loss_sup: 0.013400, loss_mps: 0.094477, loss_cps: 0.152136
[14:06:03.440] iteration 24568: total_loss: 0.373461, loss_sup: 0.024274, loss_mps: 0.126604, loss_cps: 0.222583
[14:06:03.587] iteration 24569: total_loss: 0.331432, loss_sup: 0.014289, loss_mps: 0.114393, loss_cps: 0.202751
[14:06:03.733] iteration 24570: total_loss: 0.435886, loss_sup: 0.150016, loss_mps: 0.103328, loss_cps: 0.182541
[14:06:03.884] iteration 24571: total_loss: 0.926990, loss_sup: 0.113802, loss_mps: 0.247290, loss_cps: 0.565898
[14:06:04.030] iteration 24572: total_loss: 0.375853, loss_sup: 0.036247, loss_mps: 0.122045, loss_cps: 0.217560
[14:06:04.177] iteration 24573: total_loss: 0.390599, loss_sup: 0.033345, loss_mps: 0.129554, loss_cps: 0.227699
[14:06:04.323] iteration 24574: total_loss: 0.333194, loss_sup: 0.047908, loss_mps: 0.099654, loss_cps: 0.185632
[14:06:04.470] iteration 24575: total_loss: 0.596642, loss_sup: 0.077153, loss_mps: 0.173467, loss_cps: 0.346022
[14:06:04.621] iteration 24576: total_loss: 0.595090, loss_sup: 0.101111, loss_mps: 0.158504, loss_cps: 0.335475
[14:06:04.768] iteration 24577: total_loss: 0.533401, loss_sup: 0.009716, loss_mps: 0.163365, loss_cps: 0.360320
[14:06:04.914] iteration 24578: total_loss: 0.716646, loss_sup: 0.285412, loss_mps: 0.144377, loss_cps: 0.286857
[14:06:05.061] iteration 24579: total_loss: 0.284374, loss_sup: 0.032128, loss_mps: 0.089839, loss_cps: 0.162407
[14:06:05.210] iteration 24580: total_loss: 0.246005, loss_sup: 0.023490, loss_mps: 0.082721, loss_cps: 0.139794
[14:06:05.357] iteration 24581: total_loss: 0.375242, loss_sup: 0.021692, loss_mps: 0.127264, loss_cps: 0.226286
[14:06:05.503] iteration 24582: total_loss: 0.462681, loss_sup: 0.070383, loss_mps: 0.145129, loss_cps: 0.247170
[14:06:05.649] iteration 24583: total_loss: 0.244078, loss_sup: 0.034449, loss_mps: 0.077633, loss_cps: 0.131996
[14:06:05.794] iteration 24584: total_loss: 0.500746, loss_sup: 0.005654, loss_mps: 0.153928, loss_cps: 0.341165
[14:06:05.942] iteration 24585: total_loss: 0.805666, loss_sup: 0.080880, loss_mps: 0.221510, loss_cps: 0.503277
[14:06:06.088] iteration 24586: total_loss: 0.544418, loss_sup: 0.095671, loss_mps: 0.152475, loss_cps: 0.296272
[14:06:06.238] iteration 24587: total_loss: 0.512308, loss_sup: 0.058805, loss_mps: 0.153099, loss_cps: 0.300404
[14:06:06.385] iteration 24588: total_loss: 0.358006, loss_sup: 0.061484, loss_mps: 0.098000, loss_cps: 0.198521
[14:06:06.535] iteration 24589: total_loss: 0.429842, loss_sup: 0.016479, loss_mps: 0.145520, loss_cps: 0.267843
[14:06:06.682] iteration 24590: total_loss: 0.617584, loss_sup: 0.159659, loss_mps: 0.148070, loss_cps: 0.309854
[14:06:06.828] iteration 24591: total_loss: 0.429692, loss_sup: 0.055497, loss_mps: 0.126025, loss_cps: 0.248169
[14:06:06.975] iteration 24592: total_loss: 0.399435, loss_sup: 0.007431, loss_mps: 0.125976, loss_cps: 0.266028
[14:06:07.122] iteration 24593: total_loss: 0.315394, loss_sup: 0.024721, loss_mps: 0.101290, loss_cps: 0.189384
[14:06:07.268] iteration 24594: total_loss: 0.389236, loss_sup: 0.025319, loss_mps: 0.121861, loss_cps: 0.242057
[14:06:07.414] iteration 24595: total_loss: 0.421044, loss_sup: 0.012029, loss_mps: 0.137630, loss_cps: 0.271384
[14:06:07.561] iteration 24596: total_loss: 0.196580, loss_sup: 0.047899, loss_mps: 0.056700, loss_cps: 0.091981
[14:06:07.707] iteration 24597: total_loss: 0.249939, loss_sup: 0.006357, loss_mps: 0.093546, loss_cps: 0.150037
[14:06:07.853] iteration 24598: total_loss: 0.344635, loss_sup: 0.087001, loss_mps: 0.089129, loss_cps: 0.168506
[14:06:08.003] iteration 24599: total_loss: 0.428920, loss_sup: 0.086876, loss_mps: 0.118122, loss_cps: 0.223922
[14:06:08.150] iteration 24600: total_loss: 0.673545, loss_sup: 0.131953, loss_mps: 0.171814, loss_cps: 0.369777
[14:06:08.150] Evaluation Started ==>
[14:06:19.495] ==> valid iteration 24600: unet metrics: {'dc': 0.6406399990775196, 'jc': 0.5252801949837262, 'pre': 0.8144877557480888, 'hd': 5.322520454769774}, ynet metrics: {'dc': 0.6102368112418426, 'jc': 0.49693719380473494, 'pre': 0.8182188244952985, 'hd': 5.249010658109007}.
[14:06:19.497] Evaluation Finished!⏹️
[14:06:19.648] iteration 24601: total_loss: 0.427207, loss_sup: 0.059127, loss_mps: 0.126161, loss_cps: 0.241919
[14:06:19.796] iteration 24602: total_loss: 0.571255, loss_sup: 0.051674, loss_mps: 0.162243, loss_cps: 0.357338
[14:06:19.942] iteration 24603: total_loss: 0.361162, loss_sup: 0.120699, loss_mps: 0.088245, loss_cps: 0.152218
[14:06:20.090] iteration 24604: total_loss: 0.308252, loss_sup: 0.020582, loss_mps: 0.100691, loss_cps: 0.186979
[14:06:20.236] iteration 24605: total_loss: 0.272356, loss_sup: 0.018396, loss_mps: 0.093301, loss_cps: 0.160658
[14:06:20.382] iteration 24606: total_loss: 0.498128, loss_sup: 0.120496, loss_mps: 0.126478, loss_cps: 0.251154
[14:06:20.528] iteration 24607: total_loss: 0.374364, loss_sup: 0.055342, loss_mps: 0.114925, loss_cps: 0.204098
[14:06:20.676] iteration 24608: total_loss: 0.419719, loss_sup: 0.045025, loss_mps: 0.126913, loss_cps: 0.247782
[14:06:20.823] iteration 24609: total_loss: 0.527016, loss_sup: 0.011092, loss_mps: 0.158823, loss_cps: 0.357100
[14:06:20.969] iteration 24610: total_loss: 0.483876, loss_sup: 0.051077, loss_mps: 0.152890, loss_cps: 0.279909
[14:06:21.118] iteration 24611: total_loss: 0.488186, loss_sup: 0.031525, loss_mps: 0.149249, loss_cps: 0.307413
[14:06:21.267] iteration 24612: total_loss: 0.430045, loss_sup: 0.112990, loss_mps: 0.117090, loss_cps: 0.199964
[14:06:21.415] iteration 24613: total_loss: 0.391846, loss_sup: 0.029047, loss_mps: 0.124278, loss_cps: 0.238521
[14:06:21.561] iteration 24614: total_loss: 0.323518, loss_sup: 0.019767, loss_mps: 0.108988, loss_cps: 0.194763
[14:06:21.715] iteration 24615: total_loss: 0.269973, loss_sup: 0.013701, loss_mps: 0.094387, loss_cps: 0.161885
[14:06:21.861] iteration 24616: total_loss: 0.513032, loss_sup: 0.049688, loss_mps: 0.155987, loss_cps: 0.307357
[14:06:22.007] iteration 24617: total_loss: 0.546790, loss_sup: 0.181730, loss_mps: 0.123182, loss_cps: 0.241878
[14:06:22.153] iteration 24618: total_loss: 0.328465, loss_sup: 0.076272, loss_mps: 0.092012, loss_cps: 0.160181
[14:06:22.299] iteration 24619: total_loss: 0.643237, loss_sup: 0.270113, loss_mps: 0.123027, loss_cps: 0.250097
[14:06:22.445] iteration 24620: total_loss: 0.365933, loss_sup: 0.010669, loss_mps: 0.124122, loss_cps: 0.231141
[14:06:22.592] iteration 24621: total_loss: 0.253537, loss_sup: 0.007190, loss_mps: 0.086625, loss_cps: 0.159723
[14:06:22.738] iteration 24622: total_loss: 0.667140, loss_sup: 0.065200, loss_mps: 0.194814, loss_cps: 0.407126
[14:06:22.883] iteration 24623: total_loss: 0.298820, loss_sup: 0.054399, loss_mps: 0.087263, loss_cps: 0.157157
[14:06:23.028] iteration 24624: total_loss: 0.480096, loss_sup: 0.118470, loss_mps: 0.121971, loss_cps: 0.239655
[14:06:23.174] iteration 24625: total_loss: 0.361086, loss_sup: 0.021331, loss_mps: 0.122046, loss_cps: 0.217709
[14:06:23.320] iteration 24626: total_loss: 0.552574, loss_sup: 0.126732, loss_mps: 0.139734, loss_cps: 0.286108
[14:06:23.465] iteration 24627: total_loss: 0.346899, loss_sup: 0.110481, loss_mps: 0.085602, loss_cps: 0.150815
[14:06:23.611] iteration 24628: total_loss: 0.568498, loss_sup: 0.112560, loss_mps: 0.159329, loss_cps: 0.296609
[14:06:23.757] iteration 24629: total_loss: 0.982440, loss_sup: 0.297194, loss_mps: 0.229153, loss_cps: 0.456093
[14:06:23.903] iteration 24630: total_loss: 0.600030, loss_sup: 0.088059, loss_mps: 0.169152, loss_cps: 0.342819
[14:06:24.048] iteration 24631: total_loss: 0.278052, loss_sup: 0.048249, loss_mps: 0.081880, loss_cps: 0.147924
[14:06:24.194] iteration 24632: total_loss: 0.395752, loss_sup: 0.075367, loss_mps: 0.115862, loss_cps: 0.204523
[14:06:24.341] iteration 24633: total_loss: 0.732854, loss_sup: 0.075573, loss_mps: 0.219048, loss_cps: 0.438233
[14:06:24.486] iteration 24634: total_loss: 0.740730, loss_sup: 0.106267, loss_mps: 0.200481, loss_cps: 0.433982
[14:06:24.632] iteration 24635: total_loss: 0.298208, loss_sup: 0.015533, loss_mps: 0.101850, loss_cps: 0.180826
[14:06:24.778] iteration 24636: total_loss: 0.582181, loss_sup: 0.070728, loss_mps: 0.180997, loss_cps: 0.330455
[14:06:24.926] iteration 24637: total_loss: 0.411143, loss_sup: 0.042108, loss_mps: 0.130296, loss_cps: 0.238739
[14:06:25.073] iteration 24638: total_loss: 0.561493, loss_sup: 0.048768, loss_mps: 0.169304, loss_cps: 0.343421
[14:06:25.218] iteration 24639: total_loss: 1.026046, loss_sup: 0.318780, loss_mps: 0.212446, loss_cps: 0.494820
[14:06:25.364] iteration 24640: total_loss: 0.778290, loss_sup: 0.143957, loss_mps: 0.202402, loss_cps: 0.431930
[14:06:25.510] iteration 24641: total_loss: 0.484722, loss_sup: 0.055492, loss_mps: 0.154525, loss_cps: 0.274705
[14:06:25.656] iteration 24642: total_loss: 0.355179, loss_sup: 0.045795, loss_mps: 0.113401, loss_cps: 0.195982
[14:06:25.802] iteration 24643: total_loss: 0.255071, loss_sup: 0.028614, loss_mps: 0.080978, loss_cps: 0.145479
[14:06:25.947] iteration 24644: total_loss: 0.639184, loss_sup: 0.117408, loss_mps: 0.173043, loss_cps: 0.348733
[14:06:26.093] iteration 24645: total_loss: 0.426606, loss_sup: 0.021539, loss_mps: 0.139525, loss_cps: 0.265542
[14:06:26.239] iteration 24646: total_loss: 0.586747, loss_sup: 0.146521, loss_mps: 0.149087, loss_cps: 0.291139
[14:06:26.384] iteration 24647: total_loss: 0.328386, loss_sup: 0.006094, loss_mps: 0.114777, loss_cps: 0.207515
[14:06:26.530] iteration 24648: total_loss: 0.241697, loss_sup: 0.018828, loss_mps: 0.079504, loss_cps: 0.143364
[14:06:26.676] iteration 24649: total_loss: 0.585570, loss_sup: 0.083051, loss_mps: 0.153998, loss_cps: 0.348520
[14:06:26.822] iteration 24650: total_loss: 0.383019, loss_sup: 0.013560, loss_mps: 0.127990, loss_cps: 0.241468
[14:06:26.967] iteration 24651: total_loss: 0.282840, loss_sup: 0.019274, loss_mps: 0.099533, loss_cps: 0.164032
[14:06:27.112] iteration 24652: total_loss: 0.487942, loss_sup: 0.031643, loss_mps: 0.155999, loss_cps: 0.300301
[14:06:27.258] iteration 24653: total_loss: 0.547083, loss_sup: 0.082784, loss_mps: 0.147665, loss_cps: 0.316634
[14:06:27.403] iteration 24654: total_loss: 0.709618, loss_sup: 0.019618, loss_mps: 0.213921, loss_cps: 0.476079
[14:06:27.549] iteration 24655: total_loss: 0.528448, loss_sup: 0.019616, loss_mps: 0.170364, loss_cps: 0.338467
[14:06:27.695] iteration 24656: total_loss: 0.586996, loss_sup: 0.018701, loss_mps: 0.182802, loss_cps: 0.385493
[14:06:27.840] iteration 24657: total_loss: 0.575599, loss_sup: 0.071797, loss_mps: 0.157806, loss_cps: 0.345996
[14:06:27.986] iteration 24658: total_loss: 0.317746, loss_sup: 0.043578, loss_mps: 0.101997, loss_cps: 0.172171
[14:06:28.132] iteration 24659: total_loss: 0.437931, loss_sup: 0.045893, loss_mps: 0.147264, loss_cps: 0.244773
[14:06:28.277] iteration 24660: total_loss: 0.512523, loss_sup: 0.045413, loss_mps: 0.162697, loss_cps: 0.304413
[14:06:28.423] iteration 24661: total_loss: 0.714466, loss_sup: 0.158172, loss_mps: 0.174607, loss_cps: 0.381687
[14:06:28.489] iteration 24662: total_loss: 0.469962, loss_sup: 0.101972, loss_mps: 0.128623, loss_cps: 0.239367
[14:06:29.715] iteration 24663: total_loss: 0.214009, loss_sup: 0.016293, loss_mps: 0.073854, loss_cps: 0.123861
[14:06:29.864] iteration 24664: total_loss: 0.403030, loss_sup: 0.039042, loss_mps: 0.128656, loss_cps: 0.235332
[14:06:30.011] iteration 24665: total_loss: 0.524268, loss_sup: 0.201760, loss_mps: 0.117342, loss_cps: 0.205166
[14:06:30.158] iteration 24666: total_loss: 0.685584, loss_sup: 0.009896, loss_mps: 0.220317, loss_cps: 0.455371
[14:06:30.306] iteration 24667: total_loss: 0.266591, loss_sup: 0.045072, loss_mps: 0.083912, loss_cps: 0.137607
[14:06:30.462] iteration 24668: total_loss: 0.374936, loss_sup: 0.018611, loss_mps: 0.126841, loss_cps: 0.229484
[14:06:30.608] iteration 24669: total_loss: 0.444332, loss_sup: 0.099160, loss_mps: 0.122766, loss_cps: 0.222406
[14:06:30.763] iteration 24670: total_loss: 0.272829, loss_sup: 0.070080, loss_mps: 0.073103, loss_cps: 0.129647
[14:06:30.910] iteration 24671: total_loss: 0.365492, loss_sup: 0.190506, loss_mps: 0.066996, loss_cps: 0.107989
[14:06:31.056] iteration 24672: total_loss: 0.496694, loss_sup: 0.045450, loss_mps: 0.152322, loss_cps: 0.298922
[14:06:31.203] iteration 24673: total_loss: 0.814827, loss_sup: 0.113908, loss_mps: 0.218564, loss_cps: 0.482354
[14:06:31.349] iteration 24674: total_loss: 0.655622, loss_sup: 0.187820, loss_mps: 0.150662, loss_cps: 0.317140
[14:06:31.495] iteration 24675: total_loss: 0.860946, loss_sup: 0.093653, loss_mps: 0.243594, loss_cps: 0.523699
[14:06:31.641] iteration 24676: total_loss: 0.307000, loss_sup: 0.037107, loss_mps: 0.096067, loss_cps: 0.173827
[14:06:31.787] iteration 24677: total_loss: 0.461281, loss_sup: 0.063186, loss_mps: 0.130755, loss_cps: 0.267340
[14:06:31.934] iteration 24678: total_loss: 0.714867, loss_sup: 0.022172, loss_mps: 0.218603, loss_cps: 0.474091
[14:06:32.080] iteration 24679: total_loss: 0.623712, loss_sup: 0.227924, loss_mps: 0.147929, loss_cps: 0.247859
[14:06:32.227] iteration 24680: total_loss: 0.838686, loss_sup: 0.123808, loss_mps: 0.223671, loss_cps: 0.491208
[14:06:32.373] iteration 24681: total_loss: 0.242341, loss_sup: 0.016129, loss_mps: 0.087997, loss_cps: 0.138215
[14:06:32.520] iteration 24682: total_loss: 0.297082, loss_sup: 0.026907, loss_mps: 0.095386, loss_cps: 0.174790
[14:06:32.667] iteration 24683: total_loss: 0.370245, loss_sup: 0.019522, loss_mps: 0.119523, loss_cps: 0.231200
[14:06:32.813] iteration 24684: total_loss: 0.426907, loss_sup: 0.093469, loss_mps: 0.113186, loss_cps: 0.220252
[14:06:32.960] iteration 24685: total_loss: 0.576043, loss_sup: 0.015093, loss_mps: 0.187426, loss_cps: 0.373524
[14:06:33.107] iteration 24686: total_loss: 0.488030, loss_sup: 0.061480, loss_mps: 0.143425, loss_cps: 0.283126
[14:06:33.256] iteration 24687: total_loss: 0.188243, loss_sup: 0.007084, loss_mps: 0.066335, loss_cps: 0.114824
[14:06:33.404] iteration 24688: total_loss: 0.617435, loss_sup: 0.077174, loss_mps: 0.178439, loss_cps: 0.361821
[14:06:33.551] iteration 24689: total_loss: 0.594194, loss_sup: 0.043718, loss_mps: 0.177391, loss_cps: 0.373085
[14:06:33.697] iteration 24690: total_loss: 0.329461, loss_sup: 0.038708, loss_mps: 0.102432, loss_cps: 0.188322
[14:06:33.844] iteration 24691: total_loss: 0.334558, loss_sup: 0.001882, loss_mps: 0.114010, loss_cps: 0.218666
[14:06:33.991] iteration 24692: total_loss: 0.452737, loss_sup: 0.018420, loss_mps: 0.150132, loss_cps: 0.284185
[14:06:34.137] iteration 24693: total_loss: 0.510329, loss_sup: 0.253164, loss_mps: 0.098754, loss_cps: 0.158411
[14:06:34.285] iteration 24694: total_loss: 0.408533, loss_sup: 0.025022, loss_mps: 0.136136, loss_cps: 0.247375
[14:06:34.434] iteration 24695: total_loss: 0.516263, loss_sup: 0.025034, loss_mps: 0.162699, loss_cps: 0.328530
[14:06:34.582] iteration 24696: total_loss: 0.238923, loss_sup: 0.013778, loss_mps: 0.084166, loss_cps: 0.140979
[14:06:34.734] iteration 24697: total_loss: 0.474945, loss_sup: 0.138604, loss_mps: 0.119801, loss_cps: 0.216541
[14:06:34.881] iteration 24698: total_loss: 0.454459, loss_sup: 0.021974, loss_mps: 0.149256, loss_cps: 0.283230
[14:06:35.027] iteration 24699: total_loss: 0.439753, loss_sup: 0.026564, loss_mps: 0.137415, loss_cps: 0.275774
[14:06:35.175] iteration 24700: total_loss: 0.198344, loss_sup: 0.045704, loss_mps: 0.060365, loss_cps: 0.092275
[14:06:35.175] Evaluation Started ==>
[14:06:46.538] ==> valid iteration 24700: unet metrics: {'dc': 0.6384683314268147, 'jc': 0.5185813874988168, 'pre': 0.8025523208728426, 'hd': 5.551006769153118}, ynet metrics: {'dc': 0.5740437765544486, 'jc': 0.46374360028449646, 'pre': 0.8071425694015155, 'hd': 5.475776380546018}.
[14:06:46.539] Evaluation Finished!⏹️
[14:06:46.693] iteration 24701: total_loss: 0.458813, loss_sup: 0.013274, loss_mps: 0.142820, loss_cps: 0.302719
[14:06:46.844] iteration 24702: total_loss: 0.803478, loss_sup: 0.051901, loss_mps: 0.248303, loss_cps: 0.503274
[14:06:46.991] iteration 24703: total_loss: 0.546347, loss_sup: 0.088672, loss_mps: 0.149326, loss_cps: 0.308349
[14:06:47.137] iteration 24704: total_loss: 0.261337, loss_sup: 0.047163, loss_mps: 0.078000, loss_cps: 0.136175
[14:06:47.285] iteration 24705: total_loss: 0.336551, loss_sup: 0.025899, loss_mps: 0.111250, loss_cps: 0.199402
[14:06:47.431] iteration 24706: total_loss: 0.855641, loss_sup: 0.039657, loss_mps: 0.240564, loss_cps: 0.575421
[14:06:47.577] iteration 24707: total_loss: 0.347075, loss_sup: 0.003321, loss_mps: 0.121941, loss_cps: 0.221813
[14:06:47.723] iteration 24708: total_loss: 0.520654, loss_sup: 0.013169, loss_mps: 0.162034, loss_cps: 0.345451
[14:06:47.869] iteration 24709: total_loss: 0.328461, loss_sup: 0.016300, loss_mps: 0.103520, loss_cps: 0.208642
[14:06:48.015] iteration 24710: total_loss: 0.508977, loss_sup: 0.085249, loss_mps: 0.142856, loss_cps: 0.280872
[14:06:48.160] iteration 24711: total_loss: 1.029441, loss_sup: 0.047873, loss_mps: 0.298077, loss_cps: 0.683491
[14:06:48.307] iteration 24712: total_loss: 0.823794, loss_sup: 0.042499, loss_mps: 0.244371, loss_cps: 0.536924
[14:06:48.455] iteration 24713: total_loss: 0.491027, loss_sup: 0.046563, loss_mps: 0.151466, loss_cps: 0.292998
[14:06:48.600] iteration 24714: total_loss: 0.381833, loss_sup: 0.095908, loss_mps: 0.107656, loss_cps: 0.178268
[14:06:48.746] iteration 24715: total_loss: 0.487709, loss_sup: 0.051556, loss_mps: 0.149432, loss_cps: 0.286721
[14:06:48.893] iteration 24716: total_loss: 0.918422, loss_sup: 0.147653, loss_mps: 0.232734, loss_cps: 0.538035
[14:06:49.041] iteration 24717: total_loss: 0.388496, loss_sup: 0.030312, loss_mps: 0.121489, loss_cps: 0.236694
[14:06:49.187] iteration 24718: total_loss: 1.033956, loss_sup: 0.126637, loss_mps: 0.284835, loss_cps: 0.622484
[14:06:49.333] iteration 24719: total_loss: 0.528164, loss_sup: 0.100400, loss_mps: 0.144879, loss_cps: 0.282886
[14:06:49.479] iteration 24720: total_loss: 0.445226, loss_sup: 0.144204, loss_mps: 0.111984, loss_cps: 0.189039
[14:06:49.624] iteration 24721: total_loss: 0.397132, loss_sup: 0.055952, loss_mps: 0.117105, loss_cps: 0.224074
[14:06:49.770] iteration 24722: total_loss: 0.216540, loss_sup: 0.004547, loss_mps: 0.079483, loss_cps: 0.132510
[14:06:49.916] iteration 24723: total_loss: 0.280927, loss_sup: 0.026252, loss_mps: 0.089859, loss_cps: 0.164816
[14:06:50.062] iteration 24724: total_loss: 0.616735, loss_sup: 0.105632, loss_mps: 0.173284, loss_cps: 0.337819
[14:06:50.210] iteration 24725: total_loss: 0.756095, loss_sup: 0.120700, loss_mps: 0.208192, loss_cps: 0.427203
[14:06:50.356] iteration 24726: total_loss: 0.398626, loss_sup: 0.011266, loss_mps: 0.135631, loss_cps: 0.251729
[14:06:50.502] iteration 24727: total_loss: 0.325555, loss_sup: 0.013546, loss_mps: 0.111892, loss_cps: 0.200117
[14:06:50.648] iteration 24728: total_loss: 0.488146, loss_sup: 0.052097, loss_mps: 0.139523, loss_cps: 0.296525
[14:06:50.797] iteration 24729: total_loss: 0.362505, loss_sup: 0.097161, loss_mps: 0.097135, loss_cps: 0.168209
[14:06:50.943] iteration 24730: total_loss: 0.759679, loss_sup: 0.320903, loss_mps: 0.149455, loss_cps: 0.289321
[14:06:51.090] iteration 24731: total_loss: 0.471360, loss_sup: 0.030483, loss_mps: 0.144491, loss_cps: 0.296386
[14:06:51.235] iteration 24732: total_loss: 0.448716, loss_sup: 0.024855, loss_mps: 0.145114, loss_cps: 0.278747
[14:06:51.382] iteration 24733: total_loss: 0.218321, loss_sup: 0.018033, loss_mps: 0.075253, loss_cps: 0.125035
[14:06:51.529] iteration 24734: total_loss: 0.320428, loss_sup: 0.007208, loss_mps: 0.110450, loss_cps: 0.202770
[14:06:51.675] iteration 24735: total_loss: 0.472194, loss_sup: 0.015969, loss_mps: 0.151753, loss_cps: 0.304472
[14:06:51.822] iteration 24736: total_loss: 0.265423, loss_sup: 0.005017, loss_mps: 0.091827, loss_cps: 0.168579
[14:06:51.968] iteration 24737: total_loss: 0.526295, loss_sup: 0.014726, loss_mps: 0.163826, loss_cps: 0.347744
[14:06:52.115] iteration 24738: total_loss: 0.700290, loss_sup: 0.077468, loss_mps: 0.208067, loss_cps: 0.414755
[14:06:52.262] iteration 24739: total_loss: 0.301959, loss_sup: 0.018293, loss_mps: 0.098941, loss_cps: 0.184725
[14:06:52.411] iteration 24740: total_loss: 0.258106, loss_sup: 0.011650, loss_mps: 0.094210, loss_cps: 0.152246
[14:06:52.557] iteration 24741: total_loss: 0.310824, loss_sup: 0.047352, loss_mps: 0.091568, loss_cps: 0.171903
[14:06:52.704] iteration 24742: total_loss: 0.922506, loss_sup: 0.267395, loss_mps: 0.225231, loss_cps: 0.429881
[14:06:52.851] iteration 24743: total_loss: 0.330431, loss_sup: 0.028042, loss_mps: 0.108232, loss_cps: 0.194157
[14:06:52.998] iteration 24744: total_loss: 0.293025, loss_sup: 0.034400, loss_mps: 0.091880, loss_cps: 0.166746
[14:06:53.145] iteration 24745: total_loss: 0.320063, loss_sup: 0.044016, loss_mps: 0.102580, loss_cps: 0.173466
[14:06:53.291] iteration 24746: total_loss: 0.370560, loss_sup: 0.019290, loss_mps: 0.126216, loss_cps: 0.225053
[14:06:53.437] iteration 24747: total_loss: 0.310493, loss_sup: 0.020588, loss_mps: 0.105746, loss_cps: 0.184160
[14:06:53.583] iteration 24748: total_loss: 0.396133, loss_sup: 0.008097, loss_mps: 0.130007, loss_cps: 0.258030
[14:06:53.729] iteration 24749: total_loss: 0.591470, loss_sup: 0.087467, loss_mps: 0.161772, loss_cps: 0.342231
[14:06:53.875] iteration 24750: total_loss: 0.560975, loss_sup: 0.084149, loss_mps: 0.151612, loss_cps: 0.325214
[14:06:54.023] iteration 24751: total_loss: 0.590401, loss_sup: 0.314068, loss_mps: 0.098518, loss_cps: 0.177815
[14:06:54.169] iteration 24752: total_loss: 0.289518, loss_sup: 0.034544, loss_mps: 0.095614, loss_cps: 0.159360
[14:06:54.316] iteration 24753: total_loss: 0.475312, loss_sup: 0.021260, loss_mps: 0.153800, loss_cps: 0.300252
[14:06:54.462] iteration 24754: total_loss: 0.678590, loss_sup: 0.133609, loss_mps: 0.167831, loss_cps: 0.377150
[14:06:54.611] iteration 24755: total_loss: 0.361567, loss_sup: 0.030328, loss_mps: 0.121782, loss_cps: 0.209456
[14:06:54.756] iteration 24756: total_loss: 0.322699, loss_sup: 0.047785, loss_mps: 0.096674, loss_cps: 0.178240
[14:06:54.904] iteration 24757: total_loss: 0.341126, loss_sup: 0.027420, loss_mps: 0.105524, loss_cps: 0.208182
[14:06:55.051] iteration 24758: total_loss: 0.509415, loss_sup: 0.082816, loss_mps: 0.148579, loss_cps: 0.278019
[14:06:55.200] iteration 24759: total_loss: 0.206935, loss_sup: 0.002317, loss_mps: 0.079574, loss_cps: 0.125044
[14:06:55.347] iteration 24760: total_loss: 0.496833, loss_sup: 0.070924, loss_mps: 0.145815, loss_cps: 0.280093
[14:06:55.495] iteration 24761: total_loss: 0.318037, loss_sup: 0.022314, loss_mps: 0.108573, loss_cps: 0.187149
[14:06:55.641] iteration 24762: total_loss: 0.511486, loss_sup: 0.053638, loss_mps: 0.148840, loss_cps: 0.309008
[14:06:55.787] iteration 24763: total_loss: 0.279249, loss_sup: 0.038106, loss_mps: 0.090730, loss_cps: 0.150414
[14:06:55.936] iteration 24764: total_loss: 0.339297, loss_sup: 0.020966, loss_mps: 0.114041, loss_cps: 0.204290
[14:06:56.084] iteration 24765: total_loss: 0.737045, loss_sup: 0.221704, loss_mps: 0.172538, loss_cps: 0.342802
[14:06:56.231] iteration 24766: total_loss: 0.307194, loss_sup: 0.027761, loss_mps: 0.099809, loss_cps: 0.179623
[14:06:56.378] iteration 24767: total_loss: 0.670807, loss_sup: 0.081805, loss_mps: 0.202815, loss_cps: 0.386188
[14:06:56.524] iteration 24768: total_loss: 0.325606, loss_sup: 0.016220, loss_mps: 0.112030, loss_cps: 0.197356
[14:06:56.670] iteration 24769: total_loss: 0.412857, loss_sup: 0.062938, loss_mps: 0.138201, loss_cps: 0.211719
[14:06:56.816] iteration 24770: total_loss: 0.284836, loss_sup: 0.021137, loss_mps: 0.096696, loss_cps: 0.167003
[14:06:56.963] iteration 24771: total_loss: 0.249281, loss_sup: 0.005521, loss_mps: 0.084976, loss_cps: 0.158784
[14:06:57.110] iteration 24772: total_loss: 0.332179, loss_sup: 0.003247, loss_mps: 0.114004, loss_cps: 0.214927
[14:06:57.257] iteration 24773: total_loss: 0.278314, loss_sup: 0.003874, loss_mps: 0.095977, loss_cps: 0.178464
[14:06:57.404] iteration 24774: total_loss: 0.435624, loss_sup: 0.088444, loss_mps: 0.124634, loss_cps: 0.222546
[14:06:57.550] iteration 24775: total_loss: 0.257404, loss_sup: 0.000883, loss_mps: 0.094535, loss_cps: 0.161986
[14:06:57.696] iteration 24776: total_loss: 0.920631, loss_sup: 0.069665, loss_mps: 0.257889, loss_cps: 0.593077
[14:06:57.844] iteration 24777: total_loss: 0.374694, loss_sup: 0.108521, loss_mps: 0.093545, loss_cps: 0.172627
[14:06:57.990] iteration 24778: total_loss: 0.558212, loss_sup: 0.046667, loss_mps: 0.171098, loss_cps: 0.340448
[14:06:58.138] iteration 24779: total_loss: 0.306169, loss_sup: 0.008028, loss_mps: 0.100038, loss_cps: 0.198102
[14:06:58.285] iteration 24780: total_loss: 0.193006, loss_sup: 0.015012, loss_mps: 0.066836, loss_cps: 0.111158
[14:06:58.432] iteration 24781: total_loss: 0.336330, loss_sup: 0.033654, loss_mps: 0.104231, loss_cps: 0.198444
[14:06:58.579] iteration 24782: total_loss: 0.297270, loss_sup: 0.032016, loss_mps: 0.094345, loss_cps: 0.170910
[14:06:58.727] iteration 24783: total_loss: 0.480981, loss_sup: 0.011529, loss_mps: 0.151623, loss_cps: 0.317829
[14:06:58.874] iteration 24784: total_loss: 0.248218, loss_sup: 0.015002, loss_mps: 0.088580, loss_cps: 0.144636
[14:06:59.020] iteration 24785: total_loss: 0.332963, loss_sup: 0.004708, loss_mps: 0.117016, loss_cps: 0.211239
[14:06:59.166] iteration 24786: total_loss: 0.451232, loss_sup: 0.079957, loss_mps: 0.126931, loss_cps: 0.244344
[14:06:59.313] iteration 24787: total_loss: 0.349699, loss_sup: 0.001016, loss_mps: 0.114874, loss_cps: 0.233809
[14:06:59.459] iteration 24788: total_loss: 0.433354, loss_sup: 0.005186, loss_mps: 0.142564, loss_cps: 0.285603
[14:06:59.609] iteration 24789: total_loss: 0.357335, loss_sup: 0.061886, loss_mps: 0.107089, loss_cps: 0.188360
[14:06:59.756] iteration 24790: total_loss: 0.225381, loss_sup: 0.002116, loss_mps: 0.080691, loss_cps: 0.142573
[14:06:59.902] iteration 24791: total_loss: 0.330921, loss_sup: 0.032061, loss_mps: 0.106462, loss_cps: 0.192397
[14:07:00.049] iteration 24792: total_loss: 0.745773, loss_sup: 0.133601, loss_mps: 0.203361, loss_cps: 0.408811
[14:07:00.196] iteration 24793: total_loss: 0.721878, loss_sup: 0.185229, loss_mps: 0.178414, loss_cps: 0.358235
[14:07:00.343] iteration 24794: total_loss: 0.637506, loss_sup: 0.026174, loss_mps: 0.197490, loss_cps: 0.413842
[14:07:00.490] iteration 24795: total_loss: 0.299458, loss_sup: 0.019769, loss_mps: 0.098065, loss_cps: 0.181623
[14:07:00.636] iteration 24796: total_loss: 0.480593, loss_sup: 0.096490, loss_mps: 0.134424, loss_cps: 0.249679
[14:07:00.782] iteration 24797: total_loss: 0.387981, loss_sup: 0.024275, loss_mps: 0.126039, loss_cps: 0.237667
[14:07:00.931] iteration 24798: total_loss: 0.587710, loss_sup: 0.115246, loss_mps: 0.158020, loss_cps: 0.314444
[14:07:01.077] iteration 24799: total_loss: 0.446052, loss_sup: 0.181575, loss_mps: 0.093125, loss_cps: 0.171353
[14:07:01.223] iteration 24800: total_loss: 0.701969, loss_sup: 0.043491, loss_mps: 0.209799, loss_cps: 0.448680
[14:07:01.223] Evaluation Started ==>
[14:07:12.575] ==> valid iteration 24800: unet metrics: {'dc': 0.6408082004386857, 'jc': 0.5266013615651729, 'pre': 0.7923944875745974, 'hd': 5.315464110352236}, ynet metrics: {'dc': 0.6156730067603174, 'jc': 0.5052846932881658, 'pre': 0.798400871046732, 'hd': 5.412514879174989}.
[14:07:12.577] Evaluation Finished!⏹️
[14:07:12.730] iteration 24801: total_loss: 0.736390, loss_sup: 0.269258, loss_mps: 0.152225, loss_cps: 0.314908
[14:07:12.879] iteration 24802: total_loss: 0.385156, loss_sup: 0.063926, loss_mps: 0.109007, loss_cps: 0.212223
[14:07:13.025] iteration 24803: total_loss: 0.451710, loss_sup: 0.064187, loss_mps: 0.133252, loss_cps: 0.254271
[14:07:13.170] iteration 24804: total_loss: 0.432420, loss_sup: 0.019342, loss_mps: 0.139645, loss_cps: 0.273433
[14:07:13.315] iteration 24805: total_loss: 0.980152, loss_sup: 0.181966, loss_mps: 0.241540, loss_cps: 0.556646
[14:07:13.461] iteration 24806: total_loss: 0.357864, loss_sup: 0.022774, loss_mps: 0.115381, loss_cps: 0.219709
[14:07:13.607] iteration 24807: total_loss: 0.386010, loss_sup: 0.023074, loss_mps: 0.124522, loss_cps: 0.238415
[14:07:13.754] iteration 24808: total_loss: 0.635427, loss_sup: 0.099653, loss_mps: 0.175665, loss_cps: 0.360109
[14:07:13.901] iteration 24809: total_loss: 0.506202, loss_sup: 0.057061, loss_mps: 0.145499, loss_cps: 0.303642
[14:07:14.050] iteration 24810: total_loss: 0.768779, loss_sup: 0.067662, loss_mps: 0.224531, loss_cps: 0.476586
[14:07:14.196] iteration 24811: total_loss: 0.722331, loss_sup: 0.061530, loss_mps: 0.203940, loss_cps: 0.456861
[14:07:14.342] iteration 24812: total_loss: 0.380327, loss_sup: 0.029228, loss_mps: 0.130528, loss_cps: 0.220571
[14:07:14.488] iteration 24813: total_loss: 0.294348, loss_sup: 0.023892, loss_mps: 0.097941, loss_cps: 0.172515
[14:07:14.635] iteration 24814: total_loss: 0.985486, loss_sup: 0.494098, loss_mps: 0.169130, loss_cps: 0.322258
[14:07:14.780] iteration 24815: total_loss: 0.366467, loss_sup: 0.127049, loss_mps: 0.086623, loss_cps: 0.152796
[14:07:14.926] iteration 24816: total_loss: 0.253925, loss_sup: 0.023732, loss_mps: 0.082352, loss_cps: 0.147840
[14:07:15.073] iteration 24817: total_loss: 0.327260, loss_sup: 0.016681, loss_mps: 0.108671, loss_cps: 0.201908
[14:07:15.219] iteration 24818: total_loss: 0.259596, loss_sup: 0.032610, loss_mps: 0.082820, loss_cps: 0.144167
[14:07:15.365] iteration 24819: total_loss: 0.344268, loss_sup: 0.064744, loss_mps: 0.098252, loss_cps: 0.181272
[14:07:15.510] iteration 24820: total_loss: 0.456495, loss_sup: 0.012101, loss_mps: 0.155751, loss_cps: 0.288643
[14:07:15.656] iteration 24821: total_loss: 0.231351, loss_sup: 0.002092, loss_mps: 0.084066, loss_cps: 0.145194
[14:07:15.801] iteration 24822: total_loss: 0.397751, loss_sup: 0.021410, loss_mps: 0.129784, loss_cps: 0.246557
[14:07:15.947] iteration 24823: total_loss: 0.269851, loss_sup: 0.026553, loss_mps: 0.095683, loss_cps: 0.147615
[14:07:16.092] iteration 24824: total_loss: 0.412881, loss_sup: 0.016567, loss_mps: 0.135054, loss_cps: 0.261260
[14:07:16.238] iteration 24825: total_loss: 0.910919, loss_sup: 0.168529, loss_mps: 0.234190, loss_cps: 0.508199
[14:07:16.384] iteration 24826: total_loss: 0.290773, loss_sup: 0.025647, loss_mps: 0.094316, loss_cps: 0.170809
[14:07:16.529] iteration 24827: total_loss: 0.492917, loss_sup: 0.082675, loss_mps: 0.137225, loss_cps: 0.273017
[14:07:16.675] iteration 24828: total_loss: 0.566863, loss_sup: 0.067961, loss_mps: 0.169857, loss_cps: 0.329046
[14:07:16.821] iteration 24829: total_loss: 0.720865, loss_sup: 0.086837, loss_mps: 0.200188, loss_cps: 0.433839
[14:07:16.966] iteration 24830: total_loss: 0.322862, loss_sup: 0.069427, loss_mps: 0.091212, loss_cps: 0.162223
[14:07:17.112] iteration 24831: total_loss: 0.260463, loss_sup: 0.006340, loss_mps: 0.091525, loss_cps: 0.162597
[14:07:17.258] iteration 24832: total_loss: 0.269751, loss_sup: 0.068946, loss_mps: 0.078277, loss_cps: 0.122529
[14:07:17.406] iteration 24833: total_loss: 0.612607, loss_sup: 0.056495, loss_mps: 0.188471, loss_cps: 0.367641
[14:07:17.551] iteration 24834: total_loss: 0.505620, loss_sup: 0.069736, loss_mps: 0.152578, loss_cps: 0.283306
[14:07:17.697] iteration 24835: total_loss: 0.204211, loss_sup: 0.007763, loss_mps: 0.074054, loss_cps: 0.122394
[14:07:17.842] iteration 24836: total_loss: 0.405709, loss_sup: 0.042106, loss_mps: 0.133916, loss_cps: 0.229687
[14:07:17.988] iteration 24837: total_loss: 0.828086, loss_sup: 0.110170, loss_mps: 0.225903, loss_cps: 0.492013
[14:07:18.133] iteration 24838: total_loss: 0.641953, loss_sup: 0.113329, loss_mps: 0.173809, loss_cps: 0.354814
[14:07:18.279] iteration 24839: total_loss: 0.619598, loss_sup: 0.073020, loss_mps: 0.180121, loss_cps: 0.366458
[14:07:18.424] iteration 24840: total_loss: 0.419950, loss_sup: 0.055595, loss_mps: 0.130979, loss_cps: 0.233375
[14:07:18.570] iteration 24841: total_loss: 0.511049, loss_sup: 0.020624, loss_mps: 0.166282, loss_cps: 0.324144
[14:07:18.716] iteration 24842: total_loss: 0.343217, loss_sup: 0.061322, loss_mps: 0.096242, loss_cps: 0.185652
[14:07:18.861] iteration 24843: total_loss: 0.396288, loss_sup: 0.016377, loss_mps: 0.133477, loss_cps: 0.246434
[14:07:19.007] iteration 24844: total_loss: 0.423221, loss_sup: 0.006718, loss_mps: 0.140041, loss_cps: 0.276462
[14:07:19.152] iteration 24845: total_loss: 0.708926, loss_sup: 0.180738, loss_mps: 0.175998, loss_cps: 0.352190
[14:07:19.298] iteration 24846: total_loss: 0.375395, loss_sup: 0.016903, loss_mps: 0.124469, loss_cps: 0.234022
[14:07:19.443] iteration 24847: total_loss: 0.878342, loss_sup: 0.318377, loss_mps: 0.185411, loss_cps: 0.374553
[14:07:19.588] iteration 24848: total_loss: 0.320158, loss_sup: 0.021117, loss_mps: 0.108193, loss_cps: 0.190849
[14:07:19.734] iteration 24849: total_loss: 0.220283, loss_sup: 0.013605, loss_mps: 0.079590, loss_cps: 0.127089
[14:07:19.880] iteration 24850: total_loss: 0.579946, loss_sup: 0.113575, loss_mps: 0.148967, loss_cps: 0.317404
[14:07:20.025] iteration 24851: total_loss: 0.584247, loss_sup: 0.020197, loss_mps: 0.186557, loss_cps: 0.377492
[14:07:20.171] iteration 24852: total_loss: 0.372643, loss_sup: 0.041194, loss_mps: 0.119333, loss_cps: 0.212115
[14:07:20.317] iteration 24853: total_loss: 0.487072, loss_sup: 0.006595, loss_mps: 0.159016, loss_cps: 0.321461
[14:07:20.462] iteration 24854: total_loss: 0.416892, loss_sup: 0.045464, loss_mps: 0.126430, loss_cps: 0.244998
[14:07:20.608] iteration 24855: total_loss: 0.532720, loss_sup: 0.042319, loss_mps: 0.158603, loss_cps: 0.331798
[14:07:20.753] iteration 24856: total_loss: 0.631406, loss_sup: 0.226855, loss_mps: 0.147922, loss_cps: 0.256629
[14:07:20.900] iteration 24857: total_loss: 0.374793, loss_sup: 0.020113, loss_mps: 0.123633, loss_cps: 0.231046
[14:07:21.045] iteration 24858: total_loss: 0.377503, loss_sup: 0.110042, loss_mps: 0.090257, loss_cps: 0.177205
[14:07:21.191] iteration 24859: total_loss: 0.407539, loss_sup: 0.065361, loss_mps: 0.121592, loss_cps: 0.220585
[14:07:21.336] iteration 24860: total_loss: 0.291871, loss_sup: 0.048350, loss_mps: 0.088825, loss_cps: 0.154696
[14:07:21.481] iteration 24861: total_loss: 0.457670, loss_sup: 0.004952, loss_mps: 0.157927, loss_cps: 0.294792
[14:07:21.627] iteration 24862: total_loss: 0.283620, loss_sup: 0.005949, loss_mps: 0.099242, loss_cps: 0.178429
[14:07:21.774] iteration 24863: total_loss: 0.729720, loss_sup: 0.019614, loss_mps: 0.224305, loss_cps: 0.485802
[14:07:21.923] iteration 24864: total_loss: 0.985780, loss_sup: 0.063825, loss_mps: 0.285874, loss_cps: 0.636081
[14:07:22.068] iteration 24865: total_loss: 0.571568, loss_sup: 0.036731, loss_mps: 0.177335, loss_cps: 0.357502
[14:07:22.214] iteration 24866: total_loss: 0.374537, loss_sup: 0.047474, loss_mps: 0.116435, loss_cps: 0.210628
[14:07:22.362] iteration 24867: total_loss: 0.292662, loss_sup: 0.023677, loss_mps: 0.091814, loss_cps: 0.177170
[14:07:22.508] iteration 24868: total_loss: 0.453885, loss_sup: 0.096283, loss_mps: 0.128593, loss_cps: 0.229010
[14:07:22.653] iteration 24869: total_loss: 0.438508, loss_sup: 0.087841, loss_mps: 0.128273, loss_cps: 0.222393
[14:07:22.800] iteration 24870: total_loss: 0.718695, loss_sup: 0.058003, loss_mps: 0.204850, loss_cps: 0.455842
[14:07:22.946] iteration 24871: total_loss: 0.292305, loss_sup: 0.005517, loss_mps: 0.105260, loss_cps: 0.181528
[14:07:23.092] iteration 24872: total_loss: 0.295013, loss_sup: 0.008832, loss_mps: 0.104030, loss_cps: 0.182151
[14:07:23.238] iteration 24873: total_loss: 0.307753, loss_sup: 0.019268, loss_mps: 0.109523, loss_cps: 0.178962
[14:07:23.384] iteration 24874: total_loss: 0.575954, loss_sup: 0.054948, loss_mps: 0.175510, loss_cps: 0.345496
[14:07:23.530] iteration 24875: total_loss: 0.297644, loss_sup: 0.025209, loss_mps: 0.096034, loss_cps: 0.176401
[14:07:23.676] iteration 24876: total_loss: 0.322916, loss_sup: 0.006848, loss_mps: 0.118213, loss_cps: 0.197855
[14:07:23.822] iteration 24877: total_loss: 0.693466, loss_sup: 0.117789, loss_mps: 0.191361, loss_cps: 0.384316
[14:07:23.968] iteration 24878: total_loss: 0.296045, loss_sup: 0.040930, loss_mps: 0.089883, loss_cps: 0.165232
[14:07:24.113] iteration 24879: total_loss: 0.388440, loss_sup: 0.026148, loss_mps: 0.126432, loss_cps: 0.235860
[14:07:24.259] iteration 24880: total_loss: 0.442626, loss_sup: 0.057326, loss_mps: 0.135232, loss_cps: 0.250069
[14:07:24.404] iteration 24881: total_loss: 0.351954, loss_sup: 0.039253, loss_mps: 0.113483, loss_cps: 0.199218
[14:07:24.550] iteration 24882: total_loss: 0.280915, loss_sup: 0.001792, loss_mps: 0.100092, loss_cps: 0.179031
[14:07:24.696] iteration 24883: total_loss: 0.332335, loss_sup: 0.035274, loss_mps: 0.106298, loss_cps: 0.190763
[14:07:24.843] iteration 24884: total_loss: 0.335396, loss_sup: 0.105936, loss_mps: 0.087164, loss_cps: 0.142296
[14:07:24.989] iteration 24885: total_loss: 0.663255, loss_sup: 0.043805, loss_mps: 0.198245, loss_cps: 0.421204
[14:07:25.134] iteration 24886: total_loss: 0.398170, loss_sup: 0.015170, loss_mps: 0.133920, loss_cps: 0.249080
[14:07:25.280] iteration 24887: total_loss: 0.596326, loss_sup: 0.142456, loss_mps: 0.152375, loss_cps: 0.301495
[14:07:25.426] iteration 24888: total_loss: 0.940717, loss_sup: 0.039357, loss_mps: 0.271730, loss_cps: 0.629629
[14:07:25.571] iteration 24889: total_loss: 0.330610, loss_sup: 0.012426, loss_mps: 0.112531, loss_cps: 0.205653
[14:07:25.719] iteration 24890: total_loss: 0.528827, loss_sup: 0.074949, loss_mps: 0.161321, loss_cps: 0.292557
[14:07:25.866] iteration 24891: total_loss: 0.637800, loss_sup: 0.078698, loss_mps: 0.177645, loss_cps: 0.381458
[14:07:26.011] iteration 24892: total_loss: 0.526880, loss_sup: 0.088878, loss_mps: 0.149703, loss_cps: 0.288299
[14:07:26.157] iteration 24893: total_loss: 0.279622, loss_sup: 0.002514, loss_mps: 0.098591, loss_cps: 0.178517
[14:07:26.303] iteration 24894: total_loss: 0.365257, loss_sup: 0.080200, loss_mps: 0.103253, loss_cps: 0.181804
[14:07:26.449] iteration 24895: total_loss: 0.597820, loss_sup: 0.002753, loss_mps: 0.184911, loss_cps: 0.410156
[14:07:26.595] iteration 24896: total_loss: 0.368235, loss_sup: 0.007117, loss_mps: 0.123905, loss_cps: 0.237213
[14:07:26.741] iteration 24897: total_loss: 0.355328, loss_sup: 0.034585, loss_mps: 0.112786, loss_cps: 0.207957
[14:07:26.888] iteration 24898: total_loss: 0.717411, loss_sup: 0.073792, loss_mps: 0.204369, loss_cps: 0.439250
[14:07:27.034] iteration 24899: total_loss: 0.370594, loss_sup: 0.008379, loss_mps: 0.119515, loss_cps: 0.242700
[14:07:27.180] iteration 24900: total_loss: 0.403949, loss_sup: 0.032825, loss_mps: 0.132554, loss_cps: 0.238570
[14:07:27.180] Evaluation Started ==>
[14:07:38.504] ==> valid iteration 24900: unet metrics: {'dc': 0.6709369347396377, 'jc': 0.5569573064011887, 'pre': 0.7980290253308145, 'hd': 5.415167591801311}, ynet metrics: {'dc': 0.6200197780974559, 'jc': 0.5093420937370008, 'pre': 0.7993011832915153, 'hd': 5.272676632839445}.
[14:07:38.506] Evaluation Finished!⏹️
[14:07:38.660] iteration 24901: total_loss: 0.378402, loss_sup: 0.066175, loss_mps: 0.114021, loss_cps: 0.198206
[14:07:38.812] iteration 24902: total_loss: 0.440741, loss_sup: 0.030627, loss_mps: 0.143234, loss_cps: 0.266880
[14:07:38.958] iteration 24903: total_loss: 0.395333, loss_sup: 0.071205, loss_mps: 0.118289, loss_cps: 0.205840
[14:07:39.103] iteration 24904: total_loss: 0.484572, loss_sup: 0.054490, loss_mps: 0.139138, loss_cps: 0.290944
[14:07:39.249] iteration 24905: total_loss: 0.481130, loss_sup: 0.028694, loss_mps: 0.153393, loss_cps: 0.299043
[14:07:39.396] iteration 24906: total_loss: 0.676437, loss_sup: 0.250837, loss_mps: 0.140161, loss_cps: 0.285439
[14:07:39.543] iteration 24907: total_loss: 0.373502, loss_sup: 0.043524, loss_mps: 0.115141, loss_cps: 0.214837
[14:07:39.689] iteration 24908: total_loss: 0.721131, loss_sup: 0.119991, loss_mps: 0.194721, loss_cps: 0.406419
[14:07:39.835] iteration 24909: total_loss: 0.467436, loss_sup: 0.059927, loss_mps: 0.140326, loss_cps: 0.267182
[14:07:39.982] iteration 24910: total_loss: 0.509126, loss_sup: 0.058897, loss_mps: 0.142357, loss_cps: 0.307872
[14:07:40.128] iteration 24911: total_loss: 0.340949, loss_sup: 0.036476, loss_mps: 0.109325, loss_cps: 0.195148
[14:07:40.275] iteration 24912: total_loss: 0.387978, loss_sup: 0.015678, loss_mps: 0.135001, loss_cps: 0.237299
[14:07:40.423] iteration 24913: total_loss: 0.582849, loss_sup: 0.019419, loss_mps: 0.183205, loss_cps: 0.380225
[14:07:40.570] iteration 24914: total_loss: 0.461077, loss_sup: 0.018745, loss_mps: 0.152406, loss_cps: 0.289926
[14:07:40.717] iteration 24915: total_loss: 0.704444, loss_sup: 0.160866, loss_mps: 0.176770, loss_cps: 0.366808
[14:07:40.863] iteration 24916: total_loss: 0.490324, loss_sup: 0.040520, loss_mps: 0.145516, loss_cps: 0.304288
[14:07:41.010] iteration 24917: total_loss: 0.463415, loss_sup: 0.025684, loss_mps: 0.142901, loss_cps: 0.294830
[14:07:41.156] iteration 24918: total_loss: 0.365655, loss_sup: 0.065160, loss_mps: 0.109945, loss_cps: 0.190549
[14:07:41.302] iteration 24919: total_loss: 0.595472, loss_sup: 0.074331, loss_mps: 0.168082, loss_cps: 0.353059
[14:07:41.448] iteration 24920: total_loss: 0.506438, loss_sup: 0.015443, loss_mps: 0.159774, loss_cps: 0.331221
[14:07:41.594] iteration 24921: total_loss: 0.646034, loss_sup: 0.067902, loss_mps: 0.191178, loss_cps: 0.386955
[14:07:41.740] iteration 24922: total_loss: 0.441055, loss_sup: 0.009125, loss_mps: 0.160219, loss_cps: 0.271712
[14:07:41.887] iteration 24923: total_loss: 0.376220, loss_sup: 0.063330, loss_mps: 0.107606, loss_cps: 0.205284
[14:07:42.033] iteration 24924: total_loss: 0.409247, loss_sup: 0.139139, loss_mps: 0.093012, loss_cps: 0.177096
[14:07:42.179] iteration 24925: total_loss: 0.288003, loss_sup: 0.061823, loss_mps: 0.083796, loss_cps: 0.142383
[14:07:42.325] iteration 24926: total_loss: 0.263670, loss_sup: 0.017076, loss_mps: 0.090719, loss_cps: 0.155875
[14:07:42.471] iteration 24927: total_loss: 0.508849, loss_sup: 0.065833, loss_mps: 0.145997, loss_cps: 0.297019
[14:07:42.617] iteration 24928: total_loss: 0.365227, loss_sup: 0.008778, loss_mps: 0.130835, loss_cps: 0.225614
[14:07:42.763] iteration 24929: total_loss: 0.299601, loss_sup: 0.011938, loss_mps: 0.103695, loss_cps: 0.183968
[14:07:42.911] iteration 24930: total_loss: 0.423599, loss_sup: 0.140748, loss_mps: 0.101225, loss_cps: 0.181626
[14:07:43.058] iteration 24931: total_loss: 0.599386, loss_sup: 0.031615, loss_mps: 0.182289, loss_cps: 0.385483
[14:07:43.203] iteration 24932: total_loss: 0.426085, loss_sup: 0.022653, loss_mps: 0.134017, loss_cps: 0.269415
[14:07:43.350] iteration 24933: total_loss: 0.718752, loss_sup: 0.051654, loss_mps: 0.206164, loss_cps: 0.460935
[14:07:43.498] iteration 24934: total_loss: 0.602715, loss_sup: 0.028167, loss_mps: 0.183960, loss_cps: 0.390588
[14:07:43.644] iteration 24935: total_loss: 0.385581, loss_sup: 0.092916, loss_mps: 0.111842, loss_cps: 0.180823
[14:07:43.790] iteration 24936: total_loss: 0.605745, loss_sup: 0.259229, loss_mps: 0.126359, loss_cps: 0.220158
[14:07:43.936] iteration 24937: total_loss: 0.300411, loss_sup: 0.050188, loss_mps: 0.090014, loss_cps: 0.160209
[14:07:44.082] iteration 24938: total_loss: 0.408437, loss_sup: 0.086899, loss_mps: 0.119400, loss_cps: 0.202138
[14:07:44.228] iteration 24939: total_loss: 0.455144, loss_sup: 0.004455, loss_mps: 0.143097, loss_cps: 0.307591
[14:07:44.374] iteration 24940: total_loss: 0.579595, loss_sup: 0.019693, loss_mps: 0.183421, loss_cps: 0.376481
[14:07:44.520] iteration 24941: total_loss: 0.714171, loss_sup: 0.036047, loss_mps: 0.210783, loss_cps: 0.467341
[14:07:44.665] iteration 24942: total_loss: 0.289360, loss_sup: 0.025556, loss_mps: 0.097800, loss_cps: 0.166004
[14:07:44.813] iteration 24943: total_loss: 0.781042, loss_sup: 0.183902, loss_mps: 0.203431, loss_cps: 0.393709
[14:07:44.959] iteration 24944: total_loss: 0.361896, loss_sup: 0.085168, loss_mps: 0.103423, loss_cps: 0.173305
[14:07:45.105] iteration 24945: total_loss: 0.511377, loss_sup: 0.056327, loss_mps: 0.151971, loss_cps: 0.303079
[14:07:45.252] iteration 24946: total_loss: 0.254338, loss_sup: 0.023036, loss_mps: 0.086404, loss_cps: 0.144898
[14:07:45.399] iteration 24947: total_loss: 0.226412, loss_sup: 0.027291, loss_mps: 0.074174, loss_cps: 0.124948
[14:07:45.545] iteration 24948: total_loss: 0.601916, loss_sup: 0.078054, loss_mps: 0.173307, loss_cps: 0.350556
[14:07:45.691] iteration 24949: total_loss: 0.419294, loss_sup: 0.057312, loss_mps: 0.127957, loss_cps: 0.234025
[14:07:45.837] iteration 24950: total_loss: 0.180124, loss_sup: 0.004452, loss_mps: 0.068158, loss_cps: 0.107514
[14:07:45.983] iteration 24951: total_loss: 0.322002, loss_sup: 0.026900, loss_mps: 0.106219, loss_cps: 0.188883
[14:07:46.129] iteration 24952: total_loss: 0.476144, loss_sup: 0.032406, loss_mps: 0.153875, loss_cps: 0.289864
[14:07:46.275] iteration 24953: total_loss: 0.402511, loss_sup: 0.176504, loss_mps: 0.079195, loss_cps: 0.146811
[14:07:46.421] iteration 24954: total_loss: 0.357828, loss_sup: 0.003570, loss_mps: 0.123525, loss_cps: 0.230732
[14:07:46.567] iteration 24955: total_loss: 0.751714, loss_sup: 0.120599, loss_mps: 0.207872, loss_cps: 0.423243
[14:07:46.715] iteration 24956: total_loss: 0.427475, loss_sup: 0.063045, loss_mps: 0.120000, loss_cps: 0.244429
[14:07:46.862] iteration 24957: total_loss: 0.271062, loss_sup: 0.025244, loss_mps: 0.094806, loss_cps: 0.151012
[14:07:47.010] iteration 24958: total_loss: 0.462932, loss_sup: 0.079015, loss_mps: 0.129373, loss_cps: 0.254543
[14:07:47.156] iteration 24959: total_loss: 0.531591, loss_sup: 0.072069, loss_mps: 0.161072, loss_cps: 0.298450
[14:07:47.303] iteration 24960: total_loss: 0.383126, loss_sup: 0.013881, loss_mps: 0.131461, loss_cps: 0.237783
[14:07:47.451] iteration 24961: total_loss: 0.549238, loss_sup: 0.090040, loss_mps: 0.157921, loss_cps: 0.301277
[14:07:47.600] iteration 24962: total_loss: 0.692826, loss_sup: 0.069622, loss_mps: 0.191893, loss_cps: 0.431311
[14:07:47.749] iteration 24963: total_loss: 0.353979, loss_sup: 0.098268, loss_mps: 0.090353, loss_cps: 0.165359
[14:07:47.896] iteration 24964: total_loss: 0.377468, loss_sup: 0.113449, loss_mps: 0.101289, loss_cps: 0.162730
[14:07:48.041] iteration 24965: total_loss: 0.291258, loss_sup: 0.055462, loss_mps: 0.093783, loss_cps: 0.142013
[14:07:48.188] iteration 24966: total_loss: 0.505429, loss_sup: 0.166569, loss_mps: 0.131957, loss_cps: 0.206903
[14:07:48.335] iteration 24967: total_loss: 0.458676, loss_sup: 0.031907, loss_mps: 0.140720, loss_cps: 0.286048
[14:07:48.483] iteration 24968: total_loss: 0.557529, loss_sup: 0.027975, loss_mps: 0.171947, loss_cps: 0.357607
[14:07:48.630] iteration 24969: total_loss: 0.508440, loss_sup: 0.075485, loss_mps: 0.145579, loss_cps: 0.287376
[14:07:48.776] iteration 24970: total_loss: 0.480598, loss_sup: 0.094756, loss_mps: 0.127060, loss_cps: 0.258782
[14:07:48.928] iteration 24971: total_loss: 0.420577, loss_sup: 0.066590, loss_mps: 0.129425, loss_cps: 0.224562
[14:07:49.074] iteration 24972: total_loss: 0.329119, loss_sup: 0.112550, loss_mps: 0.081938, loss_cps: 0.134631
[14:07:49.220] iteration 24973: total_loss: 0.462473, loss_sup: 0.011299, loss_mps: 0.136979, loss_cps: 0.314194
[14:07:49.367] iteration 24974: total_loss: 0.255969, loss_sup: 0.018088, loss_mps: 0.090100, loss_cps: 0.147782
[14:07:49.514] iteration 24975: total_loss: 0.246573, loss_sup: 0.002948, loss_mps: 0.090892, loss_cps: 0.152733
[14:07:49.665] iteration 24976: total_loss: 0.518818, loss_sup: 0.026585, loss_mps: 0.167093, loss_cps: 0.325141
[14:07:49.812] iteration 24977: total_loss: 0.377645, loss_sup: 0.040650, loss_mps: 0.116525, loss_cps: 0.220471
[14:07:49.959] iteration 24978: total_loss: 0.242867, loss_sup: 0.018488, loss_mps: 0.084622, loss_cps: 0.139758
[14:07:50.107] iteration 24979: total_loss: 0.366421, loss_sup: 0.146153, loss_mps: 0.083889, loss_cps: 0.136379
[14:07:50.255] iteration 24980: total_loss: 0.412906, loss_sup: 0.033895, loss_mps: 0.128306, loss_cps: 0.250704
[14:07:50.401] iteration 24981: total_loss: 0.666289, loss_sup: 0.187733, loss_mps: 0.156039, loss_cps: 0.322518
[14:07:50.548] iteration 24982: total_loss: 0.382545, loss_sup: 0.034072, loss_mps: 0.118284, loss_cps: 0.230188
[14:07:50.695] iteration 24983: total_loss: 0.604907, loss_sup: 0.016262, loss_mps: 0.199454, loss_cps: 0.389191
[14:07:50.841] iteration 24984: total_loss: 0.436872, loss_sup: 0.044887, loss_mps: 0.131358, loss_cps: 0.260627
[14:07:50.987] iteration 24985: total_loss: 0.507899, loss_sup: 0.084548, loss_mps: 0.148487, loss_cps: 0.274864
[14:07:51.134] iteration 24986: total_loss: 0.401174, loss_sup: 0.052675, loss_mps: 0.114768, loss_cps: 0.233732
[14:07:51.279] iteration 24987: total_loss: 1.086810, loss_sup: 0.198045, loss_mps: 0.275835, loss_cps: 0.612929
[14:07:51.426] iteration 24988: total_loss: 0.376694, loss_sup: 0.007484, loss_mps: 0.122497, loss_cps: 0.246714
[14:07:51.572] iteration 24989: total_loss: 0.249940, loss_sup: 0.025442, loss_mps: 0.082762, loss_cps: 0.141736
[14:07:51.718] iteration 24990: total_loss: 0.241511, loss_sup: 0.006820, loss_mps: 0.088522, loss_cps: 0.146169
[14:07:51.866] iteration 24991: total_loss: 0.364275, loss_sup: 0.043316, loss_mps: 0.115829, loss_cps: 0.205130
[14:07:52.012] iteration 24992: total_loss: 0.297760, loss_sup: 0.032117, loss_mps: 0.099736, loss_cps: 0.165906
[14:07:52.158] iteration 24993: total_loss: 1.329764, loss_sup: 0.504662, loss_mps: 0.276308, loss_cps: 0.548794
[14:07:52.307] iteration 24994: total_loss: 0.501111, loss_sup: 0.061924, loss_mps: 0.147417, loss_cps: 0.291770
[14:07:52.453] iteration 24995: total_loss: 0.637067, loss_sup: 0.204273, loss_mps: 0.139817, loss_cps: 0.292977
[14:07:52.600] iteration 24996: total_loss: 0.276758, loss_sup: 0.011767, loss_mps: 0.094591, loss_cps: 0.170401
[14:07:52.747] iteration 24997: total_loss: 0.527315, loss_sup: 0.152768, loss_mps: 0.126124, loss_cps: 0.248423
[14:07:52.896] iteration 24998: total_loss: 0.522538, loss_sup: 0.029918, loss_mps: 0.164680, loss_cps: 0.327940
[14:07:53.045] iteration 24999: total_loss: 0.427664, loss_sup: 0.039435, loss_mps: 0.136143, loss_cps: 0.252086
[14:07:53.191] iteration 25000: total_loss: 0.572789, loss_sup: 0.045456, loss_mps: 0.166011, loss_cps: 0.361322
[14:07:53.192] Evaluation Started ==>
[14:08:04.606] ==> valid iteration 25000: unet metrics: {'dc': 0.6419862337854843, 'jc': 0.5292288099220623, 'pre': 0.7883516040381584, 'hd': 5.356723123934009}, ynet metrics: {'dc': 0.6071948590876883, 'jc': 0.4938126017830643, 'pre': 0.7990699624862766, 'hd': 5.449602530418645}.
[14:08:04.608] Evaluation Finished!⏹️
[14:08:04.758] iteration 25001: total_loss: 0.367220, loss_sup: 0.046348, loss_mps: 0.120170, loss_cps: 0.200702
[14:08:04.906] iteration 25002: total_loss: 0.606894, loss_sup: 0.013128, loss_mps: 0.179765, loss_cps: 0.414001
[14:08:05.051] iteration 25003: total_loss: 0.550296, loss_sup: 0.128485, loss_mps: 0.146521, loss_cps: 0.275290
[14:08:05.196] iteration 25004: total_loss: 0.796188, loss_sup: 0.075860, loss_mps: 0.221167, loss_cps: 0.499161
[14:08:05.342] iteration 25005: total_loss: 0.288075, loss_sup: 0.024490, loss_mps: 0.095939, loss_cps: 0.167646
[14:08:05.487] iteration 25006: total_loss: 0.825209, loss_sup: 0.070219, loss_mps: 0.235892, loss_cps: 0.519098
[14:08:05.636] iteration 25007: total_loss: 0.541811, loss_sup: 0.097105, loss_mps: 0.152041, loss_cps: 0.292665
[14:08:05.781] iteration 25008: total_loss: 0.407890, loss_sup: 0.112224, loss_mps: 0.113403, loss_cps: 0.182264
[14:08:05.928] iteration 25009: total_loss: 0.436553, loss_sup: 0.121606, loss_mps: 0.109900, loss_cps: 0.205046
[14:08:06.078] iteration 25010: total_loss: 0.642980, loss_sup: 0.020211, loss_mps: 0.211773, loss_cps: 0.410996
[14:08:06.224] iteration 25011: total_loss: 0.494143, loss_sup: 0.040359, loss_mps: 0.156144, loss_cps: 0.297640
[14:08:06.370] iteration 25012: total_loss: 0.405038, loss_sup: 0.042188, loss_mps: 0.124685, loss_cps: 0.238165
[14:08:06.516] iteration 25013: total_loss: 0.479804, loss_sup: 0.126035, loss_mps: 0.124896, loss_cps: 0.228872
[14:08:06.662] iteration 25014: total_loss: 1.171541, loss_sup: 0.152046, loss_mps: 0.327140, loss_cps: 0.692355
[14:08:06.811] iteration 25015: total_loss: 0.390437, loss_sup: 0.015153, loss_mps: 0.128841, loss_cps: 0.246443
[14:08:06.957] iteration 25016: total_loss: 0.584986, loss_sup: 0.086684, loss_mps: 0.166120, loss_cps: 0.332183
[14:08:07.103] iteration 25017: total_loss: 0.562095, loss_sup: 0.051669, loss_mps: 0.173820, loss_cps: 0.336607
[14:08:07.249] iteration 25018: total_loss: 0.345211, loss_sup: 0.009886, loss_mps: 0.116622, loss_cps: 0.218703
[14:08:07.395] iteration 25019: total_loss: 0.272259, loss_sup: 0.008671, loss_mps: 0.093614, loss_cps: 0.169974
[14:08:07.542] iteration 25020: total_loss: 0.292850, loss_sup: 0.048364, loss_mps: 0.090390, loss_cps: 0.154096
[14:08:07.688] iteration 25021: total_loss: 0.302018, loss_sup: 0.027672, loss_mps: 0.098895, loss_cps: 0.175451
[14:08:07.834] iteration 25022: total_loss: 1.280709, loss_sup: 0.336884, loss_mps: 0.306466, loss_cps: 0.637358
[14:08:07.980] iteration 25023: total_loss: 0.724973, loss_sup: 0.163840, loss_mps: 0.173684, loss_cps: 0.387449
[14:08:08.126] iteration 25024: total_loss: 0.388723, loss_sup: 0.041554, loss_mps: 0.121969, loss_cps: 0.225199
[14:08:08.272] iteration 25025: total_loss: 0.324760, loss_sup: 0.106971, loss_mps: 0.080944, loss_cps: 0.136845
[14:08:08.418] iteration 25026: total_loss: 0.463599, loss_sup: 0.066998, loss_mps: 0.133883, loss_cps: 0.262718
[14:08:08.565] iteration 25027: total_loss: 0.584867, loss_sup: 0.155122, loss_mps: 0.145656, loss_cps: 0.284088
[14:08:08.717] iteration 25028: total_loss: 0.484745, loss_sup: 0.090853, loss_mps: 0.136754, loss_cps: 0.257138
[14:08:08.863] iteration 25029: total_loss: 0.196475, loss_sup: 0.009559, loss_mps: 0.072579, loss_cps: 0.114338
[14:08:09.010] iteration 25030: total_loss: 0.289535, loss_sup: 0.015155, loss_mps: 0.100751, loss_cps: 0.173629
[14:08:09.155] iteration 25031: total_loss: 0.653412, loss_sup: 0.102669, loss_mps: 0.183486, loss_cps: 0.367258
[14:08:09.304] iteration 25032: total_loss: 0.483767, loss_sup: 0.096311, loss_mps: 0.136318, loss_cps: 0.251139
[14:08:09.450] iteration 25033: total_loss: 0.321788, loss_sup: 0.004525, loss_mps: 0.119434, loss_cps: 0.197830
[14:08:09.596] iteration 25034: total_loss: 0.202512, loss_sup: 0.024552, loss_mps: 0.068748, loss_cps: 0.109212
[14:08:09.742] iteration 25035: total_loss: 0.393364, loss_sup: 0.054496, loss_mps: 0.120063, loss_cps: 0.218806
[14:08:09.888] iteration 25036: total_loss: 0.317146, loss_sup: 0.035368, loss_mps: 0.102929, loss_cps: 0.178849
[14:08:10.035] iteration 25037: total_loss: 0.659284, loss_sup: 0.137317, loss_mps: 0.174706, loss_cps: 0.347262
[14:08:10.184] iteration 25038: total_loss: 0.405015, loss_sup: 0.158830, loss_mps: 0.088609, loss_cps: 0.157576
[14:08:10.331] iteration 25039: total_loss: 0.571313, loss_sup: 0.075772, loss_mps: 0.157677, loss_cps: 0.337865
[14:08:10.476] iteration 25040: total_loss: 0.612100, loss_sup: 0.153885, loss_mps: 0.157343, loss_cps: 0.300871
[14:08:10.622] iteration 25041: total_loss: 0.571590, loss_sup: 0.090919, loss_mps: 0.162802, loss_cps: 0.317869
[14:08:10.769] iteration 25042: total_loss: 0.454441, loss_sup: 0.024109, loss_mps: 0.146930, loss_cps: 0.283402
[14:08:10.915] iteration 25043: total_loss: 0.350875, loss_sup: 0.065031, loss_mps: 0.101468, loss_cps: 0.184376
[14:08:11.061] iteration 25044: total_loss: 0.353216, loss_sup: 0.013481, loss_mps: 0.120107, loss_cps: 0.219628
[14:08:11.209] iteration 25045: total_loss: 0.422845, loss_sup: 0.032241, loss_mps: 0.144318, loss_cps: 0.246286
[14:08:11.355] iteration 25046: total_loss: 0.243771, loss_sup: 0.043268, loss_mps: 0.075239, loss_cps: 0.125264
[14:08:11.502] iteration 25047: total_loss: 0.259917, loss_sup: 0.071015, loss_mps: 0.074427, loss_cps: 0.114474
[14:08:11.649] iteration 25048: total_loss: 0.343549, loss_sup: 0.004032, loss_mps: 0.125038, loss_cps: 0.214478
[14:08:11.796] iteration 25049: total_loss: 0.395162, loss_sup: 0.027846, loss_mps: 0.130464, loss_cps: 0.236853
[14:08:11.942] iteration 25050: total_loss: 0.533092, loss_sup: 0.035083, loss_mps: 0.171084, loss_cps: 0.326925
[14:08:12.088] iteration 25051: total_loss: 0.273636, loss_sup: 0.006391, loss_mps: 0.100253, loss_cps: 0.166992
[14:08:12.235] iteration 25052: total_loss: 0.508888, loss_sup: 0.083316, loss_mps: 0.133756, loss_cps: 0.291816
[14:08:12.382] iteration 25053: total_loss: 0.278293, loss_sup: 0.016193, loss_mps: 0.095389, loss_cps: 0.166711
[14:08:12.529] iteration 25054: total_loss: 0.494049, loss_sup: 0.159630, loss_mps: 0.120054, loss_cps: 0.214365
[14:08:12.674] iteration 25055: total_loss: 0.431129, loss_sup: 0.105527, loss_mps: 0.115972, loss_cps: 0.209629
[14:08:12.821] iteration 25056: total_loss: 0.566915, loss_sup: 0.026785, loss_mps: 0.193831, loss_cps: 0.346299
[14:08:12.967] iteration 25057: total_loss: 0.525746, loss_sup: 0.072155, loss_mps: 0.160834, loss_cps: 0.292757
[14:08:13.113] iteration 25058: total_loss: 0.678785, loss_sup: 0.087137, loss_mps: 0.191637, loss_cps: 0.400010
[14:08:13.260] iteration 25059: total_loss: 0.334652, loss_sup: 0.058341, loss_mps: 0.097410, loss_cps: 0.178901
[14:08:13.406] iteration 25060: total_loss: 0.501238, loss_sup: 0.054863, loss_mps: 0.150129, loss_cps: 0.296246
[14:08:13.552] iteration 25061: total_loss: 0.281095, loss_sup: 0.005543, loss_mps: 0.099264, loss_cps: 0.176288
[14:08:13.698] iteration 25062: total_loss: 0.288735, loss_sup: 0.008843, loss_mps: 0.102749, loss_cps: 0.177144
[14:08:13.844] iteration 25063: total_loss: 0.269147, loss_sup: 0.011982, loss_mps: 0.090714, loss_cps: 0.166451
[14:08:13.993] iteration 25064: total_loss: 0.383149, loss_sup: 0.031731, loss_mps: 0.125859, loss_cps: 0.225559
[14:08:14.140] iteration 25065: total_loss: 0.542479, loss_sup: 0.039925, loss_mps: 0.173115, loss_cps: 0.329439
[14:08:14.285] iteration 25066: total_loss: 0.324776, loss_sup: 0.068779, loss_mps: 0.092524, loss_cps: 0.163473
[14:08:14.431] iteration 25067: total_loss: 0.382122, loss_sup: 0.031317, loss_mps: 0.126407, loss_cps: 0.224398
[14:08:14.577] iteration 25068: total_loss: 0.486496, loss_sup: 0.242447, loss_mps: 0.093460, loss_cps: 0.150589
[14:08:14.723] iteration 25069: total_loss: 0.489115, loss_sup: 0.036537, loss_mps: 0.152936, loss_cps: 0.299642
[14:08:14.869] iteration 25070: total_loss: 0.529070, loss_sup: 0.200903, loss_mps: 0.109486, loss_cps: 0.218681
[14:08:15.016] iteration 25071: total_loss: 0.318993, loss_sup: 0.051134, loss_mps: 0.098711, loss_cps: 0.169148
[14:08:15.161] iteration 25072: total_loss: 0.335538, loss_sup: 0.053110, loss_mps: 0.100437, loss_cps: 0.181992
[14:08:15.307] iteration 25073: total_loss: 0.595183, loss_sup: 0.036899, loss_mps: 0.180856, loss_cps: 0.377429
[14:08:15.453] iteration 25074: total_loss: 0.424464, loss_sup: 0.095199, loss_mps: 0.114205, loss_cps: 0.215060
[14:08:15.599] iteration 25075: total_loss: 0.447640, loss_sup: 0.128838, loss_mps: 0.113377, loss_cps: 0.205425
[14:08:15.746] iteration 25076: total_loss: 0.319096, loss_sup: 0.018421, loss_mps: 0.106892, loss_cps: 0.193783
[14:08:15.891] iteration 25077: total_loss: 0.295447, loss_sup: 0.006492, loss_mps: 0.103566, loss_cps: 0.185390
[14:08:16.038] iteration 25078: total_loss: 0.286912, loss_sup: 0.034472, loss_mps: 0.089671, loss_cps: 0.162769
[14:08:16.184] iteration 25079: total_loss: 0.593082, loss_sup: 0.053572, loss_mps: 0.184246, loss_cps: 0.355265
[14:08:16.247] iteration 25080: total_loss: 0.324292, loss_sup: 0.047801, loss_mps: 0.093846, loss_cps: 0.182646
[14:08:17.476] iteration 25081: total_loss: 0.417607, loss_sup: 0.015429, loss_mps: 0.138798, loss_cps: 0.263381
[14:08:17.626] iteration 25082: total_loss: 0.536858, loss_sup: 0.050535, loss_mps: 0.156410, loss_cps: 0.329913
[14:08:17.773] iteration 25083: total_loss: 0.388583, loss_sup: 0.007738, loss_mps: 0.124305, loss_cps: 0.256540
[14:08:17.923] iteration 25084: total_loss: 0.261276, loss_sup: 0.044693, loss_mps: 0.076240, loss_cps: 0.140343
[14:08:18.071] iteration 25085: total_loss: 0.281269, loss_sup: 0.019307, loss_mps: 0.094574, loss_cps: 0.167388
[14:08:18.218] iteration 25086: total_loss: 0.311862, loss_sup: 0.016066, loss_mps: 0.109231, loss_cps: 0.186565
[14:08:18.365] iteration 25087: total_loss: 0.493259, loss_sup: 0.017371, loss_mps: 0.158582, loss_cps: 0.317305
[14:08:18.516] iteration 25088: total_loss: 0.347587, loss_sup: 0.091761, loss_mps: 0.091911, loss_cps: 0.163915
[14:08:18.663] iteration 25089: total_loss: 0.340984, loss_sup: 0.021287, loss_mps: 0.119786, loss_cps: 0.199912
[14:08:18.809] iteration 25090: total_loss: 0.374736, loss_sup: 0.056965, loss_mps: 0.108142, loss_cps: 0.209629
[14:08:18.955] iteration 25091: total_loss: 0.463390, loss_sup: 0.138704, loss_mps: 0.109060, loss_cps: 0.215626
[14:08:19.102] iteration 25092: total_loss: 0.316924, loss_sup: 0.014288, loss_mps: 0.112032, loss_cps: 0.190604
[14:08:19.249] iteration 25093: total_loss: 0.368362, loss_sup: 0.007065, loss_mps: 0.120782, loss_cps: 0.240516
[14:08:19.395] iteration 25094: total_loss: 0.274104, loss_sup: 0.068217, loss_mps: 0.076528, loss_cps: 0.129359
[14:08:19.543] iteration 25095: total_loss: 0.357232, loss_sup: 0.015923, loss_mps: 0.119579, loss_cps: 0.221730
[14:08:19.690] iteration 25096: total_loss: 0.640169, loss_sup: 0.109524, loss_mps: 0.163051, loss_cps: 0.367593
[14:08:19.838] iteration 25097: total_loss: 0.459173, loss_sup: 0.056794, loss_mps: 0.128549, loss_cps: 0.273830
[14:08:19.985] iteration 25098: total_loss: 0.650565, loss_sup: 0.389211, loss_mps: 0.103233, loss_cps: 0.158121
[14:08:20.132] iteration 25099: total_loss: 0.129536, loss_sup: 0.003054, loss_mps: 0.047510, loss_cps: 0.078972
[14:08:20.281] iteration 25100: total_loss: 0.332094, loss_sup: 0.032480, loss_mps: 0.102595, loss_cps: 0.197018
[14:08:20.281] Evaluation Started ==>
[14:08:31.636] ==> valid iteration 25100: unet metrics: {'dc': 0.6485648397258029, 'jc': 0.5358129376834333, 'pre': 0.7964569398173784, 'hd': 5.324411236706904}, ynet metrics: {'dc': 0.5938829297633738, 'jc': 0.4849081139976676, 'pre': 0.8015795937334838, 'hd': 5.2504485176105105}.
[14:08:31.638] Evaluation Finished!⏹️
[14:08:31.789] iteration 25101: total_loss: 0.567610, loss_sup: 0.132324, loss_mps: 0.146228, loss_cps: 0.289057
[14:08:31.937] iteration 25102: total_loss: 0.392305, loss_sup: 0.011955, loss_mps: 0.124258, loss_cps: 0.256091
[14:08:32.084] iteration 25103: total_loss: 0.290187, loss_sup: 0.025591, loss_mps: 0.091524, loss_cps: 0.173071
[14:08:32.230] iteration 25104: total_loss: 0.712657, loss_sup: 0.243334, loss_mps: 0.149799, loss_cps: 0.319524
[14:08:32.375] iteration 25105: total_loss: 0.425959, loss_sup: 0.082910, loss_mps: 0.120368, loss_cps: 0.222681
[14:08:32.522] iteration 25106: total_loss: 0.418521, loss_sup: 0.067382, loss_mps: 0.125159, loss_cps: 0.225980
[14:08:32.668] iteration 25107: total_loss: 0.364338, loss_sup: 0.063971, loss_mps: 0.109244, loss_cps: 0.191124
[14:08:32.814] iteration 25108: total_loss: 0.339125, loss_sup: 0.044036, loss_mps: 0.105443, loss_cps: 0.189647
[14:08:32.962] iteration 25109: total_loss: 0.584224, loss_sup: 0.078036, loss_mps: 0.173652, loss_cps: 0.332536
[14:08:33.108] iteration 25110: total_loss: 0.344868, loss_sup: 0.032614, loss_mps: 0.111854, loss_cps: 0.200400
[14:08:33.255] iteration 25111: total_loss: 0.384167, loss_sup: 0.026238, loss_mps: 0.127656, loss_cps: 0.230273
[14:08:33.401] iteration 25112: total_loss: 0.838003, loss_sup: 0.098589, loss_mps: 0.220647, loss_cps: 0.518768
[14:08:33.550] iteration 25113: total_loss: 0.219206, loss_sup: 0.003966, loss_mps: 0.080149, loss_cps: 0.135091
[14:08:33.695] iteration 25114: total_loss: 0.510852, loss_sup: 0.125217, loss_mps: 0.131492, loss_cps: 0.254143
[14:08:33.845] iteration 25115: total_loss: 0.342344, loss_sup: 0.063966, loss_mps: 0.092406, loss_cps: 0.185972
[14:08:33.991] iteration 25116: total_loss: 0.438097, loss_sup: 0.039014, loss_mps: 0.136813, loss_cps: 0.262270
[14:08:34.138] iteration 25117: total_loss: 0.557283, loss_sup: 0.012118, loss_mps: 0.174020, loss_cps: 0.371146
[14:08:34.284] iteration 25118: total_loss: 0.320831, loss_sup: 0.039669, loss_mps: 0.101403, loss_cps: 0.179758
[14:08:34.430] iteration 25119: total_loss: 0.315329, loss_sup: 0.063481, loss_mps: 0.099743, loss_cps: 0.152106
[14:08:34.576] iteration 25120: total_loss: 0.316461, loss_sup: 0.018847, loss_mps: 0.103070, loss_cps: 0.194545
[14:08:34.722] iteration 25121: total_loss: 0.372937, loss_sup: 0.057951, loss_mps: 0.109790, loss_cps: 0.205197
[14:08:34.868] iteration 25122: total_loss: 0.714587, loss_sup: 0.042060, loss_mps: 0.203874, loss_cps: 0.468653
[14:08:35.013] iteration 25123: total_loss: 0.356024, loss_sup: 0.009326, loss_mps: 0.122633, loss_cps: 0.224064
[14:08:35.160] iteration 25124: total_loss: 0.399107, loss_sup: 0.031116, loss_mps: 0.130189, loss_cps: 0.237802
[14:08:35.307] iteration 25125: total_loss: 0.317636, loss_sup: 0.023382, loss_mps: 0.104059, loss_cps: 0.190195
[14:08:35.453] iteration 25126: total_loss: 0.380133, loss_sup: 0.014199, loss_mps: 0.121480, loss_cps: 0.244453
[14:08:35.599] iteration 25127: total_loss: 0.382513, loss_sup: 0.042489, loss_mps: 0.115732, loss_cps: 0.224292
[14:08:35.745] iteration 25128: total_loss: 0.305129, loss_sup: 0.019056, loss_mps: 0.106102, loss_cps: 0.179970
[14:08:35.892] iteration 25129: total_loss: 0.301015, loss_sup: 0.011653, loss_mps: 0.097638, loss_cps: 0.191724
[14:08:36.039] iteration 25130: total_loss: 0.382272, loss_sup: 0.029865, loss_mps: 0.120893, loss_cps: 0.231514
[14:08:36.185] iteration 25131: total_loss: 0.257334, loss_sup: 0.004012, loss_mps: 0.090503, loss_cps: 0.162819
[14:08:36.333] iteration 25132: total_loss: 0.438094, loss_sup: 0.033918, loss_mps: 0.129486, loss_cps: 0.274689
[14:08:36.481] iteration 25133: total_loss: 0.343961, loss_sup: 0.017011, loss_mps: 0.111187, loss_cps: 0.215763
[14:08:36.628] iteration 25134: total_loss: 0.330609, loss_sup: 0.060829, loss_mps: 0.095453, loss_cps: 0.174326
[14:08:36.775] iteration 25135: total_loss: 0.561156, loss_sup: 0.064022, loss_mps: 0.169567, loss_cps: 0.327566
[14:08:36.921] iteration 25136: total_loss: 0.306088, loss_sup: 0.027367, loss_mps: 0.094925, loss_cps: 0.183796
[14:08:37.067] iteration 25137: total_loss: 0.426452, loss_sup: 0.072930, loss_mps: 0.129194, loss_cps: 0.224328
[14:08:37.214] iteration 25138: total_loss: 0.318890, loss_sup: 0.007678, loss_mps: 0.105425, loss_cps: 0.205787
[14:08:37.360] iteration 25139: total_loss: 0.335009, loss_sup: 0.010811, loss_mps: 0.117902, loss_cps: 0.206296
[14:08:37.506] iteration 25140: total_loss: 0.582732, loss_sup: 0.062513, loss_mps: 0.178081, loss_cps: 0.342139
[14:08:37.653] iteration 25141: total_loss: 0.441245, loss_sup: 0.129020, loss_mps: 0.105298, loss_cps: 0.206926
[14:08:37.799] iteration 25142: total_loss: 0.345508, loss_sup: 0.060090, loss_mps: 0.108059, loss_cps: 0.177359
[14:08:37.946] iteration 25143: total_loss: 0.629231, loss_sup: 0.171829, loss_mps: 0.148264, loss_cps: 0.309138
[14:08:38.092] iteration 25144: total_loss: 0.472608, loss_sup: 0.045154, loss_mps: 0.149141, loss_cps: 0.278313
[14:08:38.238] iteration 25145: total_loss: 0.456033, loss_sup: 0.053881, loss_mps: 0.132694, loss_cps: 0.269458
[14:08:38.385] iteration 25146: total_loss: 0.298868, loss_sup: 0.021033, loss_mps: 0.100075, loss_cps: 0.177761
[14:08:38.532] iteration 25147: total_loss: 0.457032, loss_sup: 0.079889, loss_mps: 0.129665, loss_cps: 0.247478
[14:08:38.680] iteration 25148: total_loss: 0.428755, loss_sup: 0.076889, loss_mps: 0.124746, loss_cps: 0.227120
[14:08:38.826] iteration 25149: total_loss: 0.670320, loss_sup: 0.277589, loss_mps: 0.137221, loss_cps: 0.255510
[14:08:38.972] iteration 25150: total_loss: 0.346775, loss_sup: 0.108178, loss_mps: 0.081831, loss_cps: 0.156766
[14:08:39.118] iteration 25151: total_loss: 0.441790, loss_sup: 0.059673, loss_mps: 0.133424, loss_cps: 0.248693
[14:08:39.267] iteration 25152: total_loss: 0.506362, loss_sup: 0.074234, loss_mps: 0.149091, loss_cps: 0.283037
[14:08:39.413] iteration 25153: total_loss: 0.253154, loss_sup: 0.009013, loss_mps: 0.086032, loss_cps: 0.158108
[14:08:39.560] iteration 25154: total_loss: 0.417832, loss_sup: 0.001371, loss_mps: 0.138696, loss_cps: 0.277766
[14:08:39.706] iteration 25155: total_loss: 0.565612, loss_sup: 0.134225, loss_mps: 0.144509, loss_cps: 0.286878
[14:08:39.852] iteration 25156: total_loss: 0.321335, loss_sup: 0.048236, loss_mps: 0.098616, loss_cps: 0.174482
[14:08:39.999] iteration 25157: total_loss: 0.340603, loss_sup: 0.073782, loss_mps: 0.099410, loss_cps: 0.167411
[14:08:40.145] iteration 25158: total_loss: 0.396861, loss_sup: 0.056660, loss_mps: 0.122252, loss_cps: 0.217949
[14:08:40.293] iteration 25159: total_loss: 0.512717, loss_sup: 0.230697, loss_mps: 0.104952, loss_cps: 0.177069
[14:08:40.439] iteration 25160: total_loss: 0.591821, loss_sup: 0.045917, loss_mps: 0.171700, loss_cps: 0.374204
[14:08:40.585] iteration 25161: total_loss: 0.440391, loss_sup: 0.044853, loss_mps: 0.139376, loss_cps: 0.256162
[14:08:40.731] iteration 25162: total_loss: 0.730321, loss_sup: 0.041332, loss_mps: 0.228616, loss_cps: 0.460373
[14:08:40.877] iteration 25163: total_loss: 0.533535, loss_sup: 0.092689, loss_mps: 0.141055, loss_cps: 0.299791
[14:08:41.024] iteration 25164: total_loss: 0.326062, loss_sup: 0.063847, loss_mps: 0.097093, loss_cps: 0.165121
[14:08:41.170] iteration 25165: total_loss: 0.339974, loss_sup: 0.003793, loss_mps: 0.115414, loss_cps: 0.220767
[14:08:41.319] iteration 25166: total_loss: 0.332482, loss_sup: 0.015268, loss_mps: 0.106934, loss_cps: 0.210279
[14:08:41.468] iteration 25167: total_loss: 0.405842, loss_sup: 0.043375, loss_mps: 0.124346, loss_cps: 0.238121
[14:08:41.615] iteration 25168: total_loss: 0.545348, loss_sup: 0.114714, loss_mps: 0.139241, loss_cps: 0.291393
[14:08:41.761] iteration 25169: total_loss: 0.338980, loss_sup: 0.024754, loss_mps: 0.111466, loss_cps: 0.202760
[14:08:41.908] iteration 25170: total_loss: 0.259789, loss_sup: 0.023160, loss_mps: 0.086057, loss_cps: 0.150572
[14:08:42.056] iteration 25171: total_loss: 0.723495, loss_sup: 0.061559, loss_mps: 0.219390, loss_cps: 0.442546
[14:08:42.205] iteration 25172: total_loss: 0.764067, loss_sup: 0.149598, loss_mps: 0.193648, loss_cps: 0.420822
[14:08:42.351] iteration 25173: total_loss: 0.594387, loss_sup: 0.099431, loss_mps: 0.154709, loss_cps: 0.340247
[14:08:42.498] iteration 25174: total_loss: 0.882686, loss_sup: 0.137076, loss_mps: 0.228867, loss_cps: 0.516743
[14:08:42.644] iteration 25175: total_loss: 0.285315, loss_sup: 0.063508, loss_mps: 0.079709, loss_cps: 0.142098
[14:08:42.791] iteration 25176: total_loss: 0.828463, loss_sup: 0.065291, loss_mps: 0.242701, loss_cps: 0.520471
[14:08:42.937] iteration 25177: total_loss: 0.869897, loss_sup: 0.118866, loss_mps: 0.239678, loss_cps: 0.511353
[14:08:43.084] iteration 25178: total_loss: 0.449965, loss_sup: 0.028951, loss_mps: 0.142962, loss_cps: 0.278052
[14:08:43.230] iteration 25179: total_loss: 0.863036, loss_sup: 0.245429, loss_mps: 0.194472, loss_cps: 0.423135
[14:08:43.377] iteration 25180: total_loss: 0.491720, loss_sup: 0.018561, loss_mps: 0.156073, loss_cps: 0.317086
[14:08:43.524] iteration 25181: total_loss: 0.305309, loss_sup: 0.028325, loss_mps: 0.102984, loss_cps: 0.174000
[14:08:43.671] iteration 25182: total_loss: 0.440235, loss_sup: 0.034783, loss_mps: 0.136840, loss_cps: 0.268612
[14:08:43.820] iteration 25183: total_loss: 0.415529, loss_sup: 0.039816, loss_mps: 0.144983, loss_cps: 0.230730
[14:08:43.967] iteration 25184: total_loss: 0.619373, loss_sup: 0.210741, loss_mps: 0.134468, loss_cps: 0.274164
[14:08:44.113] iteration 25185: total_loss: 0.475401, loss_sup: 0.164716, loss_mps: 0.113889, loss_cps: 0.196796
[14:08:44.259] iteration 25186: total_loss: 0.406100, loss_sup: 0.125588, loss_mps: 0.099482, loss_cps: 0.181030
[14:08:44.406] iteration 25187: total_loss: 0.224883, loss_sup: 0.017288, loss_mps: 0.073468, loss_cps: 0.134128
[14:08:44.552] iteration 25188: total_loss: 0.461985, loss_sup: 0.031885, loss_mps: 0.153509, loss_cps: 0.276592
[14:08:44.698] iteration 25189: total_loss: 0.697933, loss_sup: 0.140960, loss_mps: 0.173271, loss_cps: 0.383702
[14:08:44.844] iteration 25190: total_loss: 0.276325, loss_sup: 0.001212, loss_mps: 0.098735, loss_cps: 0.176379
[14:08:44.991] iteration 25191: total_loss: 0.517296, loss_sup: 0.030251, loss_mps: 0.153764, loss_cps: 0.333280
[14:08:45.138] iteration 25192: total_loss: 0.454488, loss_sup: 0.028738, loss_mps: 0.150478, loss_cps: 0.275272
[14:08:45.284] iteration 25193: total_loss: 0.275010, loss_sup: 0.005089, loss_mps: 0.100239, loss_cps: 0.169682
[14:08:45.431] iteration 25194: total_loss: 0.322908, loss_sup: 0.014381, loss_mps: 0.112050, loss_cps: 0.196477
[14:08:45.579] iteration 25195: total_loss: 0.371754, loss_sup: 0.018067, loss_mps: 0.130093, loss_cps: 0.223594
[14:08:45.725] iteration 25196: total_loss: 0.389664, loss_sup: 0.028133, loss_mps: 0.123649, loss_cps: 0.237882
[14:08:45.874] iteration 25197: total_loss: 0.445481, loss_sup: 0.071606, loss_mps: 0.135595, loss_cps: 0.238279
[14:08:46.020] iteration 25198: total_loss: 0.233878, loss_sup: 0.055069, loss_mps: 0.067904, loss_cps: 0.110905
[14:08:46.169] iteration 25199: total_loss: 0.242024, loss_sup: 0.008764, loss_mps: 0.090884, loss_cps: 0.142376
[14:08:46.316] iteration 25200: total_loss: 0.449737, loss_sup: 0.042906, loss_mps: 0.132972, loss_cps: 0.273859
[14:08:46.317] Evaluation Started ==>
[14:08:57.681] ==> valid iteration 25200: unet metrics: {'dc': 0.6596731056124178, 'jc': 0.5475524012157857, 'pre': 0.7923929877818754, 'hd': 5.32746570944546}, ynet metrics: {'dc': 0.6256887762741443, 'jc': 0.5145892997414442, 'pre': 0.8038137025146812, 'hd': 5.298543207583697}.
[14:08:57.684] Evaluation Finished!⏹️
[14:08:57.837] iteration 25201: total_loss: 0.476516, loss_sup: 0.002983, loss_mps: 0.149104, loss_cps: 0.324429
[14:08:57.984] iteration 25202: total_loss: 0.488058, loss_sup: 0.043241, loss_mps: 0.152664, loss_cps: 0.292153
[14:08:58.130] iteration 25203: total_loss: 0.569246, loss_sup: 0.014784, loss_mps: 0.176367, loss_cps: 0.378095
[14:08:58.277] iteration 25204: total_loss: 0.342272, loss_sup: 0.080614, loss_mps: 0.097834, loss_cps: 0.163824
[14:08:58.423] iteration 25205: total_loss: 0.694870, loss_sup: 0.101615, loss_mps: 0.193498, loss_cps: 0.399757
[14:08:58.568] iteration 25206: total_loss: 0.710995, loss_sup: 0.082808, loss_mps: 0.197618, loss_cps: 0.430570
[14:08:58.714] iteration 25207: total_loss: 0.918277, loss_sup: 0.097532, loss_mps: 0.256744, loss_cps: 0.564001
[14:08:58.859] iteration 25208: total_loss: 0.546215, loss_sup: 0.183090, loss_mps: 0.129547, loss_cps: 0.233578
[14:08:59.004] iteration 25209: total_loss: 0.266978, loss_sup: 0.002343, loss_mps: 0.096883, loss_cps: 0.167752
[14:08:59.150] iteration 25210: total_loss: 0.243276, loss_sup: 0.010664, loss_mps: 0.085828, loss_cps: 0.146783
[14:08:59.297] iteration 25211: total_loss: 0.471995, loss_sup: 0.022856, loss_mps: 0.149475, loss_cps: 0.299664
[14:08:59.443] iteration 25212: total_loss: 0.438466, loss_sup: 0.050986, loss_mps: 0.139579, loss_cps: 0.247902
[14:08:59.589] iteration 25213: total_loss: 0.577687, loss_sup: 0.021031, loss_mps: 0.177855, loss_cps: 0.378801
[14:08:59.735] iteration 25214: total_loss: 0.325895, loss_sup: 0.014869, loss_mps: 0.113742, loss_cps: 0.197285
[14:08:59.881] iteration 25215: total_loss: 0.598218, loss_sup: 0.162793, loss_mps: 0.160458, loss_cps: 0.274968
[14:09:00.026] iteration 25216: total_loss: 0.479922, loss_sup: 0.036670, loss_mps: 0.147600, loss_cps: 0.295652
[14:09:00.172] iteration 25217: total_loss: 0.247559, loss_sup: 0.016505, loss_mps: 0.085656, loss_cps: 0.145398
[14:09:00.317] iteration 25218: total_loss: 0.306797, loss_sup: 0.042828, loss_mps: 0.094011, loss_cps: 0.169957
[14:09:00.463] iteration 25219: total_loss: 0.399021, loss_sup: 0.061861, loss_mps: 0.120594, loss_cps: 0.216566
[14:09:00.610] iteration 25220: total_loss: 0.255077, loss_sup: 0.029454, loss_mps: 0.081511, loss_cps: 0.144113
[14:09:00.756] iteration 25221: total_loss: 0.507570, loss_sup: 0.042859, loss_mps: 0.145865, loss_cps: 0.318846
[14:09:00.903] iteration 25222: total_loss: 0.471283, loss_sup: 0.008832, loss_mps: 0.160898, loss_cps: 0.301553
[14:09:01.048] iteration 25223: total_loss: 0.269870, loss_sup: 0.004901, loss_mps: 0.092367, loss_cps: 0.172602
[14:09:01.197] iteration 25224: total_loss: 0.522961, loss_sup: 0.018729, loss_mps: 0.169701, loss_cps: 0.334531
[14:09:01.346] iteration 25225: total_loss: 0.625983, loss_sup: 0.023536, loss_mps: 0.195763, loss_cps: 0.406684
[14:09:01.493] iteration 25226: total_loss: 0.366443, loss_sup: 0.002752, loss_mps: 0.122009, loss_cps: 0.241682
[14:09:01.641] iteration 25227: total_loss: 0.392712, loss_sup: 0.024719, loss_mps: 0.125784, loss_cps: 0.242209
[14:09:01.787] iteration 25228: total_loss: 0.581459, loss_sup: 0.048089, loss_mps: 0.171410, loss_cps: 0.361960
[14:09:01.935] iteration 25229: total_loss: 0.276367, loss_sup: 0.016275, loss_mps: 0.095440, loss_cps: 0.164652
[14:09:02.081] iteration 25230: total_loss: 0.632807, loss_sup: 0.086862, loss_mps: 0.189024, loss_cps: 0.356921
[14:09:02.226] iteration 25231: total_loss: 0.311794, loss_sup: 0.017400, loss_mps: 0.100197, loss_cps: 0.194198
[14:09:02.372] iteration 25232: total_loss: 0.339890, loss_sup: 0.157901, loss_mps: 0.070249, loss_cps: 0.111740
[14:09:02.519] iteration 25233: total_loss: 0.727753, loss_sup: 0.119441, loss_mps: 0.200337, loss_cps: 0.407975
[14:09:02.664] iteration 25234: total_loss: 0.285467, loss_sup: 0.021679, loss_mps: 0.096264, loss_cps: 0.167524
[14:09:02.810] iteration 25235: total_loss: 0.606746, loss_sup: 0.039600, loss_mps: 0.188701, loss_cps: 0.378445
[14:09:02.955] iteration 25236: total_loss: 0.243774, loss_sup: 0.017395, loss_mps: 0.086616, loss_cps: 0.139763
[14:09:03.101] iteration 25237: total_loss: 0.233182, loss_sup: 0.007270, loss_mps: 0.085718, loss_cps: 0.140194
[14:09:03.248] iteration 25238: total_loss: 0.582990, loss_sup: 0.067695, loss_mps: 0.176535, loss_cps: 0.338760
[14:09:03.394] iteration 25239: total_loss: 0.592497, loss_sup: 0.213830, loss_mps: 0.141689, loss_cps: 0.236978
[14:09:03.540] iteration 25240: total_loss: 0.373660, loss_sup: 0.019064, loss_mps: 0.119245, loss_cps: 0.235351
[14:09:03.686] iteration 25241: total_loss: 0.531196, loss_sup: 0.099959, loss_mps: 0.140389, loss_cps: 0.290848
[14:09:03.832] iteration 25242: total_loss: 0.553285, loss_sup: 0.110806, loss_mps: 0.147811, loss_cps: 0.294668
[14:09:03.977] iteration 25243: total_loss: 0.236038, loss_sup: 0.006070, loss_mps: 0.088214, loss_cps: 0.141754
[14:09:04.125] iteration 25244: total_loss: 0.465087, loss_sup: 0.135080, loss_mps: 0.117647, loss_cps: 0.212360
[14:09:04.275] iteration 25245: total_loss: 0.770479, loss_sup: 0.061390, loss_mps: 0.227165, loss_cps: 0.481924
[14:09:04.422] iteration 25246: total_loss: 0.562651, loss_sup: 0.137049, loss_mps: 0.141362, loss_cps: 0.284241
[14:09:04.570] iteration 25247: total_loss: 0.425689, loss_sup: 0.016669, loss_mps: 0.140223, loss_cps: 0.268798
[14:09:04.716] iteration 25248: total_loss: 0.557021, loss_sup: 0.105903, loss_mps: 0.163613, loss_cps: 0.287505
[14:09:04.865] iteration 25249: total_loss: 0.740540, loss_sup: 0.051334, loss_mps: 0.218239, loss_cps: 0.470967
[14:09:05.011] iteration 25250: total_loss: 0.437567, loss_sup: 0.046635, loss_mps: 0.149320, loss_cps: 0.241613
[14:09:05.158] iteration 25251: total_loss: 0.281237, loss_sup: 0.003582, loss_mps: 0.103714, loss_cps: 0.173941
[14:09:05.305] iteration 25252: total_loss: 0.310781, loss_sup: 0.066316, loss_mps: 0.091206, loss_cps: 0.153259
[14:09:05.452] iteration 25253: total_loss: 0.727966, loss_sup: 0.064828, loss_mps: 0.203036, loss_cps: 0.460101
[14:09:05.597] iteration 25254: total_loss: 0.343070, loss_sup: 0.041573, loss_mps: 0.100849, loss_cps: 0.200649
[14:09:05.745] iteration 25255: total_loss: 0.215263, loss_sup: 0.008960, loss_mps: 0.078834, loss_cps: 0.127469
[14:09:05.891] iteration 25256: total_loss: 0.523129, loss_sup: 0.096351, loss_mps: 0.149189, loss_cps: 0.277589
[14:09:06.037] iteration 25257: total_loss: 0.307387, loss_sup: 0.016411, loss_mps: 0.105755, loss_cps: 0.185220
[14:09:06.183] iteration 25258: total_loss: 0.271973, loss_sup: 0.010129, loss_mps: 0.092961, loss_cps: 0.168883
[14:09:06.331] iteration 25259: total_loss: 0.409663, loss_sup: 0.056512, loss_mps: 0.120273, loss_cps: 0.232878
[14:09:06.476] iteration 25260: total_loss: 0.353685, loss_sup: 0.048831, loss_mps: 0.110968, loss_cps: 0.193886
[14:09:06.622] iteration 25261: total_loss: 0.420193, loss_sup: 0.094276, loss_mps: 0.118656, loss_cps: 0.207262
[14:09:06.768] iteration 25262: total_loss: 0.703193, loss_sup: 0.247799, loss_mps: 0.158043, loss_cps: 0.297351
[14:09:06.915] iteration 25263: total_loss: 0.277876, loss_sup: 0.043996, loss_mps: 0.086872, loss_cps: 0.147008
[14:09:07.061] iteration 25264: total_loss: 0.351109, loss_sup: 0.030923, loss_mps: 0.110927, loss_cps: 0.209259
[14:09:07.207] iteration 25265: total_loss: 0.595345, loss_sup: 0.052875, loss_mps: 0.174929, loss_cps: 0.367541
[14:09:07.354] iteration 25266: total_loss: 0.355335, loss_sup: 0.041296, loss_mps: 0.101936, loss_cps: 0.212102
[14:09:07.503] iteration 25267: total_loss: 0.486486, loss_sup: 0.042345, loss_mps: 0.146135, loss_cps: 0.298006
[14:09:07.649] iteration 25268: total_loss: 0.269564, loss_sup: 0.013344, loss_mps: 0.092134, loss_cps: 0.164085
[14:09:07.794] iteration 25269: total_loss: 0.373217, loss_sup: 0.032924, loss_mps: 0.112159, loss_cps: 0.228134
[14:09:07.940] iteration 25270: total_loss: 0.523100, loss_sup: 0.089225, loss_mps: 0.153179, loss_cps: 0.280695
[14:09:08.087] iteration 25271: total_loss: 0.449984, loss_sup: 0.024091, loss_mps: 0.143606, loss_cps: 0.282287
[14:09:08.233] iteration 25272: total_loss: 0.600365, loss_sup: 0.064131, loss_mps: 0.180397, loss_cps: 0.355837
[14:09:08.384] iteration 25273: total_loss: 0.629894, loss_sup: 0.096499, loss_mps: 0.176593, loss_cps: 0.356803
[14:09:08.530] iteration 25274: total_loss: 0.328747, loss_sup: 0.013237, loss_mps: 0.109674, loss_cps: 0.205836
[14:09:08.675] iteration 25275: total_loss: 0.466023, loss_sup: 0.164674, loss_mps: 0.106949, loss_cps: 0.194400
[14:09:08.824] iteration 25276: total_loss: 0.153797, loss_sup: 0.008231, loss_mps: 0.057620, loss_cps: 0.087946
[14:09:08.970] iteration 25277: total_loss: 0.341254, loss_sup: 0.030209, loss_mps: 0.114864, loss_cps: 0.196181
[14:09:09.116] iteration 25278: total_loss: 0.375291, loss_sup: 0.072345, loss_mps: 0.112319, loss_cps: 0.190627
[14:09:09.262] iteration 25279: total_loss: 0.257878, loss_sup: 0.012708, loss_mps: 0.090636, loss_cps: 0.154534
[14:09:09.409] iteration 25280: total_loss: 0.606508, loss_sup: 0.231477, loss_mps: 0.123994, loss_cps: 0.251037
[14:09:09.558] iteration 25281: total_loss: 0.264482, loss_sup: 0.011109, loss_mps: 0.094553, loss_cps: 0.158821
[14:09:09.704] iteration 25282: total_loss: 0.508794, loss_sup: 0.128922, loss_mps: 0.130356, loss_cps: 0.249516
[14:09:09.852] iteration 25283: total_loss: 0.295895, loss_sup: 0.006625, loss_mps: 0.099596, loss_cps: 0.189675
[14:09:10.000] iteration 25284: total_loss: 0.745644, loss_sup: 0.084034, loss_mps: 0.212467, loss_cps: 0.449143
[14:09:10.148] iteration 25285: total_loss: 0.327768, loss_sup: 0.027380, loss_mps: 0.101848, loss_cps: 0.198541
[14:09:10.294] iteration 25286: total_loss: 0.401251, loss_sup: 0.095451, loss_mps: 0.120486, loss_cps: 0.185314
[14:09:10.441] iteration 25287: total_loss: 0.254268, loss_sup: 0.062988, loss_mps: 0.073428, loss_cps: 0.117852
[14:09:10.587] iteration 25288: total_loss: 0.483280, loss_sup: 0.058333, loss_mps: 0.144656, loss_cps: 0.280291
[14:09:10.733] iteration 25289: total_loss: 0.430114, loss_sup: 0.031005, loss_mps: 0.133174, loss_cps: 0.265936
[14:09:10.880] iteration 25290: total_loss: 0.425536, loss_sup: 0.107983, loss_mps: 0.105205, loss_cps: 0.212347
[14:09:11.026] iteration 25291: total_loss: 0.225642, loss_sup: 0.043098, loss_mps: 0.069054, loss_cps: 0.113489
[14:09:11.172] iteration 25292: total_loss: 0.409757, loss_sup: 0.033297, loss_mps: 0.127469, loss_cps: 0.248991
[14:09:11.318] iteration 25293: total_loss: 0.257479, loss_sup: 0.002617, loss_mps: 0.091565, loss_cps: 0.163297
[14:09:11.469] iteration 25294: total_loss: 0.713239, loss_sup: 0.358025, loss_mps: 0.129895, loss_cps: 0.225320
[14:09:11.615] iteration 25295: total_loss: 0.378421, loss_sup: 0.007324, loss_mps: 0.121721, loss_cps: 0.249377
[14:09:11.761] iteration 25296: total_loss: 0.417726, loss_sup: 0.036507, loss_mps: 0.128039, loss_cps: 0.253180
[14:09:11.907] iteration 25297: total_loss: 0.449390, loss_sup: 0.031713, loss_mps: 0.137256, loss_cps: 0.280421
[14:09:12.053] iteration 25298: total_loss: 0.229014, loss_sup: 0.013864, loss_mps: 0.079648, loss_cps: 0.135503
[14:09:12.199] iteration 25299: total_loss: 0.343719, loss_sup: 0.021896, loss_mps: 0.105723, loss_cps: 0.216100
[14:09:12.346] iteration 25300: total_loss: 0.508833, loss_sup: 0.102060, loss_mps: 0.137688, loss_cps: 0.269084
[14:09:12.346] Evaluation Started ==>
[14:09:23.708] ==> valid iteration 25300: unet metrics: {'dc': 0.6468846267820436, 'jc': 0.532332533297327, 'pre': 0.8004985225661886, 'hd': 5.308578016683129}, ynet metrics: {'dc': 0.6114422451048137, 'jc': 0.49674980175600897, 'pre': 0.8037088985259322, 'hd': 5.41179358605389}.
[14:09:23.710] Evaluation Finished!⏹️
[14:09:23.866] iteration 25301: total_loss: 0.486325, loss_sup: 0.016652, loss_mps: 0.156570, loss_cps: 0.313103
[14:09:24.015] iteration 25302: total_loss: 0.383753, loss_sup: 0.044369, loss_mps: 0.119755, loss_cps: 0.219629
[14:09:24.161] iteration 25303: total_loss: 0.392700, loss_sup: 0.064931, loss_mps: 0.111814, loss_cps: 0.215955
[14:09:24.306] iteration 25304: total_loss: 0.451694, loss_sup: 0.030776, loss_mps: 0.132854, loss_cps: 0.288064
[14:09:24.452] iteration 25305: total_loss: 0.321811, loss_sup: 0.011571, loss_mps: 0.106540, loss_cps: 0.203700
[14:09:24.597] iteration 25306: total_loss: 0.302162, loss_sup: 0.048647, loss_mps: 0.091866, loss_cps: 0.161650
[14:09:24.744] iteration 25307: total_loss: 0.493564, loss_sup: 0.047191, loss_mps: 0.145616, loss_cps: 0.300756
[14:09:24.890] iteration 25308: total_loss: 0.662781, loss_sup: 0.234641, loss_mps: 0.149693, loss_cps: 0.278447
[14:09:25.038] iteration 25309: total_loss: 0.610846, loss_sup: 0.075496, loss_mps: 0.171784, loss_cps: 0.363566
[14:09:25.184] iteration 25310: total_loss: 0.315497, loss_sup: 0.030824, loss_mps: 0.099029, loss_cps: 0.185644
[14:09:25.329] iteration 25311: total_loss: 0.451081, loss_sup: 0.137819, loss_mps: 0.109268, loss_cps: 0.203994
[14:09:25.476] iteration 25312: total_loss: 0.459029, loss_sup: 0.049172, loss_mps: 0.134947, loss_cps: 0.274910
[14:09:25.624] iteration 25313: total_loss: 0.735772, loss_sup: 0.119534, loss_mps: 0.216704, loss_cps: 0.399534
[14:09:25.772] iteration 25314: total_loss: 0.467008, loss_sup: 0.048452, loss_mps: 0.138450, loss_cps: 0.280106
[14:09:25.921] iteration 25315: total_loss: 0.331751, loss_sup: 0.004215, loss_mps: 0.117563, loss_cps: 0.209974
[14:09:26.070] iteration 25316: total_loss: 0.528235, loss_sup: 0.081201, loss_mps: 0.154920, loss_cps: 0.292115
[14:09:26.215] iteration 25317: total_loss: 0.415760, loss_sup: 0.070696, loss_mps: 0.121936, loss_cps: 0.223128
[14:09:26.361] iteration 25318: total_loss: 0.368508, loss_sup: 0.031721, loss_mps: 0.118643, loss_cps: 0.218144
[14:09:26.508] iteration 25319: total_loss: 0.348987, loss_sup: 0.021307, loss_mps: 0.117051, loss_cps: 0.210629
[14:09:26.654] iteration 25320: total_loss: 0.307886, loss_sup: 0.095464, loss_mps: 0.079716, loss_cps: 0.132707
[14:09:26.800] iteration 25321: total_loss: 0.279541, loss_sup: 0.049744, loss_mps: 0.086195, loss_cps: 0.143601
[14:09:26.946] iteration 25322: total_loss: 0.633093, loss_sup: 0.112789, loss_mps: 0.174806, loss_cps: 0.345498
[14:09:27.092] iteration 25323: total_loss: 0.294401, loss_sup: 0.011657, loss_mps: 0.096376, loss_cps: 0.186368
[14:09:27.242] iteration 25324: total_loss: 0.614696, loss_sup: 0.129285, loss_mps: 0.159474, loss_cps: 0.325937
[14:09:27.389] iteration 25325: total_loss: 0.494051, loss_sup: 0.007444, loss_mps: 0.152527, loss_cps: 0.334080
[14:09:27.535] iteration 25326: total_loss: 0.453692, loss_sup: 0.038323, loss_mps: 0.144827, loss_cps: 0.270542
[14:09:27.681] iteration 25327: total_loss: 0.829925, loss_sup: 0.258652, loss_mps: 0.186060, loss_cps: 0.385214
[14:09:27.827] iteration 25328: total_loss: 0.382617, loss_sup: 0.007106, loss_mps: 0.127846, loss_cps: 0.247665
[14:09:27.972] iteration 25329: total_loss: 0.348471, loss_sup: 0.078246, loss_mps: 0.097640, loss_cps: 0.172585
[14:09:28.122] iteration 25330: total_loss: 0.281922, loss_sup: 0.018405, loss_mps: 0.095963, loss_cps: 0.167554
[14:09:28.269] iteration 25331: total_loss: 0.319348, loss_sup: 0.020572, loss_mps: 0.105185, loss_cps: 0.193590
[14:09:28.415] iteration 25332: total_loss: 0.260673, loss_sup: 0.016127, loss_mps: 0.089375, loss_cps: 0.155171
[14:09:28.561] iteration 25333: total_loss: 0.424493, loss_sup: 0.018642, loss_mps: 0.137377, loss_cps: 0.268473
[14:09:28.706] iteration 25334: total_loss: 0.603526, loss_sup: 0.032747, loss_mps: 0.180827, loss_cps: 0.389953
[14:09:28.852] iteration 25335: total_loss: 0.333465, loss_sup: 0.004996, loss_mps: 0.113127, loss_cps: 0.215342
[14:09:28.998] iteration 25336: total_loss: 0.449358, loss_sup: 0.156253, loss_mps: 0.104738, loss_cps: 0.188367
[14:09:29.144] iteration 25337: total_loss: 0.253174, loss_sup: 0.011993, loss_mps: 0.090071, loss_cps: 0.151110
[14:09:29.289] iteration 25338: total_loss: 0.564873, loss_sup: 0.059031, loss_mps: 0.169839, loss_cps: 0.336003
[14:09:29.436] iteration 25339: total_loss: 0.395345, loss_sup: 0.008001, loss_mps: 0.135627, loss_cps: 0.251717
[14:09:29.582] iteration 25340: total_loss: 0.295705, loss_sup: 0.006224, loss_mps: 0.101220, loss_cps: 0.188261
[14:09:29.728] iteration 25341: total_loss: 0.403064, loss_sup: 0.006112, loss_mps: 0.134118, loss_cps: 0.262834
[14:09:29.874] iteration 25342: total_loss: 0.483670, loss_sup: 0.083107, loss_mps: 0.141940, loss_cps: 0.258624
[14:09:30.020] iteration 25343: total_loss: 0.508666, loss_sup: 0.069671, loss_mps: 0.147430, loss_cps: 0.291566
[14:09:30.166] iteration 25344: total_loss: 0.297797, loss_sup: 0.036057, loss_mps: 0.098657, loss_cps: 0.163083
[14:09:30.313] iteration 25345: total_loss: 0.562984, loss_sup: 0.071020, loss_mps: 0.163222, loss_cps: 0.328743
[14:09:30.459] iteration 25346: total_loss: 0.395237, loss_sup: 0.103451, loss_mps: 0.109663, loss_cps: 0.182124
[14:09:30.605] iteration 25347: total_loss: 0.618001, loss_sup: 0.276197, loss_mps: 0.122459, loss_cps: 0.219345
[14:09:30.751] iteration 25348: total_loss: 0.284349, loss_sup: 0.037377, loss_mps: 0.086677, loss_cps: 0.160295
[14:09:30.898] iteration 25349: total_loss: 0.427359, loss_sup: 0.017242, loss_mps: 0.140976, loss_cps: 0.269142
[14:09:31.045] iteration 25350: total_loss: 0.206803, loss_sup: 0.002991, loss_mps: 0.077099, loss_cps: 0.126713
[14:09:31.191] iteration 25351: total_loss: 0.438660, loss_sup: 0.052899, loss_mps: 0.138290, loss_cps: 0.247472
[14:09:31.337] iteration 25352: total_loss: 0.320329, loss_sup: 0.012254, loss_mps: 0.105933, loss_cps: 0.202142
[14:09:31.483] iteration 25353: total_loss: 0.623977, loss_sup: 0.130042, loss_mps: 0.166394, loss_cps: 0.327541
[14:09:31.629] iteration 25354: total_loss: 0.378236, loss_sup: 0.073913, loss_mps: 0.108646, loss_cps: 0.195677
[14:09:31.775] iteration 25355: total_loss: 0.256582, loss_sup: 0.024915, loss_mps: 0.084398, loss_cps: 0.147269
[14:09:31.922] iteration 25356: total_loss: 0.311834, loss_sup: 0.026258, loss_mps: 0.105939, loss_cps: 0.179637
[14:09:32.068] iteration 25357: total_loss: 0.521862, loss_sup: 0.163163, loss_mps: 0.129309, loss_cps: 0.229390
[14:09:32.215] iteration 25358: total_loss: 0.757874, loss_sup: 0.020335, loss_mps: 0.221422, loss_cps: 0.516116
[14:09:32.362] iteration 25359: total_loss: 0.833268, loss_sup: 0.319914, loss_mps: 0.167949, loss_cps: 0.345405
[14:09:32.508] iteration 25360: total_loss: 0.492963, loss_sup: 0.045903, loss_mps: 0.148804, loss_cps: 0.298256
[14:09:32.654] iteration 25361: total_loss: 0.308846, loss_sup: 0.079802, loss_mps: 0.083486, loss_cps: 0.145558
[14:09:32.801] iteration 25362: total_loss: 0.456006, loss_sup: 0.104185, loss_mps: 0.121127, loss_cps: 0.230694
[14:09:32.948] iteration 25363: total_loss: 0.631994, loss_sup: 0.032827, loss_mps: 0.188635, loss_cps: 0.410533
[14:09:33.094] iteration 25364: total_loss: 0.336338, loss_sup: 0.004468, loss_mps: 0.118379, loss_cps: 0.213491
[14:09:33.240] iteration 25365: total_loss: 1.130171, loss_sup: 0.291006, loss_mps: 0.263604, loss_cps: 0.575561
[14:09:33.385] iteration 25366: total_loss: 0.373119, loss_sup: 0.069028, loss_mps: 0.108456, loss_cps: 0.195635
[14:09:33.531] iteration 25367: total_loss: 0.410044, loss_sup: 0.043553, loss_mps: 0.124985, loss_cps: 0.241506
[14:09:33.679] iteration 25368: total_loss: 0.292561, loss_sup: 0.006984, loss_mps: 0.098668, loss_cps: 0.186909
[14:09:33.825] iteration 25369: total_loss: 0.681993, loss_sup: 0.198000, loss_mps: 0.158942, loss_cps: 0.325051
[14:09:33.973] iteration 25370: total_loss: 0.685892, loss_sup: 0.065725, loss_mps: 0.198929, loss_cps: 0.421237
[14:09:34.119] iteration 25371: total_loss: 0.325196, loss_sup: 0.014921, loss_mps: 0.114062, loss_cps: 0.196213
[14:09:34.265] iteration 25372: total_loss: 0.424564, loss_sup: 0.037304, loss_mps: 0.138151, loss_cps: 0.249108
[14:09:34.411] iteration 25373: total_loss: 0.515410, loss_sup: 0.061224, loss_mps: 0.147176, loss_cps: 0.307010
[14:09:34.557] iteration 25374: total_loss: 0.320118, loss_sup: 0.040221, loss_mps: 0.106558, loss_cps: 0.173339
[14:09:34.703] iteration 25375: total_loss: 0.804026, loss_sup: 0.201522, loss_mps: 0.204279, loss_cps: 0.398225
[14:09:34.850] iteration 25376: total_loss: 0.819655, loss_sup: 0.080302, loss_mps: 0.224024, loss_cps: 0.515328
[14:09:34.998] iteration 25377: total_loss: 0.313668, loss_sup: 0.043229, loss_mps: 0.098606, loss_cps: 0.171833
[14:09:35.144] iteration 25378: total_loss: 0.294631, loss_sup: 0.065928, loss_mps: 0.083990, loss_cps: 0.144713
[14:09:35.290] iteration 25379: total_loss: 0.333450, loss_sup: 0.116220, loss_mps: 0.084717, loss_cps: 0.132513
[14:09:35.436] iteration 25380: total_loss: 0.490831, loss_sup: 0.138497, loss_mps: 0.113399, loss_cps: 0.238935
[14:09:35.583] iteration 25381: total_loss: 0.510963, loss_sup: 0.062719, loss_mps: 0.150758, loss_cps: 0.297486
[14:09:35.728] iteration 25382: total_loss: 0.448683, loss_sup: 0.029191, loss_mps: 0.143542, loss_cps: 0.275950
[14:09:35.874] iteration 25383: total_loss: 0.462793, loss_sup: 0.032312, loss_mps: 0.143993, loss_cps: 0.286488
[14:09:36.020] iteration 25384: total_loss: 0.241451, loss_sup: 0.039788, loss_mps: 0.076505, loss_cps: 0.125159
[14:09:36.166] iteration 25385: total_loss: 0.248979, loss_sup: 0.008405, loss_mps: 0.089507, loss_cps: 0.151067
[14:09:36.312] iteration 25386: total_loss: 0.278838, loss_sup: 0.008433, loss_mps: 0.095089, loss_cps: 0.175316
[14:09:36.458] iteration 25387: total_loss: 0.397894, loss_sup: 0.011793, loss_mps: 0.138162, loss_cps: 0.247939
[14:09:36.604] iteration 25388: total_loss: 0.343634, loss_sup: 0.049851, loss_mps: 0.102498, loss_cps: 0.191285
[14:09:36.750] iteration 25389: total_loss: 0.388643, loss_sup: 0.029558, loss_mps: 0.122034, loss_cps: 0.237052
[14:09:36.896] iteration 25390: total_loss: 0.251182, loss_sup: 0.012151, loss_mps: 0.088549, loss_cps: 0.150482
[14:09:37.043] iteration 25391: total_loss: 0.378649, loss_sup: 0.052315, loss_mps: 0.111680, loss_cps: 0.214655
[14:09:37.188] iteration 25392: total_loss: 0.429313, loss_sup: 0.173973, loss_mps: 0.095658, loss_cps: 0.159682
[14:09:37.334] iteration 25393: total_loss: 0.373376, loss_sup: 0.074489, loss_mps: 0.104206, loss_cps: 0.194682
[14:09:37.480] iteration 25394: total_loss: 0.452446, loss_sup: 0.055401, loss_mps: 0.138299, loss_cps: 0.258747
[14:09:37.626] iteration 25395: total_loss: 0.571042, loss_sup: 0.049930, loss_mps: 0.173236, loss_cps: 0.347876
[14:09:37.772] iteration 25396: total_loss: 0.389195, loss_sup: 0.033636, loss_mps: 0.120893, loss_cps: 0.234667
[14:09:37.919] iteration 25397: total_loss: 0.454401, loss_sup: 0.015037, loss_mps: 0.142448, loss_cps: 0.296916
[14:09:38.066] iteration 25398: total_loss: 0.545237, loss_sup: 0.183077, loss_mps: 0.128938, loss_cps: 0.233222
[14:09:38.212] iteration 25399: total_loss: 0.319488, loss_sup: 0.029498, loss_mps: 0.107287, loss_cps: 0.182702
[14:09:38.360] iteration 25400: total_loss: 0.221165, loss_sup: 0.044683, loss_mps: 0.070231, loss_cps: 0.106251
[14:09:38.361] Evaluation Started ==>
[14:09:49.698] ==> valid iteration 25400: unet metrics: {'dc': 0.6514877577704491, 'jc': 0.5360827100868252, 'pre': 0.8111064015218831, 'hd': 5.373815466006928}, ynet metrics: {'dc': 0.5922371490566996, 'jc': 0.4817514530237288, 'pre': 0.8042479958037243, 'hd': 5.27096608009627}.
[14:09:49.700] Evaluation Finished!⏹️
[14:09:49.853] iteration 25401: total_loss: 0.551154, loss_sup: 0.049030, loss_mps: 0.166787, loss_cps: 0.335337
[14:09:50.006] iteration 25402: total_loss: 0.439656, loss_sup: 0.039583, loss_mps: 0.131424, loss_cps: 0.268648
[14:09:50.153] iteration 25403: total_loss: 0.439383, loss_sup: 0.090332, loss_mps: 0.126230, loss_cps: 0.222821
[14:09:50.298] iteration 25404: total_loss: 0.354438, loss_sup: 0.001602, loss_mps: 0.113563, loss_cps: 0.239273
[14:09:50.444] iteration 25405: total_loss: 0.688884, loss_sup: 0.118157, loss_mps: 0.174303, loss_cps: 0.396424
[14:09:50.590] iteration 25406: total_loss: 0.477039, loss_sup: 0.016341, loss_mps: 0.153706, loss_cps: 0.306993
[14:09:50.736] iteration 25407: total_loss: 0.414496, loss_sup: 0.025451, loss_mps: 0.137622, loss_cps: 0.251423
[14:09:50.885] iteration 25408: total_loss: 0.600425, loss_sup: 0.037904, loss_mps: 0.184294, loss_cps: 0.378226
[14:09:51.033] iteration 25409: total_loss: 0.465418, loss_sup: 0.048430, loss_mps: 0.138411, loss_cps: 0.278577
[14:09:51.178] iteration 25410: total_loss: 0.507819, loss_sup: 0.161747, loss_mps: 0.126009, loss_cps: 0.220063
[14:09:51.327] iteration 25411: total_loss: 0.296308, loss_sup: 0.020455, loss_mps: 0.100844, loss_cps: 0.175008
[14:09:51.473] iteration 25412: total_loss: 0.348499, loss_sup: 0.053672, loss_mps: 0.108669, loss_cps: 0.186158
[14:09:51.620] iteration 25413: total_loss: 0.342046, loss_sup: 0.014821, loss_mps: 0.110490, loss_cps: 0.216736
[14:09:51.766] iteration 25414: total_loss: 0.282736, loss_sup: 0.055801, loss_mps: 0.086601, loss_cps: 0.140335
[14:09:51.912] iteration 25415: total_loss: 0.489824, loss_sup: 0.111964, loss_mps: 0.129003, loss_cps: 0.248857
[14:09:52.060] iteration 25416: total_loss: 0.420423, loss_sup: 0.042567, loss_mps: 0.133462, loss_cps: 0.244394
[14:09:52.206] iteration 25417: total_loss: 0.483056, loss_sup: 0.095280, loss_mps: 0.129620, loss_cps: 0.258157
[14:09:52.352] iteration 25418: total_loss: 0.426393, loss_sup: 0.053154, loss_mps: 0.138178, loss_cps: 0.235061
[14:09:52.499] iteration 25419: total_loss: 0.466171, loss_sup: 0.119186, loss_mps: 0.124324, loss_cps: 0.222661
[14:09:52.645] iteration 25420: total_loss: 0.496239, loss_sup: 0.111398, loss_mps: 0.133555, loss_cps: 0.251287
[14:09:52.792] iteration 25421: total_loss: 0.432703, loss_sup: 0.031998, loss_mps: 0.132929, loss_cps: 0.267776
[14:09:52.940] iteration 25422: total_loss: 0.343008, loss_sup: 0.059666, loss_mps: 0.097783, loss_cps: 0.185558
[14:09:53.087] iteration 25423: total_loss: 0.472498, loss_sup: 0.045499, loss_mps: 0.143847, loss_cps: 0.283152
[14:09:53.233] iteration 25424: total_loss: 0.578512, loss_sup: 0.018019, loss_mps: 0.175921, loss_cps: 0.384572
[14:09:53.381] iteration 25425: total_loss: 0.241930, loss_sup: 0.005348, loss_mps: 0.088999, loss_cps: 0.147583
[14:09:53.527] iteration 25426: total_loss: 0.368363, loss_sup: 0.057209, loss_mps: 0.106144, loss_cps: 0.205011
[14:09:53.674] iteration 25427: total_loss: 0.244696, loss_sup: 0.008098, loss_mps: 0.088721, loss_cps: 0.147877
[14:09:53.825] iteration 25428: total_loss: 0.202955, loss_sup: 0.010427, loss_mps: 0.070533, loss_cps: 0.121995
[14:09:53.972] iteration 25429: total_loss: 0.266948, loss_sup: 0.054463, loss_mps: 0.077147, loss_cps: 0.135338
[14:09:54.118] iteration 25430: total_loss: 0.712006, loss_sup: 0.150471, loss_mps: 0.195882, loss_cps: 0.365653
[14:09:54.267] iteration 25431: total_loss: 0.381872, loss_sup: 0.022987, loss_mps: 0.120620, loss_cps: 0.238265
[14:09:54.414] iteration 25432: total_loss: 0.672188, loss_sup: 0.028324, loss_mps: 0.198913, loss_cps: 0.444951
[14:09:54.562] iteration 25433: total_loss: 0.241060, loss_sup: 0.019027, loss_mps: 0.080815, loss_cps: 0.141219
[14:09:54.709] iteration 25434: total_loss: 0.488265, loss_sup: 0.137416, loss_mps: 0.116068, loss_cps: 0.234782
[14:09:54.855] iteration 25435: total_loss: 0.364337, loss_sup: 0.048931, loss_mps: 0.111428, loss_cps: 0.203978
[14:09:55.001] iteration 25436: total_loss: 0.546271, loss_sup: 0.046671, loss_mps: 0.163233, loss_cps: 0.336367
[14:09:55.147] iteration 25437: total_loss: 0.267365, loss_sup: 0.024374, loss_mps: 0.086753, loss_cps: 0.156238
[14:09:55.293] iteration 25438: total_loss: 0.228161, loss_sup: 0.003923, loss_mps: 0.084590, loss_cps: 0.139647
[14:09:55.439] iteration 25439: total_loss: 0.253776, loss_sup: 0.001566, loss_mps: 0.092314, loss_cps: 0.159897
[14:09:55.586] iteration 25440: total_loss: 0.646385, loss_sup: 0.044380, loss_mps: 0.194986, loss_cps: 0.407019
[14:09:55.734] iteration 25441: total_loss: 0.355065, loss_sup: 0.082465, loss_mps: 0.091461, loss_cps: 0.181139
[14:09:55.881] iteration 25442: total_loss: 0.477974, loss_sup: 0.059034, loss_mps: 0.142940, loss_cps: 0.276000
[14:09:56.028] iteration 25443: total_loss: 0.726373, loss_sup: 0.157338, loss_mps: 0.182487, loss_cps: 0.386548
[14:09:56.174] iteration 25444: total_loss: 0.279131, loss_sup: 0.023408, loss_mps: 0.088378, loss_cps: 0.167345
[14:09:56.321] iteration 25445: total_loss: 0.518309, loss_sup: 0.092522, loss_mps: 0.136843, loss_cps: 0.288944
[14:09:56.468] iteration 25446: total_loss: 0.289186, loss_sup: 0.024438, loss_mps: 0.102421, loss_cps: 0.162326
[14:09:56.615] iteration 25447: total_loss: 0.453384, loss_sup: 0.115268, loss_mps: 0.114036, loss_cps: 0.224080
[14:09:56.761] iteration 25448: total_loss: 0.300607, loss_sup: 0.012891, loss_mps: 0.103997, loss_cps: 0.183718
[14:09:56.910] iteration 25449: total_loss: 0.397204, loss_sup: 0.096877, loss_mps: 0.107162, loss_cps: 0.193165
[14:09:57.057] iteration 25450: total_loss: 0.348706, loss_sup: 0.079130, loss_mps: 0.094249, loss_cps: 0.175326
[14:09:57.205] iteration 25451: total_loss: 0.268982, loss_sup: 0.015019, loss_mps: 0.086652, loss_cps: 0.167312
[14:09:57.354] iteration 25452: total_loss: 0.232710, loss_sup: 0.015363, loss_mps: 0.080876, loss_cps: 0.136471
[14:09:57.501] iteration 25453: total_loss: 0.198465, loss_sup: 0.003938, loss_mps: 0.072312, loss_cps: 0.122215
[14:09:57.652] iteration 25454: total_loss: 0.320110, loss_sup: 0.071429, loss_mps: 0.089213, loss_cps: 0.159469
[14:09:57.799] iteration 25455: total_loss: 0.474877, loss_sup: 0.006295, loss_mps: 0.152250, loss_cps: 0.316332
[14:09:57.948] iteration 25456: total_loss: 0.293845, loss_sup: 0.024246, loss_mps: 0.095082, loss_cps: 0.174517
[14:09:58.094] iteration 25457: total_loss: 0.346350, loss_sup: 0.005444, loss_mps: 0.118541, loss_cps: 0.222365
[14:09:58.240] iteration 25458: total_loss: 0.419315, loss_sup: 0.041796, loss_mps: 0.127194, loss_cps: 0.250325
[14:09:58.387] iteration 25459: total_loss: 0.310263, loss_sup: 0.015131, loss_mps: 0.098988, loss_cps: 0.196144
[14:09:58.534] iteration 25460: total_loss: 0.247924, loss_sup: 0.001722, loss_mps: 0.085562, loss_cps: 0.160640
[14:09:58.680] iteration 25461: total_loss: 0.250928, loss_sup: 0.037491, loss_mps: 0.081125, loss_cps: 0.132312
[14:09:58.827] iteration 25462: total_loss: 0.525174, loss_sup: 0.080383, loss_mps: 0.151940, loss_cps: 0.292851
[14:09:58.974] iteration 25463: total_loss: 0.238212, loss_sup: 0.034555, loss_mps: 0.077047, loss_cps: 0.126610
[14:09:59.120] iteration 25464: total_loss: 0.512022, loss_sup: 0.066812, loss_mps: 0.155571, loss_cps: 0.289639
[14:09:59.266] iteration 25465: total_loss: 0.381642, loss_sup: 0.020548, loss_mps: 0.117234, loss_cps: 0.243860
[14:09:59.412] iteration 25466: total_loss: 0.613420, loss_sup: 0.070974, loss_mps: 0.171926, loss_cps: 0.370521
[14:09:59.559] iteration 25467: total_loss: 0.363942, loss_sup: 0.087820, loss_mps: 0.097958, loss_cps: 0.178165
[14:09:59.706] iteration 25468: total_loss: 0.310966, loss_sup: 0.003652, loss_mps: 0.107879, loss_cps: 0.199434
[14:09:59.852] iteration 25469: total_loss: 0.316621, loss_sup: 0.010391, loss_mps: 0.108179, loss_cps: 0.198051
[14:09:59.998] iteration 25470: total_loss: 0.417049, loss_sup: 0.088483, loss_mps: 0.112184, loss_cps: 0.216381
[14:10:00.146] iteration 25471: total_loss: 0.226504, loss_sup: 0.005610, loss_mps: 0.079164, loss_cps: 0.141729
[14:10:00.292] iteration 25472: total_loss: 0.429155, loss_sup: 0.039625, loss_mps: 0.129126, loss_cps: 0.260405
[14:10:00.438] iteration 25473: total_loss: 0.277388, loss_sup: 0.009800, loss_mps: 0.094540, loss_cps: 0.173048
[14:10:00.585] iteration 25474: total_loss: 0.348548, loss_sup: 0.064198, loss_mps: 0.098455, loss_cps: 0.185895
[14:10:00.731] iteration 25475: total_loss: 0.807080, loss_sup: 0.045532, loss_mps: 0.242266, loss_cps: 0.519281
[14:10:00.877] iteration 25476: total_loss: 0.481244, loss_sup: 0.029648, loss_mps: 0.155159, loss_cps: 0.296436
[14:10:01.022] iteration 25477: total_loss: 0.486264, loss_sup: 0.253861, loss_mps: 0.088787, loss_cps: 0.143615
[14:10:01.168] iteration 25478: total_loss: 0.276462, loss_sup: 0.014558, loss_mps: 0.093646, loss_cps: 0.168258
[14:10:01.314] iteration 25479: total_loss: 0.274835, loss_sup: 0.090499, loss_mps: 0.072059, loss_cps: 0.112277
[14:10:01.460] iteration 25480: total_loss: 0.295384, loss_sup: 0.001734, loss_mps: 0.098651, loss_cps: 0.194999
[14:10:01.606] iteration 25481: total_loss: 0.359893, loss_sup: 0.002875, loss_mps: 0.120783, loss_cps: 0.236235
[14:10:01.752] iteration 25482: total_loss: 0.379184, loss_sup: 0.058232, loss_mps: 0.110246, loss_cps: 0.210706
[14:10:01.898] iteration 25483: total_loss: 0.587103, loss_sup: 0.224774, loss_mps: 0.130542, loss_cps: 0.231787
[14:10:02.045] iteration 25484: total_loss: 0.267251, loss_sup: 0.005853, loss_mps: 0.094738, loss_cps: 0.166660
[14:10:02.191] iteration 25485: total_loss: 0.354437, loss_sup: 0.079997, loss_mps: 0.103783, loss_cps: 0.170657
[14:10:02.337] iteration 25486: total_loss: 0.305023, loss_sup: 0.019625, loss_mps: 0.099769, loss_cps: 0.185629
[14:10:02.485] iteration 25487: total_loss: 0.246594, loss_sup: 0.005496, loss_mps: 0.085214, loss_cps: 0.155885
[14:10:02.631] iteration 25488: total_loss: 0.366206, loss_sup: 0.027087, loss_mps: 0.115500, loss_cps: 0.223619
[14:10:02.777] iteration 25489: total_loss: 0.301538, loss_sup: 0.047793, loss_mps: 0.090239, loss_cps: 0.163507
[14:10:02.923] iteration 25490: total_loss: 0.498877, loss_sup: 0.039459, loss_mps: 0.140640, loss_cps: 0.318779
[14:10:03.068] iteration 25491: total_loss: 0.364383, loss_sup: 0.109559, loss_mps: 0.089422, loss_cps: 0.165401
[14:10:03.215] iteration 25492: total_loss: 0.367852, loss_sup: 0.006952, loss_mps: 0.119948, loss_cps: 0.240951
[14:10:03.360] iteration 25493: total_loss: 0.998418, loss_sup: 0.028545, loss_mps: 0.295066, loss_cps: 0.674808
[14:10:03.506] iteration 25494: total_loss: 0.888874, loss_sup: 0.284016, loss_mps: 0.177440, loss_cps: 0.427419
[14:10:03.652] iteration 25495: total_loss: 0.291214, loss_sup: 0.118367, loss_mps: 0.060742, loss_cps: 0.112105
[14:10:03.798] iteration 25496: total_loss: 0.218186, loss_sup: 0.002476, loss_mps: 0.081455, loss_cps: 0.134255
[14:10:03.944] iteration 25497: total_loss: 0.564883, loss_sup: 0.075370, loss_mps: 0.156963, loss_cps: 0.332550
[14:10:04.005] iteration 25498: total_loss: 0.369745, loss_sup: 0.006782, loss_mps: 0.128882, loss_cps: 0.234080
[14:10:05.243] iteration 25499: total_loss: 0.587218, loss_sup: 0.084347, loss_mps: 0.149051, loss_cps: 0.353821
[14:10:05.393] iteration 25500: total_loss: 0.319566, loss_sup: 0.029220, loss_mps: 0.102702, loss_cps: 0.187644
[14:10:05.393] Evaluation Started ==>
[14:10:16.770] ==> valid iteration 25500: unet metrics: {'dc': 0.6597866705524122, 'jc': 0.5450942798952014, 'pre': 0.7893935400487263, 'hd': 5.4667708334760095}, ynet metrics: {'dc': 0.5985987177704646, 'jc': 0.4872454815138846, 'pre': 0.8097783567760485, 'hd': 5.355639279985904}.
[14:10:16.772] Evaluation Finished!⏹️
[14:10:16.922] iteration 25501: total_loss: 0.242599, loss_sup: 0.000721, loss_mps: 0.084772, loss_cps: 0.157106
[14:10:17.071] iteration 25502: total_loss: 0.630896, loss_sup: 0.035973, loss_mps: 0.180028, loss_cps: 0.414895
[14:10:17.218] iteration 25503: total_loss: 0.337740, loss_sup: 0.046918, loss_mps: 0.104885, loss_cps: 0.185938
[14:10:17.364] iteration 25504: total_loss: 0.746057, loss_sup: 0.075524, loss_mps: 0.203254, loss_cps: 0.467279
[14:10:17.509] iteration 25505: total_loss: 0.630988, loss_sup: 0.095511, loss_mps: 0.173896, loss_cps: 0.361581
[14:10:17.656] iteration 25506: total_loss: 0.306798, loss_sup: 0.021323, loss_mps: 0.100876, loss_cps: 0.184600
[14:10:17.803] iteration 25507: total_loss: 0.459243, loss_sup: 0.164925, loss_mps: 0.102402, loss_cps: 0.191916
[14:10:17.949] iteration 25508: total_loss: 0.157438, loss_sup: 0.010785, loss_mps: 0.057045, loss_cps: 0.089608
[14:10:18.095] iteration 25509: total_loss: 0.317700, loss_sup: 0.023174, loss_mps: 0.098812, loss_cps: 0.195715
[14:10:18.240] iteration 25510: total_loss: 0.364611, loss_sup: 0.030282, loss_mps: 0.114092, loss_cps: 0.220237
[14:10:18.386] iteration 25511: total_loss: 0.516100, loss_sup: 0.052053, loss_mps: 0.158958, loss_cps: 0.305089
[14:10:18.531] iteration 25512: total_loss: 0.756022, loss_sup: 0.083145, loss_mps: 0.203942, loss_cps: 0.468935
[14:10:18.677] iteration 25513: total_loss: 0.440714, loss_sup: 0.057953, loss_mps: 0.133913, loss_cps: 0.248848
[14:10:18.822] iteration 25514: total_loss: 0.255756, loss_sup: 0.010961, loss_mps: 0.088665, loss_cps: 0.156130
[14:10:18.968] iteration 25515: total_loss: 0.607871, loss_sup: 0.039103, loss_mps: 0.176556, loss_cps: 0.392211
[14:10:19.114] iteration 25516: total_loss: 0.328206, loss_sup: 0.008831, loss_mps: 0.108284, loss_cps: 0.211092
[14:10:19.263] iteration 25517: total_loss: 0.629674, loss_sup: 0.169142, loss_mps: 0.152050, loss_cps: 0.308481
[14:10:19.408] iteration 25518: total_loss: 0.443553, loss_sup: 0.025370, loss_mps: 0.153089, loss_cps: 0.265094
[14:10:19.553] iteration 25519: total_loss: 0.295462, loss_sup: 0.040674, loss_mps: 0.090251, loss_cps: 0.164538
[14:10:19.699] iteration 25520: total_loss: 0.410717, loss_sup: 0.099691, loss_mps: 0.111460, loss_cps: 0.199566
[14:10:19.845] iteration 25521: total_loss: 0.625889, loss_sup: 0.055740, loss_mps: 0.186645, loss_cps: 0.383503
[14:10:19.991] iteration 25522: total_loss: 0.411528, loss_sup: 0.045647, loss_mps: 0.124356, loss_cps: 0.241525
[14:10:20.137] iteration 25523: total_loss: 0.357125, loss_sup: 0.002979, loss_mps: 0.116612, loss_cps: 0.237534
[14:10:20.283] iteration 25524: total_loss: 0.365577, loss_sup: 0.029812, loss_mps: 0.130021, loss_cps: 0.205744
[14:10:20.430] iteration 25525: total_loss: 0.278657, loss_sup: 0.030853, loss_mps: 0.088308, loss_cps: 0.159495
[14:10:20.575] iteration 25526: total_loss: 0.509411, loss_sup: 0.068393, loss_mps: 0.146581, loss_cps: 0.294437
[14:10:20.721] iteration 25527: total_loss: 0.631721, loss_sup: 0.014060, loss_mps: 0.196709, loss_cps: 0.420952
[14:10:20.867] iteration 25528: total_loss: 0.443662, loss_sup: 0.062542, loss_mps: 0.135953, loss_cps: 0.245167
[14:10:21.013] iteration 25529: total_loss: 0.434444, loss_sup: 0.006006, loss_mps: 0.143959, loss_cps: 0.284479
[14:10:21.159] iteration 25530: total_loss: 0.252518, loss_sup: 0.017279, loss_mps: 0.084842, loss_cps: 0.150397
[14:10:21.306] iteration 25531: total_loss: 0.390124, loss_sup: 0.021566, loss_mps: 0.120159, loss_cps: 0.248398
[14:10:21.451] iteration 25532: total_loss: 0.312132, loss_sup: 0.001509, loss_mps: 0.104021, loss_cps: 0.206602
[14:10:21.597] iteration 25533: total_loss: 0.432354, loss_sup: 0.048265, loss_mps: 0.133411, loss_cps: 0.250678
[14:10:21.743] iteration 25534: total_loss: 0.511953, loss_sup: 0.177082, loss_mps: 0.118533, loss_cps: 0.216338
[14:10:21.894] iteration 25535: total_loss: 0.639859, loss_sup: 0.116212, loss_mps: 0.171230, loss_cps: 0.352417
[14:10:22.039] iteration 25536: total_loss: 0.420026, loss_sup: 0.036402, loss_mps: 0.132623, loss_cps: 0.251001
[14:10:22.185] iteration 25537: total_loss: 0.335584, loss_sup: 0.050194, loss_mps: 0.098941, loss_cps: 0.186448
[14:10:22.330] iteration 25538: total_loss: 0.389305, loss_sup: 0.006327, loss_mps: 0.133672, loss_cps: 0.249306
[14:10:22.496] iteration 25539: total_loss: 0.338791, loss_sup: 0.149199, loss_mps: 0.075102, loss_cps: 0.114490
[14:10:22.641] iteration 25540: total_loss: 0.669411, loss_sup: 0.073794, loss_mps: 0.188111, loss_cps: 0.407507
[14:10:22.787] iteration 25541: total_loss: 0.675153, loss_sup: 0.089579, loss_mps: 0.193293, loss_cps: 0.392282
[14:10:22.932] iteration 25542: total_loss: 0.186532, loss_sup: 0.003746, loss_mps: 0.071473, loss_cps: 0.111314
[14:10:23.078] iteration 25543: total_loss: 0.322226, loss_sup: 0.053433, loss_mps: 0.103689, loss_cps: 0.165104
[14:10:23.223] iteration 25544: total_loss: 0.321601, loss_sup: 0.017655, loss_mps: 0.105256, loss_cps: 0.198689
[14:10:23.369] iteration 25545: total_loss: 0.331769, loss_sup: 0.025966, loss_mps: 0.102847, loss_cps: 0.202956
[14:10:23.514] iteration 25546: total_loss: 0.284763, loss_sup: 0.021636, loss_mps: 0.094315, loss_cps: 0.168813
[14:10:23.660] iteration 25547: total_loss: 0.314777, loss_sup: 0.006039, loss_mps: 0.108838, loss_cps: 0.199900
[14:10:23.806] iteration 25548: total_loss: 0.266340, loss_sup: 0.023910, loss_mps: 0.094372, loss_cps: 0.148058
[14:10:23.951] iteration 25549: total_loss: 0.330168, loss_sup: 0.038840, loss_mps: 0.106424, loss_cps: 0.184904
[14:10:24.097] iteration 25550: total_loss: 0.530438, loss_sup: 0.080715, loss_mps: 0.151741, loss_cps: 0.297982
[14:10:24.244] iteration 25551: total_loss: 0.505064, loss_sup: 0.093660, loss_mps: 0.139624, loss_cps: 0.271780
[14:10:24.389] iteration 25552: total_loss: 0.454925, loss_sup: 0.046477, loss_mps: 0.133545, loss_cps: 0.274903
[14:10:24.535] iteration 25553: total_loss: 0.328124, loss_sup: 0.037339, loss_mps: 0.101252, loss_cps: 0.189532
[14:10:24.680] iteration 25554: total_loss: 0.433678, loss_sup: 0.055738, loss_mps: 0.129714, loss_cps: 0.248226
[14:10:24.826] iteration 25555: total_loss: 0.435994, loss_sup: 0.021198, loss_mps: 0.136807, loss_cps: 0.277989
[14:10:24.971] iteration 25556: total_loss: 0.439152, loss_sup: 0.108425, loss_mps: 0.120403, loss_cps: 0.210324
[14:10:25.117] iteration 25557: total_loss: 0.455913, loss_sup: 0.046044, loss_mps: 0.143396, loss_cps: 0.266472
[14:10:25.265] iteration 25558: total_loss: 0.244046, loss_sup: 0.009030, loss_mps: 0.085524, loss_cps: 0.149492
[14:10:25.411] iteration 25559: total_loss: 0.325419, loss_sup: 0.106182, loss_mps: 0.077561, loss_cps: 0.141676
[14:10:25.556] iteration 25560: total_loss: 0.372853, loss_sup: 0.044253, loss_mps: 0.114955, loss_cps: 0.213645
[14:10:25.702] iteration 25561: total_loss: 0.229599, loss_sup: 0.022190, loss_mps: 0.078098, loss_cps: 0.129310
[14:10:25.848] iteration 25562: total_loss: 0.316494, loss_sup: 0.023864, loss_mps: 0.102238, loss_cps: 0.190392
[14:10:25.995] iteration 25563: total_loss: 0.308714, loss_sup: 0.009859, loss_mps: 0.108443, loss_cps: 0.190412
[14:10:26.141] iteration 25564: total_loss: 0.508067, loss_sup: 0.020879, loss_mps: 0.159361, loss_cps: 0.327826
[14:10:26.286] iteration 25565: total_loss: 0.405523, loss_sup: 0.011640, loss_mps: 0.130491, loss_cps: 0.263392
[14:10:26.432] iteration 25566: total_loss: 0.294457, loss_sup: 0.007512, loss_mps: 0.097972, loss_cps: 0.188973
[14:10:26.580] iteration 25567: total_loss: 0.464168, loss_sup: 0.048543, loss_mps: 0.145342, loss_cps: 0.270282
[14:10:26.725] iteration 25568: total_loss: 1.240896, loss_sup: 0.131251, loss_mps: 0.330624, loss_cps: 0.779020
[14:10:26.872] iteration 25569: total_loss: 0.316550, loss_sup: 0.013556, loss_mps: 0.098514, loss_cps: 0.204479
[14:10:27.018] iteration 25570: total_loss: 0.706717, loss_sup: 0.037156, loss_mps: 0.222044, loss_cps: 0.447518
[14:10:27.164] iteration 25571: total_loss: 2.319173, loss_sup: 0.619968, loss_mps: 0.498238, loss_cps: 1.200967
[14:10:27.309] iteration 25572: total_loss: 0.388855, loss_sup: 0.053679, loss_mps: 0.122200, loss_cps: 0.212976
[14:10:27.456] iteration 25573: total_loss: 0.248997, loss_sup: 0.025673, loss_mps: 0.080543, loss_cps: 0.142782
[14:10:27.602] iteration 25574: total_loss: 0.374540, loss_sup: 0.045489, loss_mps: 0.117704, loss_cps: 0.211347
[14:10:27.747] iteration 25575: total_loss: 0.984698, loss_sup: 0.385643, loss_mps: 0.186587, loss_cps: 0.412468
[14:10:27.893] iteration 25576: total_loss: 0.769783, loss_sup: 0.076369, loss_mps: 0.213584, loss_cps: 0.479830
[14:10:28.039] iteration 25577: total_loss: 0.858430, loss_sup: 0.124928, loss_mps: 0.220980, loss_cps: 0.512522
[14:10:28.184] iteration 25578: total_loss: 0.623578, loss_sup: 0.105929, loss_mps: 0.168650, loss_cps: 0.348998
[14:10:28.333] iteration 25579: total_loss: 0.759277, loss_sup: 0.046881, loss_mps: 0.226858, loss_cps: 0.485538
[14:10:28.479] iteration 25580: total_loss: 0.530030, loss_sup: 0.114465, loss_mps: 0.141061, loss_cps: 0.274503
[14:10:28.625] iteration 25581: total_loss: 0.373927, loss_sup: 0.077119, loss_mps: 0.104987, loss_cps: 0.191821
[14:10:28.773] iteration 25582: total_loss: 0.261510, loss_sup: 0.015292, loss_mps: 0.084172, loss_cps: 0.162046
[14:10:28.919] iteration 25583: total_loss: 0.472417, loss_sup: 0.014358, loss_mps: 0.160827, loss_cps: 0.297233
[14:10:29.065] iteration 25584: total_loss: 0.200060, loss_sup: 0.034761, loss_mps: 0.062117, loss_cps: 0.103181
[14:10:29.211] iteration 25585: total_loss: 0.544376, loss_sup: 0.063783, loss_mps: 0.157913, loss_cps: 0.322680
[14:10:29.358] iteration 25586: total_loss: 0.339491, loss_sup: 0.033300, loss_mps: 0.112446, loss_cps: 0.193745
[14:10:29.503] iteration 25587: total_loss: 0.447940, loss_sup: 0.067951, loss_mps: 0.128977, loss_cps: 0.251012
[14:10:29.649] iteration 25588: total_loss: 0.463207, loss_sup: 0.036136, loss_mps: 0.135697, loss_cps: 0.291373
[14:10:29.795] iteration 25589: total_loss: 0.412636, loss_sup: 0.020411, loss_mps: 0.123118, loss_cps: 0.269107
[14:10:29.942] iteration 25590: total_loss: 0.286171, loss_sup: 0.008317, loss_mps: 0.100357, loss_cps: 0.177497
[14:10:30.088] iteration 25591: total_loss: 0.384137, loss_sup: 0.041931, loss_mps: 0.117961, loss_cps: 0.224246
[14:10:30.234] iteration 25592: total_loss: 0.338878, loss_sup: 0.066974, loss_mps: 0.100097, loss_cps: 0.171807
[14:10:30.380] iteration 25593: total_loss: 0.191157, loss_sup: 0.003643, loss_mps: 0.072709, loss_cps: 0.114806
[14:10:30.526] iteration 25594: total_loss: 1.102021, loss_sup: 0.361768, loss_mps: 0.235760, loss_cps: 0.504493
[14:10:30.674] iteration 25595: total_loss: 0.431003, loss_sup: 0.039661, loss_mps: 0.136058, loss_cps: 0.255284
[14:10:30.820] iteration 25596: total_loss: 0.598037, loss_sup: 0.021324, loss_mps: 0.190959, loss_cps: 0.385754
[14:10:30.965] iteration 25597: total_loss: 0.537655, loss_sup: 0.039075, loss_mps: 0.161272, loss_cps: 0.337307
[14:10:31.111] iteration 25598: total_loss: 0.556343, loss_sup: 0.129326, loss_mps: 0.144522, loss_cps: 0.282495
[14:10:31.258] iteration 25599: total_loss: 0.497530, loss_sup: 0.029102, loss_mps: 0.147277, loss_cps: 0.321152
[14:10:31.408] iteration 25600: total_loss: 0.664526, loss_sup: 0.001226, loss_mps: 0.199647, loss_cps: 0.463653
[14:10:31.408] Evaluation Started ==>
[14:10:42.820] ==> valid iteration 25600: unet metrics: {'dc': 0.6294988458546932, 'jc': 0.5139315150958854, 'pre': 0.7906853676212481, 'hd': 5.504421486171381}, ynet metrics: {'dc': 0.6238577070941937, 'jc': 0.5125954224962018, 'pre': 0.7868788797400479, 'hd': 5.3696124906502}.
[14:10:42.823] Evaluation Finished!⏹️
[14:10:42.976] iteration 25601: total_loss: 0.313390, loss_sup: 0.067217, loss_mps: 0.090828, loss_cps: 0.155344
[14:10:43.124] iteration 25602: total_loss: 0.558674, loss_sup: 0.044633, loss_mps: 0.166180, loss_cps: 0.347861
[14:10:43.270] iteration 25603: total_loss: 0.326278, loss_sup: 0.046049, loss_mps: 0.099007, loss_cps: 0.181222
[14:10:43.417] iteration 25604: total_loss: 0.647059, loss_sup: 0.065626, loss_mps: 0.182724, loss_cps: 0.398709
[14:10:43.563] iteration 25605: total_loss: 0.578232, loss_sup: 0.028969, loss_mps: 0.165105, loss_cps: 0.384158
[14:10:43.708] iteration 25606: total_loss: 0.569736, loss_sup: 0.182119, loss_mps: 0.131566, loss_cps: 0.256052
[14:10:43.854] iteration 25607: total_loss: 0.863474, loss_sup: 0.093304, loss_mps: 0.243151, loss_cps: 0.527018
[14:10:44.002] iteration 25608: total_loss: 0.444972, loss_sup: 0.097335, loss_mps: 0.131919, loss_cps: 0.215717
[14:10:44.153] iteration 25609: total_loss: 0.233539, loss_sup: 0.005017, loss_mps: 0.082901, loss_cps: 0.145621
[14:10:44.299] iteration 25610: total_loss: 0.398341, loss_sup: 0.087600, loss_mps: 0.109976, loss_cps: 0.200765
[14:10:44.448] iteration 25611: total_loss: 0.261810, loss_sup: 0.042365, loss_mps: 0.083675, loss_cps: 0.135770
[14:10:44.594] iteration 25612: total_loss: 0.655008, loss_sup: 0.113519, loss_mps: 0.198798, loss_cps: 0.342690
[14:10:44.742] iteration 25613: total_loss: 0.344008, loss_sup: 0.085931, loss_mps: 0.094190, loss_cps: 0.163888
[14:10:44.892] iteration 25614: total_loss: 0.404628, loss_sup: 0.073860, loss_mps: 0.116038, loss_cps: 0.214729
[14:10:45.038] iteration 25615: total_loss: 0.598043, loss_sup: 0.119904, loss_mps: 0.162863, loss_cps: 0.315276
[14:10:45.183] iteration 25616: total_loss: 0.415293, loss_sup: 0.146144, loss_mps: 0.098056, loss_cps: 0.171093
[14:10:45.330] iteration 25617: total_loss: 0.301284, loss_sup: 0.028132, loss_mps: 0.098465, loss_cps: 0.174687
[14:10:45.476] iteration 25618: total_loss: 0.313373, loss_sup: 0.037087, loss_mps: 0.105333, loss_cps: 0.170953
[14:10:45.623] iteration 25619: total_loss: 0.316153, loss_sup: 0.015718, loss_mps: 0.112271, loss_cps: 0.188164
[14:10:45.769] iteration 25620: total_loss: 0.227067, loss_sup: 0.011636, loss_mps: 0.084670, loss_cps: 0.130761
[14:10:45.916] iteration 25621: total_loss: 0.636324, loss_sup: 0.102523, loss_mps: 0.185271, loss_cps: 0.348530
[14:10:46.062] iteration 25622: total_loss: 0.309574, loss_sup: 0.039134, loss_mps: 0.098413, loss_cps: 0.172027
[14:10:46.209] iteration 25623: total_loss: 0.379215, loss_sup: 0.044340, loss_mps: 0.119510, loss_cps: 0.215366
[14:10:46.355] iteration 25624: total_loss: 0.246633, loss_sup: 0.009130, loss_mps: 0.092403, loss_cps: 0.145100
[14:10:46.501] iteration 25625: total_loss: 0.419232, loss_sup: 0.032378, loss_mps: 0.129653, loss_cps: 0.257200
[14:10:46.647] iteration 25626: total_loss: 0.438092, loss_sup: 0.083838, loss_mps: 0.133284, loss_cps: 0.220971
[14:10:46.794] iteration 25627: total_loss: 0.565389, loss_sup: 0.039575, loss_mps: 0.177895, loss_cps: 0.347919
[14:10:46.940] iteration 25628: total_loss: 0.652511, loss_sup: 0.098223, loss_mps: 0.184686, loss_cps: 0.369602
[14:10:47.086] iteration 25629: total_loss: 0.692827, loss_sup: 0.109558, loss_mps: 0.182305, loss_cps: 0.400963
[14:10:47.231] iteration 25630: total_loss: 0.242101, loss_sup: 0.073312, loss_mps: 0.068422, loss_cps: 0.100367
[14:10:47.377] iteration 25631: total_loss: 0.432525, loss_sup: 0.063771, loss_mps: 0.119679, loss_cps: 0.249075
[14:10:47.523] iteration 25632: total_loss: 0.267950, loss_sup: 0.008955, loss_mps: 0.091810, loss_cps: 0.167185
[14:10:47.669] iteration 25633: total_loss: 0.329500, loss_sup: 0.014738, loss_mps: 0.116508, loss_cps: 0.198254
[14:10:47.820] iteration 25634: total_loss: 0.290058, loss_sup: 0.034952, loss_mps: 0.093155, loss_cps: 0.161951
[14:10:47.968] iteration 25635: total_loss: 0.519610, loss_sup: 0.145284, loss_mps: 0.131182, loss_cps: 0.243144
[14:10:48.114] iteration 25636: total_loss: 0.581152, loss_sup: 0.071540, loss_mps: 0.176322, loss_cps: 0.333291
[14:10:48.259] iteration 25637: total_loss: 0.433828, loss_sup: 0.216863, loss_mps: 0.081202, loss_cps: 0.135763
[14:10:48.404] iteration 25638: total_loss: 0.478029, loss_sup: 0.132407, loss_mps: 0.121137, loss_cps: 0.224485
[14:10:48.550] iteration 25639: total_loss: 0.396761, loss_sup: 0.018898, loss_mps: 0.131264, loss_cps: 0.246598
[14:10:48.696] iteration 25640: total_loss: 0.399981, loss_sup: 0.052791, loss_mps: 0.122500, loss_cps: 0.224689
[14:10:48.842] iteration 25641: total_loss: 0.556421, loss_sup: 0.023126, loss_mps: 0.182908, loss_cps: 0.350387
[14:10:48.988] iteration 25642: total_loss: 0.440568, loss_sup: 0.081362, loss_mps: 0.117660, loss_cps: 0.241547
[14:10:49.133] iteration 25643: total_loss: 0.222250, loss_sup: 0.008142, loss_mps: 0.084662, loss_cps: 0.129446
[14:10:49.279] iteration 25644: total_loss: 0.455795, loss_sup: 0.108438, loss_mps: 0.122259, loss_cps: 0.225098
[14:10:49.424] iteration 25645: total_loss: 0.504995, loss_sup: 0.246188, loss_mps: 0.101375, loss_cps: 0.157432
[14:10:49.570] iteration 25646: total_loss: 0.291780, loss_sup: 0.009471, loss_mps: 0.105758, loss_cps: 0.176551
[14:10:49.716] iteration 25647: total_loss: 0.235211, loss_sup: 0.006509, loss_mps: 0.083355, loss_cps: 0.145348
[14:10:49.861] iteration 25648: total_loss: 0.454014, loss_sup: 0.156075, loss_mps: 0.114128, loss_cps: 0.183811
[14:10:50.008] iteration 25649: total_loss: 0.446975, loss_sup: 0.150066, loss_mps: 0.103759, loss_cps: 0.193150
[14:10:50.154] iteration 25650: total_loss: 0.413215, loss_sup: 0.013637, loss_mps: 0.139355, loss_cps: 0.260222
[14:10:50.300] iteration 25651: total_loss: 0.574940, loss_sup: 0.108744, loss_mps: 0.153958, loss_cps: 0.312237
[14:10:50.445] iteration 25652: total_loss: 0.357087, loss_sup: 0.030292, loss_mps: 0.113415, loss_cps: 0.213380
[14:10:50.592] iteration 25653: total_loss: 0.368350, loss_sup: 0.028767, loss_mps: 0.117041, loss_cps: 0.222542
[14:10:50.737] iteration 25654: total_loss: 0.788578, loss_sup: 0.013166, loss_mps: 0.241225, loss_cps: 0.534187
[14:10:50.883] iteration 25655: total_loss: 0.511524, loss_sup: 0.008321, loss_mps: 0.163879, loss_cps: 0.339324
[14:10:51.029] iteration 25656: total_loss: 0.327574, loss_sup: 0.010452, loss_mps: 0.114169, loss_cps: 0.202953
[14:10:51.174] iteration 25657: total_loss: 0.564389, loss_sup: 0.075845, loss_mps: 0.170767, loss_cps: 0.317776
[14:10:51.320] iteration 25658: total_loss: 0.572020, loss_sup: 0.009554, loss_mps: 0.181516, loss_cps: 0.380950
[14:10:51.466] iteration 25659: total_loss: 0.471784, loss_sup: 0.042419, loss_mps: 0.151988, loss_cps: 0.277377
[14:10:51.611] iteration 25660: total_loss: 0.500375, loss_sup: 0.043028, loss_mps: 0.151006, loss_cps: 0.306341
[14:10:51.757] iteration 25661: total_loss: 0.836751, loss_sup: 0.063152, loss_mps: 0.247445, loss_cps: 0.526153
[14:10:51.903] iteration 25662: total_loss: 0.419238, loss_sup: 0.093063, loss_mps: 0.117756, loss_cps: 0.208419
[14:10:52.048] iteration 25663: total_loss: 0.448655, loss_sup: 0.023646, loss_mps: 0.147617, loss_cps: 0.277393
[14:10:52.194] iteration 25664: total_loss: 0.339332, loss_sup: 0.025941, loss_mps: 0.118243, loss_cps: 0.195147
[14:10:52.341] iteration 25665: total_loss: 0.303659, loss_sup: 0.023034, loss_mps: 0.104021, loss_cps: 0.176604
[14:10:52.486] iteration 25666: total_loss: 0.418272, loss_sup: 0.128207, loss_mps: 0.107996, loss_cps: 0.182069
[14:10:52.632] iteration 25667: total_loss: 0.286070, loss_sup: 0.038787, loss_mps: 0.094464, loss_cps: 0.152819
[14:10:52.782] iteration 25668: total_loss: 0.615364, loss_sup: 0.046299, loss_mps: 0.181784, loss_cps: 0.387281
[14:10:52.928] iteration 25669: total_loss: 0.474935, loss_sup: 0.089168, loss_mps: 0.131715, loss_cps: 0.254052
[14:10:53.074] iteration 25670: total_loss: 0.278729, loss_sup: 0.017813, loss_mps: 0.089965, loss_cps: 0.170950
[14:10:53.220] iteration 25671: total_loss: 0.336241, loss_sup: 0.025129, loss_mps: 0.102771, loss_cps: 0.208340
[14:10:53.365] iteration 25672: total_loss: 0.578634, loss_sup: 0.066155, loss_mps: 0.167490, loss_cps: 0.344989
[14:10:53.512] iteration 25673: total_loss: 0.262814, loss_sup: 0.027763, loss_mps: 0.082554, loss_cps: 0.152496
[14:10:53.658] iteration 25674: total_loss: 0.239965, loss_sup: 0.011294, loss_mps: 0.083579, loss_cps: 0.145093
[14:10:53.807] iteration 25675: total_loss: 0.464158, loss_sup: 0.091242, loss_mps: 0.119214, loss_cps: 0.253702
[14:10:53.952] iteration 25676: total_loss: 0.584877, loss_sup: 0.116745, loss_mps: 0.167354, loss_cps: 0.300778
[14:10:54.098] iteration 25677: total_loss: 0.139774, loss_sup: 0.003797, loss_mps: 0.052313, loss_cps: 0.083664
[14:10:54.243] iteration 25678: total_loss: 0.267914, loss_sup: 0.007069, loss_mps: 0.092663, loss_cps: 0.168183
[14:10:54.389] iteration 25679: total_loss: 0.211246, loss_sup: 0.016874, loss_mps: 0.074922, loss_cps: 0.119450
[14:10:54.535] iteration 25680: total_loss: 0.360574, loss_sup: 0.004928, loss_mps: 0.118622, loss_cps: 0.237024
[14:10:54.680] iteration 25681: total_loss: 0.474683, loss_sup: 0.131364, loss_mps: 0.127479, loss_cps: 0.215840
[14:10:54.826] iteration 25682: total_loss: 0.243409, loss_sup: 0.004974, loss_mps: 0.088452, loss_cps: 0.149983
[14:10:54.972] iteration 25683: total_loss: 0.842699, loss_sup: 0.088192, loss_mps: 0.233715, loss_cps: 0.520792
[14:10:55.118] iteration 25684: total_loss: 0.334854, loss_sup: 0.008931, loss_mps: 0.128642, loss_cps: 0.197281
[14:10:55.264] iteration 25685: total_loss: 0.443879, loss_sup: 0.056544, loss_mps: 0.136828, loss_cps: 0.250507
[14:10:55.411] iteration 25686: total_loss: 0.519128, loss_sup: 0.139049, loss_mps: 0.128476, loss_cps: 0.251603
[14:10:55.557] iteration 25687: total_loss: 0.559956, loss_sup: 0.160984, loss_mps: 0.138393, loss_cps: 0.260579
[14:10:55.702] iteration 25688: total_loss: 0.315028, loss_sup: 0.007874, loss_mps: 0.113227, loss_cps: 0.193926
[14:10:55.848] iteration 25689: total_loss: 0.229265, loss_sup: 0.023051, loss_mps: 0.082300, loss_cps: 0.123914
[14:10:55.994] iteration 25690: total_loss: 0.274504, loss_sup: 0.052179, loss_mps: 0.080302, loss_cps: 0.142023
[14:10:56.141] iteration 25691: total_loss: 0.293666, loss_sup: 0.057580, loss_mps: 0.085175, loss_cps: 0.150911
[14:10:56.287] iteration 25692: total_loss: 0.294202, loss_sup: 0.003971, loss_mps: 0.105049, loss_cps: 0.185182
[14:10:56.434] iteration 25693: total_loss: 0.225195, loss_sup: 0.025974, loss_mps: 0.078129, loss_cps: 0.121093
[14:10:56.587] iteration 25694: total_loss: 0.303388, loss_sup: 0.117430, loss_mps: 0.069228, loss_cps: 0.116731
[14:10:56.734] iteration 25695: total_loss: 0.232169, loss_sup: 0.003869, loss_mps: 0.084543, loss_cps: 0.143756
[14:10:56.883] iteration 25696: total_loss: 0.264410, loss_sup: 0.018728, loss_mps: 0.091097, loss_cps: 0.154585
[14:10:57.029] iteration 25697: total_loss: 0.239240, loss_sup: 0.032611, loss_mps: 0.077567, loss_cps: 0.129062
[14:10:57.176] iteration 25698: total_loss: 0.330828, loss_sup: 0.015677, loss_mps: 0.117884, loss_cps: 0.197267
[14:10:57.328] iteration 25699: total_loss: 0.257411, loss_sup: 0.004854, loss_mps: 0.091664, loss_cps: 0.160894
[14:10:57.474] iteration 25700: total_loss: 0.187342, loss_sup: 0.014758, loss_mps: 0.065280, loss_cps: 0.107304
[14:10:57.474] Evaluation Started ==>
[14:11:08.760] ==> valid iteration 25700: unet metrics: {'dc': 0.6688627348905025, 'jc': 0.5544940334343283, 'pre': 0.8078087411762214, 'hd': 5.286530308805121}, ynet metrics: {'dc': 0.5979160001943798, 'jc': 0.48675047490751155, 'pre': 0.8014245310286644, 'hd': 5.236849460787387}.
[14:11:08.761] Evaluation Finished!⏹️
[14:11:08.911] iteration 25701: total_loss: 0.236922, loss_sup: 0.007322, loss_mps: 0.088391, loss_cps: 0.141209
[14:11:09.059] iteration 25702: total_loss: 0.544785, loss_sup: 0.113205, loss_mps: 0.148513, loss_cps: 0.283067
[14:11:09.204] iteration 25703: total_loss: 0.473394, loss_sup: 0.194360, loss_mps: 0.101608, loss_cps: 0.177426
[14:11:09.351] iteration 25704: total_loss: 0.427885, loss_sup: 0.051162, loss_mps: 0.126667, loss_cps: 0.250056
[14:11:09.496] iteration 25705: total_loss: 0.241386, loss_sup: 0.035611, loss_mps: 0.071938, loss_cps: 0.133837
[14:11:09.641] iteration 25706: total_loss: 0.629619, loss_sup: 0.098089, loss_mps: 0.180601, loss_cps: 0.350930
[14:11:09.787] iteration 25707: total_loss: 0.724719, loss_sup: 0.279834, loss_mps: 0.142637, loss_cps: 0.302248
[14:11:09.933] iteration 25708: total_loss: 0.695136, loss_sup: 0.161178, loss_mps: 0.183657, loss_cps: 0.350302
[14:11:10.080] iteration 25709: total_loss: 0.291515, loss_sup: 0.025872, loss_mps: 0.095258, loss_cps: 0.170384
[14:11:10.226] iteration 25710: total_loss: 0.422943, loss_sup: 0.012159, loss_mps: 0.135502, loss_cps: 0.275282
[14:11:10.372] iteration 25711: total_loss: 0.327658, loss_sup: 0.002881, loss_mps: 0.114329, loss_cps: 0.210448
[14:11:10.517] iteration 25712: total_loss: 0.241199, loss_sup: 0.024566, loss_mps: 0.078887, loss_cps: 0.137746
[14:11:10.664] iteration 25713: total_loss: 0.332823, loss_sup: 0.068768, loss_mps: 0.092232, loss_cps: 0.171822
[14:11:10.811] iteration 25714: total_loss: 0.368421, loss_sup: 0.022242, loss_mps: 0.113199, loss_cps: 0.232979
[14:11:10.957] iteration 25715: total_loss: 0.426189, loss_sup: 0.124200, loss_mps: 0.102780, loss_cps: 0.199210
[14:11:11.103] iteration 25716: total_loss: 0.246550, loss_sup: 0.010507, loss_mps: 0.083839, loss_cps: 0.152204
[14:11:11.249] iteration 25717: total_loss: 0.395973, loss_sup: 0.164531, loss_mps: 0.082426, loss_cps: 0.149016
[14:11:11.395] iteration 25718: total_loss: 0.597044, loss_sup: 0.118548, loss_mps: 0.160205, loss_cps: 0.318291
[14:11:11.541] iteration 25719: total_loss: 0.262650, loss_sup: 0.041217, loss_mps: 0.082934, loss_cps: 0.138500
[14:11:11.688] iteration 25720: total_loss: 0.400410, loss_sup: 0.068142, loss_mps: 0.106377, loss_cps: 0.225891
[14:11:11.834] iteration 25721: total_loss: 0.344802, loss_sup: 0.118377, loss_mps: 0.083088, loss_cps: 0.143338
[14:11:11.981] iteration 25722: total_loss: 0.584176, loss_sup: 0.031616, loss_mps: 0.170875, loss_cps: 0.381685
[14:11:12.126] iteration 25723: total_loss: 0.457182, loss_sup: 0.033967, loss_mps: 0.136236, loss_cps: 0.286979
[14:11:12.272] iteration 25724: total_loss: 0.406242, loss_sup: 0.094858, loss_mps: 0.112340, loss_cps: 0.199045
[14:11:12.419] iteration 25725: total_loss: 0.507021, loss_sup: 0.112345, loss_mps: 0.124699, loss_cps: 0.269977
[14:11:12.565] iteration 25726: total_loss: 0.689285, loss_sup: 0.041350, loss_mps: 0.211198, loss_cps: 0.436737
[14:11:12.713] iteration 25727: total_loss: 0.478120, loss_sup: 0.059455, loss_mps: 0.134698, loss_cps: 0.283966
[14:11:12.859] iteration 25728: total_loss: 0.281168, loss_sup: 0.010780, loss_mps: 0.101636, loss_cps: 0.168752
[14:11:13.005] iteration 25729: total_loss: 0.314883, loss_sup: 0.027519, loss_mps: 0.099628, loss_cps: 0.187735
[14:11:13.151] iteration 25730: total_loss: 0.193730, loss_sup: 0.019103, loss_mps: 0.062809, loss_cps: 0.111818
[14:11:13.296] iteration 25731: total_loss: 0.597276, loss_sup: 0.251899, loss_mps: 0.120540, loss_cps: 0.224836
[14:11:13.443] iteration 25732: total_loss: 0.231163, loss_sup: 0.013937, loss_mps: 0.084320, loss_cps: 0.132906
[14:11:13.591] iteration 25733: total_loss: 0.484997, loss_sup: 0.052402, loss_mps: 0.146412, loss_cps: 0.286183
[14:11:13.739] iteration 25734: total_loss: 0.536275, loss_sup: 0.011411, loss_mps: 0.161144, loss_cps: 0.363720
[14:11:13.885] iteration 25735: total_loss: 0.438157, loss_sup: 0.122954, loss_mps: 0.125320, loss_cps: 0.189883
[14:11:14.030] iteration 25736: total_loss: 0.722312, loss_sup: 0.291697, loss_mps: 0.160339, loss_cps: 0.270277
[14:11:14.176] iteration 25737: total_loss: 0.307868, loss_sup: 0.019435, loss_mps: 0.101786, loss_cps: 0.186648
[14:11:14.322] iteration 25738: total_loss: 0.286980, loss_sup: 0.013753, loss_mps: 0.095207, loss_cps: 0.178020
[14:11:14.468] iteration 25739: total_loss: 0.524071, loss_sup: 0.027458, loss_mps: 0.154071, loss_cps: 0.342542
[14:11:14.614] iteration 25740: total_loss: 0.723762, loss_sup: 0.134968, loss_mps: 0.193797, loss_cps: 0.394997
[14:11:14.765] iteration 25741: total_loss: 0.345037, loss_sup: 0.034649, loss_mps: 0.109677, loss_cps: 0.200711
[14:11:14.911] iteration 25742: total_loss: 0.460342, loss_sup: 0.059333, loss_mps: 0.131206, loss_cps: 0.269803
[14:11:15.056] iteration 25743: total_loss: 0.285825, loss_sup: 0.055442, loss_mps: 0.085625, loss_cps: 0.144758
[14:11:15.202] iteration 25744: total_loss: 0.539030, loss_sup: 0.033815, loss_mps: 0.168724, loss_cps: 0.336491
[14:11:15.348] iteration 25745: total_loss: 0.491666, loss_sup: 0.124049, loss_mps: 0.134001, loss_cps: 0.233616
[14:11:15.497] iteration 25746: total_loss: 0.259435, loss_sup: 0.048165, loss_mps: 0.076110, loss_cps: 0.135160
[14:11:15.644] iteration 25747: total_loss: 0.242948, loss_sup: 0.016750, loss_mps: 0.081490, loss_cps: 0.144709
[14:11:15.792] iteration 25748: total_loss: 0.318775, loss_sup: 0.083152, loss_mps: 0.076557, loss_cps: 0.159066
[14:11:15.938] iteration 25749: total_loss: 0.521573, loss_sup: 0.011082, loss_mps: 0.156297, loss_cps: 0.354194
[14:11:16.084] iteration 25750: total_loss: 0.210930, loss_sup: 0.006135, loss_mps: 0.076234, loss_cps: 0.128561
[14:11:16.231] iteration 25751: total_loss: 0.359379, loss_sup: 0.012467, loss_mps: 0.129581, loss_cps: 0.217331
[14:11:16.382] iteration 25752: total_loss: 0.439990, loss_sup: 0.019735, loss_mps: 0.142833, loss_cps: 0.277422
[14:11:16.529] iteration 25753: total_loss: 0.696814, loss_sup: 0.044147, loss_mps: 0.216624, loss_cps: 0.436043
[14:11:16.675] iteration 25754: total_loss: 0.463609, loss_sup: 0.138427, loss_mps: 0.113077, loss_cps: 0.212104
[14:11:16.822] iteration 25755: total_loss: 0.385769, loss_sup: 0.021838, loss_mps: 0.129150, loss_cps: 0.234780
[14:11:16.969] iteration 25756: total_loss: 0.292368, loss_sup: 0.032514, loss_mps: 0.093816, loss_cps: 0.166038
[14:11:17.116] iteration 25757: total_loss: 0.459619, loss_sup: 0.107518, loss_mps: 0.122936, loss_cps: 0.229165
[14:11:17.263] iteration 25758: total_loss: 0.414984, loss_sup: 0.071871, loss_mps: 0.117688, loss_cps: 0.225425
[14:11:17.410] iteration 25759: total_loss: 0.272292, loss_sup: 0.004314, loss_mps: 0.093970, loss_cps: 0.174009
[14:11:17.561] iteration 25760: total_loss: 0.563982, loss_sup: 0.180425, loss_mps: 0.121667, loss_cps: 0.261890
[14:11:17.708] iteration 25761: total_loss: 0.600163, loss_sup: 0.054746, loss_mps: 0.177457, loss_cps: 0.367959
[14:11:17.854] iteration 25762: total_loss: 0.404580, loss_sup: 0.104345, loss_mps: 0.102166, loss_cps: 0.198069
[14:11:18.002] iteration 25763: total_loss: 0.663514, loss_sup: 0.261859, loss_mps: 0.130253, loss_cps: 0.271402
[14:11:18.149] iteration 25764: total_loss: 0.301328, loss_sup: 0.010555, loss_mps: 0.103245, loss_cps: 0.187527
[14:11:18.295] iteration 25765: total_loss: 0.320596, loss_sup: 0.015939, loss_mps: 0.111354, loss_cps: 0.193304
[14:11:18.443] iteration 25766: total_loss: 0.243383, loss_sup: 0.005179, loss_mps: 0.090378, loss_cps: 0.147825
[14:11:18.593] iteration 25767: total_loss: 0.629114, loss_sup: 0.133379, loss_mps: 0.161400, loss_cps: 0.334335
[14:11:18.739] iteration 25768: total_loss: 0.318819, loss_sup: 0.045581, loss_mps: 0.097583, loss_cps: 0.175654
[14:11:18.885] iteration 25769: total_loss: 0.356657, loss_sup: 0.068356, loss_mps: 0.115282, loss_cps: 0.173019
[14:11:19.031] iteration 25770: total_loss: 0.293021, loss_sup: 0.027326, loss_mps: 0.096647, loss_cps: 0.169048
[14:11:19.180] iteration 25771: total_loss: 0.216348, loss_sup: 0.037457, loss_mps: 0.067754, loss_cps: 0.111137
[14:11:19.328] iteration 25772: total_loss: 0.160271, loss_sup: 0.001745, loss_mps: 0.060054, loss_cps: 0.098472
[14:11:19.478] iteration 25773: total_loss: 1.097957, loss_sup: 0.485196, loss_mps: 0.196821, loss_cps: 0.415940
[14:11:19.625] iteration 25774: total_loss: 0.421064, loss_sup: 0.057668, loss_mps: 0.123687, loss_cps: 0.239708
[14:11:19.774] iteration 25775: total_loss: 0.387498, loss_sup: 0.007385, loss_mps: 0.128901, loss_cps: 0.251211
[14:11:19.920] iteration 25776: total_loss: 0.772617, loss_sup: 0.029569, loss_mps: 0.235487, loss_cps: 0.507561
[14:11:20.070] iteration 25777: total_loss: 0.543424, loss_sup: 0.075373, loss_mps: 0.152460, loss_cps: 0.315591
[14:11:20.220] iteration 25778: total_loss: 0.351584, loss_sup: 0.007146, loss_mps: 0.116288, loss_cps: 0.228150
[14:11:20.367] iteration 25779: total_loss: 0.184000, loss_sup: 0.002031, loss_mps: 0.066485, loss_cps: 0.115484
[14:11:20.513] iteration 25780: total_loss: 0.409108, loss_sup: 0.024042, loss_mps: 0.140059, loss_cps: 0.245008
[14:11:20.660] iteration 25781: total_loss: 0.390508, loss_sup: 0.015029, loss_mps: 0.124057, loss_cps: 0.251422
[14:11:20.806] iteration 25782: total_loss: 0.399826, loss_sup: 0.025837, loss_mps: 0.131060, loss_cps: 0.242929
[14:11:20.952] iteration 25783: total_loss: 0.229842, loss_sup: 0.008455, loss_mps: 0.082632, loss_cps: 0.138756
[14:11:21.097] iteration 25784: total_loss: 0.604619, loss_sup: 0.074220, loss_mps: 0.170960, loss_cps: 0.359438
[14:11:21.244] iteration 25785: total_loss: 0.339154, loss_sup: 0.048319, loss_mps: 0.108051, loss_cps: 0.182785
[14:11:21.389] iteration 25786: total_loss: 0.242740, loss_sup: 0.018823, loss_mps: 0.076772, loss_cps: 0.147145
[14:11:21.539] iteration 25787: total_loss: 0.341136, loss_sup: 0.023453, loss_mps: 0.117099, loss_cps: 0.200583
[14:11:21.686] iteration 25788: total_loss: 0.362923, loss_sup: 0.050764, loss_mps: 0.108890, loss_cps: 0.203270
[14:11:21.832] iteration 25789: total_loss: 0.710688, loss_sup: 0.372806, loss_mps: 0.115751, loss_cps: 0.222130
[14:11:21.978] iteration 25790: total_loss: 0.232823, loss_sup: 0.002175, loss_mps: 0.083598, loss_cps: 0.147050
[14:11:22.124] iteration 25791: total_loss: 0.518021, loss_sup: 0.020061, loss_mps: 0.171196, loss_cps: 0.326764
[14:11:22.270] iteration 25792: total_loss: 0.335643, loss_sup: 0.073655, loss_mps: 0.091416, loss_cps: 0.170571
[14:11:22.416] iteration 25793: total_loss: 0.282841, loss_sup: 0.025038, loss_mps: 0.099381, loss_cps: 0.158422
[14:11:22.562] iteration 25794: total_loss: 0.442769, loss_sup: 0.158173, loss_mps: 0.102897, loss_cps: 0.181699
[14:11:22.713] iteration 25795: total_loss: 0.397985, loss_sup: 0.007297, loss_mps: 0.128758, loss_cps: 0.261930
[14:11:22.859] iteration 25796: total_loss: 0.300913, loss_sup: 0.033477, loss_mps: 0.087061, loss_cps: 0.180376
[14:11:23.005] iteration 25797: total_loss: 0.249662, loss_sup: 0.031206, loss_mps: 0.076988, loss_cps: 0.141468
[14:11:23.151] iteration 25798: total_loss: 0.329623, loss_sup: 0.012740, loss_mps: 0.108986, loss_cps: 0.207897
[14:11:23.297] iteration 25799: total_loss: 0.241950, loss_sup: 0.011969, loss_mps: 0.082389, loss_cps: 0.147592
[14:11:23.444] iteration 25800: total_loss: 0.319977, loss_sup: 0.012111, loss_mps: 0.108863, loss_cps: 0.199003
[14:11:23.444] Evaluation Started ==>
[14:11:34.829] ==> valid iteration 25800: unet metrics: {'dc': 0.6608128006230349, 'jc': 0.5449711972705514, 'pre': 0.8101539934486583, 'hd': 5.427521093144645}, ynet metrics: {'dc': 0.6260861430539698, 'jc': 0.5124743053685992, 'pre': 0.8033854014536393, 'hd': 5.400019396645111}.
[14:11:34.832] Evaluation Finished!⏹️
[14:11:34.986] iteration 25801: total_loss: 0.366665, loss_sup: 0.039351, loss_mps: 0.118375, loss_cps: 0.208939
[14:11:35.138] iteration 25802: total_loss: 0.331565, loss_sup: 0.049247, loss_mps: 0.106730, loss_cps: 0.175587
[14:11:35.288] iteration 25803: total_loss: 0.886621, loss_sup: 0.228893, loss_mps: 0.208925, loss_cps: 0.448803
[14:11:35.433] iteration 25804: total_loss: 0.507315, loss_sup: 0.065988, loss_mps: 0.146594, loss_cps: 0.294733
[14:11:35.579] iteration 25805: total_loss: 0.544711, loss_sup: 0.001770, loss_mps: 0.172935, loss_cps: 0.370006
[14:11:35.724] iteration 25806: total_loss: 0.324505, loss_sup: 0.036120, loss_mps: 0.100363, loss_cps: 0.188022
[14:11:35.869] iteration 25807: total_loss: 0.712715, loss_sup: 0.123126, loss_mps: 0.188369, loss_cps: 0.401220
[14:11:36.016] iteration 25808: total_loss: 0.343763, loss_sup: 0.043685, loss_mps: 0.105052, loss_cps: 0.195026
[14:11:36.162] iteration 25809: total_loss: 0.337327, loss_sup: 0.015787, loss_mps: 0.118368, loss_cps: 0.203173
[14:11:36.308] iteration 25810: total_loss: 0.339862, loss_sup: 0.046494, loss_mps: 0.109855, loss_cps: 0.183514
[14:11:36.453] iteration 25811: total_loss: 0.406612, loss_sup: 0.018183, loss_mps: 0.133610, loss_cps: 0.254819
[14:11:36.599] iteration 25812: total_loss: 0.293123, loss_sup: 0.007153, loss_mps: 0.100502, loss_cps: 0.185467
[14:11:36.745] iteration 25813: total_loss: 0.287008, loss_sup: 0.030914, loss_mps: 0.095377, loss_cps: 0.160717
[14:11:36.891] iteration 25814: total_loss: 0.625957, loss_sup: 0.263610, loss_mps: 0.123382, loss_cps: 0.238965
[14:11:37.036] iteration 25815: total_loss: 0.349229, loss_sup: 0.007673, loss_mps: 0.115812, loss_cps: 0.225743
[14:11:37.182] iteration 25816: total_loss: 0.325807, loss_sup: 0.057874, loss_mps: 0.096607, loss_cps: 0.171326
[14:11:37.327] iteration 25817: total_loss: 0.597935, loss_sup: 0.158357, loss_mps: 0.150937, loss_cps: 0.288641
[14:11:37.474] iteration 25818: total_loss: 0.351885, loss_sup: 0.029021, loss_mps: 0.108521, loss_cps: 0.214343
[14:11:37.620] iteration 25819: total_loss: 0.279490, loss_sup: 0.053625, loss_mps: 0.080338, loss_cps: 0.145527
[14:11:37.769] iteration 25820: total_loss: 0.307935, loss_sup: 0.006934, loss_mps: 0.099611, loss_cps: 0.201391
[14:11:37.915] iteration 25821: total_loss: 0.514311, loss_sup: 0.103084, loss_mps: 0.135686, loss_cps: 0.275541
[14:11:38.064] iteration 25822: total_loss: 0.281984, loss_sup: 0.013420, loss_mps: 0.090184, loss_cps: 0.178380
[14:11:38.209] iteration 25823: total_loss: 0.516382, loss_sup: 0.232276, loss_mps: 0.096647, loss_cps: 0.187459
[14:11:38.361] iteration 25824: total_loss: 0.354275, loss_sup: 0.051543, loss_mps: 0.115089, loss_cps: 0.187643
[14:11:38.509] iteration 25825: total_loss: 0.353141, loss_sup: 0.107403, loss_mps: 0.087548, loss_cps: 0.158190
[14:11:38.655] iteration 25826: total_loss: 0.211126, loss_sup: 0.008307, loss_mps: 0.078793, loss_cps: 0.124026
[14:11:38.803] iteration 25827: total_loss: 0.227008, loss_sup: 0.066237, loss_mps: 0.062934, loss_cps: 0.097837
[14:11:38.949] iteration 25828: total_loss: 0.388831, loss_sup: 0.031394, loss_mps: 0.130313, loss_cps: 0.227124
[14:11:39.094] iteration 25829: total_loss: 0.505789, loss_sup: 0.127795, loss_mps: 0.132458, loss_cps: 0.245536
[14:11:39.244] iteration 25830: total_loss: 0.260873, loss_sup: 0.027915, loss_mps: 0.083382, loss_cps: 0.149576
[14:11:39.390] iteration 25831: total_loss: 0.622002, loss_sup: 0.097564, loss_mps: 0.173397, loss_cps: 0.351040
[14:11:39.536] iteration 25832: total_loss: 0.157492, loss_sup: 0.006673, loss_mps: 0.059500, loss_cps: 0.091319
[14:11:39.682] iteration 25833: total_loss: 0.332672, loss_sup: 0.060629, loss_mps: 0.095851, loss_cps: 0.176193
[14:11:39.827] iteration 25834: total_loss: 0.565324, loss_sup: 0.087877, loss_mps: 0.167182, loss_cps: 0.310265
[14:11:39.974] iteration 25835: total_loss: 0.468229, loss_sup: 0.041564, loss_mps: 0.141592, loss_cps: 0.285073
[14:11:40.119] iteration 25836: total_loss: 0.271058, loss_sup: 0.026181, loss_mps: 0.087497, loss_cps: 0.157380
[14:11:40.265] iteration 25837: total_loss: 0.269962, loss_sup: 0.058864, loss_mps: 0.077835, loss_cps: 0.133263
[14:11:40.410] iteration 25838: total_loss: 0.306290, loss_sup: 0.018770, loss_mps: 0.104603, loss_cps: 0.182917
[14:11:40.556] iteration 25839: total_loss: 0.510434, loss_sup: 0.028261, loss_mps: 0.162036, loss_cps: 0.320137
[14:11:40.702] iteration 25840: total_loss: 0.249892, loss_sup: 0.015802, loss_mps: 0.088821, loss_cps: 0.145269
[14:11:40.848] iteration 25841: total_loss: 0.728076, loss_sup: 0.025003, loss_mps: 0.212264, loss_cps: 0.490809
[14:11:40.994] iteration 25842: total_loss: 0.369658, loss_sup: 0.059382, loss_mps: 0.103935, loss_cps: 0.206341
[14:11:41.139] iteration 25843: total_loss: 0.624051, loss_sup: 0.079212, loss_mps: 0.174115, loss_cps: 0.370724
[14:11:41.285] iteration 25844: total_loss: 0.482330, loss_sup: 0.029250, loss_mps: 0.149644, loss_cps: 0.303436
[14:11:41.430] iteration 25845: total_loss: 0.406835, loss_sup: 0.062060, loss_mps: 0.120231, loss_cps: 0.224544
[14:11:41.576] iteration 25846: total_loss: 0.406872, loss_sup: 0.016035, loss_mps: 0.129858, loss_cps: 0.260978
[14:11:41.723] iteration 25847: total_loss: 0.615097, loss_sup: 0.079253, loss_mps: 0.179674, loss_cps: 0.356170
[14:11:41.868] iteration 25848: total_loss: 0.320210, loss_sup: 0.128478, loss_mps: 0.068591, loss_cps: 0.123140
[14:11:42.017] iteration 25849: total_loss: 0.564896, loss_sup: 0.016693, loss_mps: 0.188761, loss_cps: 0.359442
[14:11:42.164] iteration 25850: total_loss: 0.574907, loss_sup: 0.047763, loss_mps: 0.180075, loss_cps: 0.347069
[14:11:42.310] iteration 25851: total_loss: 0.330680, loss_sup: 0.011706, loss_mps: 0.113987, loss_cps: 0.204987
[14:11:42.457] iteration 25852: total_loss: 0.280823, loss_sup: 0.015198, loss_mps: 0.101076, loss_cps: 0.164550
[14:11:42.607] iteration 25853: total_loss: 0.213782, loss_sup: 0.005792, loss_mps: 0.079256, loss_cps: 0.128734
[14:11:42.755] iteration 25854: total_loss: 0.381099, loss_sup: 0.089834, loss_mps: 0.107369, loss_cps: 0.183896
[14:11:42.901] iteration 25855: total_loss: 0.632486, loss_sup: 0.162378, loss_mps: 0.160787, loss_cps: 0.309321
[14:11:43.047] iteration 25856: total_loss: 0.340831, loss_sup: 0.049680, loss_mps: 0.101326, loss_cps: 0.189825
[14:11:43.193] iteration 25857: total_loss: 0.659140, loss_sup: 0.002387, loss_mps: 0.219430, loss_cps: 0.437322
[14:11:43.339] iteration 25858: total_loss: 0.287084, loss_sup: 0.026891, loss_mps: 0.096501, loss_cps: 0.163691
[14:11:43.487] iteration 25859: total_loss: 0.220451, loss_sup: 0.027712, loss_mps: 0.067888, loss_cps: 0.124852
[14:11:43.634] iteration 25860: total_loss: 0.445916, loss_sup: 0.076508, loss_mps: 0.115182, loss_cps: 0.254227
[14:11:43.780] iteration 25861: total_loss: 0.452366, loss_sup: 0.032751, loss_mps: 0.143370, loss_cps: 0.276245
[14:11:43.926] iteration 25862: total_loss: 0.318752, loss_sup: 0.077466, loss_mps: 0.085440, loss_cps: 0.155845
[14:11:44.072] iteration 25863: total_loss: 0.453089, loss_sup: 0.089751, loss_mps: 0.133162, loss_cps: 0.230177
[14:11:44.219] iteration 25864: total_loss: 0.413545, loss_sup: 0.026682, loss_mps: 0.134789, loss_cps: 0.252074
[14:11:44.365] iteration 25865: total_loss: 0.424455, loss_sup: 0.077074, loss_mps: 0.114140, loss_cps: 0.233241
[14:11:44.511] iteration 25866: total_loss: 0.316285, loss_sup: 0.016147, loss_mps: 0.108927, loss_cps: 0.191211
[14:11:44.658] iteration 25867: total_loss: 0.445641, loss_sup: 0.043564, loss_mps: 0.138606, loss_cps: 0.263472
[14:11:44.805] iteration 25868: total_loss: 0.282080, loss_sup: 0.035544, loss_mps: 0.090041, loss_cps: 0.156495
[14:11:44.953] iteration 25869: total_loss: 0.279164, loss_sup: 0.053116, loss_mps: 0.082378, loss_cps: 0.143669
[14:11:45.099] iteration 25870: total_loss: 0.486636, loss_sup: 0.085818, loss_mps: 0.129123, loss_cps: 0.271695
[14:11:45.245] iteration 25871: total_loss: 0.391646, loss_sup: 0.077330, loss_mps: 0.113327, loss_cps: 0.200989
[14:11:45.391] iteration 25872: total_loss: 0.593174, loss_sup: 0.073863, loss_mps: 0.158778, loss_cps: 0.360532
[14:11:45.538] iteration 25873: total_loss: 0.414931, loss_sup: 0.011941, loss_mps: 0.134599, loss_cps: 0.268391
[14:11:45.683] iteration 25874: total_loss: 0.657918, loss_sup: 0.030533, loss_mps: 0.193931, loss_cps: 0.433453
[14:11:45.830] iteration 25875: total_loss: 0.652991, loss_sup: 0.031123, loss_mps: 0.204503, loss_cps: 0.417365
[14:11:45.976] iteration 25876: total_loss: 0.280688, loss_sup: 0.019194, loss_mps: 0.098026, loss_cps: 0.163468
[14:11:46.122] iteration 25877: total_loss: 0.363568, loss_sup: 0.043045, loss_mps: 0.118443, loss_cps: 0.202080
[14:11:46.269] iteration 25878: total_loss: 0.254300, loss_sup: 0.004316, loss_mps: 0.087143, loss_cps: 0.162841
[14:11:46.416] iteration 25879: total_loss: 0.251185, loss_sup: 0.012082, loss_mps: 0.085872, loss_cps: 0.153230
[14:11:46.564] iteration 25880: total_loss: 0.296780, loss_sup: 0.010597, loss_mps: 0.104296, loss_cps: 0.181887
[14:11:46.711] iteration 25881: total_loss: 0.308719, loss_sup: 0.014036, loss_mps: 0.103976, loss_cps: 0.190708
[14:11:46.858] iteration 25882: total_loss: 0.276593, loss_sup: 0.002178, loss_mps: 0.095651, loss_cps: 0.178764
[14:11:47.006] iteration 25883: total_loss: 0.192024, loss_sup: 0.006253, loss_mps: 0.069037, loss_cps: 0.116735
[14:11:47.152] iteration 25884: total_loss: 0.310210, loss_sup: 0.029212, loss_mps: 0.101694, loss_cps: 0.179304
[14:11:47.300] iteration 25885: total_loss: 0.277912, loss_sup: 0.045315, loss_mps: 0.079909, loss_cps: 0.152688
[14:11:47.446] iteration 25886: total_loss: 0.441364, loss_sup: 0.064709, loss_mps: 0.128932, loss_cps: 0.247722
[14:11:47.593] iteration 25887: total_loss: 0.216871, loss_sup: 0.030530, loss_mps: 0.069815, loss_cps: 0.116526
[14:11:47.741] iteration 25888: total_loss: 0.557809, loss_sup: 0.122846, loss_mps: 0.145574, loss_cps: 0.289388
[14:11:47.886] iteration 25889: total_loss: 0.474068, loss_sup: 0.016777, loss_mps: 0.154334, loss_cps: 0.302956
[14:11:48.032] iteration 25890: total_loss: 0.416947, loss_sup: 0.117706, loss_mps: 0.107415, loss_cps: 0.191825
[14:11:48.178] iteration 25891: total_loss: 0.608379, loss_sup: 0.119447, loss_mps: 0.166129, loss_cps: 0.322802
[14:11:48.324] iteration 25892: total_loss: 0.365028, loss_sup: 0.112312, loss_mps: 0.086465, loss_cps: 0.166251
[14:11:48.470] iteration 25893: total_loss: 0.413939, loss_sup: 0.069194, loss_mps: 0.119408, loss_cps: 0.225337
[14:11:48.619] iteration 25894: total_loss: 0.241188, loss_sup: 0.048372, loss_mps: 0.069627, loss_cps: 0.123188
[14:11:48.765] iteration 25895: total_loss: 0.524493, loss_sup: 0.008916, loss_mps: 0.168457, loss_cps: 0.347120
[14:11:48.910] iteration 25896: total_loss: 0.227081, loss_sup: 0.004813, loss_mps: 0.079862, loss_cps: 0.142406
[14:11:49.056] iteration 25897: total_loss: 0.394343, loss_sup: 0.009115, loss_mps: 0.135874, loss_cps: 0.249354
[14:11:49.202] iteration 25898: total_loss: 0.682773, loss_sup: 0.064487, loss_mps: 0.203485, loss_cps: 0.414800
[14:11:49.348] iteration 25899: total_loss: 0.266125, loss_sup: 0.043478, loss_mps: 0.082781, loss_cps: 0.139866
[14:11:49.495] iteration 25900: total_loss: 0.305115, loss_sup: 0.078515, loss_mps: 0.079810, loss_cps: 0.146789
[14:11:49.495] Evaluation Started ==>
[14:12:00.868] ==> valid iteration 25900: unet metrics: {'dc': 0.6463279848089697, 'jc': 0.5315179367659801, 'pre': 0.8051958783962155, 'hd': 5.474322764393174}, ynet metrics: {'dc': 0.6147848021939242, 'jc': 0.500929877369305, 'pre': 0.7913910081931649, 'hd': 5.532233332435776}.
[14:12:00.870] Evaluation Finished!⏹️
[14:12:01.023] iteration 25901: total_loss: 0.335836, loss_sup: 0.054898, loss_mps: 0.101432, loss_cps: 0.179506
[14:12:01.173] iteration 25902: total_loss: 0.240838, loss_sup: 0.011304, loss_mps: 0.084664, loss_cps: 0.144870
[14:12:01.318] iteration 25903: total_loss: 0.396726, loss_sup: 0.026340, loss_mps: 0.123227, loss_cps: 0.247159
[14:12:01.465] iteration 25904: total_loss: 0.564582, loss_sup: 0.139915, loss_mps: 0.137227, loss_cps: 0.287439
[14:12:01.612] iteration 25905: total_loss: 0.622805, loss_sup: 0.094865, loss_mps: 0.169934, loss_cps: 0.358005
[14:12:01.757] iteration 25906: total_loss: 0.373162, loss_sup: 0.046538, loss_mps: 0.107700, loss_cps: 0.218924
[14:12:01.902] iteration 25907: total_loss: 1.288922, loss_sup: 0.319373, loss_mps: 0.304819, loss_cps: 0.664730
[14:12:02.048] iteration 25908: total_loss: 0.462312, loss_sup: 0.003418, loss_mps: 0.162289, loss_cps: 0.296605
[14:12:02.194] iteration 25909: total_loss: 0.219801, loss_sup: 0.010674, loss_mps: 0.073568, loss_cps: 0.135558
[14:12:02.340] iteration 25910: total_loss: 0.535269, loss_sup: 0.003119, loss_mps: 0.171356, loss_cps: 0.360794
[14:12:02.485] iteration 25911: total_loss: 0.452425, loss_sup: 0.102348, loss_mps: 0.121025, loss_cps: 0.229052
[14:12:02.631] iteration 25912: total_loss: 0.580920, loss_sup: 0.155891, loss_mps: 0.141887, loss_cps: 0.283142
[14:12:02.777] iteration 25913: total_loss: 0.479320, loss_sup: 0.016788, loss_mps: 0.157995, loss_cps: 0.304537
[14:12:02.922] iteration 25914: total_loss: 0.546287, loss_sup: 0.182134, loss_mps: 0.123346, loss_cps: 0.240806
[14:12:03.068] iteration 25915: total_loss: 0.198000, loss_sup: 0.011795, loss_mps: 0.068978, loss_cps: 0.117227
[14:12:03.133] iteration 25916: total_loss: 0.187884, loss_sup: 0.019128, loss_mps: 0.065211, loss_cps: 0.103545
[14:12:04.372] iteration 25917: total_loss: 0.620379, loss_sup: 0.296698, loss_mps: 0.113345, loss_cps: 0.210336
[14:12:04.521] iteration 25918: total_loss: 0.329126, loss_sup: 0.009429, loss_mps: 0.112740, loss_cps: 0.206957
[14:12:04.670] iteration 25919: total_loss: 0.248903, loss_sup: 0.011799, loss_mps: 0.086482, loss_cps: 0.150622
[14:12:04.819] iteration 25920: total_loss: 0.216352, loss_sup: 0.006376, loss_mps: 0.074018, loss_cps: 0.135958
[14:12:04.966] iteration 25921: total_loss: 0.534600, loss_sup: 0.180907, loss_mps: 0.127776, loss_cps: 0.225917
[14:12:05.113] iteration 25922: total_loss: 0.494405, loss_sup: 0.171548, loss_mps: 0.114377, loss_cps: 0.208479
[14:12:05.259] iteration 25923: total_loss: 0.463069, loss_sup: 0.084720, loss_mps: 0.125129, loss_cps: 0.253220
[14:12:05.407] iteration 25924: total_loss: 0.736452, loss_sup: 0.059896, loss_mps: 0.206850, loss_cps: 0.469706
[14:12:05.555] iteration 25925: total_loss: 0.195822, loss_sup: 0.021155, loss_mps: 0.063616, loss_cps: 0.111051
[14:12:05.701] iteration 25926: total_loss: 0.533720, loss_sup: 0.137380, loss_mps: 0.130653, loss_cps: 0.265688
[14:12:05.850] iteration 25927: total_loss: 0.339445, loss_sup: 0.148863, loss_mps: 0.070759, loss_cps: 0.119823
[14:12:05.997] iteration 25928: total_loss: 0.480156, loss_sup: 0.010880, loss_mps: 0.159272, loss_cps: 0.310004
[14:12:06.144] iteration 25929: total_loss: 0.363110, loss_sup: 0.024848, loss_mps: 0.114514, loss_cps: 0.223748
[14:12:06.291] iteration 25930: total_loss: 0.243395, loss_sup: 0.015401, loss_mps: 0.081810, loss_cps: 0.146185
[14:12:06.437] iteration 25931: total_loss: 0.305627, loss_sup: 0.005085, loss_mps: 0.117416, loss_cps: 0.183126
[14:12:06.584] iteration 25932: total_loss: 0.287519, loss_sup: 0.001173, loss_mps: 0.105338, loss_cps: 0.181008
[14:12:06.732] iteration 25933: total_loss: 0.191511, loss_sup: 0.002064, loss_mps: 0.069970, loss_cps: 0.119477
[14:12:06.878] iteration 25934: total_loss: 0.558200, loss_sup: 0.014228, loss_mps: 0.186602, loss_cps: 0.357369
[14:12:07.025] iteration 25935: total_loss: 0.375961, loss_sup: 0.075522, loss_mps: 0.109198, loss_cps: 0.191241
[14:12:07.173] iteration 25936: total_loss: 0.292408, loss_sup: 0.043458, loss_mps: 0.088465, loss_cps: 0.160485
[14:12:07.319] iteration 25937: total_loss: 0.694843, loss_sup: 0.254100, loss_mps: 0.141152, loss_cps: 0.299591
[14:12:07.465] iteration 25938: total_loss: 0.861838, loss_sup: 0.049273, loss_mps: 0.250323, loss_cps: 0.562242
[14:12:07.612] iteration 25939: total_loss: 0.279268, loss_sup: 0.038451, loss_mps: 0.086874, loss_cps: 0.153943
[14:12:07.758] iteration 25940: total_loss: 0.467020, loss_sup: 0.015002, loss_mps: 0.159297, loss_cps: 0.292721
[14:12:07.905] iteration 25941: total_loss: 0.300135, loss_sup: 0.032793, loss_mps: 0.098528, loss_cps: 0.168814
[14:12:08.051] iteration 25942: total_loss: 0.521787, loss_sup: 0.075038, loss_mps: 0.149785, loss_cps: 0.296964
[14:12:08.197] iteration 25943: total_loss: 0.450107, loss_sup: 0.022473, loss_mps: 0.139048, loss_cps: 0.288585
[14:12:08.343] iteration 25944: total_loss: 0.477790, loss_sup: 0.109384, loss_mps: 0.135634, loss_cps: 0.232772
[14:12:08.490] iteration 25945: total_loss: 0.301334, loss_sup: 0.040507, loss_mps: 0.092689, loss_cps: 0.168138
[14:12:08.636] iteration 25946: total_loss: 0.804947, loss_sup: 0.138677, loss_mps: 0.209304, loss_cps: 0.456967
[14:12:08.785] iteration 25947: total_loss: 0.694235, loss_sup: 0.045517, loss_mps: 0.209409, loss_cps: 0.439308
[14:12:08.931] iteration 25948: total_loss: 0.477174, loss_sup: 0.156437, loss_mps: 0.115388, loss_cps: 0.205349
[14:12:09.077] iteration 25949: total_loss: 0.327852, loss_sup: 0.057083, loss_mps: 0.097152, loss_cps: 0.173616
[14:12:09.224] iteration 25950: total_loss: 0.308101, loss_sup: 0.146502, loss_mps: 0.061793, loss_cps: 0.099806
[14:12:09.370] iteration 25951: total_loss: 0.448523, loss_sup: 0.002274, loss_mps: 0.141802, loss_cps: 0.304446
[14:12:09.517] iteration 25952: total_loss: 0.451835, loss_sup: 0.026186, loss_mps: 0.135799, loss_cps: 0.289850
[14:12:09.663] iteration 25953: total_loss: 0.283242, loss_sup: 0.019408, loss_mps: 0.095345, loss_cps: 0.168489
[14:12:09.812] iteration 25954: total_loss: 0.354390, loss_sup: 0.047815, loss_mps: 0.106952, loss_cps: 0.199623
[14:12:09.958] iteration 25955: total_loss: 0.517822, loss_sup: 0.015220, loss_mps: 0.171187, loss_cps: 0.331415
[14:12:10.104] iteration 25956: total_loss: 0.332528, loss_sup: 0.018949, loss_mps: 0.107499, loss_cps: 0.206080
[14:12:10.251] iteration 25957: total_loss: 0.625285, loss_sup: 0.011865, loss_mps: 0.198134, loss_cps: 0.415286
[14:12:10.397] iteration 25958: total_loss: 0.358760, loss_sup: 0.038995, loss_mps: 0.108757, loss_cps: 0.211008
[14:12:10.546] iteration 25959: total_loss: 0.285574, loss_sup: 0.006618, loss_mps: 0.101425, loss_cps: 0.177532
[14:12:10.692] iteration 25960: total_loss: 0.416273, loss_sup: 0.017034, loss_mps: 0.131779, loss_cps: 0.267460
[14:12:10.840] iteration 25961: total_loss: 0.262062, loss_sup: 0.004346, loss_mps: 0.091231, loss_cps: 0.166484
[14:12:10.987] iteration 25962: total_loss: 0.371093, loss_sup: 0.015409, loss_mps: 0.118719, loss_cps: 0.236965
[14:12:11.132] iteration 25963: total_loss: 0.278500, loss_sup: 0.003974, loss_mps: 0.092628, loss_cps: 0.181898
[14:12:11.279] iteration 25964: total_loss: 0.667791, loss_sup: 0.177706, loss_mps: 0.169087, loss_cps: 0.320998
[14:12:11.425] iteration 25965: total_loss: 0.280476, loss_sup: 0.031485, loss_mps: 0.088642, loss_cps: 0.160349
[14:12:11.571] iteration 25966: total_loss: 0.266871, loss_sup: 0.014516, loss_mps: 0.091417, loss_cps: 0.160938
[14:12:11.718] iteration 25967: total_loss: 0.324603, loss_sup: 0.048973, loss_mps: 0.096634, loss_cps: 0.178996
[14:12:11.864] iteration 25968: total_loss: 0.278473, loss_sup: 0.026549, loss_mps: 0.094167, loss_cps: 0.157757
[14:12:12.011] iteration 25969: total_loss: 0.355732, loss_sup: 0.034631, loss_mps: 0.115422, loss_cps: 0.205679
[14:12:12.157] iteration 25970: total_loss: 0.220271, loss_sup: 0.024427, loss_mps: 0.072084, loss_cps: 0.123760
[14:12:12.303] iteration 25971: total_loss: 0.306443, loss_sup: 0.003731, loss_mps: 0.103823, loss_cps: 0.198889
[14:12:12.450] iteration 25972: total_loss: 0.601173, loss_sup: 0.018576, loss_mps: 0.190764, loss_cps: 0.391833
[14:12:12.596] iteration 25973: total_loss: 0.420321, loss_sup: 0.067711, loss_mps: 0.121281, loss_cps: 0.231328
[14:12:12.742] iteration 25974: total_loss: 0.421175, loss_sup: 0.011512, loss_mps: 0.136986, loss_cps: 0.272676
[14:12:12.889] iteration 25975: total_loss: 0.326040, loss_sup: 0.024152, loss_mps: 0.110491, loss_cps: 0.191397
[14:12:13.035] iteration 25976: total_loss: 0.336069, loss_sup: 0.113631, loss_mps: 0.085502, loss_cps: 0.136936
[14:12:13.181] iteration 25977: total_loss: 0.549698, loss_sup: 0.180231, loss_mps: 0.128375, loss_cps: 0.241091
[14:12:13.328] iteration 25978: total_loss: 0.289875, loss_sup: 0.054836, loss_mps: 0.086854, loss_cps: 0.148186
[14:12:13.474] iteration 25979: total_loss: 0.549573, loss_sup: 0.273053, loss_mps: 0.102226, loss_cps: 0.174294
[14:12:13.620] iteration 25980: total_loss: 0.408704, loss_sup: 0.083092, loss_mps: 0.113234, loss_cps: 0.212378
[14:12:13.767] iteration 25981: total_loss: 0.637713, loss_sup: 0.051753, loss_mps: 0.183169, loss_cps: 0.402791
[14:12:13.915] iteration 25982: total_loss: 0.481134, loss_sup: 0.203079, loss_mps: 0.097306, loss_cps: 0.180749
[14:12:14.061] iteration 25983: total_loss: 0.417416, loss_sup: 0.054842, loss_mps: 0.124527, loss_cps: 0.238047
[14:12:14.207] iteration 25984: total_loss: 0.297635, loss_sup: 0.048205, loss_mps: 0.088955, loss_cps: 0.160475
[14:12:14.353] iteration 25985: total_loss: 0.292965, loss_sup: 0.036454, loss_mps: 0.093690, loss_cps: 0.162821
[14:12:14.500] iteration 25986: total_loss: 0.337768, loss_sup: 0.013745, loss_mps: 0.109561, loss_cps: 0.214461
[14:12:14.647] iteration 25987: total_loss: 0.178990, loss_sup: 0.039619, loss_mps: 0.051054, loss_cps: 0.088317
[14:12:14.793] iteration 25988: total_loss: 0.554525, loss_sup: 0.065723, loss_mps: 0.167950, loss_cps: 0.320852
[14:12:14.943] iteration 25989: total_loss: 0.393724, loss_sup: 0.031157, loss_mps: 0.129903, loss_cps: 0.232664
[14:12:15.090] iteration 25990: total_loss: 0.157163, loss_sup: 0.001723, loss_mps: 0.060417, loss_cps: 0.095023
[14:12:15.236] iteration 25991: total_loss: 0.293711, loss_sup: 0.025233, loss_mps: 0.103495, loss_cps: 0.164983
[14:12:15.382] iteration 25992: total_loss: 0.340678, loss_sup: 0.050840, loss_mps: 0.112345, loss_cps: 0.177492
[14:12:15.529] iteration 25993: total_loss: 0.445472, loss_sup: 0.029600, loss_mps: 0.137045, loss_cps: 0.278827
[14:12:15.675] iteration 25994: total_loss: 0.359130, loss_sup: 0.084815, loss_mps: 0.100380, loss_cps: 0.173935
[14:12:15.821] iteration 25995: total_loss: 0.296820, loss_sup: 0.028313, loss_mps: 0.093485, loss_cps: 0.175023
[14:12:15.968] iteration 25996: total_loss: 0.419402, loss_sup: 0.005991, loss_mps: 0.130573, loss_cps: 0.282838
[14:12:16.115] iteration 25997: total_loss: 0.219358, loss_sup: 0.002504, loss_mps: 0.077676, loss_cps: 0.139178
[14:12:16.262] iteration 25998: total_loss: 0.473398, loss_sup: 0.068267, loss_mps: 0.135774, loss_cps: 0.269356
[14:12:16.410] iteration 25999: total_loss: 0.172029, loss_sup: 0.006610, loss_mps: 0.061484, loss_cps: 0.103935
[14:12:16.557] iteration 26000: total_loss: 0.462157, loss_sup: 0.048037, loss_mps: 0.144739, loss_cps: 0.269381
[14:12:16.557] Evaluation Started ==>
[14:12:27.910] ==> valid iteration 26000: unet metrics: {'dc': 0.6519981355208632, 'jc': 0.5360740506792592, 'pre': 0.805092187234986, 'hd': 5.476878447491567}, ynet metrics: {'dc': 0.6105935346512883, 'jc': 0.4974796375118273, 'pre': 0.8024101266919056, 'hd': 5.376612852714426}.
[14:12:27.912] Evaluation Finished!⏹️
[14:12:28.065] iteration 26001: total_loss: 0.559498, loss_sup: 0.035222, loss_mps: 0.167183, loss_cps: 0.357093
[14:12:28.213] iteration 26002: total_loss: 0.307741, loss_sup: 0.002562, loss_mps: 0.106268, loss_cps: 0.198912
[14:12:28.359] iteration 26003: total_loss: 0.555569, loss_sup: 0.199138, loss_mps: 0.126673, loss_cps: 0.229758
[14:12:28.505] iteration 26004: total_loss: 0.242124, loss_sup: 0.017282, loss_mps: 0.082961, loss_cps: 0.141881
[14:12:28.650] iteration 26005: total_loss: 0.224347, loss_sup: 0.042825, loss_mps: 0.067157, loss_cps: 0.114365
[14:12:28.797] iteration 26006: total_loss: 0.404825, loss_sup: 0.074669, loss_mps: 0.118764, loss_cps: 0.211392
[14:12:28.946] iteration 26007: total_loss: 0.355459, loss_sup: 0.087187, loss_mps: 0.101521, loss_cps: 0.166751
[14:12:29.091] iteration 26008: total_loss: 0.274319, loss_sup: 0.002319, loss_mps: 0.097881, loss_cps: 0.174120
[14:12:29.240] iteration 26009: total_loss: 0.283192, loss_sup: 0.023555, loss_mps: 0.090666, loss_cps: 0.168971
[14:12:29.385] iteration 26010: total_loss: 0.766167, loss_sup: 0.113874, loss_mps: 0.206493, loss_cps: 0.445800
[14:12:29.531] iteration 26011: total_loss: 0.347614, loss_sup: 0.068130, loss_mps: 0.104962, loss_cps: 0.174522
[14:12:29.676] iteration 26012: total_loss: 0.461189, loss_sup: 0.012661, loss_mps: 0.147319, loss_cps: 0.301209
[14:12:29.822] iteration 26013: total_loss: 0.349061, loss_sup: 0.012509, loss_mps: 0.121470, loss_cps: 0.215082
[14:12:29.970] iteration 26014: total_loss: 0.245393, loss_sup: 0.025144, loss_mps: 0.079168, loss_cps: 0.141081
[14:12:30.115] iteration 26015: total_loss: 0.312646, loss_sup: 0.068072, loss_mps: 0.087833, loss_cps: 0.156740
[14:12:30.263] iteration 26016: total_loss: 0.261305, loss_sup: 0.023265, loss_mps: 0.092057, loss_cps: 0.145984
[14:12:30.410] iteration 26017: total_loss: 0.299613, loss_sup: 0.029420, loss_mps: 0.100939, loss_cps: 0.169255
[14:12:30.560] iteration 26018: total_loss: 0.324033, loss_sup: 0.079232, loss_mps: 0.092679, loss_cps: 0.152122
[14:12:30.706] iteration 26019: total_loss: 0.315239, loss_sup: 0.003927, loss_mps: 0.102716, loss_cps: 0.208596
[14:12:30.852] iteration 26020: total_loss: 0.166375, loss_sup: 0.007480, loss_mps: 0.061840, loss_cps: 0.097055
[14:12:31.001] iteration 26021: total_loss: 0.506445, loss_sup: 0.054772, loss_mps: 0.163363, loss_cps: 0.288310
[14:12:31.147] iteration 26022: total_loss: 0.396460, loss_sup: 0.078148, loss_mps: 0.119481, loss_cps: 0.198831
[14:12:31.294] iteration 26023: total_loss: 0.382287, loss_sup: 0.018220, loss_mps: 0.118107, loss_cps: 0.245960
[14:12:31.439] iteration 26024: total_loss: 0.300214, loss_sup: 0.048127, loss_mps: 0.093788, loss_cps: 0.158300
[14:12:31.585] iteration 26025: total_loss: 0.583889, loss_sup: 0.199244, loss_mps: 0.127955, loss_cps: 0.256689
[14:12:31.730] iteration 26026: total_loss: 0.275030, loss_sup: 0.042595, loss_mps: 0.089730, loss_cps: 0.142705
[14:12:31.876] iteration 26027: total_loss: 0.287123, loss_sup: 0.034437, loss_mps: 0.092855, loss_cps: 0.159831
[14:12:32.021] iteration 26028: total_loss: 0.355226, loss_sup: 0.021921, loss_mps: 0.117382, loss_cps: 0.215923
[14:12:32.166] iteration 26029: total_loss: 0.293348, loss_sup: 0.010498, loss_mps: 0.099982, loss_cps: 0.182867
[14:12:32.314] iteration 26030: total_loss: 0.215350, loss_sup: 0.039550, loss_mps: 0.066508, loss_cps: 0.109292
[14:12:32.459] iteration 26031: total_loss: 0.345680, loss_sup: 0.007134, loss_mps: 0.109441, loss_cps: 0.229105
[14:12:32.605] iteration 26032: total_loss: 0.438287, loss_sup: 0.011762, loss_mps: 0.150572, loss_cps: 0.275954
[14:12:32.751] iteration 26033: total_loss: 0.754530, loss_sup: 0.011846, loss_mps: 0.234346, loss_cps: 0.508339
[14:12:32.898] iteration 26034: total_loss: 0.369524, loss_sup: 0.018149, loss_mps: 0.127703, loss_cps: 0.223673
[14:12:33.044] iteration 26035: total_loss: 0.241900, loss_sup: 0.052862, loss_mps: 0.068733, loss_cps: 0.120305
[14:12:33.190] iteration 26036: total_loss: 0.450983, loss_sup: 0.047961, loss_mps: 0.141096, loss_cps: 0.261926
[14:12:33.339] iteration 26037: total_loss: 0.855242, loss_sup: 0.008111, loss_mps: 0.252444, loss_cps: 0.594687
[14:12:33.485] iteration 26038: total_loss: 0.429142, loss_sup: 0.044337, loss_mps: 0.126981, loss_cps: 0.257825
[14:12:33.631] iteration 26039: total_loss: 0.993615, loss_sup: 0.086401, loss_mps: 0.269669, loss_cps: 0.637544
[14:12:33.784] iteration 26040: total_loss: 0.981419, loss_sup: 0.016596, loss_mps: 0.299600, loss_cps: 0.665223
[14:12:33.930] iteration 26041: total_loss: 0.352898, loss_sup: 0.018210, loss_mps: 0.113558, loss_cps: 0.221130
[14:12:34.077] iteration 26042: total_loss: 0.448331, loss_sup: 0.142835, loss_mps: 0.103448, loss_cps: 0.202048
[14:12:34.223] iteration 26043: total_loss: 0.511818, loss_sup: 0.243525, loss_mps: 0.092315, loss_cps: 0.175978
[14:12:34.370] iteration 26044: total_loss: 0.440331, loss_sup: 0.046699, loss_mps: 0.132423, loss_cps: 0.261209
[14:12:34.516] iteration 26045: total_loss: 0.451317, loss_sup: 0.113008, loss_mps: 0.117351, loss_cps: 0.220958
[14:12:34.664] iteration 26046: total_loss: 0.390651, loss_sup: 0.110592, loss_mps: 0.098762, loss_cps: 0.181297
[14:12:34.811] iteration 26047: total_loss: 0.402141, loss_sup: 0.068968, loss_mps: 0.114883, loss_cps: 0.218290
[14:12:34.957] iteration 26048: total_loss: 0.464237, loss_sup: 0.127710, loss_mps: 0.120573, loss_cps: 0.215953
[14:12:35.103] iteration 26049: total_loss: 0.227460, loss_sup: 0.004247, loss_mps: 0.083362, loss_cps: 0.139852
[14:12:35.249] iteration 26050: total_loss: 0.443351, loss_sup: 0.078122, loss_mps: 0.123990, loss_cps: 0.241239
[14:12:35.395] iteration 26051: total_loss: 0.763141, loss_sup: 0.121242, loss_mps: 0.193980, loss_cps: 0.447920
[14:12:35.541] iteration 26052: total_loss: 0.605756, loss_sup: 0.015259, loss_mps: 0.182182, loss_cps: 0.408315
[14:12:35.686] iteration 26053: total_loss: 0.295962, loss_sup: 0.004057, loss_mps: 0.101811, loss_cps: 0.190094
[14:12:35.831] iteration 26054: total_loss: 0.292909, loss_sup: 0.046569, loss_mps: 0.084483, loss_cps: 0.161858
[14:12:35.979] iteration 26055: total_loss: 0.448373, loss_sup: 0.069340, loss_mps: 0.132750, loss_cps: 0.246283
[14:12:36.124] iteration 26056: total_loss: 0.749025, loss_sup: 0.314737, loss_mps: 0.142073, loss_cps: 0.292215
[14:12:36.270] iteration 26057: total_loss: 0.204046, loss_sup: 0.002176, loss_mps: 0.076442, loss_cps: 0.125428
[14:12:36.417] iteration 26058: total_loss: 0.556602, loss_sup: 0.137125, loss_mps: 0.139905, loss_cps: 0.279571
[14:12:36.562] iteration 26059: total_loss: 0.247615, loss_sup: 0.031751, loss_mps: 0.080671, loss_cps: 0.135193
[14:12:36.708] iteration 26060: total_loss: 0.310835, loss_sup: 0.024716, loss_mps: 0.107686, loss_cps: 0.178434
[14:12:36.856] iteration 26061: total_loss: 0.228415, loss_sup: 0.016322, loss_mps: 0.078345, loss_cps: 0.133747
[14:12:37.015] iteration 26062: total_loss: 0.345599, loss_sup: 0.047683, loss_mps: 0.100188, loss_cps: 0.197728
[14:12:37.161] iteration 26063: total_loss: 0.273241, loss_sup: 0.011993, loss_mps: 0.098295, loss_cps: 0.162953
[14:12:37.307] iteration 26064: total_loss: 0.399901, loss_sup: 0.005044, loss_mps: 0.129950, loss_cps: 0.264907
[14:12:37.458] iteration 26065: total_loss: 0.312202, loss_sup: 0.031385, loss_mps: 0.095168, loss_cps: 0.185650
[14:12:37.604] iteration 26066: total_loss: 0.238978, loss_sup: 0.025379, loss_mps: 0.084836, loss_cps: 0.128763
[14:12:37.750] iteration 26067: total_loss: 0.381250, loss_sup: 0.102893, loss_mps: 0.105473, loss_cps: 0.172885
[14:12:37.898] iteration 26068: total_loss: 0.314804, loss_sup: 0.061872, loss_mps: 0.089277, loss_cps: 0.163655
[14:12:38.044] iteration 26069: total_loss: 0.436496, loss_sup: 0.041986, loss_mps: 0.125038, loss_cps: 0.269472
[14:12:38.189] iteration 26070: total_loss: 0.225000, loss_sup: 0.016606, loss_mps: 0.079729, loss_cps: 0.128665
[14:12:38.336] iteration 26071: total_loss: 0.552360, loss_sup: 0.096703, loss_mps: 0.143781, loss_cps: 0.311877
[14:12:38.485] iteration 26072: total_loss: 0.231352, loss_sup: 0.011774, loss_mps: 0.076465, loss_cps: 0.143112
[14:12:38.630] iteration 26073: total_loss: 0.647223, loss_sup: 0.181712, loss_mps: 0.157662, loss_cps: 0.307849
[14:12:38.777] iteration 26074: total_loss: 0.292590, loss_sup: 0.008344, loss_mps: 0.097514, loss_cps: 0.186732
[14:12:38.924] iteration 26075: total_loss: 0.396823, loss_sup: 0.010675, loss_mps: 0.126927, loss_cps: 0.259221
[14:12:39.070] iteration 26076: total_loss: 0.606211, loss_sup: 0.109692, loss_mps: 0.167592, loss_cps: 0.328927
[14:12:39.217] iteration 26077: total_loss: 0.349198, loss_sup: 0.152924, loss_mps: 0.072386, loss_cps: 0.123889
[14:12:39.367] iteration 26078: total_loss: 0.270105, loss_sup: 0.001609, loss_mps: 0.100379, loss_cps: 0.168118
[14:12:39.514] iteration 26079: total_loss: 0.395241, loss_sup: 0.046983, loss_mps: 0.123830, loss_cps: 0.224428
[14:12:39.663] iteration 26080: total_loss: 0.317025, loss_sup: 0.078623, loss_mps: 0.089779, loss_cps: 0.148622
[14:12:39.809] iteration 26081: total_loss: 0.425690, loss_sup: 0.141719, loss_mps: 0.110774, loss_cps: 0.173196
[14:12:39.957] iteration 26082: total_loss: 0.281258, loss_sup: 0.012942, loss_mps: 0.090291, loss_cps: 0.178024
[14:12:40.103] iteration 26083: total_loss: 0.273732, loss_sup: 0.040124, loss_mps: 0.088067, loss_cps: 0.145540
[14:12:40.248] iteration 26084: total_loss: 0.972099, loss_sup: 0.008364, loss_mps: 0.295713, loss_cps: 0.668022
[14:12:40.395] iteration 26085: total_loss: 0.681293, loss_sup: 0.070971, loss_mps: 0.184413, loss_cps: 0.425908
[14:12:40.543] iteration 26086: total_loss: 0.240237, loss_sup: 0.021115, loss_mps: 0.084800, loss_cps: 0.134322
[14:12:40.693] iteration 26087: total_loss: 0.256192, loss_sup: 0.055811, loss_mps: 0.071537, loss_cps: 0.128844
[14:12:40.839] iteration 26088: total_loss: 0.305253, loss_sup: 0.013109, loss_mps: 0.097895, loss_cps: 0.194249
[14:12:40.986] iteration 26089: total_loss: 0.265904, loss_sup: 0.006157, loss_mps: 0.094698, loss_cps: 0.165049
[14:12:41.131] iteration 26090: total_loss: 0.313568, loss_sup: 0.019118, loss_mps: 0.105031, loss_cps: 0.189419
[14:12:41.280] iteration 26091: total_loss: 0.229853, loss_sup: 0.030453, loss_mps: 0.074134, loss_cps: 0.125266
[14:12:41.425] iteration 26092: total_loss: 0.249777, loss_sup: 0.026463, loss_mps: 0.082353, loss_cps: 0.140962
[14:12:41.571] iteration 26093: total_loss: 0.886589, loss_sup: 0.318164, loss_mps: 0.180857, loss_cps: 0.387569
[14:12:41.718] iteration 26094: total_loss: 0.351154, loss_sup: 0.078299, loss_mps: 0.100283, loss_cps: 0.172572
[14:12:41.864] iteration 26095: total_loss: 0.602713, loss_sup: 0.008841, loss_mps: 0.190769, loss_cps: 0.403104
[14:12:42.010] iteration 26096: total_loss: 0.322759, loss_sup: 0.046809, loss_mps: 0.092988, loss_cps: 0.182962
[14:12:42.156] iteration 26097: total_loss: 0.452188, loss_sup: 0.088138, loss_mps: 0.131695, loss_cps: 0.232356
[14:12:42.302] iteration 26098: total_loss: 0.315732, loss_sup: 0.076549, loss_mps: 0.089271, loss_cps: 0.149911
[14:12:42.448] iteration 26099: total_loss: 0.347114, loss_sup: 0.055748, loss_mps: 0.100190, loss_cps: 0.191177
[14:12:42.594] iteration 26100: total_loss: 0.397576, loss_sup: 0.015445, loss_mps: 0.136695, loss_cps: 0.245436
[14:12:42.594] Evaluation Started ==>
[14:12:53.921] ==> valid iteration 26100: unet metrics: {'dc': 0.6623814500313644, 'jc': 0.5471717414629433, 'pre': 0.8061378120037144, 'hd': 5.344077485915098}, ynet metrics: {'dc': 0.6070324622432838, 'jc': 0.4956011860901724, 'pre': 0.8088463214201117, 'hd': 5.401323293424848}.
[14:12:53.924] Evaluation Finished!⏹️
[14:12:54.075] iteration 26101: total_loss: 0.434303, loss_sup: 0.094036, loss_mps: 0.123737, loss_cps: 0.216531
[14:12:54.222] iteration 26102: total_loss: 0.465683, loss_sup: 0.053812, loss_mps: 0.132918, loss_cps: 0.278953
[14:12:54.369] iteration 26103: total_loss: 0.441126, loss_sup: 0.041090, loss_mps: 0.136112, loss_cps: 0.263924
[14:12:54.514] iteration 26104: total_loss: 0.584262, loss_sup: 0.128152, loss_mps: 0.148516, loss_cps: 0.307594
[14:12:54.659] iteration 26105: total_loss: 0.302268, loss_sup: 0.005622, loss_mps: 0.104246, loss_cps: 0.192401
[14:12:54.804] iteration 26106: total_loss: 0.394747, loss_sup: 0.086109, loss_mps: 0.101311, loss_cps: 0.207328
[14:12:54.950] iteration 26107: total_loss: 0.348382, loss_sup: 0.009786, loss_mps: 0.115082, loss_cps: 0.223515
[14:12:55.097] iteration 26108: total_loss: 0.536152, loss_sup: 0.108667, loss_mps: 0.134202, loss_cps: 0.293283
[14:12:55.243] iteration 26109: total_loss: 0.346825, loss_sup: 0.062440, loss_mps: 0.098169, loss_cps: 0.186216
[14:12:55.390] iteration 26110: total_loss: 0.380726, loss_sup: 0.014640, loss_mps: 0.125209, loss_cps: 0.240877
[14:12:55.536] iteration 26111: total_loss: 0.500622, loss_sup: 0.041991, loss_mps: 0.153137, loss_cps: 0.305494
[14:12:55.681] iteration 26112: total_loss: 0.340029, loss_sup: 0.071186, loss_mps: 0.098317, loss_cps: 0.170525
[14:12:55.827] iteration 26113: total_loss: 0.491784, loss_sup: 0.073728, loss_mps: 0.139172, loss_cps: 0.278883
[14:12:55.972] iteration 26114: total_loss: 0.231395, loss_sup: 0.009570, loss_mps: 0.082476, loss_cps: 0.139348
[14:12:56.118] iteration 26115: total_loss: 0.162238, loss_sup: 0.018263, loss_mps: 0.056579, loss_cps: 0.087396
[14:12:56.265] iteration 26116: total_loss: 0.816138, loss_sup: 0.114949, loss_mps: 0.219260, loss_cps: 0.481929
[14:12:56.411] iteration 26117: total_loss: 0.509772, loss_sup: 0.100086, loss_mps: 0.141956, loss_cps: 0.267730
[14:12:56.560] iteration 26118: total_loss: 0.572827, loss_sup: 0.094017, loss_mps: 0.166878, loss_cps: 0.311931
[14:12:56.706] iteration 26119: total_loss: 0.390056, loss_sup: 0.030051, loss_mps: 0.125321, loss_cps: 0.234684
[14:12:56.851] iteration 26120: total_loss: 0.366288, loss_sup: 0.151233, loss_mps: 0.077979, loss_cps: 0.137076
[14:12:56.999] iteration 26121: total_loss: 0.250552, loss_sup: 0.052352, loss_mps: 0.075529, loss_cps: 0.122670
[14:12:57.144] iteration 26122: total_loss: 0.332543, loss_sup: 0.045110, loss_mps: 0.098162, loss_cps: 0.189272
[14:12:57.289] iteration 26123: total_loss: 0.756759, loss_sup: 0.065759, loss_mps: 0.216920, loss_cps: 0.474080
[14:12:57.435] iteration 26124: total_loss: 0.525778, loss_sup: 0.310592, loss_mps: 0.078793, loss_cps: 0.136392
[14:12:57.581] iteration 26125: total_loss: 0.377286, loss_sup: 0.023710, loss_mps: 0.122137, loss_cps: 0.231439
[14:12:57.727] iteration 26126: total_loss: 0.383381, loss_sup: 0.059096, loss_mps: 0.111136, loss_cps: 0.213149
[14:12:57.875] iteration 26127: total_loss: 0.411417, loss_sup: 0.021193, loss_mps: 0.132014, loss_cps: 0.258210
[14:12:58.021] iteration 26128: total_loss: 0.316121, loss_sup: 0.024766, loss_mps: 0.106465, loss_cps: 0.184890
[14:12:58.167] iteration 26129: total_loss: 0.340108, loss_sup: 0.011352, loss_mps: 0.111468, loss_cps: 0.217288
[14:12:58.315] iteration 26130: total_loss: 0.881830, loss_sup: 0.018969, loss_mps: 0.258122, loss_cps: 0.604738
[14:12:58.462] iteration 26131: total_loss: 0.207978, loss_sup: 0.007070, loss_mps: 0.074331, loss_cps: 0.126577
[14:12:58.607] iteration 26132: total_loss: 0.225482, loss_sup: 0.000808, loss_mps: 0.080963, loss_cps: 0.143712
[14:12:58.757] iteration 26133: total_loss: 0.358176, loss_sup: 0.097250, loss_mps: 0.095551, loss_cps: 0.165376
[14:12:58.902] iteration 26134: total_loss: 0.630197, loss_sup: 0.089421, loss_mps: 0.163282, loss_cps: 0.377494
[14:12:59.049] iteration 26135: total_loss: 0.240891, loss_sup: 0.006239, loss_mps: 0.086825, loss_cps: 0.147827
[14:12:59.194] iteration 26136: total_loss: 0.328266, loss_sup: 0.012761, loss_mps: 0.117265, loss_cps: 0.198240
[14:12:59.340] iteration 26137: total_loss: 0.654071, loss_sup: 0.100337, loss_mps: 0.183128, loss_cps: 0.370606
[14:12:59.485] iteration 26138: total_loss: 0.282231, loss_sup: 0.005028, loss_mps: 0.099311, loss_cps: 0.177893
[14:12:59.631] iteration 26139: total_loss: 0.466086, loss_sup: 0.096131, loss_mps: 0.120525, loss_cps: 0.249430
[14:12:59.781] iteration 26140: total_loss: 0.282167, loss_sup: 0.010110, loss_mps: 0.105955, loss_cps: 0.166102
[14:12:59.927] iteration 26141: total_loss: 0.772195, loss_sup: 0.467468, loss_mps: 0.097826, loss_cps: 0.206901
[14:13:00.074] iteration 26142: total_loss: 0.326670, loss_sup: 0.014650, loss_mps: 0.105336, loss_cps: 0.206684
[14:13:00.219] iteration 26143: total_loss: 0.500503, loss_sup: 0.058946, loss_mps: 0.149255, loss_cps: 0.292301
[14:13:00.364] iteration 26144: total_loss: 0.326928, loss_sup: 0.058222, loss_mps: 0.096768, loss_cps: 0.171938
[14:13:00.510] iteration 26145: total_loss: 0.548574, loss_sup: 0.158233, loss_mps: 0.127034, loss_cps: 0.263308
[14:13:00.655] iteration 26146: total_loss: 0.602296, loss_sup: 0.145557, loss_mps: 0.157040, loss_cps: 0.299699
[14:13:00.801] iteration 26147: total_loss: 0.604382, loss_sup: 0.076893, loss_mps: 0.171660, loss_cps: 0.355829
[14:13:00.949] iteration 26148: total_loss: 0.432334, loss_sup: 0.084994, loss_mps: 0.119451, loss_cps: 0.227889
[14:13:01.095] iteration 26149: total_loss: 0.300145, loss_sup: 0.023878, loss_mps: 0.096862, loss_cps: 0.179405
[14:13:01.241] iteration 26150: total_loss: 1.016820, loss_sup: 0.386540, loss_mps: 0.201431, loss_cps: 0.428849
[14:13:01.386] iteration 26151: total_loss: 0.280381, loss_sup: 0.002572, loss_mps: 0.103308, loss_cps: 0.174501
[14:13:01.532] iteration 26152: total_loss: 0.487346, loss_sup: 0.073646, loss_mps: 0.138598, loss_cps: 0.275102
[14:13:01.677] iteration 26153: total_loss: 0.673810, loss_sup: 0.132779, loss_mps: 0.178133, loss_cps: 0.362898
[14:13:01.823] iteration 26154: total_loss: 0.295411, loss_sup: 0.010284, loss_mps: 0.105138, loss_cps: 0.179989
[14:13:01.968] iteration 26155: total_loss: 0.414629, loss_sup: 0.023881, loss_mps: 0.135232, loss_cps: 0.255516
[14:13:02.113] iteration 26156: total_loss: 0.573857, loss_sup: 0.025598, loss_mps: 0.183995, loss_cps: 0.364263
[14:13:02.260] iteration 26157: total_loss: 0.377950, loss_sup: 0.018269, loss_mps: 0.121571, loss_cps: 0.238110
[14:13:02.406] iteration 26158: total_loss: 0.299269, loss_sup: 0.035742, loss_mps: 0.095196, loss_cps: 0.168330
[14:13:02.554] iteration 26159: total_loss: 0.393854, loss_sup: 0.148549, loss_mps: 0.092637, loss_cps: 0.152668
[14:13:02.702] iteration 26160: total_loss: 0.268169, loss_sup: 0.006705, loss_mps: 0.095534, loss_cps: 0.165930
[14:13:02.849] iteration 26161: total_loss: 0.359438, loss_sup: 0.015871, loss_mps: 0.120181, loss_cps: 0.223386
[14:13:02.995] iteration 26162: total_loss: 0.445450, loss_sup: 0.017712, loss_mps: 0.142255, loss_cps: 0.285483
[14:13:03.141] iteration 26163: total_loss: 0.440502, loss_sup: 0.102091, loss_mps: 0.122717, loss_cps: 0.215694
[14:13:03.289] iteration 26164: total_loss: 0.416083, loss_sup: 0.019247, loss_mps: 0.137172, loss_cps: 0.259664
[14:13:03.435] iteration 26165: total_loss: 0.263325, loss_sup: 0.001042, loss_mps: 0.092090, loss_cps: 0.170192
[14:13:03.581] iteration 26166: total_loss: 0.372973, loss_sup: 0.027529, loss_mps: 0.123010, loss_cps: 0.222435
[14:13:03.730] iteration 26167: total_loss: 0.376672, loss_sup: 0.038504, loss_mps: 0.121136, loss_cps: 0.217031
[14:13:03.882] iteration 26168: total_loss: 0.341576, loss_sup: 0.013362, loss_mps: 0.109489, loss_cps: 0.218725
[14:13:04.030] iteration 26169: total_loss: 0.876567, loss_sup: 0.069954, loss_mps: 0.256243, loss_cps: 0.550371
[14:13:04.176] iteration 26170: total_loss: 0.563707, loss_sup: 0.237596, loss_mps: 0.110594, loss_cps: 0.215517
[14:13:04.321] iteration 26171: total_loss: 0.267854, loss_sup: 0.072530, loss_mps: 0.073206, loss_cps: 0.122119
[14:13:04.468] iteration 26172: total_loss: 0.324469, loss_sup: 0.030758, loss_mps: 0.103866, loss_cps: 0.189845
[14:13:04.615] iteration 26173: total_loss: 0.338559, loss_sup: 0.026015, loss_mps: 0.113121, loss_cps: 0.199422
[14:13:04.761] iteration 26174: total_loss: 0.286912, loss_sup: 0.009444, loss_mps: 0.096076, loss_cps: 0.181393
[14:13:04.907] iteration 26175: total_loss: 0.333349, loss_sup: 0.021372, loss_mps: 0.108929, loss_cps: 0.203048
[14:13:05.056] iteration 26176: total_loss: 0.248450, loss_sup: 0.006071, loss_mps: 0.085981, loss_cps: 0.156397
[14:13:05.201] iteration 26177: total_loss: 0.244995, loss_sup: 0.001909, loss_mps: 0.083896, loss_cps: 0.159190
[14:13:05.347] iteration 26178: total_loss: 0.389905, loss_sup: 0.070221, loss_mps: 0.110028, loss_cps: 0.209656
[14:13:05.493] iteration 26179: total_loss: 0.436364, loss_sup: 0.070957, loss_mps: 0.119562, loss_cps: 0.245844
[14:13:05.639] iteration 26180: total_loss: 0.208398, loss_sup: 0.020117, loss_mps: 0.070429, loss_cps: 0.117851
[14:13:05.785] iteration 26181: total_loss: 0.356324, loss_sup: 0.025542, loss_mps: 0.110499, loss_cps: 0.220283
[14:13:05.932] iteration 26182: total_loss: 0.374044, loss_sup: 0.020848, loss_mps: 0.123199, loss_cps: 0.229997
[14:13:06.078] iteration 26183: total_loss: 0.406971, loss_sup: 0.154806, loss_mps: 0.096287, loss_cps: 0.155879
[14:13:06.224] iteration 26184: total_loss: 0.351173, loss_sup: 0.066103, loss_mps: 0.100649, loss_cps: 0.184421
[14:13:06.370] iteration 26185: total_loss: 0.588516, loss_sup: 0.108558, loss_mps: 0.163012, loss_cps: 0.316945
[14:13:06.516] iteration 26186: total_loss: 0.447566, loss_sup: 0.009112, loss_mps: 0.137462, loss_cps: 0.300993
[14:13:06.662] iteration 26187: total_loss: 0.368780, loss_sup: 0.001211, loss_mps: 0.128611, loss_cps: 0.238958
[14:13:06.808] iteration 26188: total_loss: 0.242418, loss_sup: 0.034210, loss_mps: 0.079750, loss_cps: 0.128458
[14:13:06.955] iteration 26189: total_loss: 0.334991, loss_sup: 0.012074, loss_mps: 0.115367, loss_cps: 0.207549
[14:13:07.102] iteration 26190: total_loss: 0.211713, loss_sup: 0.009560, loss_mps: 0.074325, loss_cps: 0.127828
[14:13:07.249] iteration 26191: total_loss: 0.324826, loss_sup: 0.090871, loss_mps: 0.084878, loss_cps: 0.149077
[14:13:07.395] iteration 26192: total_loss: 0.389615, loss_sup: 0.031387, loss_mps: 0.126306, loss_cps: 0.231922
[14:13:07.544] iteration 26193: total_loss: 0.295136, loss_sup: 0.014733, loss_mps: 0.101048, loss_cps: 0.179355
[14:13:07.693] iteration 26194: total_loss: 0.347044, loss_sup: 0.034497, loss_mps: 0.103350, loss_cps: 0.209196
[14:13:07.841] iteration 26195: total_loss: 0.426784, loss_sup: 0.137204, loss_mps: 0.107370, loss_cps: 0.182210
[14:13:07.988] iteration 26196: total_loss: 0.441332, loss_sup: 0.056805, loss_mps: 0.152895, loss_cps: 0.231632
[14:13:08.135] iteration 26197: total_loss: 0.567930, loss_sup: 0.147882, loss_mps: 0.150155, loss_cps: 0.269893
[14:13:08.281] iteration 26198: total_loss: 0.215435, loss_sup: 0.009455, loss_mps: 0.075322, loss_cps: 0.130658
[14:13:08.435] iteration 26199: total_loss: 0.296954, loss_sup: 0.013671, loss_mps: 0.097432, loss_cps: 0.185851
[14:13:08.582] iteration 26200: total_loss: 0.359709, loss_sup: 0.039304, loss_mps: 0.112829, loss_cps: 0.207577
[14:13:08.583] Evaluation Started ==>
[14:13:19.924] ==> valid iteration 26200: unet metrics: {'dc': 0.6430431064831966, 'jc': 0.5267124459868471, 'pre': 0.803087666194953, 'hd': 5.432154550171857}, ynet metrics: {'dc': 0.5994367775869014, 'jc': 0.48811410711093356, 'pre': 0.8011631330932898, 'hd': 5.333363797280888}.
[14:13:19.926] Evaluation Finished!⏹️
[14:13:20.077] iteration 26201: total_loss: 0.474714, loss_sup: 0.036871, loss_mps: 0.153124, loss_cps: 0.284719
[14:13:20.224] iteration 26202: total_loss: 0.983759, loss_sup: 0.466437, loss_mps: 0.190495, loss_cps: 0.326827
[14:13:20.370] iteration 26203: total_loss: 0.297865, loss_sup: 0.024936, loss_mps: 0.092241, loss_cps: 0.180688
[14:13:20.516] iteration 26204: total_loss: 0.154494, loss_sup: 0.013444, loss_mps: 0.055214, loss_cps: 0.085836
[14:13:20.662] iteration 26205: total_loss: 0.209083, loss_sup: 0.002159, loss_mps: 0.075392, loss_cps: 0.131532
[14:13:20.808] iteration 26206: total_loss: 0.478630, loss_sup: 0.012589, loss_mps: 0.151953, loss_cps: 0.314088
[14:13:20.953] iteration 26207: total_loss: 0.363746, loss_sup: 0.044920, loss_mps: 0.104725, loss_cps: 0.214101
[14:13:21.099] iteration 26208: total_loss: 0.166782, loss_sup: 0.005061, loss_mps: 0.062190, loss_cps: 0.099532
[14:13:21.245] iteration 26209: total_loss: 0.209802, loss_sup: 0.003203, loss_mps: 0.081276, loss_cps: 0.125323
[14:13:21.390] iteration 26210: total_loss: 0.738340, loss_sup: 0.070129, loss_mps: 0.213129, loss_cps: 0.455083
[14:13:21.535] iteration 26211: total_loss: 0.320662, loss_sup: 0.042892, loss_mps: 0.100835, loss_cps: 0.176935
[14:13:21.682] iteration 26212: total_loss: 0.341706, loss_sup: 0.053600, loss_mps: 0.105506, loss_cps: 0.182600
[14:13:21.827] iteration 26213: total_loss: 0.610206, loss_sup: 0.043577, loss_mps: 0.196923, loss_cps: 0.369706
[14:13:21.973] iteration 26214: total_loss: 0.325247, loss_sup: 0.063222, loss_mps: 0.092856, loss_cps: 0.169169
[14:13:22.121] iteration 26215: total_loss: 0.246050, loss_sup: 0.028996, loss_mps: 0.081234, loss_cps: 0.135820
[14:13:22.268] iteration 26216: total_loss: 0.248407, loss_sup: 0.013366, loss_mps: 0.089803, loss_cps: 0.145238
[14:13:22.414] iteration 26217: total_loss: 0.439887, loss_sup: 0.133785, loss_mps: 0.108061, loss_cps: 0.198041
[14:13:22.561] iteration 26218: total_loss: 0.651818, loss_sup: 0.008841, loss_mps: 0.189554, loss_cps: 0.453423
[14:13:22.706] iteration 26219: total_loss: 0.249097, loss_sup: 0.027122, loss_mps: 0.077146, loss_cps: 0.144828
[14:13:22.852] iteration 26220: total_loss: 0.145458, loss_sup: 0.000592, loss_mps: 0.057111, loss_cps: 0.087755
[14:13:22.997] iteration 26221: total_loss: 0.430489, loss_sup: 0.008648, loss_mps: 0.152214, loss_cps: 0.269627
[14:13:23.142] iteration 26222: total_loss: 0.391428, loss_sup: 0.058423, loss_mps: 0.119133, loss_cps: 0.213872
[14:13:23.288] iteration 26223: total_loss: 0.227016, loss_sup: 0.015392, loss_mps: 0.075449, loss_cps: 0.136174
[14:13:23.434] iteration 26224: total_loss: 0.384607, loss_sup: 0.027427, loss_mps: 0.123334, loss_cps: 0.233846
[14:13:23.582] iteration 26225: total_loss: 0.334943, loss_sup: 0.037331, loss_mps: 0.107644, loss_cps: 0.189968
[14:13:23.727] iteration 26226: total_loss: 0.465170, loss_sup: 0.143755, loss_mps: 0.116383, loss_cps: 0.205032
[14:13:23.873] iteration 26227: total_loss: 0.301965, loss_sup: 0.008918, loss_mps: 0.102733, loss_cps: 0.190315
[14:13:24.019] iteration 26228: total_loss: 0.208545, loss_sup: 0.002282, loss_mps: 0.076768, loss_cps: 0.129495
[14:13:24.170] iteration 26229: total_loss: 0.229283, loss_sup: 0.014081, loss_mps: 0.082081, loss_cps: 0.133121
[14:13:24.317] iteration 26230: total_loss: 0.299297, loss_sup: 0.038766, loss_mps: 0.092651, loss_cps: 0.167880
[14:13:24.467] iteration 26231: total_loss: 0.329552, loss_sup: 0.005054, loss_mps: 0.112751, loss_cps: 0.211746
[14:13:24.613] iteration 26232: total_loss: 0.561995, loss_sup: 0.019018, loss_mps: 0.180719, loss_cps: 0.362257
[14:13:24.760] iteration 26233: total_loss: 0.364837, loss_sup: 0.018770, loss_mps: 0.111165, loss_cps: 0.234902
[14:13:24.906] iteration 26234: total_loss: 0.336293, loss_sup: 0.016380, loss_mps: 0.109694, loss_cps: 0.210220
[14:13:25.055] iteration 26235: total_loss: 0.236207, loss_sup: 0.022757, loss_mps: 0.074782, loss_cps: 0.138668
[14:13:25.201] iteration 26236: total_loss: 0.190051, loss_sup: 0.044824, loss_mps: 0.055983, loss_cps: 0.089244
[14:13:25.348] iteration 26237: total_loss: 0.307036, loss_sup: 0.006449, loss_mps: 0.097847, loss_cps: 0.202740
[14:13:25.493] iteration 26238: total_loss: 0.245543, loss_sup: 0.007863, loss_mps: 0.083274, loss_cps: 0.154406
[14:13:25.639] iteration 26239: total_loss: 0.415685, loss_sup: 0.020668, loss_mps: 0.138868, loss_cps: 0.256150
[14:13:25.789] iteration 26240: total_loss: 0.280118, loss_sup: 0.030096, loss_mps: 0.088752, loss_cps: 0.161270
[14:13:25.936] iteration 26241: total_loss: 0.407914, loss_sup: 0.044378, loss_mps: 0.122059, loss_cps: 0.241476
[14:13:26.085] iteration 26242: total_loss: 0.290958, loss_sup: 0.049280, loss_mps: 0.083658, loss_cps: 0.158019
[14:13:26.231] iteration 26243: total_loss: 0.639814, loss_sup: 0.076198, loss_mps: 0.185747, loss_cps: 0.377869
[14:13:26.377] iteration 26244: total_loss: 0.237753, loss_sup: 0.029059, loss_mps: 0.075115, loss_cps: 0.133579
[14:13:26.523] iteration 26245: total_loss: 0.377103, loss_sup: 0.029908, loss_mps: 0.120969, loss_cps: 0.226225
[14:13:26.669] iteration 26246: total_loss: 0.336644, loss_sup: 0.038230, loss_mps: 0.100000, loss_cps: 0.198414
[14:13:26.815] iteration 26247: total_loss: 0.496711, loss_sup: 0.079358, loss_mps: 0.134780, loss_cps: 0.282573
[14:13:26.961] iteration 26248: total_loss: 0.344813, loss_sup: 0.019638, loss_mps: 0.109205, loss_cps: 0.215970
[14:13:27.107] iteration 26249: total_loss: 0.309485, loss_sup: 0.034122, loss_mps: 0.096040, loss_cps: 0.179323
[14:13:27.252] iteration 26250: total_loss: 0.297008, loss_sup: 0.068124, loss_mps: 0.082816, loss_cps: 0.146068
[14:13:27.399] iteration 26251: total_loss: 0.456809, loss_sup: 0.042951, loss_mps: 0.133006, loss_cps: 0.280851
[14:13:27.544] iteration 26252: total_loss: 0.291138, loss_sup: 0.077078, loss_mps: 0.076949, loss_cps: 0.137111
[14:13:27.691] iteration 26253: total_loss: 0.212769, loss_sup: 0.008795, loss_mps: 0.076754, loss_cps: 0.127220
[14:13:27.837] iteration 26254: total_loss: 0.659737, loss_sup: 0.243548, loss_mps: 0.131833, loss_cps: 0.284356
[14:13:27.983] iteration 26255: total_loss: 0.310308, loss_sup: 0.058579, loss_mps: 0.092181, loss_cps: 0.159549
[14:13:28.130] iteration 26256: total_loss: 0.282914, loss_sup: 0.043704, loss_mps: 0.082670, loss_cps: 0.156539
[14:13:28.276] iteration 26257: total_loss: 0.197942, loss_sup: 0.005896, loss_mps: 0.068487, loss_cps: 0.123560
[14:13:28.424] iteration 26258: total_loss: 0.353322, loss_sup: 0.058326, loss_mps: 0.107518, loss_cps: 0.187478
[14:13:28.570] iteration 26259: total_loss: 0.500636, loss_sup: 0.064841, loss_mps: 0.142768, loss_cps: 0.293027
[14:13:28.716] iteration 26260: total_loss: 0.691114, loss_sup: 0.251467, loss_mps: 0.140440, loss_cps: 0.299208
[14:13:28.866] iteration 26261: total_loss: 0.609065, loss_sup: 0.105810, loss_mps: 0.172389, loss_cps: 0.330866
[14:13:29.012] iteration 26262: total_loss: 0.591510, loss_sup: 0.207262, loss_mps: 0.130383, loss_cps: 0.253865
[14:13:29.158] iteration 26263: total_loss: 0.288724, loss_sup: 0.010114, loss_mps: 0.106438, loss_cps: 0.172173
[14:13:29.307] iteration 26264: total_loss: 0.353164, loss_sup: 0.008659, loss_mps: 0.114763, loss_cps: 0.229742
[14:13:29.453] iteration 26265: total_loss: 0.312128, loss_sup: 0.018983, loss_mps: 0.104099, loss_cps: 0.189046
[14:13:29.599] iteration 26266: total_loss: 0.202625, loss_sup: 0.000488, loss_mps: 0.073936, loss_cps: 0.128201
[14:13:29.747] iteration 26267: total_loss: 0.335732, loss_sup: 0.070199, loss_mps: 0.094650, loss_cps: 0.170883
[14:13:29.892] iteration 26268: total_loss: 0.268684, loss_sup: 0.033727, loss_mps: 0.086039, loss_cps: 0.148919
[14:13:30.038] iteration 26269: total_loss: 0.316021, loss_sup: 0.061196, loss_mps: 0.092112, loss_cps: 0.162713
[14:13:30.185] iteration 26270: total_loss: 0.354144, loss_sup: 0.042326, loss_mps: 0.111735, loss_cps: 0.200083
[14:13:30.335] iteration 26271: total_loss: 0.368646, loss_sup: 0.064319, loss_mps: 0.106494, loss_cps: 0.197833
[14:13:30.481] iteration 26272: total_loss: 0.205777, loss_sup: 0.015848, loss_mps: 0.072637, loss_cps: 0.117291
[14:13:30.629] iteration 26273: total_loss: 0.563696, loss_sup: 0.023059, loss_mps: 0.169185, loss_cps: 0.371451
[14:13:30.776] iteration 26274: total_loss: 0.288065, loss_sup: 0.006451, loss_mps: 0.098639, loss_cps: 0.182974
[14:13:30.923] iteration 26275: total_loss: 0.315581, loss_sup: 0.010664, loss_mps: 0.104648, loss_cps: 0.200269
[14:13:31.072] iteration 26276: total_loss: 0.301614, loss_sup: 0.080093, loss_mps: 0.082178, loss_cps: 0.139343
[14:13:31.219] iteration 26277: total_loss: 0.383367, loss_sup: 0.077323, loss_mps: 0.104788, loss_cps: 0.201256
[14:13:31.365] iteration 26278: total_loss: 0.492153, loss_sup: 0.059693, loss_mps: 0.151258, loss_cps: 0.281202
[14:13:31.510] iteration 26279: total_loss: 0.267409, loss_sup: 0.036974, loss_mps: 0.086127, loss_cps: 0.144308
[14:13:31.656] iteration 26280: total_loss: 0.425423, loss_sup: 0.022382, loss_mps: 0.129055, loss_cps: 0.273986
[14:13:31.807] iteration 26281: total_loss: 0.343634, loss_sup: 0.010480, loss_mps: 0.115730, loss_cps: 0.217424
[14:13:31.953] iteration 26282: total_loss: 0.383533, loss_sup: 0.156363, loss_mps: 0.082406, loss_cps: 0.144763
[14:13:32.101] iteration 26283: total_loss: 0.541219, loss_sup: 0.057477, loss_mps: 0.167924, loss_cps: 0.315818
[14:13:32.251] iteration 26284: total_loss: 0.301507, loss_sup: 0.039945, loss_mps: 0.093207, loss_cps: 0.168356
[14:13:32.397] iteration 26285: total_loss: 0.401350, loss_sup: 0.157417, loss_mps: 0.088612, loss_cps: 0.155321
[14:13:32.545] iteration 26286: total_loss: 0.272264, loss_sup: 0.028911, loss_mps: 0.088889, loss_cps: 0.154464
[14:13:32.692] iteration 26287: total_loss: 0.783574, loss_sup: 0.271876, loss_mps: 0.168357, loss_cps: 0.343340
[14:13:32.839] iteration 26288: total_loss: 0.367292, loss_sup: 0.071711, loss_mps: 0.106731, loss_cps: 0.188850
[14:13:32.991] iteration 26289: total_loss: 0.409428, loss_sup: 0.087537, loss_mps: 0.114588, loss_cps: 0.207303
[14:13:33.138] iteration 26290: total_loss: 0.387351, loss_sup: 0.106940, loss_mps: 0.099782, loss_cps: 0.180629
[14:13:33.284] iteration 26291: total_loss: 0.378649, loss_sup: 0.106570, loss_mps: 0.097188, loss_cps: 0.174891
[14:13:33.430] iteration 26292: total_loss: 0.211950, loss_sup: 0.021962, loss_mps: 0.073607, loss_cps: 0.116380
[14:13:33.578] iteration 26293: total_loss: 0.406694, loss_sup: 0.001362, loss_mps: 0.134549, loss_cps: 0.270783
[14:13:33.724] iteration 26294: total_loss: 0.380177, loss_sup: 0.023323, loss_mps: 0.123809, loss_cps: 0.233045
[14:13:33.872] iteration 26295: total_loss: 0.392460, loss_sup: 0.119434, loss_mps: 0.099227, loss_cps: 0.173799
[14:13:34.018] iteration 26296: total_loss: 0.263549, loss_sup: 0.020612, loss_mps: 0.086367, loss_cps: 0.156570
[14:13:34.164] iteration 26297: total_loss: 0.356408, loss_sup: 0.057823, loss_mps: 0.106643, loss_cps: 0.191943
[14:13:34.313] iteration 26298: total_loss: 0.233646, loss_sup: 0.005036, loss_mps: 0.081823, loss_cps: 0.146787
[14:13:34.459] iteration 26299: total_loss: 0.303423, loss_sup: 0.036118, loss_mps: 0.095802, loss_cps: 0.171504
[14:13:34.606] iteration 26300: total_loss: 0.353401, loss_sup: 0.035454, loss_mps: 0.112727, loss_cps: 0.205219
[14:13:34.606] Evaluation Started ==>
[14:13:45.944] ==> valid iteration 26300: unet metrics: {'dc': 0.6498848612534777, 'jc': 0.5333000612500064, 'pre': 0.7956134997393435, 'hd': 5.4472085855927475}, ynet metrics: {'dc': 0.6067624357553009, 'jc': 0.49542747815336313, 'pre': 0.8008310689761702, 'hd': 5.28766200526893}.
[14:13:45.946] Evaluation Finished!⏹️
[14:13:46.101] iteration 26301: total_loss: 0.224597, loss_sup: 0.000798, loss_mps: 0.079534, loss_cps: 0.144266
[14:13:46.249] iteration 26302: total_loss: 0.643340, loss_sup: 0.175335, loss_mps: 0.158612, loss_cps: 0.309392
[14:13:46.394] iteration 26303: total_loss: 0.298880, loss_sup: 0.030007, loss_mps: 0.096301, loss_cps: 0.172571
[14:13:46.539] iteration 26304: total_loss: 0.379928, loss_sup: 0.006222, loss_mps: 0.124386, loss_cps: 0.249321
[14:13:46.685] iteration 26305: total_loss: 0.261223, loss_sup: 0.042139, loss_mps: 0.079464, loss_cps: 0.139620
[14:13:46.830] iteration 26306: total_loss: 0.488981, loss_sup: 0.015667, loss_mps: 0.150499, loss_cps: 0.322815
[14:13:46.976] iteration 26307: total_loss: 0.287566, loss_sup: 0.025830, loss_mps: 0.087665, loss_cps: 0.174072
[14:13:47.123] iteration 26308: total_loss: 0.443235, loss_sup: 0.064572, loss_mps: 0.124684, loss_cps: 0.253979
[14:13:47.271] iteration 26309: total_loss: 0.241115, loss_sup: 0.009134, loss_mps: 0.088047, loss_cps: 0.143934
[14:13:47.417] iteration 26310: total_loss: 0.376233, loss_sup: 0.080452, loss_mps: 0.103059, loss_cps: 0.192722
[14:13:47.562] iteration 26311: total_loss: 0.349041, loss_sup: 0.010290, loss_mps: 0.114809, loss_cps: 0.223942
[14:13:47.708] iteration 26312: total_loss: 0.354084, loss_sup: 0.015472, loss_mps: 0.114481, loss_cps: 0.224131
[14:13:47.854] iteration 26313: total_loss: 0.353402, loss_sup: 0.130991, loss_mps: 0.084997, loss_cps: 0.137415
[14:13:48.000] iteration 26314: total_loss: 0.317016, loss_sup: 0.012362, loss_mps: 0.105022, loss_cps: 0.199632
[14:13:48.146] iteration 26315: total_loss: 0.340814, loss_sup: 0.065351, loss_mps: 0.097070, loss_cps: 0.178392
[14:13:48.295] iteration 26316: total_loss: 0.293235, loss_sup: 0.070536, loss_mps: 0.080111, loss_cps: 0.142588
[14:13:48.441] iteration 26317: total_loss: 0.295130, loss_sup: 0.012233, loss_mps: 0.101752, loss_cps: 0.181145
[14:13:48.586] iteration 26318: total_loss: 0.729827, loss_sup: 0.031508, loss_mps: 0.223527, loss_cps: 0.474792
[14:13:48.732] iteration 26319: total_loss: 0.368077, loss_sup: 0.020946, loss_mps: 0.125410, loss_cps: 0.221721
[14:13:48.878] iteration 26320: total_loss: 0.279335, loss_sup: 0.011154, loss_mps: 0.095894, loss_cps: 0.172287
[14:13:49.024] iteration 26321: total_loss: 0.145615, loss_sup: 0.006464, loss_mps: 0.053445, loss_cps: 0.085705
[14:13:49.170] iteration 26322: total_loss: 0.436624, loss_sup: 0.032477, loss_mps: 0.132754, loss_cps: 0.271393
[14:13:49.315] iteration 26323: total_loss: 0.409466, loss_sup: 0.101300, loss_mps: 0.104739, loss_cps: 0.203426
[14:13:49.460] iteration 26324: total_loss: 0.604383, loss_sup: 0.047793, loss_mps: 0.183440, loss_cps: 0.373150
[14:13:49.606] iteration 26325: total_loss: 0.323947, loss_sup: 0.006060, loss_mps: 0.107486, loss_cps: 0.210401
[14:13:49.751] iteration 26326: total_loss: 0.241760, loss_sup: 0.007704, loss_mps: 0.086110, loss_cps: 0.147945
[14:13:49.897] iteration 26327: total_loss: 0.312035, loss_sup: 0.054027, loss_mps: 0.094357, loss_cps: 0.163652
[14:13:50.044] iteration 26328: total_loss: 0.335725, loss_sup: 0.012144, loss_mps: 0.109022, loss_cps: 0.214558
[14:13:50.192] iteration 26329: total_loss: 0.289117, loss_sup: 0.094453, loss_mps: 0.069888, loss_cps: 0.124776
[14:13:50.337] iteration 26330: total_loss: 0.356737, loss_sup: 0.012265, loss_mps: 0.122543, loss_cps: 0.221928
[14:13:50.482] iteration 26331: total_loss: 0.396939, loss_sup: 0.068240, loss_mps: 0.108622, loss_cps: 0.220077
[14:13:50.627] iteration 26332: total_loss: 0.566827, loss_sup: 0.163077, loss_mps: 0.142273, loss_cps: 0.261477
[14:13:50.773] iteration 26333: total_loss: 0.324603, loss_sup: 0.038958, loss_mps: 0.094563, loss_cps: 0.191083
[14:13:50.835] iteration 26334: total_loss: 0.155736, loss_sup: 0.004626, loss_mps: 0.059368, loss_cps: 0.091742
[14:13:52.056] iteration 26335: total_loss: 0.300485, loss_sup: 0.086024, loss_mps: 0.075489, loss_cps: 0.138972
[14:13:52.205] iteration 26336: total_loss: 0.373006, loss_sup: 0.008537, loss_mps: 0.127028, loss_cps: 0.237441
[14:13:52.352] iteration 26337: total_loss: 0.368402, loss_sup: 0.085889, loss_mps: 0.093429, loss_cps: 0.189085
[14:13:52.499] iteration 26338: total_loss: 0.306682, loss_sup: 0.015386, loss_mps: 0.101768, loss_cps: 0.189528
[14:13:52.646] iteration 26339: total_loss: 0.590568, loss_sup: 0.159621, loss_mps: 0.140814, loss_cps: 0.290133
[14:13:52.792] iteration 26340: total_loss: 0.491140, loss_sup: 0.121536, loss_mps: 0.133513, loss_cps: 0.236091
[14:13:52.938] iteration 26341: total_loss: 0.709092, loss_sup: 0.247870, loss_mps: 0.151065, loss_cps: 0.310157
[14:13:53.088] iteration 26342: total_loss: 0.351072, loss_sup: 0.025753, loss_mps: 0.100948, loss_cps: 0.224371
[14:13:53.235] iteration 26343: total_loss: 0.341424, loss_sup: 0.003952, loss_mps: 0.113762, loss_cps: 0.223711
[14:13:53.382] iteration 26344: total_loss: 0.790920, loss_sup: 0.357345, loss_mps: 0.153362, loss_cps: 0.280213
[14:13:53.530] iteration 26345: total_loss: 0.148225, loss_sup: 0.003792, loss_mps: 0.054824, loss_cps: 0.089609
[14:13:53.677] iteration 26346: total_loss: 0.502242, loss_sup: 0.033973, loss_mps: 0.150085, loss_cps: 0.318184
[14:13:53.823] iteration 26347: total_loss: 0.342535, loss_sup: 0.013099, loss_mps: 0.116881, loss_cps: 0.212556
[14:13:53.969] iteration 26348: total_loss: 0.286439, loss_sup: 0.009495, loss_mps: 0.093268, loss_cps: 0.183676
[14:13:54.116] iteration 26349: total_loss: 0.596726, loss_sup: 0.052484, loss_mps: 0.177580, loss_cps: 0.366662
[14:13:54.266] iteration 26350: total_loss: 0.268709, loss_sup: 0.001913, loss_mps: 0.092212, loss_cps: 0.174585
[14:13:54.413] iteration 26351: total_loss: 0.365070, loss_sup: 0.057661, loss_mps: 0.107091, loss_cps: 0.200317
[14:13:54.559] iteration 26352: total_loss: 0.475643, loss_sup: 0.229928, loss_mps: 0.083279, loss_cps: 0.162437
[14:13:54.705] iteration 26353: total_loss: 0.345767, loss_sup: 0.019555, loss_mps: 0.113046, loss_cps: 0.213166
[14:13:54.851] iteration 26354: total_loss: 0.377519, loss_sup: 0.081365, loss_mps: 0.105457, loss_cps: 0.190696
[14:13:54.998] iteration 26355: total_loss: 0.684222, loss_sup: 0.287432, loss_mps: 0.129597, loss_cps: 0.267194
[14:13:55.144] iteration 26356: total_loss: 0.427593, loss_sup: 0.127107, loss_mps: 0.108802, loss_cps: 0.191684
[14:13:55.291] iteration 26357: total_loss: 0.443667, loss_sup: 0.045899, loss_mps: 0.127456, loss_cps: 0.270312
[14:13:55.438] iteration 26358: total_loss: 0.260384, loss_sup: 0.028251, loss_mps: 0.080042, loss_cps: 0.152092
[14:13:55.584] iteration 26359: total_loss: 0.273464, loss_sup: 0.017110, loss_mps: 0.089509, loss_cps: 0.166845
[14:13:55.730] iteration 26360: total_loss: 0.542957, loss_sup: 0.119310, loss_mps: 0.142227, loss_cps: 0.281420
[14:13:55.879] iteration 26361: total_loss: 0.289088, loss_sup: 0.010703, loss_mps: 0.097506, loss_cps: 0.180879
[14:13:56.025] iteration 26362: total_loss: 0.511471, loss_sup: 0.092812, loss_mps: 0.145149, loss_cps: 0.273510
[14:13:56.171] iteration 26363: total_loss: 0.677955, loss_sup: 0.123600, loss_mps: 0.177885, loss_cps: 0.376470
[14:13:56.317] iteration 26364: total_loss: 0.397392, loss_sup: 0.018257, loss_mps: 0.133010, loss_cps: 0.246126
[14:13:56.464] iteration 26365: total_loss: 0.595326, loss_sup: 0.008571, loss_mps: 0.188495, loss_cps: 0.398260
[14:13:56.612] iteration 26366: total_loss: 0.554245, loss_sup: 0.026769, loss_mps: 0.177178, loss_cps: 0.350298
[14:13:56.758] iteration 26367: total_loss: 0.282184, loss_sup: 0.000927, loss_mps: 0.100947, loss_cps: 0.180311
[14:13:56.904] iteration 26368: total_loss: 0.210646, loss_sup: 0.005733, loss_mps: 0.078254, loss_cps: 0.126658
[14:13:57.051] iteration 26369: total_loss: 0.598364, loss_sup: 0.057167, loss_mps: 0.181989, loss_cps: 0.359209
[14:13:57.199] iteration 26370: total_loss: 0.528721, loss_sup: 0.225625, loss_mps: 0.105738, loss_cps: 0.197358
[14:13:57.346] iteration 26371: total_loss: 0.292263, loss_sup: 0.017812, loss_mps: 0.097355, loss_cps: 0.177097
[14:13:57.497] iteration 26372: total_loss: 0.655197, loss_sup: 0.015476, loss_mps: 0.194889, loss_cps: 0.444832
[14:13:57.643] iteration 26373: total_loss: 0.322067, loss_sup: 0.007730, loss_mps: 0.110445, loss_cps: 0.203893
[14:13:57.790] iteration 26374: total_loss: 0.264145, loss_sup: 0.061406, loss_mps: 0.073446, loss_cps: 0.129294
[14:13:57.936] iteration 26375: total_loss: 0.380233, loss_sup: 0.037805, loss_mps: 0.114225, loss_cps: 0.228203
[14:13:58.084] iteration 26376: total_loss: 0.768989, loss_sup: 0.055627, loss_mps: 0.218865, loss_cps: 0.494497
[14:13:58.230] iteration 26377: total_loss: 0.534287, loss_sup: 0.039672, loss_mps: 0.163342, loss_cps: 0.331273
[14:13:58.378] iteration 26378: total_loss: 0.565068, loss_sup: 0.039337, loss_mps: 0.175119, loss_cps: 0.350612
[14:13:58.524] iteration 26379: total_loss: 0.279911, loss_sup: 0.036794, loss_mps: 0.085142, loss_cps: 0.157975
[14:13:58.671] iteration 26380: total_loss: 0.338083, loss_sup: 0.035487, loss_mps: 0.104202, loss_cps: 0.198394
[14:13:58.817] iteration 26381: total_loss: 0.815787, loss_sup: 0.276448, loss_mps: 0.188744, loss_cps: 0.350595
[14:13:58.964] iteration 26382: total_loss: 0.338734, loss_sup: 0.043949, loss_mps: 0.096929, loss_cps: 0.197856
[14:13:59.110] iteration 26383: total_loss: 0.335504, loss_sup: 0.074891, loss_mps: 0.093021, loss_cps: 0.167591
[14:13:59.257] iteration 26384: total_loss: 0.312038, loss_sup: 0.037854, loss_mps: 0.095815, loss_cps: 0.178369
[14:13:59.403] iteration 26385: total_loss: 0.288624, loss_sup: 0.003217, loss_mps: 0.101252, loss_cps: 0.184156
[14:13:59.550] iteration 26386: total_loss: 0.408239, loss_sup: 0.018953, loss_mps: 0.133753, loss_cps: 0.255533
[14:13:59.698] iteration 26387: total_loss: 0.251711, loss_sup: 0.001183, loss_mps: 0.086012, loss_cps: 0.164516
[14:13:59.846] iteration 26388: total_loss: 0.274963, loss_sup: 0.022536, loss_mps: 0.092844, loss_cps: 0.159583
[14:13:59.993] iteration 26389: total_loss: 0.332192, loss_sup: 0.011331, loss_mps: 0.113085, loss_cps: 0.207776
[14:14:00.140] iteration 26390: total_loss: 0.226535, loss_sup: 0.012177, loss_mps: 0.076354, loss_cps: 0.138004
[14:14:00.286] iteration 26391: total_loss: 0.568434, loss_sup: 0.025657, loss_mps: 0.178007, loss_cps: 0.364770
[14:14:00.433] iteration 26392: total_loss: 0.209423, loss_sup: 0.055303, loss_mps: 0.061784, loss_cps: 0.092337
[14:14:00.580] iteration 26393: total_loss: 0.258972, loss_sup: 0.031912, loss_mps: 0.083243, loss_cps: 0.143817
[14:14:00.727] iteration 26394: total_loss: 0.624392, loss_sup: 0.039309, loss_mps: 0.197133, loss_cps: 0.387950
[14:14:00.873] iteration 26395: total_loss: 0.306706, loss_sup: 0.023993, loss_mps: 0.099741, loss_cps: 0.182972
[14:14:01.020] iteration 26396: total_loss: 0.166321, loss_sup: 0.002547, loss_mps: 0.061563, loss_cps: 0.102211
[14:14:01.167] iteration 26397: total_loss: 0.270074, loss_sup: 0.018684, loss_mps: 0.095509, loss_cps: 0.155881
[14:14:01.314] iteration 26398: total_loss: 0.539131, loss_sup: 0.261627, loss_mps: 0.099506, loss_cps: 0.177998
[14:14:01.462] iteration 26399: total_loss: 0.322608, loss_sup: 0.055898, loss_mps: 0.096276, loss_cps: 0.170434
[14:14:01.612] iteration 26400: total_loss: 0.370012, loss_sup: 0.022033, loss_mps: 0.116253, loss_cps: 0.231726
[14:14:01.612] Evaluation Started ==>
[14:14:12.948] ==> valid iteration 26400: unet metrics: {'dc': 0.6373035538419726, 'jc': 0.5271090931671325, 'pre': 0.7814272399178556, 'hd': 5.258239114952869}, ynet metrics: {'dc': 0.6280849009121204, 'jc': 0.5168512937468185, 'pre': 0.8108321580398469, 'hd': 5.208966560367272}.
[14:14:12.950] Evaluation Finished!⏹️
[14:14:13.101] iteration 26401: total_loss: 0.344910, loss_sup: 0.097342, loss_mps: 0.092077, loss_cps: 0.155491
[14:14:13.248] iteration 26402: total_loss: 0.263198, loss_sup: 0.028767, loss_mps: 0.085766, loss_cps: 0.148665
[14:14:13.394] iteration 26403: total_loss: 0.348415, loss_sup: 0.040435, loss_mps: 0.105560, loss_cps: 0.202420
[14:14:13.539] iteration 26404: total_loss: 0.351814, loss_sup: 0.031252, loss_mps: 0.120392, loss_cps: 0.200170
[14:14:13.685] iteration 26405: total_loss: 0.328776, loss_sup: 0.025030, loss_mps: 0.115836, loss_cps: 0.187910
[14:14:13.830] iteration 26406: total_loss: 0.364766, loss_sup: 0.097195, loss_mps: 0.093514, loss_cps: 0.174057
[14:14:13.975] iteration 26407: total_loss: 0.264418, loss_sup: 0.031873, loss_mps: 0.087081, loss_cps: 0.145463
[14:14:14.122] iteration 26408: total_loss: 0.233510, loss_sup: 0.023359, loss_mps: 0.071732, loss_cps: 0.138420
[14:14:14.269] iteration 26409: total_loss: 0.226299, loss_sup: 0.006189, loss_mps: 0.083143, loss_cps: 0.136967
[14:14:14.416] iteration 26410: total_loss: 0.429892, loss_sup: 0.048575, loss_mps: 0.126508, loss_cps: 0.254809
[14:14:14.564] iteration 26411: total_loss: 0.333746, loss_sup: 0.033219, loss_mps: 0.105268, loss_cps: 0.195259
[14:14:14.710] iteration 26412: total_loss: 0.557589, loss_sup: 0.105295, loss_mps: 0.149062, loss_cps: 0.303232
[14:14:14.856] iteration 26413: total_loss: 0.425523, loss_sup: 0.061535, loss_mps: 0.124439, loss_cps: 0.239549
[14:14:15.001] iteration 26414: total_loss: 0.352116, loss_sup: 0.099001, loss_mps: 0.085793, loss_cps: 0.167321
[14:14:15.147] iteration 26415: total_loss: 0.436118, loss_sup: 0.070341, loss_mps: 0.126379, loss_cps: 0.239398
[14:14:15.304] iteration 26416: total_loss: 0.393876, loss_sup: 0.119847, loss_mps: 0.102960, loss_cps: 0.171070
[14:14:15.450] iteration 26417: total_loss: 0.552335, loss_sup: 0.107777, loss_mps: 0.155597, loss_cps: 0.288960
[14:14:15.596] iteration 26418: total_loss: 0.252255, loss_sup: 0.002147, loss_mps: 0.091635, loss_cps: 0.158473
[14:14:15.742] iteration 26419: total_loss: 0.209627, loss_sup: 0.004485, loss_mps: 0.077286, loss_cps: 0.127856
[14:14:15.892] iteration 26420: total_loss: 0.319033, loss_sup: 0.121444, loss_mps: 0.072962, loss_cps: 0.124627
[14:14:16.038] iteration 26421: total_loss: 0.510728, loss_sup: 0.056191, loss_mps: 0.147671, loss_cps: 0.306866
[14:14:16.183] iteration 26422: total_loss: 0.614060, loss_sup: 0.050109, loss_mps: 0.168800, loss_cps: 0.395151
[14:14:16.329] iteration 26423: total_loss: 0.444765, loss_sup: 0.177017, loss_mps: 0.100338, loss_cps: 0.167410
[14:14:16.475] iteration 26424: total_loss: 0.232267, loss_sup: 0.002127, loss_mps: 0.082034, loss_cps: 0.148107
[14:14:16.621] iteration 26425: total_loss: 0.409314, loss_sup: 0.038787, loss_mps: 0.130863, loss_cps: 0.239663
[14:14:16.768] iteration 26426: total_loss: 0.203191, loss_sup: 0.002684, loss_mps: 0.070229, loss_cps: 0.130278
[14:14:16.914] iteration 26427: total_loss: 0.341142, loss_sup: 0.020989, loss_mps: 0.114617, loss_cps: 0.205536
[14:14:17.060] iteration 26428: total_loss: 0.532694, loss_sup: 0.040017, loss_mps: 0.170817, loss_cps: 0.321860
[14:14:17.209] iteration 26429: total_loss: 0.298400, loss_sup: 0.045949, loss_mps: 0.091514, loss_cps: 0.160937
[14:14:17.356] iteration 26430: total_loss: 0.291194, loss_sup: 0.013491, loss_mps: 0.094019, loss_cps: 0.183684
[14:14:17.502] iteration 26431: total_loss: 0.170366, loss_sup: 0.012850, loss_mps: 0.058276, loss_cps: 0.099241
[14:14:17.648] iteration 26432: total_loss: 0.256932, loss_sup: 0.004768, loss_mps: 0.090252, loss_cps: 0.161912
[14:14:17.798] iteration 26433: total_loss: 0.325109, loss_sup: 0.002659, loss_mps: 0.109378, loss_cps: 0.213073
[14:14:17.944] iteration 26434: total_loss: 0.411359, loss_sup: 0.005336, loss_mps: 0.130974, loss_cps: 0.275049
[14:14:18.090] iteration 26435: total_loss: 0.387363, loss_sup: 0.081842, loss_mps: 0.100954, loss_cps: 0.204567
[14:14:18.236] iteration 26436: total_loss: 0.405766, loss_sup: 0.021036, loss_mps: 0.130474, loss_cps: 0.254257
[14:14:18.383] iteration 26437: total_loss: 0.340002, loss_sup: 0.020725, loss_mps: 0.108494, loss_cps: 0.210782
[14:14:18.529] iteration 26438: total_loss: 0.598422, loss_sup: 0.107921, loss_mps: 0.163909, loss_cps: 0.326592
[14:14:18.675] iteration 26439: total_loss: 0.425338, loss_sup: 0.079099, loss_mps: 0.113768, loss_cps: 0.232471
[14:14:18.821] iteration 26440: total_loss: 0.216415, loss_sup: 0.020651, loss_mps: 0.076262, loss_cps: 0.119502
[14:14:18.970] iteration 26441: total_loss: 0.375608, loss_sup: 0.009828, loss_mps: 0.123701, loss_cps: 0.242079
[14:14:19.116] iteration 26442: total_loss: 0.312616, loss_sup: 0.040236, loss_mps: 0.095956, loss_cps: 0.176425
[14:14:19.265] iteration 26443: total_loss: 0.350501, loss_sup: 0.007044, loss_mps: 0.114475, loss_cps: 0.228982
[14:14:19.411] iteration 26444: total_loss: 0.501717, loss_sup: 0.068754, loss_mps: 0.153481, loss_cps: 0.279481
[14:14:19.557] iteration 26445: total_loss: 0.426563, loss_sup: 0.063996, loss_mps: 0.125950, loss_cps: 0.236617
[14:14:19.708] iteration 26446: total_loss: 0.409474, loss_sup: 0.137634, loss_mps: 0.095010, loss_cps: 0.176830
[14:14:19.855] iteration 26447: total_loss: 0.504973, loss_sup: 0.002601, loss_mps: 0.166057, loss_cps: 0.336315
[14:14:20.002] iteration 26448: total_loss: 0.478025, loss_sup: 0.072413, loss_mps: 0.129397, loss_cps: 0.276215
[14:14:20.148] iteration 26449: total_loss: 0.484529, loss_sup: 0.067841, loss_mps: 0.146517, loss_cps: 0.270171
[14:14:20.295] iteration 26450: total_loss: 0.361370, loss_sup: 0.011773, loss_mps: 0.116277, loss_cps: 0.233320
[14:14:20.441] iteration 26451: total_loss: 0.240737, loss_sup: 0.017586, loss_mps: 0.079829, loss_cps: 0.143321
[14:14:20.589] iteration 26452: total_loss: 0.527187, loss_sup: 0.105107, loss_mps: 0.142868, loss_cps: 0.279213
[14:14:20.735] iteration 26453: total_loss: 0.386873, loss_sup: 0.045417, loss_mps: 0.110772, loss_cps: 0.230684
[14:14:20.882] iteration 26454: total_loss: 0.488887, loss_sup: 0.058787, loss_mps: 0.139467, loss_cps: 0.290633
[14:14:21.028] iteration 26455: total_loss: 0.509519, loss_sup: 0.099441, loss_mps: 0.139461, loss_cps: 0.270617
[14:14:21.174] iteration 26456: total_loss: 0.531021, loss_sup: 0.021334, loss_mps: 0.161327, loss_cps: 0.348360
[14:14:21.321] iteration 26457: total_loss: 0.576205, loss_sup: 0.278814, loss_mps: 0.101989, loss_cps: 0.195402
[14:14:21.467] iteration 26458: total_loss: 0.441624, loss_sup: 0.073975, loss_mps: 0.128507, loss_cps: 0.239141
[14:14:21.614] iteration 26459: total_loss: 0.188225, loss_sup: 0.012202, loss_mps: 0.066590, loss_cps: 0.109433
[14:14:21.760] iteration 26460: total_loss: 0.296839, loss_sup: 0.046676, loss_mps: 0.087894, loss_cps: 0.162269
[14:14:21.907] iteration 26461: total_loss: 0.308170, loss_sup: 0.047356, loss_mps: 0.092223, loss_cps: 0.168592
[14:14:22.055] iteration 26462: total_loss: 0.327645, loss_sup: 0.012724, loss_mps: 0.112200, loss_cps: 0.202722
[14:14:22.201] iteration 26463: total_loss: 0.290609, loss_sup: 0.030536, loss_mps: 0.088827, loss_cps: 0.171246
[14:14:22.347] iteration 26464: total_loss: 0.801644, loss_sup: 0.068487, loss_mps: 0.235766, loss_cps: 0.497391
[14:14:22.493] iteration 26465: total_loss: 0.391408, loss_sup: 0.058395, loss_mps: 0.115610, loss_cps: 0.217402
[14:14:22.640] iteration 26466: total_loss: 0.462434, loss_sup: 0.004557, loss_mps: 0.149138, loss_cps: 0.308739
[14:14:22.786] iteration 26467: total_loss: 0.263495, loss_sup: 0.014508, loss_mps: 0.086298, loss_cps: 0.162689
[14:14:22.932] iteration 26468: total_loss: 0.503056, loss_sup: 0.135156, loss_mps: 0.118711, loss_cps: 0.249189
[14:14:23.078] iteration 26469: total_loss: 0.384561, loss_sup: 0.062466, loss_mps: 0.114664, loss_cps: 0.207431
[14:14:23.228] iteration 26470: total_loss: 0.154416, loss_sup: 0.002727, loss_mps: 0.057533, loss_cps: 0.094156
[14:14:23.375] iteration 26471: total_loss: 0.571139, loss_sup: 0.082144, loss_mps: 0.151814, loss_cps: 0.337180
[14:14:23.521] iteration 26472: total_loss: 0.465165, loss_sup: 0.048049, loss_mps: 0.139343, loss_cps: 0.277773
[14:14:23.667] iteration 26473: total_loss: 0.267167, loss_sup: 0.063398, loss_mps: 0.075191, loss_cps: 0.128578
[14:14:23.813] iteration 26474: total_loss: 0.403260, loss_sup: 0.006963, loss_mps: 0.135124, loss_cps: 0.261173
[14:14:23.959] iteration 26475: total_loss: 0.468645, loss_sup: 0.117668, loss_mps: 0.123161, loss_cps: 0.227816
[14:14:24.105] iteration 26476: total_loss: 0.513807, loss_sup: 0.027080, loss_mps: 0.158255, loss_cps: 0.328472
[14:14:24.250] iteration 26477: total_loss: 0.332409, loss_sup: 0.015319, loss_mps: 0.107270, loss_cps: 0.209819
[14:14:24.396] iteration 26478: total_loss: 0.226575, loss_sup: 0.004184, loss_mps: 0.079980, loss_cps: 0.142411
[14:14:24.547] iteration 26479: total_loss: 0.203785, loss_sup: 0.017884, loss_mps: 0.067031, loss_cps: 0.118870
[14:14:24.694] iteration 26480: total_loss: 0.302034, loss_sup: 0.069887, loss_mps: 0.083465, loss_cps: 0.148682
[14:14:24.842] iteration 26481: total_loss: 0.302663, loss_sup: 0.042743, loss_mps: 0.092514, loss_cps: 0.167406
[14:14:24.988] iteration 26482: total_loss: 0.580939, loss_sup: 0.075249, loss_mps: 0.161780, loss_cps: 0.343910
[14:14:25.135] iteration 26483: total_loss: 0.515102, loss_sup: 0.298954, loss_mps: 0.082261, loss_cps: 0.133887
[14:14:25.285] iteration 26484: total_loss: 0.268945, loss_sup: 0.036357, loss_mps: 0.082658, loss_cps: 0.149930
[14:14:25.432] iteration 26485: total_loss: 0.416286, loss_sup: 0.043226, loss_mps: 0.134295, loss_cps: 0.238764
[14:14:25.578] iteration 26486: total_loss: 0.291212, loss_sup: 0.030420, loss_mps: 0.092521, loss_cps: 0.168271
[14:14:25.725] iteration 26487: total_loss: 0.280481, loss_sup: 0.008072, loss_mps: 0.095820, loss_cps: 0.176590
[14:14:25.871] iteration 26488: total_loss: 0.311700, loss_sup: 0.008654, loss_mps: 0.110741, loss_cps: 0.192306
[14:14:26.017] iteration 26489: total_loss: 0.344407, loss_sup: 0.005500, loss_mps: 0.111589, loss_cps: 0.227318
[14:14:26.163] iteration 26490: total_loss: 0.568793, loss_sup: 0.105678, loss_mps: 0.153557, loss_cps: 0.309558
[14:14:26.309] iteration 26491: total_loss: 0.568523, loss_sup: 0.248727, loss_mps: 0.110375, loss_cps: 0.209421
[14:14:26.456] iteration 26492: total_loss: 0.331406, loss_sup: 0.001878, loss_mps: 0.115005, loss_cps: 0.214523
[14:14:26.602] iteration 26493: total_loss: 0.431524, loss_sup: 0.026602, loss_mps: 0.131994, loss_cps: 0.272928
[14:14:26.748] iteration 26494: total_loss: 0.220751, loss_sup: 0.005156, loss_mps: 0.084817, loss_cps: 0.130778
[14:14:26.895] iteration 26495: total_loss: 0.415799, loss_sup: 0.019273, loss_mps: 0.140155, loss_cps: 0.256372
[14:14:27.042] iteration 26496: total_loss: 0.187788, loss_sup: 0.013163, loss_mps: 0.070687, loss_cps: 0.103937
[14:14:27.188] iteration 26497: total_loss: 0.602652, loss_sup: 0.133221, loss_mps: 0.153630, loss_cps: 0.315801
[14:14:27.334] iteration 26498: total_loss: 0.435366, loss_sup: 0.038884, loss_mps: 0.130869, loss_cps: 0.265613
[14:14:27.481] iteration 26499: total_loss: 0.434612, loss_sup: 0.027738, loss_mps: 0.136133, loss_cps: 0.270741
[14:14:27.629] iteration 26500: total_loss: 0.374416, loss_sup: 0.065572, loss_mps: 0.104565, loss_cps: 0.204279
[14:14:27.629] Evaluation Started ==>
[14:14:38.953] ==> valid iteration 26500: unet metrics: {'dc': 0.6355892176435907, 'jc': 0.5242948919211591, 'pre': 0.793128876115679, 'hd': 5.2563013478308}, ynet metrics: {'dc': 0.6022355103713757, 'jc': 0.4902669154168444, 'pre': 0.8032168021603652, 'hd': 5.38961469215016}.
[14:14:38.955] Evaluation Finished!⏹️
[14:14:39.105] iteration 26501: total_loss: 0.372897, loss_sup: 0.012284, loss_mps: 0.122205, loss_cps: 0.238409
[14:14:39.253] iteration 26502: total_loss: 0.508518, loss_sup: 0.069241, loss_mps: 0.143479, loss_cps: 0.295797
[14:14:39.398] iteration 26503: total_loss: 0.196320, loss_sup: 0.008742, loss_mps: 0.067708, loss_cps: 0.119869
[14:14:39.543] iteration 26504: total_loss: 0.389498, loss_sup: 0.010080, loss_mps: 0.129231, loss_cps: 0.250187
[14:14:39.688] iteration 26505: total_loss: 0.226097, loss_sup: 0.007743, loss_mps: 0.083084, loss_cps: 0.135270
[14:14:39.833] iteration 26506: total_loss: 0.434784, loss_sup: 0.052827, loss_mps: 0.128589, loss_cps: 0.253367
[14:14:39.979] iteration 26507: total_loss: 0.355166, loss_sup: 0.046187, loss_mps: 0.103186, loss_cps: 0.205793
[14:14:40.125] iteration 26508: total_loss: 0.250573, loss_sup: 0.045129, loss_mps: 0.073811, loss_cps: 0.131632
[14:14:40.270] iteration 26509: total_loss: 0.353598, loss_sup: 0.015071, loss_mps: 0.115583, loss_cps: 0.222944
[14:14:40.416] iteration 26510: total_loss: 0.235054, loss_sup: 0.017195, loss_mps: 0.079792, loss_cps: 0.138067
[14:14:40.561] iteration 26511: total_loss: 0.312400, loss_sup: 0.010625, loss_mps: 0.099273, loss_cps: 0.202502
[14:14:40.707] iteration 26512: total_loss: 0.287070, loss_sup: 0.020105, loss_mps: 0.090873, loss_cps: 0.176092
[14:14:40.852] iteration 26513: total_loss: 0.353330, loss_sup: 0.076576, loss_mps: 0.094061, loss_cps: 0.182693
[14:14:40.997] iteration 26514: total_loss: 0.206947, loss_sup: 0.003705, loss_mps: 0.072692, loss_cps: 0.130551
[14:14:41.146] iteration 26515: total_loss: 0.200211, loss_sup: 0.004151, loss_mps: 0.073542, loss_cps: 0.122517
[14:14:41.292] iteration 26516: total_loss: 0.469726, loss_sup: 0.071859, loss_mps: 0.137300, loss_cps: 0.260567
[14:14:41.439] iteration 26517: total_loss: 0.172142, loss_sup: 0.004505, loss_mps: 0.063566, loss_cps: 0.104071
[14:14:41.587] iteration 26518: total_loss: 0.196107, loss_sup: 0.001906, loss_mps: 0.073955, loss_cps: 0.120247
[14:14:41.735] iteration 26519: total_loss: 0.241351, loss_sup: 0.022461, loss_mps: 0.077729, loss_cps: 0.141161
[14:14:41.881] iteration 26520: total_loss: 0.251912, loss_sup: 0.059909, loss_mps: 0.072089, loss_cps: 0.119915
[14:14:42.026] iteration 26521: total_loss: 0.492876, loss_sup: 0.097369, loss_mps: 0.126724, loss_cps: 0.268783
[14:14:42.172] iteration 26522: total_loss: 0.586790, loss_sup: 0.083602, loss_mps: 0.173920, loss_cps: 0.329268
[14:14:42.318] iteration 26523: total_loss: 0.352914, loss_sup: 0.027914, loss_mps: 0.104206, loss_cps: 0.220794
[14:14:42.465] iteration 26524: total_loss: 0.318368, loss_sup: 0.029370, loss_mps: 0.102196, loss_cps: 0.186802
[14:14:42.610] iteration 26525: total_loss: 0.568389, loss_sup: 0.130943, loss_mps: 0.144635, loss_cps: 0.292812
[14:14:42.756] iteration 26526: total_loss: 0.294370, loss_sup: 0.060455, loss_mps: 0.083404, loss_cps: 0.150510
[14:14:42.902] iteration 26527: total_loss: 0.394238, loss_sup: 0.091262, loss_mps: 0.105942, loss_cps: 0.197033
[14:14:43.049] iteration 26528: total_loss: 0.302252, loss_sup: 0.008930, loss_mps: 0.104543, loss_cps: 0.188779
[14:14:43.199] iteration 26529: total_loss: 0.234651, loss_sup: 0.017620, loss_mps: 0.078682, loss_cps: 0.138349
[14:14:43.345] iteration 26530: total_loss: 0.241736, loss_sup: 0.037534, loss_mps: 0.077436, loss_cps: 0.126765
[14:14:43.490] iteration 26531: total_loss: 0.462680, loss_sup: 0.045249, loss_mps: 0.138600, loss_cps: 0.278830
[14:14:43.636] iteration 26532: total_loss: 0.423016, loss_sup: 0.025459, loss_mps: 0.130668, loss_cps: 0.266889
[14:14:43.781] iteration 26533: total_loss: 0.338681, loss_sup: 0.125736, loss_mps: 0.076130, loss_cps: 0.136814
[14:14:43.926] iteration 26534: total_loss: 0.275517, loss_sup: 0.006037, loss_mps: 0.092970, loss_cps: 0.176509
[14:14:44.072] iteration 26535: total_loss: 0.473811, loss_sup: 0.090097, loss_mps: 0.133084, loss_cps: 0.250629
[14:14:44.217] iteration 26536: total_loss: 0.580466, loss_sup: 0.061746, loss_mps: 0.164657, loss_cps: 0.354062
[14:14:44.363] iteration 26537: total_loss: 0.459876, loss_sup: 0.041003, loss_mps: 0.136012, loss_cps: 0.282861
[14:14:44.512] iteration 26538: total_loss: 0.244059, loss_sup: 0.035436, loss_mps: 0.074411, loss_cps: 0.134213
[14:14:44.658] iteration 26539: total_loss: 0.509200, loss_sup: 0.065766, loss_mps: 0.138560, loss_cps: 0.304874
[14:14:44.804] iteration 26540: total_loss: 0.388840, loss_sup: 0.163651, loss_mps: 0.088297, loss_cps: 0.136892
[14:14:44.950] iteration 26541: total_loss: 0.277305, loss_sup: 0.033307, loss_mps: 0.089317, loss_cps: 0.154681
[14:14:45.095] iteration 26542: total_loss: 0.370660, loss_sup: 0.047457, loss_mps: 0.107187, loss_cps: 0.216016
[14:14:45.241] iteration 26543: total_loss: 0.302167, loss_sup: 0.067472, loss_mps: 0.087122, loss_cps: 0.147572
[14:14:45.386] iteration 26544: total_loss: 0.228955, loss_sup: 0.003903, loss_mps: 0.082167, loss_cps: 0.142885
[14:14:45.542] iteration 26545: total_loss: 0.240913, loss_sup: 0.028989, loss_mps: 0.079761, loss_cps: 0.132163
[14:14:45.689] iteration 26546: total_loss: 0.297024, loss_sup: 0.019009, loss_mps: 0.098247, loss_cps: 0.179767
[14:14:45.837] iteration 26547: total_loss: 0.260579, loss_sup: 0.015025, loss_mps: 0.091888, loss_cps: 0.153666
[14:14:45.988] iteration 26548: total_loss: 0.289540, loss_sup: 0.093109, loss_mps: 0.068700, loss_cps: 0.127730
[14:14:46.134] iteration 26549: total_loss: 0.690537, loss_sup: 0.142491, loss_mps: 0.187792, loss_cps: 0.360254
[14:14:46.280] iteration 26550: total_loss: 0.281288, loss_sup: 0.016447, loss_mps: 0.092035, loss_cps: 0.172805
[14:14:46.426] iteration 26551: total_loss: 0.244823, loss_sup: 0.047367, loss_mps: 0.075960, loss_cps: 0.121496
[14:14:46.576] iteration 26552: total_loss: 0.614696, loss_sup: 0.051238, loss_mps: 0.181823, loss_cps: 0.381636
[14:14:46.722] iteration 26553: total_loss: 0.267566, loss_sup: 0.054489, loss_mps: 0.074110, loss_cps: 0.138967
[14:14:46.868] iteration 26554: total_loss: 0.391571, loss_sup: 0.053159, loss_mps: 0.116813, loss_cps: 0.221599
[14:14:47.014] iteration 26555: total_loss: 0.322285, loss_sup: 0.085563, loss_mps: 0.086955, loss_cps: 0.149767
[14:14:47.160] iteration 26556: total_loss: 0.332159, loss_sup: 0.034333, loss_mps: 0.106585, loss_cps: 0.191241
[14:14:47.305] iteration 26557: total_loss: 0.702383, loss_sup: 0.119356, loss_mps: 0.193538, loss_cps: 0.389488
[14:14:47.451] iteration 26558: total_loss: 0.170010, loss_sup: 0.008395, loss_mps: 0.062551, loss_cps: 0.099065
[14:14:47.597] iteration 26559: total_loss: 0.359483, loss_sup: 0.016550, loss_mps: 0.114454, loss_cps: 0.228478
[14:14:47.743] iteration 26560: total_loss: 0.362997, loss_sup: 0.002591, loss_mps: 0.120755, loss_cps: 0.239652
[14:14:47.889] iteration 26561: total_loss: 0.689045, loss_sup: 0.081705, loss_mps: 0.192615, loss_cps: 0.414725
[14:14:48.035] iteration 26562: total_loss: 0.356822, loss_sup: 0.061308, loss_mps: 0.099601, loss_cps: 0.195913
[14:14:48.181] iteration 26563: total_loss: 0.313411, loss_sup: 0.023608, loss_mps: 0.095843, loss_cps: 0.193960
[14:14:48.326] iteration 26564: total_loss: 0.334650, loss_sup: 0.013586, loss_mps: 0.112954, loss_cps: 0.208110
[14:14:48.472] iteration 26565: total_loss: 0.292807, loss_sup: 0.020812, loss_mps: 0.095723, loss_cps: 0.176272
[14:14:48.618] iteration 26566: total_loss: 0.493424, loss_sup: 0.047687, loss_mps: 0.146076, loss_cps: 0.299662
[14:14:48.764] iteration 26567: total_loss: 0.253419, loss_sup: 0.005806, loss_mps: 0.088583, loss_cps: 0.159030
[14:14:48.910] iteration 26568: total_loss: 0.479619, loss_sup: 0.056190, loss_mps: 0.142138, loss_cps: 0.281292
[14:14:49.056] iteration 26569: total_loss: 0.245987, loss_sup: 0.015438, loss_mps: 0.082099, loss_cps: 0.148450
[14:14:49.203] iteration 26570: total_loss: 0.196803, loss_sup: 0.008127, loss_mps: 0.072405, loss_cps: 0.116271
[14:14:49.348] iteration 26571: total_loss: 0.534040, loss_sup: 0.119617, loss_mps: 0.129211, loss_cps: 0.285212
[14:14:49.494] iteration 26572: total_loss: 0.406768, loss_sup: 0.022765, loss_mps: 0.121500, loss_cps: 0.262503
[14:14:49.640] iteration 26573: total_loss: 0.244511, loss_sup: 0.018535, loss_mps: 0.081022, loss_cps: 0.144954
[14:14:49.787] iteration 26574: total_loss: 0.368095, loss_sup: 0.095718, loss_mps: 0.097204, loss_cps: 0.175172
[14:14:49.936] iteration 26575: total_loss: 0.205037, loss_sup: 0.009338, loss_mps: 0.073501, loss_cps: 0.122198
[14:14:50.086] iteration 26576: total_loss: 0.307562, loss_sup: 0.056289, loss_mps: 0.091972, loss_cps: 0.159300
[14:14:50.232] iteration 26577: total_loss: 0.323295, loss_sup: 0.012620, loss_mps: 0.103646, loss_cps: 0.207029
[14:14:50.380] iteration 26578: total_loss: 0.359985, loss_sup: 0.026623, loss_mps: 0.112195, loss_cps: 0.221167
[14:14:50.526] iteration 26579: total_loss: 0.325375, loss_sup: 0.019739, loss_mps: 0.103090, loss_cps: 0.202545
[14:14:50.674] iteration 26580: total_loss: 0.368250, loss_sup: 0.041058, loss_mps: 0.117114, loss_cps: 0.210078
[14:14:50.821] iteration 26581: total_loss: 0.266301, loss_sup: 0.073587, loss_mps: 0.073983, loss_cps: 0.118731
[14:14:50.966] iteration 26582: total_loss: 0.385307, loss_sup: 0.053099, loss_mps: 0.109242, loss_cps: 0.222966
[14:14:51.119] iteration 26583: total_loss: 0.408350, loss_sup: 0.019018, loss_mps: 0.128662, loss_cps: 0.260669
[14:14:51.266] iteration 26584: total_loss: 0.375166, loss_sup: 0.143476, loss_mps: 0.085204, loss_cps: 0.146485
[14:14:51.415] iteration 26585: total_loss: 0.533548, loss_sup: 0.043901, loss_mps: 0.162272, loss_cps: 0.327375
[14:14:51.564] iteration 26586: total_loss: 0.448225, loss_sup: 0.044392, loss_mps: 0.142194, loss_cps: 0.261639
[14:14:51.710] iteration 26587: total_loss: 0.305755, loss_sup: 0.041260, loss_mps: 0.092903, loss_cps: 0.171592
[14:14:51.856] iteration 26588: total_loss: 0.441113, loss_sup: 0.079438, loss_mps: 0.122803, loss_cps: 0.238871
[14:14:52.003] iteration 26589: total_loss: 0.402375, loss_sup: 0.034857, loss_mps: 0.130241, loss_cps: 0.237277
[14:14:52.153] iteration 26590: total_loss: 0.475095, loss_sup: 0.181347, loss_mps: 0.106632, loss_cps: 0.187116
[14:14:52.299] iteration 26591: total_loss: 0.220286, loss_sup: 0.009782, loss_mps: 0.075661, loss_cps: 0.134843
[14:14:52.448] iteration 26592: total_loss: 0.486760, loss_sup: 0.028853, loss_mps: 0.150205, loss_cps: 0.307701
[14:14:52.594] iteration 26593: total_loss: 0.387339, loss_sup: 0.028670, loss_mps: 0.118634, loss_cps: 0.240035
[14:14:52.740] iteration 26594: total_loss: 0.424008, loss_sup: 0.042564, loss_mps: 0.130537, loss_cps: 0.250907
[14:14:52.886] iteration 26595: total_loss: 0.579571, loss_sup: 0.087031, loss_mps: 0.155773, loss_cps: 0.336767
[14:14:53.037] iteration 26596: total_loss: 0.328811, loss_sup: 0.034022, loss_mps: 0.105550, loss_cps: 0.189239
[14:14:53.183] iteration 26597: total_loss: 0.642904, loss_sup: 0.144359, loss_mps: 0.160527, loss_cps: 0.338019
[14:14:53.330] iteration 26598: total_loss: 0.251864, loss_sup: 0.002411, loss_mps: 0.090271, loss_cps: 0.159182
[14:14:53.478] iteration 26599: total_loss: 0.235732, loss_sup: 0.007544, loss_mps: 0.082151, loss_cps: 0.146037
[14:14:53.624] iteration 26600: total_loss: 0.331023, loss_sup: 0.027372, loss_mps: 0.109759, loss_cps: 0.193893
[14:14:53.624] Evaluation Started ==>
[14:15:04.920] ==> valid iteration 26600: unet metrics: {'dc': 0.6569869196179654, 'jc': 0.541639877704411, 'pre': 0.8061189298033732, 'hd': 5.391292366414487}, ynet metrics: {'dc': 0.5981737225367928, 'jc': 0.4878647005190749, 'pre': 0.7967766275727692, 'hd': 5.356850490278676}.
[14:15:04.922] Evaluation Finished!⏹️
[14:15:05.073] iteration 26601: total_loss: 0.331808, loss_sup: 0.111188, loss_mps: 0.082675, loss_cps: 0.137945
[14:15:05.220] iteration 26602: total_loss: 0.459287, loss_sup: 0.035869, loss_mps: 0.151011, loss_cps: 0.272407
[14:15:05.366] iteration 26603: total_loss: 0.274490, loss_sup: 0.037385, loss_mps: 0.076565, loss_cps: 0.160540
[14:15:05.512] iteration 26604: total_loss: 0.268690, loss_sup: 0.061213, loss_mps: 0.071835, loss_cps: 0.135642
[14:15:05.657] iteration 26605: total_loss: 0.340660, loss_sup: 0.033887, loss_mps: 0.107147, loss_cps: 0.199626
[14:15:05.804] iteration 26606: total_loss: 0.529384, loss_sup: 0.004784, loss_mps: 0.169524, loss_cps: 0.355075
[14:15:05.949] iteration 26607: total_loss: 0.227458, loss_sup: 0.015161, loss_mps: 0.080768, loss_cps: 0.131529
[14:15:06.095] iteration 26608: total_loss: 0.371755, loss_sup: 0.134177, loss_mps: 0.090869, loss_cps: 0.146709
[14:15:06.240] iteration 26609: total_loss: 0.298862, loss_sup: 0.102029, loss_mps: 0.071584, loss_cps: 0.125250
[14:15:06.389] iteration 26610: total_loss: 0.257759, loss_sup: 0.004566, loss_mps: 0.091621, loss_cps: 0.161572
[14:15:06.535] iteration 26611: total_loss: 0.415994, loss_sup: 0.055107, loss_mps: 0.121585, loss_cps: 0.239302
[14:15:06.682] iteration 26612: total_loss: 0.352308, loss_sup: 0.051537, loss_mps: 0.105739, loss_cps: 0.195031
[14:15:06.827] iteration 26613: total_loss: 0.229082, loss_sup: 0.087351, loss_mps: 0.053576, loss_cps: 0.088154
[14:15:06.973] iteration 26614: total_loss: 0.409797, loss_sup: 0.097137, loss_mps: 0.100814, loss_cps: 0.211846
[14:15:07.119] iteration 26615: total_loss: 0.292645, loss_sup: 0.068561, loss_mps: 0.079944, loss_cps: 0.144140
[14:15:07.265] iteration 26616: total_loss: 0.531898, loss_sup: 0.029690, loss_mps: 0.171070, loss_cps: 0.331138
[14:15:07.411] iteration 26617: total_loss: 0.526258, loss_sup: 0.037133, loss_mps: 0.159058, loss_cps: 0.330067
[14:15:07.557] iteration 26618: total_loss: 0.393162, loss_sup: 0.132325, loss_mps: 0.086842, loss_cps: 0.173995
[14:15:07.702] iteration 26619: total_loss: 0.594823, loss_sup: 0.097730, loss_mps: 0.159063, loss_cps: 0.338030
[14:15:07.849] iteration 26620: total_loss: 0.306022, loss_sup: 0.003510, loss_mps: 0.107749, loss_cps: 0.194763
[14:15:07.995] iteration 26621: total_loss: 0.208805, loss_sup: 0.016339, loss_mps: 0.072619, loss_cps: 0.119846
[14:15:08.140] iteration 26622: total_loss: 0.298958, loss_sup: 0.014775, loss_mps: 0.097650, loss_cps: 0.186532
[14:15:08.286] iteration 26623: total_loss: 0.303993, loss_sup: 0.055274, loss_mps: 0.087478, loss_cps: 0.161241
[14:15:08.431] iteration 26624: total_loss: 0.768442, loss_sup: 0.026083, loss_mps: 0.227008, loss_cps: 0.515352
[14:15:08.578] iteration 26625: total_loss: 0.227099, loss_sup: 0.016424, loss_mps: 0.077000, loss_cps: 0.133674
[14:15:08.727] iteration 26626: total_loss: 0.571689, loss_sup: 0.136824, loss_mps: 0.144093, loss_cps: 0.290771
[14:15:08.872] iteration 26627: total_loss: 0.254506, loss_sup: 0.017729, loss_mps: 0.079516, loss_cps: 0.157260
[14:15:09.018] iteration 26628: total_loss: 0.287685, loss_sup: 0.028926, loss_mps: 0.095783, loss_cps: 0.162976
[14:15:09.164] iteration 26629: total_loss: 0.808767, loss_sup: 0.120608, loss_mps: 0.228502, loss_cps: 0.459656
[14:15:09.310] iteration 26630: total_loss: 0.532550, loss_sup: 0.072161, loss_mps: 0.141931, loss_cps: 0.318458
[14:15:09.456] iteration 26631: total_loss: 0.352447, loss_sup: 0.042973, loss_mps: 0.107305, loss_cps: 0.202170
[14:15:09.602] iteration 26632: total_loss: 0.517086, loss_sup: 0.023518, loss_mps: 0.150359, loss_cps: 0.343210
[14:15:09.747] iteration 26633: total_loss: 0.617473, loss_sup: 0.181218, loss_mps: 0.143218, loss_cps: 0.293037
[14:15:09.893] iteration 26634: total_loss: 0.457839, loss_sup: 0.122315, loss_mps: 0.117445, loss_cps: 0.218080
[14:15:10.038] iteration 26635: total_loss: 0.272623, loss_sup: 0.001042, loss_mps: 0.092536, loss_cps: 0.179045
[14:15:10.185] iteration 26636: total_loss: 0.300848, loss_sup: 0.009615, loss_mps: 0.106848, loss_cps: 0.184385
[14:15:10.332] iteration 26637: total_loss: 0.320144, loss_sup: 0.058723, loss_mps: 0.086506, loss_cps: 0.174915
[14:15:10.479] iteration 26638: total_loss: 0.213552, loss_sup: 0.015369, loss_mps: 0.068079, loss_cps: 0.130104
[14:15:10.625] iteration 26639: total_loss: 0.461430, loss_sup: 0.040016, loss_mps: 0.135466, loss_cps: 0.285949
[14:15:10.771] iteration 26640: total_loss: 0.220456, loss_sup: 0.043581, loss_mps: 0.066537, loss_cps: 0.110338
[14:15:10.918] iteration 26641: total_loss: 0.236031, loss_sup: 0.063834, loss_mps: 0.066326, loss_cps: 0.105871
[14:15:11.064] iteration 26642: total_loss: 0.573828, loss_sup: 0.019011, loss_mps: 0.173017, loss_cps: 0.381801
[14:15:11.212] iteration 26643: total_loss: 0.487527, loss_sup: 0.101427, loss_mps: 0.131714, loss_cps: 0.254386
[14:15:11.358] iteration 26644: total_loss: 0.233180, loss_sup: 0.013480, loss_mps: 0.079167, loss_cps: 0.140533
[14:15:11.506] iteration 26645: total_loss: 0.488303, loss_sup: 0.078329, loss_mps: 0.137795, loss_cps: 0.272179
[14:15:11.652] iteration 26646: total_loss: 0.468443, loss_sup: 0.113301, loss_mps: 0.118388, loss_cps: 0.236755
[14:15:11.797] iteration 26647: total_loss: 0.576605, loss_sup: 0.081073, loss_mps: 0.167114, loss_cps: 0.328418
[14:15:11.943] iteration 26648: total_loss: 0.308299, loss_sup: 0.055381, loss_mps: 0.089232, loss_cps: 0.163686
[14:15:12.089] iteration 26649: total_loss: 0.472475, loss_sup: 0.270457, loss_mps: 0.075773, loss_cps: 0.126244
[14:15:12.238] iteration 26650: total_loss: 0.376871, loss_sup: 0.037314, loss_mps: 0.121432, loss_cps: 0.218125
[14:15:12.384] iteration 26651: total_loss: 0.360190, loss_sup: 0.023721, loss_mps: 0.120828, loss_cps: 0.215641
[14:15:12.530] iteration 26652: total_loss: 0.271120, loss_sup: 0.006384, loss_mps: 0.090398, loss_cps: 0.174339
[14:15:12.676] iteration 26653: total_loss: 0.350263, loss_sup: 0.014704, loss_mps: 0.124761, loss_cps: 0.210799
[14:15:12.823] iteration 26654: total_loss: 0.288999, loss_sup: 0.032056, loss_mps: 0.096887, loss_cps: 0.160056
[14:15:12.969] iteration 26655: total_loss: 0.324049, loss_sup: 0.072600, loss_mps: 0.087884, loss_cps: 0.163565
[14:15:13.115] iteration 26656: total_loss: 0.279572, loss_sup: 0.003835, loss_mps: 0.099930, loss_cps: 0.175807
[14:15:13.261] iteration 26657: total_loss: 0.415782, loss_sup: 0.024388, loss_mps: 0.124550, loss_cps: 0.266844
[14:15:13.409] iteration 26658: total_loss: 0.311471, loss_sup: 0.030743, loss_mps: 0.093117, loss_cps: 0.187610
[14:15:13.555] iteration 26659: total_loss: 0.181664, loss_sup: 0.018110, loss_mps: 0.060639, loss_cps: 0.102915
[14:15:13.701] iteration 26660: total_loss: 0.323988, loss_sup: 0.107875, loss_mps: 0.075737, loss_cps: 0.140376
[14:15:13.848] iteration 26661: total_loss: 0.293816, loss_sup: 0.030598, loss_mps: 0.094778, loss_cps: 0.168440
[14:15:13.994] iteration 26662: total_loss: 0.414618, loss_sup: 0.057010, loss_mps: 0.119053, loss_cps: 0.238555
[14:15:14.140] iteration 26663: total_loss: 0.319468, loss_sup: 0.038295, loss_mps: 0.096916, loss_cps: 0.184257
[14:15:14.287] iteration 26664: total_loss: 0.321616, loss_sup: 0.042967, loss_mps: 0.099273, loss_cps: 0.179376
[14:15:14.433] iteration 26665: total_loss: 0.315908, loss_sup: 0.001078, loss_mps: 0.101629, loss_cps: 0.213201
[14:15:14.579] iteration 26666: total_loss: 0.392477, loss_sup: 0.045985, loss_mps: 0.119357, loss_cps: 0.227134
[14:15:14.725] iteration 26667: total_loss: 0.195400, loss_sup: 0.003463, loss_mps: 0.069177, loss_cps: 0.122759
[14:15:14.871] iteration 26668: total_loss: 0.257423, loss_sup: 0.006026, loss_mps: 0.089640, loss_cps: 0.161756
[14:15:15.016] iteration 26669: total_loss: 0.567379, loss_sup: 0.331879, loss_mps: 0.093031, loss_cps: 0.142468
[14:15:15.162] iteration 26670: total_loss: 0.572760, loss_sup: 0.218050, loss_mps: 0.120246, loss_cps: 0.234464
[14:15:15.309] iteration 26671: total_loss: 0.217171, loss_sup: 0.013244, loss_mps: 0.073628, loss_cps: 0.130300
[14:15:15.455] iteration 26672: total_loss: 0.488521, loss_sup: 0.118728, loss_mps: 0.126112, loss_cps: 0.243680
[14:15:15.600] iteration 26673: total_loss: 1.002445, loss_sup: 0.075424, loss_mps: 0.285504, loss_cps: 0.641517
[14:15:15.747] iteration 26674: total_loss: 0.249262, loss_sup: 0.020287, loss_mps: 0.084754, loss_cps: 0.144222
[14:15:15.893] iteration 26675: total_loss: 0.199970, loss_sup: 0.001134, loss_mps: 0.074137, loss_cps: 0.124699
[14:15:16.039] iteration 26676: total_loss: 0.553017, loss_sup: 0.255687, loss_mps: 0.104305, loss_cps: 0.193024
[14:15:16.185] iteration 26677: total_loss: 0.255517, loss_sup: 0.036860, loss_mps: 0.080791, loss_cps: 0.137865
[14:15:16.332] iteration 26678: total_loss: 0.207584, loss_sup: 0.004174, loss_mps: 0.071832, loss_cps: 0.131578
[14:15:16.478] iteration 26679: total_loss: 0.652493, loss_sup: 0.062485, loss_mps: 0.182703, loss_cps: 0.407304
[14:15:16.625] iteration 26680: total_loss: 0.261368, loss_sup: 0.053610, loss_mps: 0.079298, loss_cps: 0.128460
[14:15:16.771] iteration 26681: total_loss: 0.432955, loss_sup: 0.026536, loss_mps: 0.133605, loss_cps: 0.272814
[14:15:16.918] iteration 26682: total_loss: 0.513651, loss_sup: 0.016325, loss_mps: 0.166488, loss_cps: 0.330838
[14:15:17.065] iteration 26683: total_loss: 0.333944, loss_sup: 0.015144, loss_mps: 0.106845, loss_cps: 0.211955
[14:15:17.220] iteration 26684: total_loss: 0.491672, loss_sup: 0.093117, loss_mps: 0.130978, loss_cps: 0.267577
[14:15:17.367] iteration 26685: total_loss: 0.234452, loss_sup: 0.002220, loss_mps: 0.082233, loss_cps: 0.150000
[14:15:17.513] iteration 26686: total_loss: 0.531304, loss_sup: 0.257320, loss_mps: 0.095263, loss_cps: 0.178721
[14:15:17.659] iteration 26687: total_loss: 0.300951, loss_sup: 0.040672, loss_mps: 0.097785, loss_cps: 0.162495
[14:15:17.807] iteration 26688: total_loss: 0.436768, loss_sup: 0.097061, loss_mps: 0.114797, loss_cps: 0.224909
[14:15:17.952] iteration 26689: total_loss: 0.200558, loss_sup: 0.044553, loss_mps: 0.058389, loss_cps: 0.097616
[14:15:18.099] iteration 26690: total_loss: 0.310774, loss_sup: 0.007864, loss_mps: 0.100361, loss_cps: 0.202549
[14:15:18.246] iteration 26691: total_loss: 0.337054, loss_sup: 0.009969, loss_mps: 0.112602, loss_cps: 0.214483
[14:15:18.397] iteration 26692: total_loss: 0.519351, loss_sup: 0.041963, loss_mps: 0.153505, loss_cps: 0.323884
[14:15:18.544] iteration 26693: total_loss: 0.522787, loss_sup: 0.099067, loss_mps: 0.148116, loss_cps: 0.275604
[14:15:18.691] iteration 26694: total_loss: 0.289429, loss_sup: 0.028800, loss_mps: 0.096087, loss_cps: 0.164542
[14:15:18.837] iteration 26695: total_loss: 0.250526, loss_sup: 0.033981, loss_mps: 0.079415, loss_cps: 0.137130
[14:15:18.986] iteration 26696: total_loss: 0.589044, loss_sup: 0.296406, loss_mps: 0.100567, loss_cps: 0.192071
[14:15:19.137] iteration 26697: total_loss: 0.409952, loss_sup: 0.052757, loss_mps: 0.127140, loss_cps: 0.230056
[14:15:19.284] iteration 26698: total_loss: 0.452026, loss_sup: 0.042441, loss_mps: 0.134758, loss_cps: 0.274826
[14:15:19.429] iteration 26699: total_loss: 0.325433, loss_sup: 0.077066, loss_mps: 0.088357, loss_cps: 0.160011
[14:15:19.575] iteration 26700: total_loss: 0.183514, loss_sup: 0.016668, loss_mps: 0.063312, loss_cps: 0.103534
[14:15:19.576] Evaluation Started ==>
[14:15:30.947] ==> valid iteration 26700: unet metrics: {'dc': 0.6493981117563393, 'jc': 0.5350280584857743, 'pre': 0.7992826768435412, 'hd': 5.393170632400258}, ynet metrics: {'dc': 0.6190028812714424, 'jc': 0.5065802197403955, 'pre': 0.7860938663618154, 'hd': 5.466669221176929}.
[14:15:30.949] Evaluation Finished!⏹️
[14:15:31.101] iteration 26701: total_loss: 0.202710, loss_sup: 0.019173, loss_mps: 0.067399, loss_cps: 0.116138
[14:15:31.248] iteration 26702: total_loss: 0.440607, loss_sup: 0.041383, loss_mps: 0.132048, loss_cps: 0.267176
[14:15:31.394] iteration 26703: total_loss: 0.280019, loss_sup: 0.029854, loss_mps: 0.095519, loss_cps: 0.154646
[14:15:31.539] iteration 26704: total_loss: 0.508017, loss_sup: 0.028533, loss_mps: 0.158454, loss_cps: 0.321029
[14:15:31.685] iteration 26705: total_loss: 0.277600, loss_sup: 0.059463, loss_mps: 0.080166, loss_cps: 0.137970
[14:15:31.830] iteration 26706: total_loss: 0.411615, loss_sup: 0.012369, loss_mps: 0.131338, loss_cps: 0.267908
[14:15:31.977] iteration 26707: total_loss: 0.305738, loss_sup: 0.029441, loss_mps: 0.100965, loss_cps: 0.175331
[14:15:32.123] iteration 26708: total_loss: 0.359473, loss_sup: 0.009251, loss_mps: 0.112097, loss_cps: 0.238126
[14:15:32.268] iteration 26709: total_loss: 0.181445, loss_sup: 0.040284, loss_mps: 0.052020, loss_cps: 0.089142
[14:15:32.414] iteration 26710: total_loss: 0.321395, loss_sup: 0.070463, loss_mps: 0.096642, loss_cps: 0.154289
[14:15:32.560] iteration 26711: total_loss: 0.214354, loss_sup: 0.001655, loss_mps: 0.080131, loss_cps: 0.132569
[14:15:32.707] iteration 26712: total_loss: 0.200970, loss_sup: 0.005026, loss_mps: 0.072086, loss_cps: 0.123858
[14:15:32.852] iteration 26713: total_loss: 0.640854, loss_sup: 0.028841, loss_mps: 0.196260, loss_cps: 0.415753
[14:15:32.997] iteration 26714: total_loss: 0.182088, loss_sup: 0.006700, loss_mps: 0.067649, loss_cps: 0.107740
[14:15:33.142] iteration 26715: total_loss: 0.683015, loss_sup: 0.073394, loss_mps: 0.198213, loss_cps: 0.411407
[14:15:33.288] iteration 26716: total_loss: 0.334828, loss_sup: 0.066213, loss_mps: 0.097936, loss_cps: 0.170679
[14:15:33.435] iteration 26717: total_loss: 0.238494, loss_sup: 0.032167, loss_mps: 0.078061, loss_cps: 0.128266
[14:15:33.581] iteration 26718: total_loss: 0.522769, loss_sup: 0.106349, loss_mps: 0.150496, loss_cps: 0.265924
[14:15:33.727] iteration 26719: total_loss: 0.344094, loss_sup: 0.063412, loss_mps: 0.093423, loss_cps: 0.187259
[14:15:33.872] iteration 26720: total_loss: 0.315796, loss_sup: 0.017655, loss_mps: 0.104382, loss_cps: 0.193759
[14:15:34.018] iteration 26721: total_loss: 0.305576, loss_sup: 0.017369, loss_mps: 0.098947, loss_cps: 0.189260
[14:15:34.163] iteration 26722: total_loss: 0.379030, loss_sup: 0.013224, loss_mps: 0.130836, loss_cps: 0.234970
[14:15:34.308] iteration 26723: total_loss: 0.242053, loss_sup: 0.055336, loss_mps: 0.070549, loss_cps: 0.116168
[14:15:34.454] iteration 26724: total_loss: 0.376853, loss_sup: 0.052881, loss_mps: 0.114966, loss_cps: 0.209006
[14:15:34.599] iteration 26725: total_loss: 0.468129, loss_sup: 0.097556, loss_mps: 0.129630, loss_cps: 0.240943
[14:15:34.746] iteration 26726: total_loss: 0.328782, loss_sup: 0.003338, loss_mps: 0.110722, loss_cps: 0.214722
[14:15:34.894] iteration 26727: total_loss: 0.347408, loss_sup: 0.024982, loss_mps: 0.115923, loss_cps: 0.206503
[14:15:35.040] iteration 26728: total_loss: 0.320808, loss_sup: 0.117130, loss_mps: 0.074503, loss_cps: 0.129175
[14:15:35.185] iteration 26729: total_loss: 0.371745, loss_sup: 0.070547, loss_mps: 0.108498, loss_cps: 0.192700
[14:15:35.332] iteration 26730: total_loss: 0.403181, loss_sup: 0.067949, loss_mps: 0.114516, loss_cps: 0.220715
[14:15:35.480] iteration 26731: total_loss: 0.377230, loss_sup: 0.018822, loss_mps: 0.127911, loss_cps: 0.230497
[14:15:35.626] iteration 26732: total_loss: 0.266664, loss_sup: 0.027860, loss_mps: 0.084400, loss_cps: 0.154404
[14:15:35.774] iteration 26733: total_loss: 0.337860, loss_sup: 0.035299, loss_mps: 0.111893, loss_cps: 0.190668
[14:15:35.920] iteration 26734: total_loss: 0.269098, loss_sup: 0.015416, loss_mps: 0.091785, loss_cps: 0.161897
[14:15:36.066] iteration 26735: total_loss: 0.274936, loss_sup: 0.018441, loss_mps: 0.090236, loss_cps: 0.166260
[14:15:36.212] iteration 26736: total_loss: 0.562055, loss_sup: 0.051970, loss_mps: 0.163433, loss_cps: 0.346652
[14:15:36.359] iteration 26737: total_loss: 0.451238, loss_sup: 0.011105, loss_mps: 0.142895, loss_cps: 0.297239
[14:15:36.506] iteration 26738: total_loss: 0.531281, loss_sup: 0.084210, loss_mps: 0.156674, loss_cps: 0.290396
[14:15:36.652] iteration 26739: total_loss: 0.584539, loss_sup: 0.087516, loss_mps: 0.160546, loss_cps: 0.336478
[14:15:36.797] iteration 26740: total_loss: 0.499389, loss_sup: 0.167608, loss_mps: 0.117202, loss_cps: 0.214579
[14:15:36.943] iteration 26741: total_loss: 0.258179, loss_sup: 0.009527, loss_mps: 0.096166, loss_cps: 0.152486
[14:15:37.088] iteration 26742: total_loss: 0.240020, loss_sup: 0.032938, loss_mps: 0.074876, loss_cps: 0.132206
[14:15:37.233] iteration 26743: total_loss: 0.336160, loss_sup: 0.009380, loss_mps: 0.117370, loss_cps: 0.209410
[14:15:37.379] iteration 26744: total_loss: 0.488512, loss_sup: 0.061458, loss_mps: 0.141123, loss_cps: 0.285931
[14:15:37.525] iteration 26745: total_loss: 0.421287, loss_sup: 0.031972, loss_mps: 0.128245, loss_cps: 0.261070
[14:15:37.672] iteration 26746: total_loss: 0.352344, loss_sup: 0.028215, loss_mps: 0.113756, loss_cps: 0.210373
[14:15:37.818] iteration 26747: total_loss: 0.548461, loss_sup: 0.005051, loss_mps: 0.171488, loss_cps: 0.371922
[14:15:37.963] iteration 26748: total_loss: 0.259074, loss_sup: 0.040038, loss_mps: 0.078144, loss_cps: 0.140892
[14:15:38.109] iteration 26749: total_loss: 0.257180, loss_sup: 0.007836, loss_mps: 0.086843, loss_cps: 0.162501
[14:15:38.255] iteration 26750: total_loss: 0.405545, loss_sup: 0.040300, loss_mps: 0.125045, loss_cps: 0.240200
[14:15:38.400] iteration 26751: total_loss: 0.591315, loss_sup: 0.012214, loss_mps: 0.184941, loss_cps: 0.394160
[14:15:38.461] iteration 26752: total_loss: 0.160994, loss_sup: 0.000162, loss_mps: 0.061474, loss_cps: 0.099359
[14:15:39.685] iteration 26753: total_loss: 0.735040, loss_sup: 0.040619, loss_mps: 0.221986, loss_cps: 0.472434
[14:15:39.833] iteration 26754: total_loss: 0.595432, loss_sup: 0.234368, loss_mps: 0.114897, loss_cps: 0.246167
[14:15:39.981] iteration 26755: total_loss: 0.293284, loss_sup: 0.010612, loss_mps: 0.098633, loss_cps: 0.184040
[14:15:40.127] iteration 26756: total_loss: 0.591825, loss_sup: 0.152210, loss_mps: 0.138044, loss_cps: 0.301570
[14:15:40.274] iteration 26757: total_loss: 0.460885, loss_sup: 0.062197, loss_mps: 0.132918, loss_cps: 0.265769
[14:15:40.420] iteration 26758: total_loss: 0.630019, loss_sup: 0.026085, loss_mps: 0.189769, loss_cps: 0.414164
[14:15:40.566] iteration 26759: total_loss: 0.516790, loss_sup: 0.017329, loss_mps: 0.159409, loss_cps: 0.340051
[14:15:40.718] iteration 26760: total_loss: 0.323669, loss_sup: 0.070324, loss_mps: 0.089681, loss_cps: 0.163665
[14:15:40.865] iteration 26761: total_loss: 0.534329, loss_sup: 0.086862, loss_mps: 0.142505, loss_cps: 0.304962
[14:15:41.011] iteration 26762: total_loss: 0.295511, loss_sup: 0.006917, loss_mps: 0.102915, loss_cps: 0.185678
[14:15:41.158] iteration 26763: total_loss: 0.236110, loss_sup: 0.005384, loss_mps: 0.080875, loss_cps: 0.149851
[14:15:41.304] iteration 26764: total_loss: 0.257539, loss_sup: 0.026976, loss_mps: 0.083696, loss_cps: 0.146867
[14:15:41.450] iteration 26765: total_loss: 0.316887, loss_sup: 0.039655, loss_mps: 0.098576, loss_cps: 0.178656
[14:15:41.597] iteration 26766: total_loss: 0.330558, loss_sup: 0.054934, loss_mps: 0.088413, loss_cps: 0.187211
[14:15:41.744] iteration 26767: total_loss: 0.320382, loss_sup: 0.009449, loss_mps: 0.110757, loss_cps: 0.200177
[14:15:41.892] iteration 26768: total_loss: 0.687618, loss_sup: 0.220607, loss_mps: 0.154775, loss_cps: 0.312236
[14:15:42.039] iteration 26769: total_loss: 0.661398, loss_sup: 0.275940, loss_mps: 0.134919, loss_cps: 0.250539
[14:15:42.186] iteration 26770: total_loss: 0.209649, loss_sup: 0.005662, loss_mps: 0.077042, loss_cps: 0.126945
[14:15:42.332] iteration 26771: total_loss: 0.291404, loss_sup: 0.021121, loss_mps: 0.103558, loss_cps: 0.166725
[14:15:42.479] iteration 26772: total_loss: 0.270382, loss_sup: 0.062130, loss_mps: 0.074684, loss_cps: 0.133568
[14:15:42.626] iteration 26773: total_loss: 0.314845, loss_sup: 0.003577, loss_mps: 0.103465, loss_cps: 0.207804
[14:15:42.773] iteration 26774: total_loss: 0.194088, loss_sup: 0.018457, loss_mps: 0.066323, loss_cps: 0.109308
[14:15:42.923] iteration 26775: total_loss: 0.382335, loss_sup: 0.003613, loss_mps: 0.119153, loss_cps: 0.259570
[14:15:43.070] iteration 26776: total_loss: 0.320818, loss_sup: 0.035220, loss_mps: 0.096582, loss_cps: 0.189016
[14:15:43.216] iteration 26777: total_loss: 0.287974, loss_sup: 0.014412, loss_mps: 0.100822, loss_cps: 0.172739
[14:15:43.364] iteration 26778: total_loss: 1.100477, loss_sup: 0.573440, loss_mps: 0.164014, loss_cps: 0.363024
[14:15:43.512] iteration 26779: total_loss: 0.217599, loss_sup: 0.016040, loss_mps: 0.073929, loss_cps: 0.127629
[14:15:43.660] iteration 26780: total_loss: 0.635245, loss_sup: 0.092064, loss_mps: 0.173685, loss_cps: 0.369496
[14:15:43.807] iteration 26781: total_loss: 0.447049, loss_sup: 0.009642, loss_mps: 0.135472, loss_cps: 0.301934
[14:15:43.955] iteration 26782: total_loss: 0.560997, loss_sup: 0.065840, loss_mps: 0.168547, loss_cps: 0.326609
[14:15:44.101] iteration 26783: total_loss: 0.332703, loss_sup: 0.071765, loss_mps: 0.090448, loss_cps: 0.170490
[14:15:44.248] iteration 26784: total_loss: 0.756301, loss_sup: 0.197685, loss_mps: 0.177731, loss_cps: 0.380885
[14:15:44.395] iteration 26785: total_loss: 0.200534, loss_sup: 0.038084, loss_mps: 0.063183, loss_cps: 0.099267
[14:15:44.543] iteration 26786: total_loss: 0.489206, loss_sup: 0.100607, loss_mps: 0.134919, loss_cps: 0.253680
[14:15:44.690] iteration 26787: total_loss: 0.176837, loss_sup: 0.065469, loss_mps: 0.044979, loss_cps: 0.066388
[14:15:44.838] iteration 26788: total_loss: 0.584112, loss_sup: 0.063524, loss_mps: 0.161072, loss_cps: 0.359516
[14:15:44.985] iteration 26789: total_loss: 0.514121, loss_sup: 0.042345, loss_mps: 0.151114, loss_cps: 0.320662
[14:15:45.132] iteration 26790: total_loss: 0.410339, loss_sup: 0.006454, loss_mps: 0.140375, loss_cps: 0.263510
[14:15:45.279] iteration 26791: total_loss: 0.281199, loss_sup: 0.011618, loss_mps: 0.097222, loss_cps: 0.172359
[14:15:45.426] iteration 26792: total_loss: 0.197939, loss_sup: 0.048017, loss_mps: 0.061273, loss_cps: 0.088649
[14:15:45.572] iteration 26793: total_loss: 0.198752, loss_sup: 0.022978, loss_mps: 0.069203, loss_cps: 0.106571
[14:15:45.719] iteration 26794: total_loss: 0.190018, loss_sup: 0.002138, loss_mps: 0.070627, loss_cps: 0.117253
[14:15:45.867] iteration 26795: total_loss: 0.265036, loss_sup: 0.039528, loss_mps: 0.088415, loss_cps: 0.137093
[14:15:46.014] iteration 26796: total_loss: 0.244217, loss_sup: 0.012270, loss_mps: 0.083742, loss_cps: 0.148205
[14:15:46.161] iteration 26797: total_loss: 0.617864, loss_sup: 0.183983, loss_mps: 0.142611, loss_cps: 0.291269
[14:15:46.308] iteration 26798: total_loss: 0.578227, loss_sup: 0.035119, loss_mps: 0.196725, loss_cps: 0.346382
[14:15:46.454] iteration 26799: total_loss: 0.282850, loss_sup: 0.019151, loss_mps: 0.095535, loss_cps: 0.168165
[14:15:46.600] iteration 26800: total_loss: 0.350427, loss_sup: 0.037572, loss_mps: 0.110555, loss_cps: 0.202299
[14:15:46.600] Evaluation Started ==>
[14:15:57.960] ==> valid iteration 26800: unet metrics: {'dc': 0.6556885519080858, 'jc': 0.5441663573708025, 'pre': 0.7982868471799099, 'hd': 5.346910710797748}, ynet metrics: {'dc': 0.6111206153651761, 'jc': 0.5004865914053929, 'pre': 0.7984607399092704, 'hd': 5.33473891174564}.
[14:15:57.962] Evaluation Finished!⏹️
[14:15:58.115] iteration 26801: total_loss: 0.490655, loss_sup: 0.049087, loss_mps: 0.149709, loss_cps: 0.291859
[14:15:58.268] iteration 26802: total_loss: 0.430043, loss_sup: 0.056843, loss_mps: 0.129707, loss_cps: 0.243494
[14:15:58.413] iteration 26803: total_loss: 0.183044, loss_sup: 0.009747, loss_mps: 0.065570, loss_cps: 0.107727
[14:15:58.559] iteration 26804: total_loss: 0.274993, loss_sup: 0.007371, loss_mps: 0.098227, loss_cps: 0.169395
[14:15:58.704] iteration 26805: total_loss: 0.590178, loss_sup: 0.075446, loss_mps: 0.173596, loss_cps: 0.341136
[14:15:58.848] iteration 26806: total_loss: 0.325110, loss_sup: 0.026432, loss_mps: 0.099470, loss_cps: 0.199207
[14:15:58.993] iteration 26807: total_loss: 0.285984, loss_sup: 0.027006, loss_mps: 0.103585, loss_cps: 0.155394
[14:15:59.137] iteration 26808: total_loss: 0.287243, loss_sup: 0.094906, loss_mps: 0.073042, loss_cps: 0.119295
[14:15:59.283] iteration 26809: total_loss: 0.365622, loss_sup: 0.040528, loss_mps: 0.111379, loss_cps: 0.213716
[14:15:59.428] iteration 26810: total_loss: 1.025798, loss_sup: 0.162890, loss_mps: 0.270279, loss_cps: 0.592629
[14:15:59.573] iteration 26811: total_loss: 0.472965, loss_sup: 0.115219, loss_mps: 0.126334, loss_cps: 0.231413
[14:15:59.719] iteration 26812: total_loss: 0.227885, loss_sup: 0.021153, loss_mps: 0.078492, loss_cps: 0.128241
[14:15:59.864] iteration 26813: total_loss: 0.442417, loss_sup: 0.093336, loss_mps: 0.120051, loss_cps: 0.229030
[14:16:00.011] iteration 26814: total_loss: 0.384995, loss_sup: 0.014502, loss_mps: 0.121777, loss_cps: 0.248716
[14:16:00.156] iteration 26815: total_loss: 0.357222, loss_sup: 0.042908, loss_mps: 0.107379, loss_cps: 0.206934
[14:16:00.302] iteration 26816: total_loss: 0.182473, loss_sup: 0.009682, loss_mps: 0.065842, loss_cps: 0.106949
[14:16:00.447] iteration 26817: total_loss: 0.207968, loss_sup: 0.003571, loss_mps: 0.075167, loss_cps: 0.129230
[14:16:00.592] iteration 26818: total_loss: 0.268609, loss_sup: 0.005583, loss_mps: 0.100193, loss_cps: 0.162833
[14:16:00.738] iteration 26819: total_loss: 0.264011, loss_sup: 0.041258, loss_mps: 0.088126, loss_cps: 0.134627
[14:16:00.883] iteration 26820: total_loss: 0.518612, loss_sup: 0.066941, loss_mps: 0.157397, loss_cps: 0.294274
[14:16:01.029] iteration 26821: total_loss: 0.281394, loss_sup: 0.029751, loss_mps: 0.091092, loss_cps: 0.160551
[14:16:01.174] iteration 26822: total_loss: 0.166787, loss_sup: 0.005642, loss_mps: 0.061185, loss_cps: 0.099959
[14:16:01.319] iteration 26823: total_loss: 0.431092, loss_sup: 0.074647, loss_mps: 0.125954, loss_cps: 0.230491
[14:16:01.465] iteration 26824: total_loss: 0.231262, loss_sup: 0.002523, loss_mps: 0.085190, loss_cps: 0.143548
[14:16:01.611] iteration 26825: total_loss: 0.335041, loss_sup: 0.043064, loss_mps: 0.104299, loss_cps: 0.187679
[14:16:01.756] iteration 26826: total_loss: 0.422496, loss_sup: 0.041860, loss_mps: 0.132612, loss_cps: 0.248024
[14:16:01.901] iteration 26827: total_loss: 0.376541, loss_sup: 0.087745, loss_mps: 0.100800, loss_cps: 0.187996
[14:16:02.047] iteration 26828: total_loss: 0.385665, loss_sup: 0.013510, loss_mps: 0.114855, loss_cps: 0.257301
[14:16:02.193] iteration 26829: total_loss: 0.340085, loss_sup: 0.047867, loss_mps: 0.099137, loss_cps: 0.193080
[14:16:02.339] iteration 26830: total_loss: 0.198490, loss_sup: 0.008409, loss_mps: 0.072577, loss_cps: 0.117504
[14:16:02.485] iteration 26831: total_loss: 0.245026, loss_sup: 0.025124, loss_mps: 0.081975, loss_cps: 0.137927
[14:16:02.630] iteration 26832: total_loss: 0.435601, loss_sup: 0.022209, loss_mps: 0.127526, loss_cps: 0.285866
[14:16:02.776] iteration 26833: total_loss: 0.324066, loss_sup: 0.067367, loss_mps: 0.087607, loss_cps: 0.169093
[14:16:02.921] iteration 26834: total_loss: 0.405166, loss_sup: 0.114076, loss_mps: 0.103138, loss_cps: 0.187952
[14:16:03.067] iteration 26835: total_loss: 0.489397, loss_sup: 0.026888, loss_mps: 0.158210, loss_cps: 0.304299
[14:16:03.215] iteration 26836: total_loss: 0.249395, loss_sup: 0.027445, loss_mps: 0.084510, loss_cps: 0.137440
[14:16:03.363] iteration 26837: total_loss: 0.236818, loss_sup: 0.017869, loss_mps: 0.081052, loss_cps: 0.137898
[14:16:03.510] iteration 26838: total_loss: 0.161822, loss_sup: 0.010478, loss_mps: 0.058454, loss_cps: 0.092890
[14:16:03.655] iteration 26839: total_loss: 0.310987, loss_sup: 0.001907, loss_mps: 0.107400, loss_cps: 0.201680
[14:16:03.803] iteration 26840: total_loss: 0.376005, loss_sup: 0.013837, loss_mps: 0.118317, loss_cps: 0.243851
[14:16:03.949] iteration 26841: total_loss: 0.658906, loss_sup: 0.190849, loss_mps: 0.171649, loss_cps: 0.296408
[14:16:04.095] iteration 26842: total_loss: 0.426768, loss_sup: 0.161981, loss_mps: 0.095902, loss_cps: 0.168885
[14:16:04.243] iteration 26843: total_loss: 0.314582, loss_sup: 0.004575, loss_mps: 0.104542, loss_cps: 0.205465
[14:16:04.390] iteration 26844: total_loss: 0.376794, loss_sup: 0.009771, loss_mps: 0.126590, loss_cps: 0.240433
[14:16:04.535] iteration 26845: total_loss: 0.265780, loss_sup: 0.029490, loss_mps: 0.083947, loss_cps: 0.152343
[14:16:04.681] iteration 26846: total_loss: 0.301615, loss_sup: 0.049648, loss_mps: 0.088589, loss_cps: 0.163379
[14:16:04.827] iteration 26847: total_loss: 0.337579, loss_sup: 0.022572, loss_mps: 0.096832, loss_cps: 0.218175
[14:16:04.973] iteration 26848: total_loss: 0.225982, loss_sup: 0.035642, loss_mps: 0.074488, loss_cps: 0.115853
[14:16:05.120] iteration 26849: total_loss: 0.511690, loss_sup: 0.061160, loss_mps: 0.150118, loss_cps: 0.300413
[14:16:05.266] iteration 26850: total_loss: 0.211739, loss_sup: 0.002004, loss_mps: 0.074973, loss_cps: 0.134761
[14:16:05.413] iteration 26851: total_loss: 0.323861, loss_sup: 0.053553, loss_mps: 0.097854, loss_cps: 0.172454
[14:16:05.558] iteration 26852: total_loss: 0.481754, loss_sup: 0.106973, loss_mps: 0.128818, loss_cps: 0.245963
[14:16:05.704] iteration 26853: total_loss: 0.244394, loss_sup: 0.056842, loss_mps: 0.066472, loss_cps: 0.121080
[14:16:05.850] iteration 26854: total_loss: 0.423121, loss_sup: 0.088729, loss_mps: 0.118372, loss_cps: 0.216020
[14:16:05.997] iteration 26855: total_loss: 0.566149, loss_sup: 0.202065, loss_mps: 0.119563, loss_cps: 0.244521
[14:16:06.143] iteration 26856: total_loss: 0.394972, loss_sup: 0.069687, loss_mps: 0.119504, loss_cps: 0.205781
[14:16:06.289] iteration 26857: total_loss: 0.168065, loss_sup: 0.002229, loss_mps: 0.062271, loss_cps: 0.103565
[14:16:06.435] iteration 26858: total_loss: 0.557491, loss_sup: 0.162411, loss_mps: 0.137911, loss_cps: 0.257169
[14:16:06.581] iteration 26859: total_loss: 0.317439, loss_sup: 0.062654, loss_mps: 0.089535, loss_cps: 0.165250
[14:16:06.727] iteration 26860: total_loss: 0.322069, loss_sup: 0.067507, loss_mps: 0.089962, loss_cps: 0.164600
[14:16:06.873] iteration 26861: total_loss: 0.446265, loss_sup: 0.165512, loss_mps: 0.102218, loss_cps: 0.178536
[14:16:07.019] iteration 26862: total_loss: 0.301052, loss_sup: 0.017161, loss_mps: 0.104745, loss_cps: 0.179146
[14:16:07.164] iteration 26863: total_loss: 0.478083, loss_sup: 0.056401, loss_mps: 0.141342, loss_cps: 0.280340
[14:16:07.311] iteration 26864: total_loss: 0.315128, loss_sup: 0.026180, loss_mps: 0.102035, loss_cps: 0.186914
[14:16:07.458] iteration 26865: total_loss: 0.431801, loss_sup: 0.054789, loss_mps: 0.130623, loss_cps: 0.246390
[14:16:07.604] iteration 26866: total_loss: 0.279521, loss_sup: 0.042157, loss_mps: 0.082941, loss_cps: 0.154423
[14:16:07.749] iteration 26867: total_loss: 0.313703, loss_sup: 0.003890, loss_mps: 0.107989, loss_cps: 0.201824
[14:16:07.895] iteration 26868: total_loss: 0.397923, loss_sup: 0.034476, loss_mps: 0.117549, loss_cps: 0.245899
[14:16:08.041] iteration 26869: total_loss: 0.413483, loss_sup: 0.174877, loss_mps: 0.088118, loss_cps: 0.150488
[14:16:08.187] iteration 26870: total_loss: 0.267867, loss_sup: 0.086161, loss_mps: 0.068114, loss_cps: 0.113592
[14:16:08.333] iteration 26871: total_loss: 0.262655, loss_sup: 0.010710, loss_mps: 0.089311, loss_cps: 0.162633
[14:16:08.479] iteration 26872: total_loss: 0.331219, loss_sup: 0.061744, loss_mps: 0.090863, loss_cps: 0.178611
[14:16:08.625] iteration 26873: total_loss: 0.278921, loss_sup: 0.027513, loss_mps: 0.089324, loss_cps: 0.162084
[14:16:08.772] iteration 26874: total_loss: 0.416800, loss_sup: 0.034273, loss_mps: 0.130531, loss_cps: 0.251997
[14:16:08.918] iteration 26875: total_loss: 0.320140, loss_sup: 0.074028, loss_mps: 0.088246, loss_cps: 0.157866
[14:16:09.063] iteration 26876: total_loss: 0.521417, loss_sup: 0.040470, loss_mps: 0.145728, loss_cps: 0.335219
[14:16:09.209] iteration 26877: total_loss: 0.413418, loss_sup: 0.032893, loss_mps: 0.125294, loss_cps: 0.255231
[14:16:09.356] iteration 26878: total_loss: 0.296935, loss_sup: 0.038086, loss_mps: 0.091756, loss_cps: 0.167093
[14:16:09.502] iteration 26879: total_loss: 0.256847, loss_sup: 0.015883, loss_mps: 0.091232, loss_cps: 0.149732
[14:16:09.652] iteration 26880: total_loss: 0.326421, loss_sup: 0.025568, loss_mps: 0.103203, loss_cps: 0.197649
[14:16:09.798] iteration 26881: total_loss: 0.213750, loss_sup: 0.033007, loss_mps: 0.064839, loss_cps: 0.115905
[14:16:09.945] iteration 26882: total_loss: 1.180439, loss_sup: 0.101862, loss_mps: 0.346921, loss_cps: 0.731655
[14:16:10.091] iteration 26883: total_loss: 0.214635, loss_sup: 0.001329, loss_mps: 0.076351, loss_cps: 0.136955
[14:16:10.237] iteration 26884: total_loss: 0.486644, loss_sup: 0.031577, loss_mps: 0.153461, loss_cps: 0.301606
[14:16:10.384] iteration 26885: total_loss: 0.252500, loss_sup: 0.006742, loss_mps: 0.083819, loss_cps: 0.161939
[14:16:10.536] iteration 26886: total_loss: 0.666238, loss_sup: 0.115207, loss_mps: 0.181228, loss_cps: 0.369803
[14:16:10.685] iteration 26887: total_loss: 0.467006, loss_sup: 0.029658, loss_mps: 0.140430, loss_cps: 0.296919
[14:16:10.833] iteration 26888: total_loss: 0.157394, loss_sup: 0.002882, loss_mps: 0.056049, loss_cps: 0.098464
[14:16:10.978] iteration 26889: total_loss: 0.351131, loss_sup: 0.001484, loss_mps: 0.114927, loss_cps: 0.234720
[14:16:11.125] iteration 26890: total_loss: 0.208315, loss_sup: 0.000682, loss_mps: 0.069497, loss_cps: 0.138136
[14:16:11.271] iteration 26891: total_loss: 0.208475, loss_sup: 0.002618, loss_mps: 0.076981, loss_cps: 0.128876
[14:16:11.417] iteration 26892: total_loss: 0.758846, loss_sup: 0.025058, loss_mps: 0.227663, loss_cps: 0.506126
[14:16:11.563] iteration 26893: total_loss: 0.314082, loss_sup: 0.013127, loss_mps: 0.102360, loss_cps: 0.198596
[14:16:11.710] iteration 26894: total_loss: 0.462162, loss_sup: 0.033668, loss_mps: 0.138046, loss_cps: 0.290448
[14:16:11.856] iteration 26895: total_loss: 0.215584, loss_sup: 0.003559, loss_mps: 0.073899, loss_cps: 0.138126
[14:16:12.002] iteration 26896: total_loss: 0.375210, loss_sup: 0.075224, loss_mps: 0.112146, loss_cps: 0.187840
[14:16:12.149] iteration 26897: total_loss: 0.179298, loss_sup: 0.010606, loss_mps: 0.061837, loss_cps: 0.106855
[14:16:12.296] iteration 26898: total_loss: 0.342385, loss_sup: 0.002455, loss_mps: 0.110974, loss_cps: 0.228956
[14:16:12.443] iteration 26899: total_loss: 0.397745, loss_sup: 0.077446, loss_mps: 0.115562, loss_cps: 0.204737
[14:16:12.590] iteration 26900: total_loss: 0.427485, loss_sup: 0.035953, loss_mps: 0.137799, loss_cps: 0.253733
[14:16:12.590] Evaluation Started ==>
[14:16:23.915] ==> valid iteration 26900: unet metrics: {'dc': 0.6702949456061323, 'jc': 0.5546769486371137, 'pre': 0.8097904070004278, 'hd': 5.436611459141283}, ynet metrics: {'dc': 0.6168217976487619, 'jc': 0.5041450385257947, 'pre': 0.7949709808650793, 'hd': 5.429237834995482}.
[14:16:23.917] Evaluation Finished!⏹️
[14:16:24.069] iteration 26901: total_loss: 0.328926, loss_sup: 0.010058, loss_mps: 0.117243, loss_cps: 0.201625
[14:16:24.216] iteration 26902: total_loss: 0.438981, loss_sup: 0.067092, loss_mps: 0.125968, loss_cps: 0.245920
[14:16:24.366] iteration 26903: total_loss: 0.358051, loss_sup: 0.037091, loss_mps: 0.113994, loss_cps: 0.206966
[14:16:24.511] iteration 26904: total_loss: 0.319214, loss_sup: 0.039995, loss_mps: 0.097894, loss_cps: 0.181324
[14:16:24.656] iteration 26905: total_loss: 0.231658, loss_sup: 0.006255, loss_mps: 0.082656, loss_cps: 0.142747
[14:16:24.802] iteration 26906: total_loss: 0.402301, loss_sup: 0.029497, loss_mps: 0.123184, loss_cps: 0.249620
[14:16:24.949] iteration 26907: total_loss: 0.713272, loss_sup: 0.136392, loss_mps: 0.191078, loss_cps: 0.385801
[14:16:25.094] iteration 26908: total_loss: 0.342960, loss_sup: 0.021244, loss_mps: 0.107953, loss_cps: 0.213764
[14:16:25.239] iteration 26909: total_loss: 0.346870, loss_sup: 0.064026, loss_mps: 0.099853, loss_cps: 0.182991
[14:16:25.389] iteration 26910: total_loss: 0.374123, loss_sup: 0.052509, loss_mps: 0.108673, loss_cps: 0.212940
[14:16:25.536] iteration 26911: total_loss: 0.416589, loss_sup: 0.050788, loss_mps: 0.119870, loss_cps: 0.245931
[14:16:25.681] iteration 26912: total_loss: 0.259521, loss_sup: 0.013084, loss_mps: 0.087841, loss_cps: 0.158597
[14:16:25.830] iteration 26913: total_loss: 0.286895, loss_sup: 0.046864, loss_mps: 0.091076, loss_cps: 0.148955
[14:16:25.976] iteration 26914: total_loss: 0.393720, loss_sup: 0.001978, loss_mps: 0.122183, loss_cps: 0.269560
[14:16:26.126] iteration 26915: total_loss: 0.567772, loss_sup: 0.128703, loss_mps: 0.154244, loss_cps: 0.284826
[14:16:26.271] iteration 26916: total_loss: 0.206686, loss_sup: 0.005487, loss_mps: 0.074516, loss_cps: 0.126683
[14:16:26.417] iteration 26917: total_loss: 0.347650, loss_sup: 0.007796, loss_mps: 0.114976, loss_cps: 0.224877
[14:16:26.562] iteration 26918: total_loss: 0.433149, loss_sup: 0.005528, loss_mps: 0.144880, loss_cps: 0.282742
[14:16:26.709] iteration 26919: total_loss: 0.505114, loss_sup: 0.047316, loss_mps: 0.148798, loss_cps: 0.309000
[14:16:26.855] iteration 26920: total_loss: 0.520639, loss_sup: 0.048354, loss_mps: 0.159456, loss_cps: 0.312829
[14:16:27.001] iteration 26921: total_loss: 0.151776, loss_sup: 0.020502, loss_mps: 0.051216, loss_cps: 0.080057
[14:16:27.147] iteration 26922: total_loss: 0.417191, loss_sup: 0.046586, loss_mps: 0.123710, loss_cps: 0.246895
[14:16:27.292] iteration 26923: total_loss: 0.280354, loss_sup: 0.066465, loss_mps: 0.076083, loss_cps: 0.137806
[14:16:27.438] iteration 26924: total_loss: 0.560162, loss_sup: 0.019279, loss_mps: 0.174639, loss_cps: 0.366244
[14:16:27.583] iteration 26925: total_loss: 0.368484, loss_sup: 0.055512, loss_mps: 0.103559, loss_cps: 0.209412
[14:16:27.729] iteration 26926: total_loss: 0.416062, loss_sup: 0.052565, loss_mps: 0.122701, loss_cps: 0.240797
[14:16:27.876] iteration 26927: total_loss: 0.268193, loss_sup: 0.013938, loss_mps: 0.086419, loss_cps: 0.167837
[14:16:28.023] iteration 26928: total_loss: 0.197891, loss_sup: 0.013606, loss_mps: 0.065898, loss_cps: 0.118387
[14:16:28.173] iteration 26929: total_loss: 0.491065, loss_sup: 0.049577, loss_mps: 0.150792, loss_cps: 0.290695
[14:16:28.321] iteration 26930: total_loss: 0.347174, loss_sup: 0.044505, loss_mps: 0.097144, loss_cps: 0.205525
[14:16:28.467] iteration 26931: total_loss: 0.334511, loss_sup: 0.031519, loss_mps: 0.102350, loss_cps: 0.200642
[14:16:28.614] iteration 26932: total_loss: 0.228036, loss_sup: 0.015582, loss_mps: 0.077992, loss_cps: 0.134462
[14:16:28.759] iteration 26933: total_loss: 0.174398, loss_sup: 0.007710, loss_mps: 0.060027, loss_cps: 0.106661
[14:16:28.908] iteration 26934: total_loss: 0.246433, loss_sup: 0.006887, loss_mps: 0.084312, loss_cps: 0.155233
[14:16:29.054] iteration 26935: total_loss: 0.389595, loss_sup: 0.066317, loss_mps: 0.108474, loss_cps: 0.214804
[14:16:29.202] iteration 26936: total_loss: 0.251547, loss_sup: 0.063155, loss_mps: 0.066142, loss_cps: 0.122250
[14:16:29.348] iteration 26937: total_loss: 0.204167, loss_sup: 0.006469, loss_mps: 0.069487, loss_cps: 0.128212
[14:16:29.493] iteration 26938: total_loss: 0.303657, loss_sup: 0.002560, loss_mps: 0.105401, loss_cps: 0.195696
[14:16:29.639] iteration 26939: total_loss: 0.218061, loss_sup: 0.020706, loss_mps: 0.072674, loss_cps: 0.124681
[14:16:29.786] iteration 26940: total_loss: 0.325949, loss_sup: 0.027049, loss_mps: 0.100292, loss_cps: 0.198608
[14:16:29.931] iteration 26941: total_loss: 0.187837, loss_sup: 0.018820, loss_mps: 0.060885, loss_cps: 0.108132
[14:16:30.077] iteration 26942: total_loss: 0.260382, loss_sup: 0.000486, loss_mps: 0.092295, loss_cps: 0.167601
[14:16:30.222] iteration 26943: total_loss: 0.348170, loss_sup: 0.110652, loss_mps: 0.086468, loss_cps: 0.151049
[14:16:30.371] iteration 26944: total_loss: 0.399785, loss_sup: 0.099058, loss_mps: 0.101340, loss_cps: 0.199387
[14:16:30.519] iteration 26945: total_loss: 0.260324, loss_sup: 0.016774, loss_mps: 0.086582, loss_cps: 0.156968
[14:16:30.665] iteration 26946: total_loss: 0.433170, loss_sup: 0.074700, loss_mps: 0.115637, loss_cps: 0.242833
[14:16:30.811] iteration 26947: total_loss: 0.405310, loss_sup: 0.003184, loss_mps: 0.132078, loss_cps: 0.270048
[14:16:30.957] iteration 26948: total_loss: 0.247227, loss_sup: 0.019238, loss_mps: 0.076849, loss_cps: 0.151139
[14:16:31.102] iteration 26949: total_loss: 0.429707, loss_sup: 0.090966, loss_mps: 0.118012, loss_cps: 0.220729
[14:16:31.248] iteration 26950: total_loss: 0.270000, loss_sup: 0.005418, loss_mps: 0.092447, loss_cps: 0.172135
[14:16:31.396] iteration 26951: total_loss: 0.508776, loss_sup: 0.040861, loss_mps: 0.143584, loss_cps: 0.324330
[14:16:31.541] iteration 26952: total_loss: 0.535570, loss_sup: 0.111829, loss_mps: 0.145961, loss_cps: 0.277780
[14:16:31.687] iteration 26953: total_loss: 0.366227, loss_sup: 0.015613, loss_mps: 0.121543, loss_cps: 0.229071
[14:16:31.834] iteration 26954: total_loss: 0.260790, loss_sup: 0.004097, loss_mps: 0.092672, loss_cps: 0.164021
[14:16:31.980] iteration 26955: total_loss: 0.243299, loss_sup: 0.016486, loss_mps: 0.077440, loss_cps: 0.149373
[14:16:32.126] iteration 26956: total_loss: 0.260731, loss_sup: 0.055808, loss_mps: 0.073544, loss_cps: 0.131379
[14:16:32.273] iteration 26957: total_loss: 0.350726, loss_sup: 0.109725, loss_mps: 0.087175, loss_cps: 0.153826
[14:16:32.419] iteration 26958: total_loss: 0.274925, loss_sup: 0.054185, loss_mps: 0.081947, loss_cps: 0.138793
[14:16:32.565] iteration 26959: total_loss: 0.577886, loss_sup: 0.080797, loss_mps: 0.145179, loss_cps: 0.351909
[14:16:32.711] iteration 26960: total_loss: 0.451064, loss_sup: 0.081758, loss_mps: 0.124471, loss_cps: 0.244834
[14:16:32.857] iteration 26961: total_loss: 0.231834, loss_sup: 0.008371, loss_mps: 0.081951, loss_cps: 0.141513
[14:16:33.003] iteration 26962: total_loss: 0.891796, loss_sup: 0.010457, loss_mps: 0.257444, loss_cps: 0.623895
[14:16:33.150] iteration 26963: total_loss: 0.300378, loss_sup: 0.028899, loss_mps: 0.099964, loss_cps: 0.171515
[14:16:33.295] iteration 26964: total_loss: 0.291242, loss_sup: 0.000685, loss_mps: 0.100092, loss_cps: 0.190465
[14:16:33.443] iteration 26965: total_loss: 0.412136, loss_sup: 0.023944, loss_mps: 0.129835, loss_cps: 0.258357
[14:16:33.589] iteration 26966: total_loss: 0.316399, loss_sup: 0.005382, loss_mps: 0.113981, loss_cps: 0.197035
[14:16:33.736] iteration 26967: total_loss: 0.608365, loss_sup: 0.263289, loss_mps: 0.118305, loss_cps: 0.226771
[14:16:33.882] iteration 26968: total_loss: 0.466313, loss_sup: 0.045529, loss_mps: 0.130930, loss_cps: 0.289854
[14:16:34.028] iteration 26969: total_loss: 0.351193, loss_sup: 0.027811, loss_mps: 0.108978, loss_cps: 0.214405
[14:16:34.175] iteration 26970: total_loss: 0.518134, loss_sup: 0.190099, loss_mps: 0.117560, loss_cps: 0.210476
[14:16:34.321] iteration 26971: total_loss: 0.286931, loss_sup: 0.042573, loss_mps: 0.086842, loss_cps: 0.157516
[14:16:34.467] iteration 26972: total_loss: 0.402932, loss_sup: 0.028922, loss_mps: 0.128762, loss_cps: 0.245248
[14:16:34.613] iteration 26973: total_loss: 0.340741, loss_sup: 0.027803, loss_mps: 0.104274, loss_cps: 0.208663
[14:16:34.760] iteration 26974: total_loss: 0.275860, loss_sup: 0.019089, loss_mps: 0.093789, loss_cps: 0.162982
[14:16:34.907] iteration 26975: total_loss: 0.366346, loss_sup: 0.030296, loss_mps: 0.114821, loss_cps: 0.221230
[14:16:35.053] iteration 26976: total_loss: 0.520361, loss_sup: 0.007461, loss_mps: 0.167707, loss_cps: 0.345193
[14:16:35.199] iteration 26977: total_loss: 0.345259, loss_sup: 0.057029, loss_mps: 0.102879, loss_cps: 0.185352
[14:16:35.345] iteration 26978: total_loss: 0.401125, loss_sup: 0.078214, loss_mps: 0.115429, loss_cps: 0.207483
[14:16:35.493] iteration 26979: total_loss: 0.353496, loss_sup: 0.027911, loss_mps: 0.109672, loss_cps: 0.215913
[14:16:35.638] iteration 26980: total_loss: 0.312931, loss_sup: 0.099131, loss_mps: 0.079527, loss_cps: 0.134272
[14:16:35.785] iteration 26981: total_loss: 0.262816, loss_sup: 0.006106, loss_mps: 0.090209, loss_cps: 0.166501
[14:16:35.931] iteration 26982: total_loss: 0.397540, loss_sup: 0.110927, loss_mps: 0.101487, loss_cps: 0.185126
[14:16:36.079] iteration 26983: total_loss: 0.261325, loss_sup: 0.021051, loss_mps: 0.092562, loss_cps: 0.147712
[14:16:36.225] iteration 26984: total_loss: 0.377523, loss_sup: 0.008874, loss_mps: 0.122259, loss_cps: 0.246389
[14:16:36.371] iteration 26985: total_loss: 0.430901, loss_sup: 0.005562, loss_mps: 0.142975, loss_cps: 0.282364
[14:16:36.517] iteration 26986: total_loss: 0.892900, loss_sup: 0.159636, loss_mps: 0.220989, loss_cps: 0.512275
[14:16:36.663] iteration 26987: total_loss: 0.374127, loss_sup: 0.045433, loss_mps: 0.117718, loss_cps: 0.210976
[14:16:36.809] iteration 26988: total_loss: 0.306687, loss_sup: 0.082765, loss_mps: 0.081498, loss_cps: 0.142423
[14:16:36.959] iteration 26989: total_loss: 0.382615, loss_sup: 0.138192, loss_mps: 0.087578, loss_cps: 0.156845
[14:16:37.106] iteration 26990: total_loss: 0.247633, loss_sup: 0.011053, loss_mps: 0.085173, loss_cps: 0.151407
[14:16:37.253] iteration 26991: total_loss: 0.189676, loss_sup: 0.001287, loss_mps: 0.068314, loss_cps: 0.120076
[14:16:37.407] iteration 26992: total_loss: 0.226393, loss_sup: 0.018505, loss_mps: 0.074770, loss_cps: 0.133117
[14:16:37.553] iteration 26993: total_loss: 0.502323, loss_sup: 0.076840, loss_mps: 0.147335, loss_cps: 0.278148
[14:16:37.700] iteration 26994: total_loss: 0.238121, loss_sup: 0.023462, loss_mps: 0.074277, loss_cps: 0.140382
[14:16:37.846] iteration 26995: total_loss: 0.382977, loss_sup: 0.009719, loss_mps: 0.127796, loss_cps: 0.245462
[14:16:37.992] iteration 26996: total_loss: 0.621804, loss_sup: 0.227286, loss_mps: 0.131988, loss_cps: 0.262530
[14:16:38.138] iteration 26997: total_loss: 0.323177, loss_sup: 0.008255, loss_mps: 0.115174, loss_cps: 0.199747
[14:16:38.284] iteration 26998: total_loss: 0.240520, loss_sup: 0.064896, loss_mps: 0.065524, loss_cps: 0.110100
[14:16:38.430] iteration 26999: total_loss: 0.559938, loss_sup: 0.034540, loss_mps: 0.171404, loss_cps: 0.353995
[14:16:38.576] iteration 27000: total_loss: 0.480297, loss_sup: 0.054911, loss_mps: 0.135615, loss_cps: 0.289771
[14:16:38.576] Evaluation Started ==>
[14:16:49.895] ==> valid iteration 27000: unet metrics: {'dc': 0.655893820221227, 'jc': 0.54115280426909, 'pre': 0.8083421994947095, 'hd': 5.3967431284619884}, ynet metrics: {'dc': 0.5998692458488167, 'jc': 0.48861117646996294, 'pre': 0.7936755385038183, 'hd': 5.425192465257813}.
[14:16:49.898] Evaluation Finished!⏹️
[14:16:50.049] iteration 27001: total_loss: 0.504469, loss_sup: 0.077936, loss_mps: 0.144320, loss_cps: 0.282213
[14:16:50.197] iteration 27002: total_loss: 0.636772, loss_sup: 0.133311, loss_mps: 0.171225, loss_cps: 0.332236
[14:16:50.342] iteration 27003: total_loss: 0.342843, loss_sup: 0.084380, loss_mps: 0.095188, loss_cps: 0.163275
[14:16:50.487] iteration 27004: total_loss: 0.381736, loss_sup: 0.057059, loss_mps: 0.114263, loss_cps: 0.210413
[14:16:50.634] iteration 27005: total_loss: 0.341459, loss_sup: 0.052652, loss_mps: 0.103203, loss_cps: 0.185604
[14:16:50.780] iteration 27006: total_loss: 0.275334, loss_sup: 0.037853, loss_mps: 0.079884, loss_cps: 0.157596
[14:16:50.925] iteration 27007: total_loss: 0.759148, loss_sup: 0.098747, loss_mps: 0.212904, loss_cps: 0.447497
[14:16:51.071] iteration 27008: total_loss: 0.223544, loss_sup: 0.044079, loss_mps: 0.070554, loss_cps: 0.108911
[14:16:51.217] iteration 27009: total_loss: 0.399441, loss_sup: 0.165477, loss_mps: 0.083341, loss_cps: 0.150622
[14:16:51.362] iteration 27010: total_loss: 0.398192, loss_sup: 0.052279, loss_mps: 0.118870, loss_cps: 0.227044
[14:16:51.514] iteration 27011: total_loss: 0.213003, loss_sup: 0.015741, loss_mps: 0.069300, loss_cps: 0.127962
[14:16:51.661] iteration 27012: total_loss: 0.351750, loss_sup: 0.079808, loss_mps: 0.092137, loss_cps: 0.179805
[14:16:51.807] iteration 27013: total_loss: 0.494052, loss_sup: 0.019491, loss_mps: 0.154473, loss_cps: 0.320089
[14:16:51.954] iteration 27014: total_loss: 0.359765, loss_sup: 0.003642, loss_mps: 0.124221, loss_cps: 0.231903
[14:16:52.099] iteration 27015: total_loss: 0.228714, loss_sup: 0.017917, loss_mps: 0.074050, loss_cps: 0.136747
[14:16:52.245] iteration 27016: total_loss: 0.226083, loss_sup: 0.018987, loss_mps: 0.083524, loss_cps: 0.123573
[14:16:52.391] iteration 27017: total_loss: 0.303861, loss_sup: 0.025431, loss_mps: 0.098397, loss_cps: 0.180033
[14:16:52.538] iteration 27018: total_loss: 0.357842, loss_sup: 0.021039, loss_mps: 0.121705, loss_cps: 0.215098
[14:16:52.684] iteration 27019: total_loss: 0.392151, loss_sup: 0.065252, loss_mps: 0.114649, loss_cps: 0.212250
[14:16:52.830] iteration 27020: total_loss: 0.184502, loss_sup: 0.004916, loss_mps: 0.067765, loss_cps: 0.111821
[14:16:52.976] iteration 27021: total_loss: 0.450251, loss_sup: 0.046348, loss_mps: 0.144788, loss_cps: 0.259114
[14:16:53.122] iteration 27022: total_loss: 0.272108, loss_sup: 0.039649, loss_mps: 0.084446, loss_cps: 0.148012
[14:16:53.267] iteration 27023: total_loss: 0.410582, loss_sup: 0.026518, loss_mps: 0.137829, loss_cps: 0.246236
[14:16:53.415] iteration 27024: total_loss: 0.425031, loss_sup: 0.041891, loss_mps: 0.135925, loss_cps: 0.247215
[14:16:53.561] iteration 27025: total_loss: 0.584826, loss_sup: 0.222082, loss_mps: 0.120056, loss_cps: 0.242688
[14:16:53.706] iteration 27026: total_loss: 0.377626, loss_sup: 0.004039, loss_mps: 0.128656, loss_cps: 0.244932
[14:16:53.854] iteration 27027: total_loss: 0.688014, loss_sup: 0.070879, loss_mps: 0.190880, loss_cps: 0.426256
[14:16:54.001] iteration 27028: total_loss: 0.265207, loss_sup: 0.033121, loss_mps: 0.085722, loss_cps: 0.146364
[14:16:54.148] iteration 27029: total_loss: 0.327283, loss_sup: 0.025889, loss_mps: 0.106875, loss_cps: 0.194519
[14:16:54.294] iteration 27030: total_loss: 0.307369, loss_sup: 0.143551, loss_mps: 0.059963, loss_cps: 0.103855
[14:16:54.439] iteration 27031: total_loss: 0.247411, loss_sup: 0.005356, loss_mps: 0.084832, loss_cps: 0.157223
[14:16:54.587] iteration 27032: total_loss: 0.233765, loss_sup: 0.027996, loss_mps: 0.073642, loss_cps: 0.132127
[14:16:54.739] iteration 27033: total_loss: 0.251983, loss_sup: 0.005749, loss_mps: 0.086027, loss_cps: 0.160208
[14:16:54.885] iteration 27034: total_loss: 0.245023, loss_sup: 0.003110, loss_mps: 0.086187, loss_cps: 0.155725
[14:16:55.030] iteration 27035: total_loss: 0.385761, loss_sup: 0.144993, loss_mps: 0.089890, loss_cps: 0.150879
[14:16:55.176] iteration 27036: total_loss: 0.289891, loss_sup: 0.086767, loss_mps: 0.077058, loss_cps: 0.126066
[14:16:55.322] iteration 27037: total_loss: 0.280936, loss_sup: 0.125916, loss_mps: 0.059340, loss_cps: 0.095680
[14:16:55.468] iteration 27038: total_loss: 0.265492, loss_sup: 0.023977, loss_mps: 0.085112, loss_cps: 0.156404
[14:16:55.614] iteration 27039: total_loss: 0.342096, loss_sup: 0.005758, loss_mps: 0.120357, loss_cps: 0.215981
[14:16:55.760] iteration 27040: total_loss: 0.295628, loss_sup: 0.045486, loss_mps: 0.086443, loss_cps: 0.163699
[14:16:55.906] iteration 27041: total_loss: 0.303694, loss_sup: 0.002050, loss_mps: 0.099062, loss_cps: 0.202581
[14:16:56.052] iteration 27042: total_loss: 0.514110, loss_sup: 0.054513, loss_mps: 0.157525, loss_cps: 0.302071
[14:16:56.199] iteration 27043: total_loss: 0.436912, loss_sup: 0.130041, loss_mps: 0.108696, loss_cps: 0.198175
[14:16:56.345] iteration 27044: total_loss: 0.498500, loss_sup: 0.043461, loss_mps: 0.153781, loss_cps: 0.301258
[14:16:56.491] iteration 27045: total_loss: 0.371160, loss_sup: 0.063134, loss_mps: 0.109310, loss_cps: 0.198717
[14:16:56.637] iteration 27046: total_loss: 0.270309, loss_sup: 0.009811, loss_mps: 0.096467, loss_cps: 0.164032
[14:16:56.783] iteration 27047: total_loss: 0.375284, loss_sup: 0.013275, loss_mps: 0.125255, loss_cps: 0.236754
[14:16:56.929] iteration 27048: total_loss: 0.277273, loss_sup: 0.039348, loss_mps: 0.085602, loss_cps: 0.152324
[14:16:57.075] iteration 27049: total_loss: 0.199865, loss_sup: 0.010993, loss_mps: 0.073205, loss_cps: 0.115666
[14:16:57.221] iteration 27050: total_loss: 0.788413, loss_sup: 0.320247, loss_mps: 0.161603, loss_cps: 0.306563
[14:16:57.366] iteration 27051: total_loss: 0.440775, loss_sup: 0.052463, loss_mps: 0.131032, loss_cps: 0.257280
[14:16:57.512] iteration 27052: total_loss: 0.604420, loss_sup: 0.260246, loss_mps: 0.121146, loss_cps: 0.223028
[14:16:57.659] iteration 27053: total_loss: 0.432390, loss_sup: 0.022894, loss_mps: 0.137915, loss_cps: 0.271581
[14:16:57.807] iteration 27054: total_loss: 0.483476, loss_sup: 0.030205, loss_mps: 0.151687, loss_cps: 0.301585
[14:16:57.952] iteration 27055: total_loss: 0.233366, loss_sup: 0.013766, loss_mps: 0.075127, loss_cps: 0.144473
[14:16:58.099] iteration 27056: total_loss: 0.215546, loss_sup: 0.014273, loss_mps: 0.075759, loss_cps: 0.125514
[14:16:58.244] iteration 27057: total_loss: 0.521667, loss_sup: 0.077784, loss_mps: 0.146418, loss_cps: 0.297465
[14:16:58.392] iteration 27058: total_loss: 0.610858, loss_sup: 0.018725, loss_mps: 0.183004, loss_cps: 0.409130
[14:16:58.537] iteration 27059: total_loss: 0.557336, loss_sup: 0.035981, loss_mps: 0.178277, loss_cps: 0.343078
[14:16:58.682] iteration 27060: total_loss: 0.355196, loss_sup: 0.014005, loss_mps: 0.118577, loss_cps: 0.222614
[14:16:58.828] iteration 27061: total_loss: 0.237263, loss_sup: 0.048862, loss_mps: 0.070466, loss_cps: 0.117935
[14:16:58.973] iteration 27062: total_loss: 0.340378, loss_sup: 0.099493, loss_mps: 0.087336, loss_cps: 0.153550
[14:16:59.119] iteration 27063: total_loss: 0.551147, loss_sup: 0.066331, loss_mps: 0.160084, loss_cps: 0.324732
[14:16:59.267] iteration 27064: total_loss: 0.557200, loss_sup: 0.034255, loss_mps: 0.168165, loss_cps: 0.354780
[14:16:59.413] iteration 27065: total_loss: 0.357123, loss_sup: 0.051450, loss_mps: 0.111975, loss_cps: 0.193698
[14:16:59.559] iteration 27066: total_loss: 0.262523, loss_sup: 0.064999, loss_mps: 0.071830, loss_cps: 0.125693
[14:16:59.705] iteration 27067: total_loss: 0.271272, loss_sup: 0.024890, loss_mps: 0.088839, loss_cps: 0.157543
[14:16:59.852] iteration 27068: total_loss: 0.183982, loss_sup: 0.041717, loss_mps: 0.054058, loss_cps: 0.088207
[14:16:59.997] iteration 27069: total_loss: 0.297546, loss_sup: 0.031416, loss_mps: 0.093363, loss_cps: 0.172767
[14:17:00.145] iteration 27070: total_loss: 0.334844, loss_sup: 0.068387, loss_mps: 0.096114, loss_cps: 0.170344
[14:17:00.291] iteration 27071: total_loss: 0.315181, loss_sup: 0.046547, loss_mps: 0.094404, loss_cps: 0.174230
[14:17:00.438] iteration 27072: total_loss: 0.459690, loss_sup: 0.125793, loss_mps: 0.115591, loss_cps: 0.218307
[14:17:00.584] iteration 27073: total_loss: 0.279919, loss_sup: 0.006811, loss_mps: 0.093775, loss_cps: 0.179333
[14:17:00.730] iteration 27074: total_loss: 0.220640, loss_sup: 0.018407, loss_mps: 0.080534, loss_cps: 0.121699
[14:17:00.875] iteration 27075: total_loss: 0.403939, loss_sup: 0.013020, loss_mps: 0.129671, loss_cps: 0.261248
[14:17:01.021] iteration 27076: total_loss: 0.320874, loss_sup: 0.032373, loss_mps: 0.097589, loss_cps: 0.190912
[14:17:01.176] iteration 27077: total_loss: 0.190759, loss_sup: 0.011892, loss_mps: 0.069744, loss_cps: 0.109123
[14:17:01.321] iteration 27078: total_loss: 0.252652, loss_sup: 0.012215, loss_mps: 0.081627, loss_cps: 0.158810
[14:17:01.466] iteration 27079: total_loss: 0.251822, loss_sup: 0.021271, loss_mps: 0.090113, loss_cps: 0.140438
[14:17:01.612] iteration 27080: total_loss: 0.333239, loss_sup: 0.047233, loss_mps: 0.106980, loss_cps: 0.179026
[14:17:01.757] iteration 27081: total_loss: 0.336807, loss_sup: 0.065066, loss_mps: 0.096481, loss_cps: 0.175260
[14:17:01.904] iteration 27082: total_loss: 0.288695, loss_sup: 0.017979, loss_mps: 0.095463, loss_cps: 0.175253
[14:17:02.050] iteration 27083: total_loss: 0.384512, loss_sup: 0.164757, loss_mps: 0.080075, loss_cps: 0.139679
[14:17:02.196] iteration 27084: total_loss: 0.594935, loss_sup: 0.162786, loss_mps: 0.135823, loss_cps: 0.296325
[14:17:02.345] iteration 27085: total_loss: 0.776870, loss_sup: 0.062004, loss_mps: 0.202017, loss_cps: 0.512848
[14:17:02.491] iteration 27086: total_loss: 0.319826, loss_sup: 0.049606, loss_mps: 0.091140, loss_cps: 0.179080
[14:17:02.637] iteration 27087: total_loss: 0.265588, loss_sup: 0.007590, loss_mps: 0.087073, loss_cps: 0.170926
[14:17:02.783] iteration 27088: total_loss: 0.462625, loss_sup: 0.098805, loss_mps: 0.121159, loss_cps: 0.242661
[14:17:02.929] iteration 27089: total_loss: 0.330896, loss_sup: 0.043421, loss_mps: 0.100037, loss_cps: 0.187438
[14:17:03.075] iteration 27090: total_loss: 0.559591, loss_sup: 0.056980, loss_mps: 0.162705, loss_cps: 0.339906
[14:17:03.220] iteration 27091: total_loss: 0.483585, loss_sup: 0.077436, loss_mps: 0.140472, loss_cps: 0.265677
[14:17:03.366] iteration 27092: total_loss: 0.446235, loss_sup: 0.126075, loss_mps: 0.103713, loss_cps: 0.216447
[14:17:03.512] iteration 27093: total_loss: 0.403428, loss_sup: 0.008866, loss_mps: 0.138561, loss_cps: 0.256001
[14:17:03.658] iteration 27094: total_loss: 0.230392, loss_sup: 0.011539, loss_mps: 0.084896, loss_cps: 0.133958
[14:17:03.806] iteration 27095: total_loss: 0.277677, loss_sup: 0.074952, loss_mps: 0.070050, loss_cps: 0.132675
[14:17:03.953] iteration 27096: total_loss: 0.505821, loss_sup: 0.142599, loss_mps: 0.119845, loss_cps: 0.243377
[14:17:04.099] iteration 27097: total_loss: 0.390399, loss_sup: 0.058130, loss_mps: 0.112414, loss_cps: 0.219855
[14:17:04.247] iteration 27098: total_loss: 0.677324, loss_sup: 0.317537, loss_mps: 0.124929, loss_cps: 0.234857
[14:17:04.394] iteration 27099: total_loss: 0.269701, loss_sup: 0.014452, loss_mps: 0.089657, loss_cps: 0.165592
[14:17:04.539] iteration 27100: total_loss: 0.241610, loss_sup: 0.006581, loss_mps: 0.083545, loss_cps: 0.151484
[14:17:04.539] Evaluation Started ==>
[14:17:15.957] ==> valid iteration 27100: unet metrics: {'dc': 0.6676349324204239, 'jc': 0.5535296801003461, 'pre': 0.8017033693992069, 'hd': 5.352902620998572}, ynet metrics: {'dc': 0.6433586678223049, 'jc': 0.5318934534046911, 'pre': 0.7881056163730612, 'hd': 5.359196485368157}.
[14:17:15.959] Evaluation Finished!⏹️
[14:17:16.113] iteration 27101: total_loss: 0.501677, loss_sup: 0.120919, loss_mps: 0.129772, loss_cps: 0.250986
[14:17:16.265] iteration 27102: total_loss: 0.417136, loss_sup: 0.037649, loss_mps: 0.134624, loss_cps: 0.244862
[14:17:16.411] iteration 27103: total_loss: 0.229107, loss_sup: 0.003069, loss_mps: 0.083532, loss_cps: 0.142507
[14:17:16.558] iteration 27104: total_loss: 0.243295, loss_sup: 0.024555, loss_mps: 0.076337, loss_cps: 0.142404
[14:17:16.703] iteration 27105: total_loss: 0.260380, loss_sup: 0.004905, loss_mps: 0.092357, loss_cps: 0.163118
[14:17:16.850] iteration 27106: total_loss: 0.409327, loss_sup: 0.003857, loss_mps: 0.136038, loss_cps: 0.269431
[14:17:16.995] iteration 27107: total_loss: 0.227278, loss_sup: 0.005517, loss_mps: 0.083983, loss_cps: 0.137778
[14:17:17.142] iteration 27108: total_loss: 0.363444, loss_sup: 0.089785, loss_mps: 0.092801, loss_cps: 0.180858
[14:17:17.290] iteration 27109: total_loss: 0.330082, loss_sup: 0.034853, loss_mps: 0.100471, loss_cps: 0.194759
[14:17:17.436] iteration 27110: total_loss: 0.313216, loss_sup: 0.065085, loss_mps: 0.088781, loss_cps: 0.159350
[14:17:17.583] iteration 27111: total_loss: 0.162524, loss_sup: 0.022096, loss_mps: 0.054036, loss_cps: 0.086392
[14:17:17.728] iteration 27112: total_loss: 0.337417, loss_sup: 0.154544, loss_mps: 0.067630, loss_cps: 0.115244
[14:17:17.874] iteration 27113: total_loss: 0.366759, loss_sup: 0.037991, loss_mps: 0.115000, loss_cps: 0.213767
[14:17:18.021] iteration 27114: total_loss: 0.259039, loss_sup: 0.028829, loss_mps: 0.080710, loss_cps: 0.149500
[14:17:18.168] iteration 27115: total_loss: 0.455393, loss_sup: 0.027903, loss_mps: 0.143463, loss_cps: 0.284028
[14:17:18.314] iteration 27116: total_loss: 0.815710, loss_sup: 0.040463, loss_mps: 0.240163, loss_cps: 0.535084
[14:17:18.462] iteration 27117: total_loss: 0.343789, loss_sup: 0.065351, loss_mps: 0.100288, loss_cps: 0.178150
[14:17:18.608] iteration 27118: total_loss: 0.387725, loss_sup: 0.001341, loss_mps: 0.126834, loss_cps: 0.259550
[14:17:18.754] iteration 27119: total_loss: 0.262463, loss_sup: 0.027484, loss_mps: 0.086209, loss_cps: 0.148769
[14:17:18.900] iteration 27120: total_loss: 0.295855, loss_sup: 0.009109, loss_mps: 0.100787, loss_cps: 0.185959
[14:17:19.046] iteration 27121: total_loss: 0.246631, loss_sup: 0.026013, loss_mps: 0.085532, loss_cps: 0.135086
[14:17:19.193] iteration 27122: total_loss: 0.357756, loss_sup: 0.040650, loss_mps: 0.116388, loss_cps: 0.200719
[14:17:19.339] iteration 27123: total_loss: 0.372705, loss_sup: 0.065511, loss_mps: 0.105230, loss_cps: 0.201965
[14:17:19.486] iteration 27124: total_loss: 0.224085, loss_sup: 0.066597, loss_mps: 0.057791, loss_cps: 0.099697
[14:17:19.632] iteration 27125: total_loss: 0.323468, loss_sup: 0.006562, loss_mps: 0.108299, loss_cps: 0.208607
[14:17:19.778] iteration 27126: total_loss: 0.353416, loss_sup: 0.040200, loss_mps: 0.101425, loss_cps: 0.211791
[14:17:19.924] iteration 27127: total_loss: 0.195155, loss_sup: 0.001556, loss_mps: 0.073985, loss_cps: 0.119613
[14:17:20.070] iteration 27128: total_loss: 0.649997, loss_sup: 0.106119, loss_mps: 0.170992, loss_cps: 0.372886
[14:17:20.218] iteration 27129: total_loss: 0.221184, loss_sup: 0.027986, loss_mps: 0.069351, loss_cps: 0.123847
[14:17:20.363] iteration 27130: total_loss: 0.367953, loss_sup: 0.021142, loss_mps: 0.117895, loss_cps: 0.228915
[14:17:20.509] iteration 27131: total_loss: 0.549974, loss_sup: 0.023017, loss_mps: 0.163503, loss_cps: 0.363455
[14:17:20.654] iteration 27132: total_loss: 0.182499, loss_sup: 0.010334, loss_mps: 0.064902, loss_cps: 0.107264
[14:17:20.800] iteration 27133: total_loss: 0.224799, loss_sup: 0.012992, loss_mps: 0.077894, loss_cps: 0.133913
[14:17:20.947] iteration 27134: total_loss: 0.327298, loss_sup: 0.077303, loss_mps: 0.088030, loss_cps: 0.161965
[14:17:21.093] iteration 27135: total_loss: 0.475152, loss_sup: 0.208442, loss_mps: 0.102089, loss_cps: 0.164620
[14:17:21.240] iteration 27136: total_loss: 0.582993, loss_sup: 0.227570, loss_mps: 0.117246, loss_cps: 0.238177
[14:17:21.386] iteration 27137: total_loss: 0.233792, loss_sup: 0.001161, loss_mps: 0.083444, loss_cps: 0.149186
[14:17:21.536] iteration 27138: total_loss: 0.252422, loss_sup: 0.001268, loss_mps: 0.088521, loss_cps: 0.162633
[14:17:21.682] iteration 27139: total_loss: 0.297496, loss_sup: 0.050954, loss_mps: 0.089857, loss_cps: 0.156685
[14:17:21.829] iteration 27140: total_loss: 0.437017, loss_sup: 0.029016, loss_mps: 0.127063, loss_cps: 0.280938
[14:17:21.975] iteration 27141: total_loss: 0.199202, loss_sup: 0.004085, loss_mps: 0.070188, loss_cps: 0.124929
[14:17:22.120] iteration 27142: total_loss: 0.334772, loss_sup: 0.047793, loss_mps: 0.101416, loss_cps: 0.185562
[14:17:22.268] iteration 27143: total_loss: 0.744230, loss_sup: 0.415827, loss_mps: 0.116643, loss_cps: 0.211760
[14:17:22.413] iteration 27144: total_loss: 0.381366, loss_sup: 0.087575, loss_mps: 0.105307, loss_cps: 0.188484
[14:17:22.559] iteration 27145: total_loss: 0.385868, loss_sup: 0.069147, loss_mps: 0.114804, loss_cps: 0.201917
[14:17:22.707] iteration 27146: total_loss: 0.429022, loss_sup: 0.089094, loss_mps: 0.111714, loss_cps: 0.228214
[14:17:22.852] iteration 27147: total_loss: 0.541611, loss_sup: 0.079682, loss_mps: 0.136049, loss_cps: 0.325881
[14:17:22.998] iteration 27148: total_loss: 0.329485, loss_sup: 0.020365, loss_mps: 0.110668, loss_cps: 0.198452
[14:17:23.144] iteration 27149: total_loss: 0.369691, loss_sup: 0.045573, loss_mps: 0.109444, loss_cps: 0.214674
[14:17:23.290] iteration 27150: total_loss: 0.403683, loss_sup: 0.034380, loss_mps: 0.123291, loss_cps: 0.246011
[14:17:23.436] iteration 27151: total_loss: 0.223887, loss_sup: 0.028665, loss_mps: 0.071583, loss_cps: 0.123639
[14:17:23.582] iteration 27152: total_loss: 0.301067, loss_sup: 0.029766, loss_mps: 0.095956, loss_cps: 0.175345
[14:17:23.727] iteration 27153: total_loss: 0.190418, loss_sup: 0.003989, loss_mps: 0.066626, loss_cps: 0.119802
[14:17:23.873] iteration 27154: total_loss: 0.277326, loss_sup: 0.083163, loss_mps: 0.070386, loss_cps: 0.123777
[14:17:24.018] iteration 27155: total_loss: 0.253405, loss_sup: 0.004098, loss_mps: 0.088004, loss_cps: 0.161303
[14:17:24.164] iteration 27156: total_loss: 0.441046, loss_sup: 0.013767, loss_mps: 0.151379, loss_cps: 0.275899
[14:17:24.309] iteration 27157: total_loss: 0.384422, loss_sup: 0.035628, loss_mps: 0.122094, loss_cps: 0.226700
[14:17:24.455] iteration 27158: total_loss: 0.548648, loss_sup: 0.093161, loss_mps: 0.142245, loss_cps: 0.313242
[14:17:24.600] iteration 27159: total_loss: 0.298704, loss_sup: 0.044068, loss_mps: 0.087386, loss_cps: 0.167250
[14:17:24.745] iteration 27160: total_loss: 0.171157, loss_sup: 0.008358, loss_mps: 0.059838, loss_cps: 0.102961
[14:17:24.891] iteration 27161: total_loss: 0.371563, loss_sup: 0.062833, loss_mps: 0.105730, loss_cps: 0.203000
[14:17:25.037] iteration 27162: total_loss: 0.261002, loss_sup: 0.068300, loss_mps: 0.073523, loss_cps: 0.119179
[14:17:25.183] iteration 27163: total_loss: 0.270672, loss_sup: 0.019713, loss_mps: 0.091909, loss_cps: 0.159049
[14:17:25.328] iteration 27164: total_loss: 0.301946, loss_sup: 0.005319, loss_mps: 0.107931, loss_cps: 0.188696
[14:17:25.474] iteration 27165: total_loss: 0.391174, loss_sup: 0.023755, loss_mps: 0.122198, loss_cps: 0.245221
[14:17:25.620] iteration 27166: total_loss: 0.220136, loss_sup: 0.008919, loss_mps: 0.072886, loss_cps: 0.138331
[14:17:25.766] iteration 27167: total_loss: 0.232993, loss_sup: 0.005779, loss_mps: 0.083282, loss_cps: 0.143932
[14:17:25.912] iteration 27168: total_loss: 0.365088, loss_sup: 0.121205, loss_mps: 0.087349, loss_cps: 0.156534
[14:17:26.058] iteration 27169: total_loss: 0.730690, loss_sup: 0.020108, loss_mps: 0.222903, loss_cps: 0.487678
[14:17:26.119] iteration 27170: total_loss: 0.235859, loss_sup: 0.001855, loss_mps: 0.085394, loss_cps: 0.148610
[14:17:27.338] iteration 27171: total_loss: 0.339702, loss_sup: 0.019245, loss_mps: 0.111710, loss_cps: 0.208747
[14:17:27.488] iteration 27172: total_loss: 0.295461, loss_sup: 0.022724, loss_mps: 0.101199, loss_cps: 0.171537
[14:17:27.635] iteration 27173: total_loss: 0.636962, loss_sup: 0.217760, loss_mps: 0.148051, loss_cps: 0.271151
[14:17:27.782] iteration 27174: total_loss: 0.273467, loss_sup: 0.089536, loss_mps: 0.070604, loss_cps: 0.113327
[14:17:27.928] iteration 27175: total_loss: 0.324075, loss_sup: 0.097820, loss_mps: 0.078152, loss_cps: 0.148103
[14:17:28.074] iteration 27176: total_loss: 0.252781, loss_sup: 0.018733, loss_mps: 0.087519, loss_cps: 0.146528
[14:17:28.223] iteration 27177: total_loss: 0.496473, loss_sup: 0.131367, loss_mps: 0.123568, loss_cps: 0.241537
[14:17:28.375] iteration 27178: total_loss: 0.362700, loss_sup: 0.035638, loss_mps: 0.112814, loss_cps: 0.214248
[14:17:28.522] iteration 27179: total_loss: 0.285397, loss_sup: 0.015083, loss_mps: 0.094488, loss_cps: 0.175826
[14:17:28.668] iteration 27180: total_loss: 0.206820, loss_sup: 0.021296, loss_mps: 0.068669, loss_cps: 0.116855
[14:17:28.815] iteration 27181: total_loss: 0.639251, loss_sup: 0.007557, loss_mps: 0.202554, loss_cps: 0.429141
[14:17:28.961] iteration 27182: total_loss: 0.311267, loss_sup: 0.050073, loss_mps: 0.087937, loss_cps: 0.173257
[14:17:29.110] iteration 27183: total_loss: 0.344981, loss_sup: 0.016889, loss_mps: 0.114109, loss_cps: 0.213983
[14:17:29.257] iteration 27184: total_loss: 0.226324, loss_sup: 0.024094, loss_mps: 0.079162, loss_cps: 0.123068
[14:17:29.404] iteration 27185: total_loss: 0.347795, loss_sup: 0.078308, loss_mps: 0.088891, loss_cps: 0.180597
[14:17:29.550] iteration 27186: total_loss: 0.425043, loss_sup: 0.148204, loss_mps: 0.103715, loss_cps: 0.173124
[14:17:29.702] iteration 27187: total_loss: 0.303313, loss_sup: 0.035381, loss_mps: 0.094558, loss_cps: 0.173374
[14:17:29.850] iteration 27188: total_loss: 0.283574, loss_sup: 0.021483, loss_mps: 0.089870, loss_cps: 0.172221
[14:17:29.997] iteration 27189: total_loss: 0.327257, loss_sup: 0.032643, loss_mps: 0.107137, loss_cps: 0.187477
[14:17:30.147] iteration 27190: total_loss: 0.349034, loss_sup: 0.017064, loss_mps: 0.104518, loss_cps: 0.227451
[14:17:30.294] iteration 27191: total_loss: 0.278513, loss_sup: 0.012780, loss_mps: 0.099123, loss_cps: 0.166610
[14:17:30.445] iteration 27192: total_loss: 0.354959, loss_sup: 0.074073, loss_mps: 0.091682, loss_cps: 0.189204
[14:17:30.593] iteration 27193: total_loss: 0.309084, loss_sup: 0.054411, loss_mps: 0.094172, loss_cps: 0.160502
[14:17:30.740] iteration 27194: total_loss: 0.394153, loss_sup: 0.031905, loss_mps: 0.120567, loss_cps: 0.241681
[14:17:30.888] iteration 27195: total_loss: 0.421512, loss_sup: 0.042481, loss_mps: 0.131066, loss_cps: 0.247964
[14:17:31.035] iteration 27196: total_loss: 0.390388, loss_sup: 0.148065, loss_mps: 0.084481, loss_cps: 0.157843
[14:17:31.183] iteration 27197: total_loss: 0.528729, loss_sup: 0.088404, loss_mps: 0.141330, loss_cps: 0.298996
[14:17:31.332] iteration 27198: total_loss: 0.604381, loss_sup: 0.183506, loss_mps: 0.146250, loss_cps: 0.274626
[14:17:31.480] iteration 27199: total_loss: 0.445080, loss_sup: 0.056845, loss_mps: 0.132855, loss_cps: 0.255380
[14:17:31.629] iteration 27200: total_loss: 0.476264, loss_sup: 0.036861, loss_mps: 0.144608, loss_cps: 0.294794
[14:17:31.629] Evaluation Started ==>
[14:17:43.047] ==> valid iteration 27200: unet metrics: {'dc': 0.660520663929187, 'jc': 0.5454624263893565, 'pre': 0.8031118726910669, 'hd': 5.384147204122011}, ynet metrics: {'dc': 0.6316055771945354, 'jc': 0.5201815898006567, 'pre': 0.7918023643738865, 'hd': 5.383692188471798}.
[14:17:43.049] Evaluation Finished!⏹️
[14:17:43.199] iteration 27201: total_loss: 0.360348, loss_sup: 0.036198, loss_mps: 0.109733, loss_cps: 0.214417
[14:17:43.347] iteration 27202: total_loss: 0.321280, loss_sup: 0.021479, loss_mps: 0.106846, loss_cps: 0.192955
[14:17:43.494] iteration 27203: total_loss: 0.525678, loss_sup: 0.083385, loss_mps: 0.144467, loss_cps: 0.297826
[14:17:43.639] iteration 27204: total_loss: 0.476464, loss_sup: 0.016243, loss_mps: 0.149402, loss_cps: 0.310818
[14:17:43.785] iteration 27205: total_loss: 0.316331, loss_sup: 0.044043, loss_mps: 0.100701, loss_cps: 0.171587
[14:17:43.930] iteration 27206: total_loss: 0.206038, loss_sup: 0.006829, loss_mps: 0.074685, loss_cps: 0.124524
[14:17:44.075] iteration 27207: total_loss: 0.306334, loss_sup: 0.015730, loss_mps: 0.105247, loss_cps: 0.185357
[14:17:44.223] iteration 27208: total_loss: 0.249947, loss_sup: 0.036192, loss_mps: 0.078315, loss_cps: 0.135440
[14:17:44.374] iteration 27209: total_loss: 0.258917, loss_sup: 0.051901, loss_mps: 0.078918, loss_cps: 0.128098
[14:17:44.523] iteration 27210: total_loss: 0.321318, loss_sup: 0.022572, loss_mps: 0.102623, loss_cps: 0.196123
[14:17:44.668] iteration 27211: total_loss: 0.249114, loss_sup: 0.001114, loss_mps: 0.092260, loss_cps: 0.155740
[14:17:44.814] iteration 27212: total_loss: 0.264218, loss_sup: 0.016202, loss_mps: 0.091230, loss_cps: 0.156786
[14:17:44.961] iteration 27213: total_loss: 0.308728, loss_sup: 0.078478, loss_mps: 0.085039, loss_cps: 0.145212
[14:17:45.109] iteration 27214: total_loss: 0.311783, loss_sup: 0.056877, loss_mps: 0.086902, loss_cps: 0.168003
[14:17:45.255] iteration 27215: total_loss: 0.161519, loss_sup: 0.001235, loss_mps: 0.060693, loss_cps: 0.099590
[14:17:45.401] iteration 27216: total_loss: 0.487866, loss_sup: 0.034134, loss_mps: 0.153377, loss_cps: 0.300355
[14:17:45.547] iteration 27217: total_loss: 0.467108, loss_sup: 0.055257, loss_mps: 0.143262, loss_cps: 0.268589
[14:17:45.692] iteration 27218: total_loss: 0.337147, loss_sup: 0.004076, loss_mps: 0.113537, loss_cps: 0.219534
[14:17:45.839] iteration 27219: total_loss: 0.261501, loss_sup: 0.047993, loss_mps: 0.080486, loss_cps: 0.133022
[14:17:45.985] iteration 27220: total_loss: 0.332308, loss_sup: 0.052061, loss_mps: 0.094994, loss_cps: 0.185254
[14:17:46.132] iteration 27221: total_loss: 0.240409, loss_sup: 0.018080, loss_mps: 0.082933, loss_cps: 0.139395
[14:17:46.278] iteration 27222: total_loss: 0.302043, loss_sup: 0.043915, loss_mps: 0.093724, loss_cps: 0.164403
[14:17:46.424] iteration 27223: total_loss: 0.426144, loss_sup: 0.047064, loss_mps: 0.127140, loss_cps: 0.251940
[14:17:46.569] iteration 27224: total_loss: 0.367781, loss_sup: 0.040666, loss_mps: 0.110714, loss_cps: 0.216401
[14:17:46.715] iteration 27225: total_loss: 0.375141, loss_sup: 0.128595, loss_mps: 0.087571, loss_cps: 0.158974
[14:17:46.862] iteration 27226: total_loss: 0.359466, loss_sup: 0.122218, loss_mps: 0.084826, loss_cps: 0.152422
[14:17:47.008] iteration 27227: total_loss: 0.300168, loss_sup: 0.015605, loss_mps: 0.096158, loss_cps: 0.188405
[14:17:47.157] iteration 27228: total_loss: 0.507300, loss_sup: 0.065498, loss_mps: 0.153598, loss_cps: 0.288204
[14:17:47.303] iteration 27229: total_loss: 0.274528, loss_sup: 0.012974, loss_mps: 0.092918, loss_cps: 0.168636
[14:17:47.450] iteration 27230: total_loss: 0.370317, loss_sup: 0.129485, loss_mps: 0.084375, loss_cps: 0.156457
[14:17:47.595] iteration 27231: total_loss: 0.363343, loss_sup: 0.115047, loss_mps: 0.091650, loss_cps: 0.156646
[14:17:47.741] iteration 27232: total_loss: 0.200452, loss_sup: 0.006605, loss_mps: 0.072782, loss_cps: 0.121065
[14:17:47.888] iteration 27233: total_loss: 0.326962, loss_sup: 0.011597, loss_mps: 0.102090, loss_cps: 0.213276
[14:17:48.034] iteration 27234: total_loss: 0.268249, loss_sup: 0.011551, loss_mps: 0.094643, loss_cps: 0.162055
[14:17:48.179] iteration 27235: total_loss: 0.222514, loss_sup: 0.040597, loss_mps: 0.068847, loss_cps: 0.113070
[14:17:48.328] iteration 27236: total_loss: 0.203077, loss_sup: 0.002829, loss_mps: 0.070017, loss_cps: 0.130231
[14:17:48.475] iteration 27237: total_loss: 0.170873, loss_sup: 0.006487, loss_mps: 0.064842, loss_cps: 0.099544
[14:17:48.622] iteration 27238: total_loss: 0.178310, loss_sup: 0.005139, loss_mps: 0.066667, loss_cps: 0.106503
[14:17:48.769] iteration 27239: total_loss: 0.649763, loss_sup: 0.158233, loss_mps: 0.159102, loss_cps: 0.332428
[14:17:48.916] iteration 27240: total_loss: 0.320862, loss_sup: 0.110290, loss_mps: 0.080053, loss_cps: 0.130519
[14:17:49.062] iteration 27241: total_loss: 0.321917, loss_sup: 0.029288, loss_mps: 0.108402, loss_cps: 0.184226
[14:17:49.208] iteration 27242: total_loss: 0.324296, loss_sup: 0.015671, loss_mps: 0.104220, loss_cps: 0.204405
[14:17:49.354] iteration 27243: total_loss: 0.303352, loss_sup: 0.026317, loss_mps: 0.105007, loss_cps: 0.172029
[14:17:49.499] iteration 27244: total_loss: 0.227031, loss_sup: 0.012801, loss_mps: 0.074577, loss_cps: 0.139653
[14:17:49.645] iteration 27245: total_loss: 0.220113, loss_sup: 0.021774, loss_mps: 0.074172, loss_cps: 0.124167
[14:17:49.792] iteration 27246: total_loss: 0.446271, loss_sup: 0.003714, loss_mps: 0.152425, loss_cps: 0.290131
[14:17:49.938] iteration 27247: total_loss: 0.531790, loss_sup: 0.261888, loss_mps: 0.090250, loss_cps: 0.179652
[14:17:50.085] iteration 27248: total_loss: 0.270907, loss_sup: 0.046431, loss_mps: 0.086935, loss_cps: 0.137541
[14:17:50.230] iteration 27249: total_loss: 0.256342, loss_sup: 0.022569, loss_mps: 0.086153, loss_cps: 0.147620
[14:17:50.377] iteration 27250: total_loss: 0.253234, loss_sup: 0.128541, loss_mps: 0.046942, loss_cps: 0.077752
[14:17:50.523] iteration 27251: total_loss: 0.626895, loss_sup: 0.037066, loss_mps: 0.178767, loss_cps: 0.411063
[14:17:50.668] iteration 27252: total_loss: 0.411958, loss_sup: 0.018376, loss_mps: 0.126939, loss_cps: 0.266642
[14:17:50.814] iteration 27253: total_loss: 0.321103, loss_sup: 0.034656, loss_mps: 0.097522, loss_cps: 0.188924
[14:17:50.961] iteration 27254: total_loss: 0.347691, loss_sup: 0.045231, loss_mps: 0.106578, loss_cps: 0.195882
[14:17:51.108] iteration 27255: total_loss: 0.346428, loss_sup: 0.011204, loss_mps: 0.106675, loss_cps: 0.228549
[14:17:51.254] iteration 27256: total_loss: 0.446141, loss_sup: 0.033415, loss_mps: 0.138658, loss_cps: 0.274068
[14:17:51.400] iteration 27257: total_loss: 0.322712, loss_sup: 0.032378, loss_mps: 0.098160, loss_cps: 0.192173
[14:17:51.546] iteration 27258: total_loss: 0.637585, loss_sup: 0.045595, loss_mps: 0.194666, loss_cps: 0.397324
[14:17:51.691] iteration 27259: total_loss: 0.869267, loss_sup: 0.044850, loss_mps: 0.251682, loss_cps: 0.572735
[14:17:51.837] iteration 27260: total_loss: 0.335143, loss_sup: 0.008179, loss_mps: 0.109754, loss_cps: 0.217210
[14:17:51.983] iteration 27261: total_loss: 0.559589, loss_sup: 0.092681, loss_mps: 0.154232, loss_cps: 0.312676
[14:17:52.129] iteration 27262: total_loss: 0.237258, loss_sup: 0.067183, loss_mps: 0.063420, loss_cps: 0.106655
[14:17:52.275] iteration 27263: total_loss: 0.555953, loss_sup: 0.038230, loss_mps: 0.167993, loss_cps: 0.349731
[14:17:52.421] iteration 27264: total_loss: 0.248506, loss_sup: 0.110695, loss_mps: 0.053931, loss_cps: 0.083880
[14:17:52.567] iteration 27265: total_loss: 0.233611, loss_sup: 0.014393, loss_mps: 0.080885, loss_cps: 0.138333
[14:17:52.712] iteration 27266: total_loss: 0.307177, loss_sup: 0.041645, loss_mps: 0.093902, loss_cps: 0.171629
[14:17:52.859] iteration 27267: total_loss: 0.302391, loss_sup: 0.003381, loss_mps: 0.098304, loss_cps: 0.200706
[14:17:53.006] iteration 27268: total_loss: 0.301794, loss_sup: 0.118941, loss_mps: 0.065452, loss_cps: 0.117401
[14:17:53.152] iteration 27269: total_loss: 0.281036, loss_sup: 0.062719, loss_mps: 0.077634, loss_cps: 0.140682
[14:17:53.298] iteration 27270: total_loss: 0.471563, loss_sup: 0.068733, loss_mps: 0.136171, loss_cps: 0.266658
[14:17:53.444] iteration 27271: total_loss: 0.400149, loss_sup: 0.062263, loss_mps: 0.110127, loss_cps: 0.227759
[14:17:53.589] iteration 27272: total_loss: 0.316787, loss_sup: 0.068752, loss_mps: 0.087837, loss_cps: 0.160198
[14:17:53.737] iteration 27273: total_loss: 0.540776, loss_sup: 0.096064, loss_mps: 0.148666, loss_cps: 0.296046
[14:17:53.883] iteration 27274: total_loss: 0.216381, loss_sup: 0.008318, loss_mps: 0.074538, loss_cps: 0.133525
[14:17:54.031] iteration 27275: total_loss: 0.292570, loss_sup: 0.005346, loss_mps: 0.102511, loss_cps: 0.184713
[14:17:54.177] iteration 27276: total_loss: 0.272036, loss_sup: 0.011754, loss_mps: 0.090401, loss_cps: 0.169881
[14:17:54.323] iteration 27277: total_loss: 0.179898, loss_sup: 0.008620, loss_mps: 0.065045, loss_cps: 0.106233
[14:17:54.469] iteration 27278: total_loss: 0.309039, loss_sup: 0.030107, loss_mps: 0.100440, loss_cps: 0.178492
[14:17:54.616] iteration 27279: total_loss: 0.396654, loss_sup: 0.073297, loss_mps: 0.111053, loss_cps: 0.212304
[14:17:54.762] iteration 27280: total_loss: 0.213616, loss_sup: 0.003166, loss_mps: 0.080194, loss_cps: 0.130256
[14:17:54.909] iteration 27281: total_loss: 0.451479, loss_sup: 0.044706, loss_mps: 0.136754, loss_cps: 0.270019
[14:17:55.057] iteration 27282: total_loss: 0.295045, loss_sup: 0.021048, loss_mps: 0.096402, loss_cps: 0.177594
[14:17:55.203] iteration 27283: total_loss: 0.406005, loss_sup: 0.063449, loss_mps: 0.113340, loss_cps: 0.229217
[14:17:55.348] iteration 27284: total_loss: 0.295041, loss_sup: 0.061957, loss_mps: 0.079604, loss_cps: 0.153481
[14:17:55.494] iteration 27285: total_loss: 1.153090, loss_sup: 0.030840, loss_mps: 0.329613, loss_cps: 0.792638
[14:17:55.641] iteration 27286: total_loss: 0.423940, loss_sup: 0.078740, loss_mps: 0.124265, loss_cps: 0.220935
[14:17:55.788] iteration 27287: total_loss: 0.536842, loss_sup: 0.093134, loss_mps: 0.154103, loss_cps: 0.289606
[14:17:55.934] iteration 27288: total_loss: 0.167540, loss_sup: 0.006808, loss_mps: 0.060894, loss_cps: 0.099838
[14:17:56.081] iteration 27289: total_loss: 0.247137, loss_sup: 0.005593, loss_mps: 0.083075, loss_cps: 0.158469
[14:17:56.227] iteration 27290: total_loss: 0.291167, loss_sup: 0.042656, loss_mps: 0.089276, loss_cps: 0.159234
[14:17:56.373] iteration 27291: total_loss: 0.331419, loss_sup: 0.009164, loss_mps: 0.111655, loss_cps: 0.210600
[14:17:56.519] iteration 27292: total_loss: 0.709398, loss_sup: 0.057822, loss_mps: 0.199494, loss_cps: 0.452083
[14:17:56.664] iteration 27293: total_loss: 0.472304, loss_sup: 0.052564, loss_mps: 0.144793, loss_cps: 0.274947
[14:17:56.811] iteration 27294: total_loss: 0.343072, loss_sup: 0.035930, loss_mps: 0.108846, loss_cps: 0.198297
[14:17:56.959] iteration 27295: total_loss: 0.622270, loss_sup: 0.006252, loss_mps: 0.194692, loss_cps: 0.421326
[14:17:57.108] iteration 27296: total_loss: 0.340455, loss_sup: 0.003756, loss_mps: 0.114113, loss_cps: 0.222586
[14:17:57.255] iteration 27297: total_loss: 0.512175, loss_sup: 0.062166, loss_mps: 0.158874, loss_cps: 0.291134
[14:17:57.403] iteration 27298: total_loss: 0.328530, loss_sup: 0.032065, loss_mps: 0.099308, loss_cps: 0.197157
[14:17:57.549] iteration 27299: total_loss: 0.376961, loss_sup: 0.015388, loss_mps: 0.124041, loss_cps: 0.237532
[14:17:57.695] iteration 27300: total_loss: 0.240269, loss_sup: 0.024456, loss_mps: 0.074548, loss_cps: 0.141266
[14:17:57.695] Evaluation Started ==>
[14:18:09.054] ==> valid iteration 27300: unet metrics: {'dc': 0.635278157150265, 'jc': 0.519824222660776, 'pre': 0.7999823022473129, 'hd': 5.481047540301234}, ynet metrics: {'dc': 0.6032122133337195, 'jc': 0.491490897499544, 'pre': 0.8018826913248851, 'hd': 5.3824934760883485}.
[14:18:09.056] Evaluation Finished!⏹️
[14:18:09.206] iteration 27301: total_loss: 0.532718, loss_sup: 0.111526, loss_mps: 0.137884, loss_cps: 0.283308
[14:18:09.354] iteration 27302: total_loss: 0.523990, loss_sup: 0.005034, loss_mps: 0.168416, loss_cps: 0.350541
[14:18:09.503] iteration 27303: total_loss: 0.699564, loss_sup: 0.037176, loss_mps: 0.203020, loss_cps: 0.459368
[14:18:09.649] iteration 27304: total_loss: 0.526065, loss_sup: 0.039222, loss_mps: 0.146970, loss_cps: 0.339873
[14:18:09.795] iteration 27305: total_loss: 0.424209, loss_sup: 0.028756, loss_mps: 0.129163, loss_cps: 0.266289
[14:18:09.940] iteration 27306: total_loss: 0.163396, loss_sup: 0.009766, loss_mps: 0.055870, loss_cps: 0.097759
[14:18:10.085] iteration 27307: total_loss: 0.163646, loss_sup: 0.001052, loss_mps: 0.060642, loss_cps: 0.101952
[14:18:10.231] iteration 27308: total_loss: 0.655061, loss_sup: 0.140321, loss_mps: 0.168960, loss_cps: 0.345779
[14:18:10.377] iteration 27309: total_loss: 0.321742, loss_sup: 0.017345, loss_mps: 0.106746, loss_cps: 0.197652
[14:18:10.524] iteration 27310: total_loss: 0.558270, loss_sup: 0.043561, loss_mps: 0.162062, loss_cps: 0.352647
[14:18:10.670] iteration 27311: total_loss: 0.554006, loss_sup: 0.103358, loss_mps: 0.151598, loss_cps: 0.299050
[14:18:10.816] iteration 27312: total_loss: 0.294117, loss_sup: 0.029359, loss_mps: 0.096163, loss_cps: 0.168595
[14:18:10.965] iteration 27313: total_loss: 0.317579, loss_sup: 0.044015, loss_mps: 0.097910, loss_cps: 0.175654
[14:18:11.113] iteration 27314: total_loss: 0.294284, loss_sup: 0.015804, loss_mps: 0.100468, loss_cps: 0.178012
[14:18:11.259] iteration 27315: total_loss: 0.285574, loss_sup: 0.019974, loss_mps: 0.090315, loss_cps: 0.175285
[14:18:11.405] iteration 27316: total_loss: 0.303393, loss_sup: 0.020332, loss_mps: 0.099278, loss_cps: 0.183783
[14:18:11.551] iteration 27317: total_loss: 0.439122, loss_sup: 0.029367, loss_mps: 0.143381, loss_cps: 0.266375
[14:18:11.697] iteration 27318: total_loss: 0.273899, loss_sup: 0.028435, loss_mps: 0.085346, loss_cps: 0.160118
[14:18:11.842] iteration 27319: total_loss: 0.410197, loss_sup: 0.057621, loss_mps: 0.123294, loss_cps: 0.229282
[14:18:11.989] iteration 27320: total_loss: 0.673771, loss_sup: 0.361954, loss_mps: 0.104767, loss_cps: 0.207051
[14:18:12.135] iteration 27321: total_loss: 0.339741, loss_sup: 0.075501, loss_mps: 0.093402, loss_cps: 0.170838
[14:18:12.281] iteration 27322: total_loss: 0.357796, loss_sup: 0.080757, loss_mps: 0.099224, loss_cps: 0.177815
[14:18:12.429] iteration 27323: total_loss: 0.505750, loss_sup: 0.038073, loss_mps: 0.157756, loss_cps: 0.309921
[14:18:12.576] iteration 27324: total_loss: 0.717051, loss_sup: 0.079999, loss_mps: 0.201587, loss_cps: 0.435465
[14:18:12.722] iteration 27325: total_loss: 0.279863, loss_sup: 0.069439, loss_mps: 0.077647, loss_cps: 0.132777
[14:18:12.868] iteration 27326: total_loss: 0.579867, loss_sup: 0.059320, loss_mps: 0.170009, loss_cps: 0.350538
[14:18:13.014] iteration 27327: total_loss: 0.505108, loss_sup: 0.219042, loss_mps: 0.099623, loss_cps: 0.186443
[14:18:13.160] iteration 27328: total_loss: 0.483301, loss_sup: 0.127994, loss_mps: 0.116302, loss_cps: 0.239005
[14:18:13.306] iteration 27329: total_loss: 0.271637, loss_sup: 0.012833, loss_mps: 0.087552, loss_cps: 0.171252
[14:18:13.451] iteration 27330: total_loss: 0.462425, loss_sup: 0.112896, loss_mps: 0.118652, loss_cps: 0.230877
[14:18:13.597] iteration 27331: total_loss: 0.194266, loss_sup: 0.026885, loss_mps: 0.061348, loss_cps: 0.106034
[14:18:13.742] iteration 27332: total_loss: 0.691908, loss_sup: 0.171951, loss_mps: 0.173958, loss_cps: 0.345998
[14:18:13.888] iteration 27333: total_loss: 0.243584, loss_sup: 0.030160, loss_mps: 0.076184, loss_cps: 0.137240
[14:18:14.035] iteration 27334: total_loss: 0.208906, loss_sup: 0.006943, loss_mps: 0.074945, loss_cps: 0.127018
[14:18:14.180] iteration 27335: total_loss: 0.282279, loss_sup: 0.034044, loss_mps: 0.089591, loss_cps: 0.158645
[14:18:14.327] iteration 27336: total_loss: 0.286821, loss_sup: 0.072720, loss_mps: 0.077196, loss_cps: 0.136904
[14:18:14.473] iteration 27337: total_loss: 0.570077, loss_sup: 0.122235, loss_mps: 0.152228, loss_cps: 0.295614
[14:18:14.619] iteration 27338: total_loss: 0.320293, loss_sup: 0.070000, loss_mps: 0.086858, loss_cps: 0.163435
[14:18:14.768] iteration 27339: total_loss: 0.245186, loss_sup: 0.025167, loss_mps: 0.076452, loss_cps: 0.143567
[14:18:14.914] iteration 27340: total_loss: 0.383343, loss_sup: 0.057434, loss_mps: 0.117034, loss_cps: 0.208875
[14:18:15.059] iteration 27341: total_loss: 0.420903, loss_sup: 0.027489, loss_mps: 0.138291, loss_cps: 0.255123
[14:18:15.206] iteration 27342: total_loss: 0.564047, loss_sup: 0.054257, loss_mps: 0.155575, loss_cps: 0.354214
[14:18:15.352] iteration 27343: total_loss: 0.394724, loss_sup: 0.046012, loss_mps: 0.116666, loss_cps: 0.232045
[14:18:15.497] iteration 27344: total_loss: 0.552279, loss_sup: 0.146998, loss_mps: 0.139910, loss_cps: 0.265370
[14:18:15.643] iteration 27345: total_loss: 0.261229, loss_sup: 0.029700, loss_mps: 0.082179, loss_cps: 0.149350
[14:18:15.788] iteration 27346: total_loss: 0.369007, loss_sup: 0.064374, loss_mps: 0.105423, loss_cps: 0.199209
[14:18:15.934] iteration 27347: total_loss: 0.414340, loss_sup: 0.151602, loss_mps: 0.097749, loss_cps: 0.164988
[14:18:16.079] iteration 27348: total_loss: 0.269187, loss_sup: 0.019803, loss_mps: 0.088296, loss_cps: 0.161087
[14:18:16.225] iteration 27349: total_loss: 0.505812, loss_sup: 0.005110, loss_mps: 0.162761, loss_cps: 0.337940
[14:18:16.371] iteration 27350: total_loss: 0.398635, loss_sup: 0.006541, loss_mps: 0.127428, loss_cps: 0.264665
[14:18:16.517] iteration 27351: total_loss: 0.250681, loss_sup: 0.035771, loss_mps: 0.077410, loss_cps: 0.137500
[14:18:16.662] iteration 27352: total_loss: 0.269840, loss_sup: 0.037033, loss_mps: 0.088007, loss_cps: 0.144800
[14:18:16.810] iteration 27353: total_loss: 0.461597, loss_sup: 0.025112, loss_mps: 0.149215, loss_cps: 0.287270
[14:18:16.956] iteration 27354: total_loss: 0.425836, loss_sup: 0.022782, loss_mps: 0.129718, loss_cps: 0.273336
[14:18:17.103] iteration 27355: total_loss: 0.278297, loss_sup: 0.091245, loss_mps: 0.066441, loss_cps: 0.120611
[14:18:17.253] iteration 27356: total_loss: 0.335091, loss_sup: 0.031154, loss_mps: 0.114818, loss_cps: 0.189119
[14:18:17.398] iteration 27357: total_loss: 0.270248, loss_sup: 0.049349, loss_mps: 0.083407, loss_cps: 0.137492
[14:18:17.545] iteration 27358: total_loss: 0.445097, loss_sup: 0.042222, loss_mps: 0.136248, loss_cps: 0.266627
[14:18:17.692] iteration 27359: total_loss: 0.650729, loss_sup: 0.105505, loss_mps: 0.178948, loss_cps: 0.366277
[14:18:17.839] iteration 27360: total_loss: 0.178201, loss_sup: 0.008252, loss_mps: 0.065188, loss_cps: 0.104761
[14:18:17.985] iteration 27361: total_loss: 0.144838, loss_sup: 0.000753, loss_mps: 0.055130, loss_cps: 0.088955
[14:18:18.133] iteration 27362: total_loss: 0.323669, loss_sup: 0.008840, loss_mps: 0.110806, loss_cps: 0.204023
[14:18:18.278] iteration 27363: total_loss: 0.692310, loss_sup: 0.000803, loss_mps: 0.216131, loss_cps: 0.475376
[14:18:18.424] iteration 27364: total_loss: 0.281224, loss_sup: 0.010360, loss_mps: 0.092265, loss_cps: 0.178598
[14:18:18.570] iteration 27365: total_loss: 0.236371, loss_sup: 0.001239, loss_mps: 0.086843, loss_cps: 0.148289
[14:18:18.716] iteration 27366: total_loss: 0.363299, loss_sup: 0.028894, loss_mps: 0.117487, loss_cps: 0.216918
[14:18:18.861] iteration 27367: total_loss: 0.303378, loss_sup: 0.033787, loss_mps: 0.090244, loss_cps: 0.179347
[14:18:19.007] iteration 27368: total_loss: 0.359954, loss_sup: 0.010384, loss_mps: 0.120172, loss_cps: 0.229399
[14:18:19.152] iteration 27369: total_loss: 0.544126, loss_sup: 0.214738, loss_mps: 0.115812, loss_cps: 0.213576
[14:18:19.298] iteration 27370: total_loss: 0.222753, loss_sup: 0.007523, loss_mps: 0.076398, loss_cps: 0.138831
[14:18:19.446] iteration 27371: total_loss: 0.328128, loss_sup: 0.021665, loss_mps: 0.104841, loss_cps: 0.201623
[14:18:19.591] iteration 27372: total_loss: 0.419594, loss_sup: 0.011177, loss_mps: 0.140682, loss_cps: 0.267734
[14:18:19.736] iteration 27373: total_loss: 0.618935, loss_sup: 0.115529, loss_mps: 0.153715, loss_cps: 0.349691
[14:18:19.881] iteration 27374: total_loss: 0.329109, loss_sup: 0.053484, loss_mps: 0.090324, loss_cps: 0.185301
[14:18:20.027] iteration 27375: total_loss: 0.592796, loss_sup: 0.127806, loss_mps: 0.161712, loss_cps: 0.303277
[14:18:20.173] iteration 27376: total_loss: 0.588862, loss_sup: 0.142001, loss_mps: 0.150040, loss_cps: 0.296821
[14:18:20.319] iteration 27377: total_loss: 0.334173, loss_sup: 0.032772, loss_mps: 0.103000, loss_cps: 0.198401
[14:18:20.466] iteration 27378: total_loss: 0.474311, loss_sup: 0.121305, loss_mps: 0.114827, loss_cps: 0.238180
[14:18:20.613] iteration 27379: total_loss: 0.201309, loss_sup: 0.029879, loss_mps: 0.065352, loss_cps: 0.106078
[14:18:20.759] iteration 27380: total_loss: 0.362472, loss_sup: 0.033151, loss_mps: 0.108981, loss_cps: 0.220340
[14:18:20.906] iteration 27381: total_loss: 0.515713, loss_sup: 0.089470, loss_mps: 0.151034, loss_cps: 0.275210
[14:18:21.053] iteration 27382: total_loss: 0.624040, loss_sup: 0.134789, loss_mps: 0.162469, loss_cps: 0.326782
[14:18:21.199] iteration 27383: total_loss: 0.204376, loss_sup: 0.000709, loss_mps: 0.074503, loss_cps: 0.129165
[14:18:21.345] iteration 27384: total_loss: 0.407239, loss_sup: 0.071060, loss_mps: 0.118237, loss_cps: 0.217942
[14:18:21.492] iteration 27385: total_loss: 0.248997, loss_sup: 0.004552, loss_mps: 0.083337, loss_cps: 0.161108
[14:18:21.638] iteration 27386: total_loss: 0.395426, loss_sup: 0.048329, loss_mps: 0.122260, loss_cps: 0.224837
[14:18:21.786] iteration 27387: total_loss: 0.452032, loss_sup: 0.052179, loss_mps: 0.132752, loss_cps: 0.267101
[14:18:21.934] iteration 27388: total_loss: 0.690575, loss_sup: 0.044188, loss_mps: 0.201372, loss_cps: 0.445014
[14:18:22.081] iteration 27389: total_loss: 0.292240, loss_sup: 0.107768, loss_mps: 0.073366, loss_cps: 0.111106
[14:18:22.227] iteration 27390: total_loss: 0.299719, loss_sup: 0.015537, loss_mps: 0.097550, loss_cps: 0.186632
[14:18:22.373] iteration 27391: total_loss: 0.305068, loss_sup: 0.053907, loss_mps: 0.085869, loss_cps: 0.165292
[14:18:22.519] iteration 27392: total_loss: 0.289351, loss_sup: 0.028716, loss_mps: 0.095361, loss_cps: 0.165275
[14:18:22.665] iteration 27393: total_loss: 0.294627, loss_sup: 0.014017, loss_mps: 0.098636, loss_cps: 0.181974
[14:18:22.813] iteration 27394: total_loss: 0.227254, loss_sup: 0.012279, loss_mps: 0.079604, loss_cps: 0.135371
[14:18:22.963] iteration 27395: total_loss: 0.371296, loss_sup: 0.020016, loss_mps: 0.123586, loss_cps: 0.227694
[14:18:23.110] iteration 27396: total_loss: 0.300826, loss_sup: 0.052927, loss_mps: 0.087096, loss_cps: 0.160803
[14:18:23.256] iteration 27397: total_loss: 0.361169, loss_sup: 0.045071, loss_mps: 0.107843, loss_cps: 0.208256
[14:18:23.402] iteration 27398: total_loss: 0.231823, loss_sup: 0.009865, loss_mps: 0.080593, loss_cps: 0.141365
[14:18:23.548] iteration 27399: total_loss: 0.201127, loss_sup: 0.011228, loss_mps: 0.070027, loss_cps: 0.119872
[14:18:23.696] iteration 27400: total_loss: 0.454513, loss_sup: 0.059733, loss_mps: 0.141779, loss_cps: 0.253001
[14:18:23.696] Evaluation Started ==>
[14:18:35.075] ==> valid iteration 27400: unet metrics: {'dc': 0.6609121905726689, 'jc': 0.5455874870046091, 'pre': 0.7932664319945509, 'hd': 5.441379389364908}, ynet metrics: {'dc': 0.6250840206998262, 'jc': 0.5139374349304373, 'pre': 0.7905323588510929, 'hd': 5.4307175062352275}.
[14:18:35.076] Evaluation Finished!⏹️
[14:18:35.226] iteration 27401: total_loss: 0.290955, loss_sup: 0.002352, loss_mps: 0.100834, loss_cps: 0.187769
[14:18:35.373] iteration 27402: total_loss: 0.200306, loss_sup: 0.017935, loss_mps: 0.073847, loss_cps: 0.108524
[14:18:35.519] iteration 27403: total_loss: 0.202112, loss_sup: 0.037735, loss_mps: 0.063463, loss_cps: 0.100913
[14:18:35.665] iteration 27404: total_loss: 0.743121, loss_sup: 0.054148, loss_mps: 0.214779, loss_cps: 0.474194
[14:18:35.810] iteration 27405: total_loss: 0.594850, loss_sup: 0.018237, loss_mps: 0.178203, loss_cps: 0.398411
[14:18:35.956] iteration 27406: total_loss: 0.415306, loss_sup: 0.069175, loss_mps: 0.117430, loss_cps: 0.228701
[14:18:36.103] iteration 27407: total_loss: 0.254690, loss_sup: 0.007732, loss_mps: 0.088150, loss_cps: 0.158808
[14:18:36.249] iteration 27408: total_loss: 0.219631, loss_sup: 0.023441, loss_mps: 0.069200, loss_cps: 0.126990
[14:18:36.395] iteration 27409: total_loss: 0.283122, loss_sup: 0.027252, loss_mps: 0.088770, loss_cps: 0.167101
[14:18:36.541] iteration 27410: total_loss: 0.176566, loss_sup: 0.021133, loss_mps: 0.056601, loss_cps: 0.098832
[14:18:36.687] iteration 27411: total_loss: 0.234423, loss_sup: 0.018305, loss_mps: 0.079086, loss_cps: 0.137032
[14:18:36.833] iteration 27412: total_loss: 0.377931, loss_sup: 0.091891, loss_mps: 0.101343, loss_cps: 0.184696
[14:18:36.978] iteration 27413: total_loss: 0.389624, loss_sup: 0.049583, loss_mps: 0.114578, loss_cps: 0.225463
[14:18:37.124] iteration 27414: total_loss: 0.157873, loss_sup: 0.006159, loss_mps: 0.059336, loss_cps: 0.092378
[14:18:37.271] iteration 27415: total_loss: 0.445978, loss_sup: 0.205321, loss_mps: 0.081861, loss_cps: 0.158796
[14:18:37.416] iteration 27416: total_loss: 0.184047, loss_sup: 0.003846, loss_mps: 0.070010, loss_cps: 0.110191
[14:18:37.561] iteration 27417: total_loss: 1.057400, loss_sup: 0.305644, loss_mps: 0.236180, loss_cps: 0.515577
[14:18:37.707] iteration 27418: total_loss: 0.401584, loss_sup: 0.037503, loss_mps: 0.129034, loss_cps: 0.235047
[14:18:37.853] iteration 27419: total_loss: 0.289284, loss_sup: 0.016569, loss_mps: 0.096010, loss_cps: 0.176705
[14:18:37.998] iteration 27420: total_loss: 0.403325, loss_sup: 0.073813, loss_mps: 0.116787, loss_cps: 0.212725
[14:18:38.144] iteration 27421: total_loss: 0.280805, loss_sup: 0.008381, loss_mps: 0.095612, loss_cps: 0.176811
[14:18:38.294] iteration 27422: total_loss: 0.250874, loss_sup: 0.022091, loss_mps: 0.082549, loss_cps: 0.146234
[14:18:38.440] iteration 27423: total_loss: 0.316788, loss_sup: 0.012555, loss_mps: 0.108921, loss_cps: 0.195312
[14:18:38.587] iteration 27424: total_loss: 0.600588, loss_sup: 0.019118, loss_mps: 0.197788, loss_cps: 0.383682
[14:18:38.733] iteration 27425: total_loss: 0.242659, loss_sup: 0.005245, loss_mps: 0.085541, loss_cps: 0.151873
[14:18:38.878] iteration 27426: total_loss: 0.705671, loss_sup: 0.063116, loss_mps: 0.212617, loss_cps: 0.429938
[14:18:39.024] iteration 27427: total_loss: 0.383222, loss_sup: 0.079120, loss_mps: 0.105703, loss_cps: 0.198399
[14:18:39.169] iteration 27428: total_loss: 0.534441, loss_sup: 0.019103, loss_mps: 0.163079, loss_cps: 0.352259
[14:18:39.315] iteration 27429: total_loss: 0.327572, loss_sup: 0.011100, loss_mps: 0.104452, loss_cps: 0.212020
[14:18:39.461] iteration 27430: total_loss: 0.187251, loss_sup: 0.005003, loss_mps: 0.067170, loss_cps: 0.115079
[14:18:39.611] iteration 27431: total_loss: 0.763282, loss_sup: 0.107152, loss_mps: 0.203822, loss_cps: 0.452309
[14:18:39.756] iteration 27432: total_loss: 0.381974, loss_sup: 0.181114, loss_mps: 0.072093, loss_cps: 0.128767
[14:18:39.902] iteration 27433: total_loss: 0.290594, loss_sup: 0.034629, loss_mps: 0.092719, loss_cps: 0.163246
[14:18:40.047] iteration 27434: total_loss: 0.410070, loss_sup: 0.075577, loss_mps: 0.114112, loss_cps: 0.220381
[14:18:40.193] iteration 27435: total_loss: 0.330439, loss_sup: 0.010177, loss_mps: 0.114079, loss_cps: 0.206183
[14:18:40.340] iteration 27436: total_loss: 0.195653, loss_sup: 0.014016, loss_mps: 0.065764, loss_cps: 0.115873
[14:18:40.489] iteration 27437: total_loss: 0.404224, loss_sup: 0.084006, loss_mps: 0.107934, loss_cps: 0.212284
[14:18:40.634] iteration 27438: total_loss: 0.268339, loss_sup: 0.003153, loss_mps: 0.093309, loss_cps: 0.171877
[14:18:40.781] iteration 27439: total_loss: 0.455166, loss_sup: 0.025275, loss_mps: 0.146629, loss_cps: 0.283262
[14:18:40.926] iteration 27440: total_loss: 0.167937, loss_sup: 0.007371, loss_mps: 0.059612, loss_cps: 0.100955
[14:18:41.072] iteration 27441: total_loss: 0.220736, loss_sup: 0.042873, loss_mps: 0.064837, loss_cps: 0.113026
[14:18:41.217] iteration 27442: total_loss: 0.322478, loss_sup: 0.042335, loss_mps: 0.094741, loss_cps: 0.185403
[14:18:41.363] iteration 27443: total_loss: 0.241636, loss_sup: 0.000695, loss_mps: 0.086641, loss_cps: 0.154300
[14:18:41.512] iteration 27444: total_loss: 0.331367, loss_sup: 0.032068, loss_mps: 0.105878, loss_cps: 0.193421
[14:18:41.660] iteration 27445: total_loss: 0.379273, loss_sup: 0.004381, loss_mps: 0.130656, loss_cps: 0.244237
[14:18:41.806] iteration 27446: total_loss: 0.443357, loss_sup: 0.093005, loss_mps: 0.124579, loss_cps: 0.225773
[14:18:41.951] iteration 27447: total_loss: 0.263118, loss_sup: 0.001214, loss_mps: 0.090638, loss_cps: 0.171266
[14:18:42.097] iteration 27448: total_loss: 0.525998, loss_sup: 0.064510, loss_mps: 0.151908, loss_cps: 0.309579
[14:18:42.246] iteration 27449: total_loss: 0.385011, loss_sup: 0.053315, loss_mps: 0.114407, loss_cps: 0.217289
[14:18:42.392] iteration 27450: total_loss: 0.246148, loss_sup: 0.016673, loss_mps: 0.085844, loss_cps: 0.143631
[14:18:42.538] iteration 27451: total_loss: 0.492569, loss_sup: 0.111152, loss_mps: 0.130254, loss_cps: 0.251162
[14:18:42.683] iteration 27452: total_loss: 0.207606, loss_sup: 0.027421, loss_mps: 0.067857, loss_cps: 0.112327
[14:18:42.829] iteration 27453: total_loss: 0.484036, loss_sup: 0.037138, loss_mps: 0.142343, loss_cps: 0.304555
[14:18:42.974] iteration 27454: total_loss: 0.289832, loss_sup: 0.076113, loss_mps: 0.072762, loss_cps: 0.140957
[14:18:43.120] iteration 27455: total_loss: 0.364088, loss_sup: 0.011333, loss_mps: 0.119396, loss_cps: 0.233359
[14:18:43.266] iteration 27456: total_loss: 0.281299, loss_sup: 0.032761, loss_mps: 0.092684, loss_cps: 0.155854
[14:18:43.411] iteration 27457: total_loss: 0.356652, loss_sup: 0.054890, loss_mps: 0.104331, loss_cps: 0.197431
[14:18:43.556] iteration 27458: total_loss: 0.184003, loss_sup: 0.002320, loss_mps: 0.069121, loss_cps: 0.112562
[14:18:43.702] iteration 27459: total_loss: 0.230373, loss_sup: 0.005461, loss_mps: 0.081476, loss_cps: 0.143436
[14:18:43.848] iteration 27460: total_loss: 0.698412, loss_sup: 0.313972, loss_mps: 0.126880, loss_cps: 0.257561
[14:18:43.994] iteration 27461: total_loss: 0.394698, loss_sup: 0.048794, loss_mps: 0.114950, loss_cps: 0.230954
[14:18:44.140] iteration 27462: total_loss: 0.245007, loss_sup: 0.010557, loss_mps: 0.088962, loss_cps: 0.145488
[14:18:44.285] iteration 27463: total_loss: 0.454508, loss_sup: 0.120259, loss_mps: 0.113180, loss_cps: 0.221069
[14:18:44.431] iteration 27464: total_loss: 0.215582, loss_sup: 0.026451, loss_mps: 0.069266, loss_cps: 0.119866
[14:18:44.576] iteration 27465: total_loss: 0.251386, loss_sup: 0.002121, loss_mps: 0.091157, loss_cps: 0.158107
[14:18:44.723] iteration 27466: total_loss: 0.319591, loss_sup: 0.003179, loss_mps: 0.106872, loss_cps: 0.209540
[14:18:44.869] iteration 27467: total_loss: 0.398594, loss_sup: 0.007089, loss_mps: 0.125808, loss_cps: 0.265697
[14:18:45.015] iteration 27468: total_loss: 0.541659, loss_sup: 0.115526, loss_mps: 0.153596, loss_cps: 0.272537
[14:18:45.161] iteration 27469: total_loss: 0.333562, loss_sup: 0.015473, loss_mps: 0.106082, loss_cps: 0.212007
[14:18:45.307] iteration 27470: total_loss: 0.265829, loss_sup: 0.007108, loss_mps: 0.089718, loss_cps: 0.169004
[14:18:45.453] iteration 27471: total_loss: 0.573005, loss_sup: 0.151822, loss_mps: 0.137147, loss_cps: 0.284036
[14:18:45.598] iteration 27472: total_loss: 0.489527, loss_sup: 0.202731, loss_mps: 0.101827, loss_cps: 0.184969
[14:18:45.744] iteration 27473: total_loss: 0.328086, loss_sup: 0.036810, loss_mps: 0.103939, loss_cps: 0.187337
[14:18:45.889] iteration 27474: total_loss: 0.262760, loss_sup: 0.026238, loss_mps: 0.083995, loss_cps: 0.152527
[14:18:46.035] iteration 27475: total_loss: 0.279623, loss_sup: 0.060039, loss_mps: 0.083589, loss_cps: 0.135995
[14:18:46.182] iteration 27476: total_loss: 0.442672, loss_sup: 0.032776, loss_mps: 0.137763, loss_cps: 0.272133
[14:18:46.327] iteration 27477: total_loss: 0.172757, loss_sup: 0.031031, loss_mps: 0.051824, loss_cps: 0.089902
[14:18:46.473] iteration 27478: total_loss: 0.437331, loss_sup: 0.172923, loss_mps: 0.094617, loss_cps: 0.169791
[14:18:46.619] iteration 27479: total_loss: 0.380863, loss_sup: 0.092364, loss_mps: 0.097705, loss_cps: 0.190794
[14:18:46.764] iteration 27480: total_loss: 0.424959, loss_sup: 0.035239, loss_mps: 0.130734, loss_cps: 0.258986
[14:18:46.910] iteration 27481: total_loss: 0.253390, loss_sup: 0.002535, loss_mps: 0.089960, loss_cps: 0.160895
[14:18:47.056] iteration 27482: total_loss: 0.205057, loss_sup: 0.002831, loss_mps: 0.073467, loss_cps: 0.128759
[14:18:47.202] iteration 27483: total_loss: 0.266347, loss_sup: 0.057805, loss_mps: 0.077083, loss_cps: 0.131458
[14:18:47.348] iteration 27484: total_loss: 0.534990, loss_sup: 0.038979, loss_mps: 0.167170, loss_cps: 0.328842
[14:18:47.497] iteration 27485: total_loss: 0.201187, loss_sup: 0.018411, loss_mps: 0.070553, loss_cps: 0.112223
[14:18:47.643] iteration 27486: total_loss: 0.245620, loss_sup: 0.016620, loss_mps: 0.085499, loss_cps: 0.143501
[14:18:47.788] iteration 27487: total_loss: 0.387835, loss_sup: 0.113685, loss_mps: 0.089988, loss_cps: 0.184161
[14:18:47.935] iteration 27488: total_loss: 0.210521, loss_sup: 0.016074, loss_mps: 0.072991, loss_cps: 0.121457
[14:18:48.081] iteration 27489: total_loss: 0.903269, loss_sup: 0.050314, loss_mps: 0.260992, loss_cps: 0.591963
[14:18:48.227] iteration 27490: total_loss: 0.304495, loss_sup: 0.076577, loss_mps: 0.086219, loss_cps: 0.141699
[14:18:48.372] iteration 27491: total_loss: 0.659570, loss_sup: 0.050921, loss_mps: 0.200747, loss_cps: 0.407902
[14:18:48.518] iteration 27492: total_loss: 0.370858, loss_sup: 0.114683, loss_mps: 0.092287, loss_cps: 0.163888
[14:18:48.664] iteration 27493: total_loss: 0.395182, loss_sup: 0.016093, loss_mps: 0.133053, loss_cps: 0.246036
[14:18:48.810] iteration 27494: total_loss: 0.339303, loss_sup: 0.079699, loss_mps: 0.092965, loss_cps: 0.166639
[14:18:48.955] iteration 27495: total_loss: 0.432180, loss_sup: 0.058455, loss_mps: 0.120825, loss_cps: 0.252900
[14:18:49.101] iteration 27496: total_loss: 0.235310, loss_sup: 0.006302, loss_mps: 0.080785, loss_cps: 0.148223
[14:18:49.246] iteration 27497: total_loss: 0.325521, loss_sup: 0.072537, loss_mps: 0.092226, loss_cps: 0.160759
[14:18:49.393] iteration 27498: total_loss: 0.178951, loss_sup: 0.012378, loss_mps: 0.065532, loss_cps: 0.101041
[14:18:49.539] iteration 27499: total_loss: 0.277319, loss_sup: 0.015648, loss_mps: 0.093595, loss_cps: 0.168076
[14:18:49.685] iteration 27500: total_loss: 0.243296, loss_sup: 0.031299, loss_mps: 0.078293, loss_cps: 0.133704
[14:18:49.685] Evaluation Started ==>
[14:19:00.991] ==> valid iteration 27500: unet metrics: {'dc': 0.6691869066655775, 'jc': 0.5546533076653554, 'pre': 0.8071650093820464, 'hd': 5.337653304431229}, ynet metrics: {'dc': 0.6106287679918857, 'jc': 0.50127397057472, 'pre': 0.7962989144032755, 'hd': 5.335889971651205}.
[14:19:00.993] Evaluation Finished!⏹️
[14:19:01.148] iteration 27501: total_loss: 0.247565, loss_sup: 0.024287, loss_mps: 0.079906, loss_cps: 0.143371
[14:19:01.296] iteration 27502: total_loss: 0.463147, loss_sup: 0.245619, loss_mps: 0.080181, loss_cps: 0.137347
[14:19:01.442] iteration 27503: total_loss: 0.305135, loss_sup: 0.023275, loss_mps: 0.093998, loss_cps: 0.187862
[14:19:01.587] iteration 27504: total_loss: 0.563118, loss_sup: 0.234850, loss_mps: 0.112483, loss_cps: 0.215784
[14:19:01.734] iteration 27505: total_loss: 0.231190, loss_sup: 0.050204, loss_mps: 0.066926, loss_cps: 0.114060
[14:19:01.880] iteration 27506: total_loss: 0.322107, loss_sup: 0.019863, loss_mps: 0.108554, loss_cps: 0.193690
[14:19:02.026] iteration 27507: total_loss: 0.299152, loss_sup: 0.086390, loss_mps: 0.075424, loss_cps: 0.137338
[14:19:02.172] iteration 27508: total_loss: 0.467126, loss_sup: 0.043250, loss_mps: 0.148946, loss_cps: 0.274930
[14:19:02.318] iteration 27509: total_loss: 0.427177, loss_sup: 0.065378, loss_mps: 0.120763, loss_cps: 0.241036
[14:19:02.463] iteration 27510: total_loss: 0.211079, loss_sup: 0.002446, loss_mps: 0.074609, loss_cps: 0.134024
[14:19:02.608] iteration 27511: total_loss: 0.250984, loss_sup: 0.014419, loss_mps: 0.084684, loss_cps: 0.151881
[14:19:02.761] iteration 27512: total_loss: 0.374758, loss_sup: 0.054877, loss_mps: 0.112614, loss_cps: 0.207266
[14:19:02.907] iteration 27513: total_loss: 0.313508, loss_sup: 0.030426, loss_mps: 0.102032, loss_cps: 0.181050
[14:19:03.053] iteration 27514: total_loss: 0.261949, loss_sup: 0.030373, loss_mps: 0.081938, loss_cps: 0.149638
[14:19:03.200] iteration 27515: total_loss: 0.561432, loss_sup: 0.024363, loss_mps: 0.169943, loss_cps: 0.367126
[14:19:03.345] iteration 27516: total_loss: 0.195605, loss_sup: 0.010338, loss_mps: 0.068338, loss_cps: 0.116929
[14:19:03.491] iteration 27517: total_loss: 0.258018, loss_sup: 0.034202, loss_mps: 0.078084, loss_cps: 0.145732
[14:19:03.638] iteration 27518: total_loss: 0.368264, loss_sup: 0.035354, loss_mps: 0.113325, loss_cps: 0.219584
[14:19:03.785] iteration 27519: total_loss: 0.175430, loss_sup: 0.010212, loss_mps: 0.060696, loss_cps: 0.104522
[14:19:03.936] iteration 27520: total_loss: 0.311749, loss_sup: 0.042487, loss_mps: 0.095214, loss_cps: 0.174049
[14:19:04.082] iteration 27521: total_loss: 0.255006, loss_sup: 0.017663, loss_mps: 0.087436, loss_cps: 0.149908
[14:19:04.232] iteration 27522: total_loss: 0.219515, loss_sup: 0.006996, loss_mps: 0.078164, loss_cps: 0.134355
[14:19:04.379] iteration 27523: total_loss: 0.456344, loss_sup: 0.091570, loss_mps: 0.130502, loss_cps: 0.234272
[14:19:04.528] iteration 27524: total_loss: 0.425305, loss_sup: 0.092159, loss_mps: 0.117514, loss_cps: 0.215632
[14:19:04.674] iteration 27525: total_loss: 0.284920, loss_sup: 0.023937, loss_mps: 0.092164, loss_cps: 0.168820
[14:19:04.819] iteration 27526: total_loss: 0.699926, loss_sup: 0.183528, loss_mps: 0.168415, loss_cps: 0.347982
[14:19:04.965] iteration 27527: total_loss: 0.336165, loss_sup: 0.014583, loss_mps: 0.105773, loss_cps: 0.215808
[14:19:05.112] iteration 27528: total_loss: 0.178356, loss_sup: 0.010699, loss_mps: 0.062077, loss_cps: 0.105581
[14:19:05.258] iteration 27529: total_loss: 0.429072, loss_sup: 0.097076, loss_mps: 0.111045, loss_cps: 0.220951
[14:19:05.405] iteration 27530: total_loss: 0.378229, loss_sup: 0.086643, loss_mps: 0.101040, loss_cps: 0.190546
[14:19:05.551] iteration 27531: total_loss: 0.290631, loss_sup: 0.001722, loss_mps: 0.100412, loss_cps: 0.188497
[14:19:05.697] iteration 27532: total_loss: 0.289456, loss_sup: 0.032576, loss_mps: 0.094653, loss_cps: 0.162227
[14:19:05.843] iteration 27533: total_loss: 0.246760, loss_sup: 0.000316, loss_mps: 0.084991, loss_cps: 0.161453
[14:19:05.989] iteration 27534: total_loss: 0.452272, loss_sup: 0.024739, loss_mps: 0.147147, loss_cps: 0.280387
[14:19:06.134] iteration 27535: total_loss: 0.347705, loss_sup: 0.019095, loss_mps: 0.107872, loss_cps: 0.220739
[14:19:06.280] iteration 27536: total_loss: 0.762516, loss_sup: 0.082714, loss_mps: 0.217084, loss_cps: 0.462717
[14:19:06.425] iteration 27537: total_loss: 0.235101, loss_sup: 0.034338, loss_mps: 0.071360, loss_cps: 0.129403
[14:19:06.571] iteration 27538: total_loss: 0.241533, loss_sup: 0.007585, loss_mps: 0.077865, loss_cps: 0.156083
[14:19:06.718] iteration 27539: total_loss: 0.449453, loss_sup: 0.109978, loss_mps: 0.119669, loss_cps: 0.219806
[14:19:06.864] iteration 27540: total_loss: 0.550118, loss_sup: 0.002384, loss_mps: 0.171698, loss_cps: 0.376036
[14:19:07.014] iteration 27541: total_loss: 0.259439, loss_sup: 0.075253, loss_mps: 0.068181, loss_cps: 0.116004
[14:19:07.160] iteration 27542: total_loss: 0.511862, loss_sup: 0.027541, loss_mps: 0.158634, loss_cps: 0.325687
[14:19:07.306] iteration 27543: total_loss: 0.241961, loss_sup: 0.003248, loss_mps: 0.086992, loss_cps: 0.151720
[14:19:07.452] iteration 27544: total_loss: 0.312164, loss_sup: 0.099036, loss_mps: 0.072463, loss_cps: 0.140665
[14:19:07.599] iteration 27545: total_loss: 0.752619, loss_sup: 0.194863, loss_mps: 0.189557, loss_cps: 0.368200
[14:19:07.744] iteration 27546: total_loss: 0.317232, loss_sup: 0.058781, loss_mps: 0.094229, loss_cps: 0.164222
[14:19:07.891] iteration 27547: total_loss: 0.388136, loss_sup: 0.125250, loss_mps: 0.091474, loss_cps: 0.171412
[14:19:08.039] iteration 27548: total_loss: 0.446911, loss_sup: 0.051493, loss_mps: 0.133342, loss_cps: 0.262076
[14:19:08.185] iteration 27549: total_loss: 0.197796, loss_sup: 0.020789, loss_mps: 0.066349, loss_cps: 0.110658
[14:19:08.332] iteration 27550: total_loss: 0.310708, loss_sup: 0.030403, loss_mps: 0.101608, loss_cps: 0.178698
[14:19:08.479] iteration 27551: total_loss: 0.275769, loss_sup: 0.032031, loss_mps: 0.088340, loss_cps: 0.155398
[14:19:08.625] iteration 27552: total_loss: 0.249405, loss_sup: 0.062185, loss_mps: 0.071978, loss_cps: 0.115242
[14:19:08.772] iteration 27553: total_loss: 0.265715, loss_sup: 0.033083, loss_mps: 0.082319, loss_cps: 0.150313
[14:19:08.920] iteration 27554: total_loss: 0.405578, loss_sup: 0.028538, loss_mps: 0.125083, loss_cps: 0.251956
[14:19:09.066] iteration 27555: total_loss: 0.273811, loss_sup: 0.053133, loss_mps: 0.078095, loss_cps: 0.142583
[14:19:09.212] iteration 27556: total_loss: 0.495863, loss_sup: 0.239473, loss_mps: 0.101166, loss_cps: 0.155225
[14:19:09.360] iteration 27557: total_loss: 0.518153, loss_sup: 0.110899, loss_mps: 0.130962, loss_cps: 0.276291
[14:19:09.506] iteration 27558: total_loss: 0.313454, loss_sup: 0.006378, loss_mps: 0.101911, loss_cps: 0.205165
[14:19:09.653] iteration 27559: total_loss: 0.192628, loss_sup: 0.012491, loss_mps: 0.067254, loss_cps: 0.112883
[14:19:09.799] iteration 27560: total_loss: 0.236815, loss_sup: 0.055481, loss_mps: 0.071197, loss_cps: 0.110137
[14:19:09.945] iteration 27561: total_loss: 0.218480, loss_sup: 0.008176, loss_mps: 0.076146, loss_cps: 0.134158
[14:19:10.091] iteration 27562: total_loss: 0.330569, loss_sup: 0.047073, loss_mps: 0.101642, loss_cps: 0.181854
[14:19:10.238] iteration 27563: total_loss: 0.279540, loss_sup: 0.015466, loss_mps: 0.091020, loss_cps: 0.173054
[14:19:10.385] iteration 27564: total_loss: 0.507642, loss_sup: 0.091048, loss_mps: 0.138957, loss_cps: 0.277636
[14:19:10.532] iteration 27565: total_loss: 0.238329, loss_sup: 0.019917, loss_mps: 0.082031, loss_cps: 0.136381
[14:19:10.678] iteration 27566: total_loss: 0.155584, loss_sup: 0.008666, loss_mps: 0.055914, loss_cps: 0.091004
[14:19:10.824] iteration 27567: total_loss: 0.292247, loss_sup: 0.007182, loss_mps: 0.099290, loss_cps: 0.185774
[14:19:10.970] iteration 27568: total_loss: 0.250689, loss_sup: 0.039058, loss_mps: 0.077042, loss_cps: 0.134590
[14:19:11.116] iteration 27569: total_loss: 0.235973, loss_sup: 0.006725, loss_mps: 0.086264, loss_cps: 0.142984
[14:19:11.262] iteration 27570: total_loss: 0.361939, loss_sup: 0.009738, loss_mps: 0.129250, loss_cps: 0.222951
[14:19:11.408] iteration 27571: total_loss: 0.496032, loss_sup: 0.089618, loss_mps: 0.141309, loss_cps: 0.265105
[14:19:11.554] iteration 27572: total_loss: 0.200520, loss_sup: 0.001653, loss_mps: 0.071542, loss_cps: 0.127325
[14:19:11.701] iteration 27573: total_loss: 0.271354, loss_sup: 0.008026, loss_mps: 0.093793, loss_cps: 0.169535
[14:19:11.847] iteration 27574: total_loss: 0.192125, loss_sup: 0.013064, loss_mps: 0.066732, loss_cps: 0.112329
[14:19:11.993] iteration 27575: total_loss: 0.243358, loss_sup: 0.015152, loss_mps: 0.080898, loss_cps: 0.147308
[14:19:12.138] iteration 27576: total_loss: 0.162400, loss_sup: 0.002507, loss_mps: 0.061316, loss_cps: 0.098577
[14:19:12.284] iteration 27577: total_loss: 0.303539, loss_sup: 0.035688, loss_mps: 0.092089, loss_cps: 0.175762
[14:19:12.430] iteration 27578: total_loss: 0.478199, loss_sup: 0.207827, loss_mps: 0.093550, loss_cps: 0.176823
[14:19:12.575] iteration 27579: total_loss: 0.321156, loss_sup: 0.007691, loss_mps: 0.122961, loss_cps: 0.190504
[14:19:12.721] iteration 27580: total_loss: 0.156597, loss_sup: 0.005132, loss_mps: 0.056429, loss_cps: 0.095037
[14:19:12.867] iteration 27581: total_loss: 0.391221, loss_sup: 0.042608, loss_mps: 0.118483, loss_cps: 0.230130
[14:19:13.014] iteration 27582: total_loss: 0.151929, loss_sup: 0.019668, loss_mps: 0.052369, loss_cps: 0.079892
[14:19:13.159] iteration 27583: total_loss: 0.377342, loss_sup: 0.036766, loss_mps: 0.121487, loss_cps: 0.219089
[14:19:13.305] iteration 27584: total_loss: 0.270019, loss_sup: 0.006698, loss_mps: 0.089729, loss_cps: 0.173592
[14:19:13.451] iteration 27585: total_loss: 0.325075, loss_sup: 0.037653, loss_mps: 0.098512, loss_cps: 0.188910
[14:19:13.597] iteration 27586: total_loss: 0.429037, loss_sup: 0.206157, loss_mps: 0.082377, loss_cps: 0.140503
[14:19:13.742] iteration 27587: total_loss: 0.292042, loss_sup: 0.045085, loss_mps: 0.086523, loss_cps: 0.160434
[14:19:13.803] iteration 27588: total_loss: 0.276946, loss_sup: 0.023893, loss_mps: 0.098574, loss_cps: 0.154480
[14:19:15.082] iteration 27589: total_loss: 0.447013, loss_sup: 0.022058, loss_mps: 0.138618, loss_cps: 0.286338
[14:19:15.235] iteration 27590: total_loss: 0.497880, loss_sup: 0.038619, loss_mps: 0.145807, loss_cps: 0.313455
[14:19:15.383] iteration 27591: total_loss: 0.561146, loss_sup: 0.016556, loss_mps: 0.181763, loss_cps: 0.362827
[14:19:15.530] iteration 27592: total_loss: 0.204724, loss_sup: 0.009507, loss_mps: 0.075943, loss_cps: 0.119274
[14:19:15.680] iteration 27593: total_loss: 0.337646, loss_sup: 0.125276, loss_mps: 0.073842, loss_cps: 0.138529
[14:19:15.826] iteration 27594: total_loss: 0.326666, loss_sup: 0.076963, loss_mps: 0.093166, loss_cps: 0.156537
[14:19:15.986] iteration 27595: total_loss: 0.373988, loss_sup: 0.117216, loss_mps: 0.088792, loss_cps: 0.167980
[14:19:16.140] iteration 27596: total_loss: 0.193567, loss_sup: 0.018491, loss_mps: 0.064302, loss_cps: 0.110774
[14:19:16.287] iteration 27597: total_loss: 0.468058, loss_sup: 0.061807, loss_mps: 0.143161, loss_cps: 0.263091
[14:19:16.434] iteration 27598: total_loss: 0.499077, loss_sup: 0.049415, loss_mps: 0.138755, loss_cps: 0.310907
[14:19:16.580] iteration 27599: total_loss: 0.253941, loss_sup: 0.013555, loss_mps: 0.086482, loss_cps: 0.153905
[14:19:16.728] iteration 27600: total_loss: 0.246139, loss_sup: 0.003920, loss_mps: 0.086230, loss_cps: 0.155989
[14:19:16.728] Evaluation Started ==>
[14:19:28.120] ==> valid iteration 27600: unet metrics: {'dc': 0.6696137107706, 'jc': 0.5546067585644777, 'pre': 0.8043698455452168, 'hd': 5.382490305775306}, ynet metrics: {'dc': 0.6120962595981395, 'jc': 0.5016370359509003, 'pre': 0.7959245191690467, 'hd': 5.340066816573777}.
[14:19:28.123] Evaluation Finished!⏹️
[14:19:28.273] iteration 27601: total_loss: 0.249048, loss_sup: 0.004566, loss_mps: 0.086870, loss_cps: 0.157612
[14:19:28.421] iteration 27602: total_loss: 0.357189, loss_sup: 0.109492, loss_mps: 0.085519, loss_cps: 0.162178
[14:19:28.567] iteration 27603: total_loss: 0.253381, loss_sup: 0.007498, loss_mps: 0.088473, loss_cps: 0.157410
[14:19:28.716] iteration 27604: total_loss: 0.267193, loss_sup: 0.013478, loss_mps: 0.089046, loss_cps: 0.164670
[14:19:28.862] iteration 27605: total_loss: 0.634427, loss_sup: 0.328354, loss_mps: 0.104729, loss_cps: 0.201344
[14:19:29.007] iteration 27606: total_loss: 0.302211, loss_sup: 0.041493, loss_mps: 0.090190, loss_cps: 0.170528
[14:19:29.153] iteration 27607: total_loss: 0.266576, loss_sup: 0.001772, loss_mps: 0.089062, loss_cps: 0.175741
[14:19:29.299] iteration 27608: total_loss: 0.334405, loss_sup: 0.014617, loss_mps: 0.110657, loss_cps: 0.209131
[14:19:29.445] iteration 27609: total_loss: 0.561472, loss_sup: 0.038548, loss_mps: 0.167307, loss_cps: 0.355618
[14:19:29.592] iteration 27610: total_loss: 0.314999, loss_sup: 0.002716, loss_mps: 0.104940, loss_cps: 0.207344
[14:19:29.738] iteration 27611: total_loss: 0.547586, loss_sup: 0.046653, loss_mps: 0.166520, loss_cps: 0.334413
[14:19:29.883] iteration 27612: total_loss: 0.187480, loss_sup: 0.006569, loss_mps: 0.065686, loss_cps: 0.115225
[14:19:30.029] iteration 27613: total_loss: 0.224682, loss_sup: 0.027171, loss_mps: 0.069161, loss_cps: 0.128350
[14:19:30.175] iteration 27614: total_loss: 0.797533, loss_sup: 0.188111, loss_mps: 0.202350, loss_cps: 0.407072
[14:19:30.321] iteration 27615: total_loss: 0.381766, loss_sup: 0.133691, loss_mps: 0.087091, loss_cps: 0.160984
[14:19:30.467] iteration 27616: total_loss: 0.243831, loss_sup: 0.068728, loss_mps: 0.065421, loss_cps: 0.109682
[14:19:30.617] iteration 27617: total_loss: 0.325353, loss_sup: 0.084114, loss_mps: 0.089801, loss_cps: 0.151437
[14:19:30.765] iteration 27618: total_loss: 0.368641, loss_sup: 0.004620, loss_mps: 0.124418, loss_cps: 0.239603
[14:19:30.911] iteration 27619: total_loss: 0.280925, loss_sup: 0.024331, loss_mps: 0.093449, loss_cps: 0.163145
[14:19:31.057] iteration 27620: total_loss: 0.456454, loss_sup: 0.004704, loss_mps: 0.141482, loss_cps: 0.310268
[14:19:31.203] iteration 27621: total_loss: 0.416687, loss_sup: 0.010945, loss_mps: 0.132319, loss_cps: 0.273422
[14:19:31.350] iteration 27622: total_loss: 0.502177, loss_sup: 0.030596, loss_mps: 0.145121, loss_cps: 0.326460
[14:19:31.495] iteration 27623: total_loss: 0.293375, loss_sup: 0.030793, loss_mps: 0.098339, loss_cps: 0.164243
[14:19:31.641] iteration 27624: total_loss: 0.242067, loss_sup: 0.026516, loss_mps: 0.078583, loss_cps: 0.136968
[14:19:31.788] iteration 27625: total_loss: 0.402432, loss_sup: 0.085005, loss_mps: 0.111894, loss_cps: 0.205532
[14:19:31.934] iteration 27626: total_loss: 0.421209, loss_sup: 0.048310, loss_mps: 0.114870, loss_cps: 0.258030
[14:19:32.080] iteration 27627: total_loss: 0.205581, loss_sup: 0.030149, loss_mps: 0.062656, loss_cps: 0.112776
[14:19:32.226] iteration 27628: total_loss: 0.383624, loss_sup: 0.042443, loss_mps: 0.121092, loss_cps: 0.220089
[14:19:32.372] iteration 27629: total_loss: 0.408901, loss_sup: 0.045950, loss_mps: 0.121415, loss_cps: 0.241536
[14:19:32.519] iteration 27630: total_loss: 0.297582, loss_sup: 0.025821, loss_mps: 0.098057, loss_cps: 0.173704
[14:19:32.666] iteration 27631: total_loss: 0.331421, loss_sup: 0.114500, loss_mps: 0.077251, loss_cps: 0.139669
[14:19:32.812] iteration 27632: total_loss: 0.267795, loss_sup: 0.039285, loss_mps: 0.082192, loss_cps: 0.146318
[14:19:32.958] iteration 27633: total_loss: 0.443566, loss_sup: 0.098502, loss_mps: 0.109735, loss_cps: 0.235330
[14:19:33.104] iteration 27634: total_loss: 0.174761, loss_sup: 0.004180, loss_mps: 0.064554, loss_cps: 0.106026
[14:19:33.250] iteration 27635: total_loss: 0.449343, loss_sup: 0.068571, loss_mps: 0.122185, loss_cps: 0.258587
[14:19:33.396] iteration 27636: total_loss: 0.449223, loss_sup: 0.047364, loss_mps: 0.134665, loss_cps: 0.267194
[14:19:33.543] iteration 27637: total_loss: 0.358659, loss_sup: 0.060128, loss_mps: 0.112325, loss_cps: 0.186206
[14:19:33.689] iteration 27638: total_loss: 0.289874, loss_sup: 0.016065, loss_mps: 0.096732, loss_cps: 0.177077
[14:19:33.836] iteration 27639: total_loss: 0.492247, loss_sup: 0.173455, loss_mps: 0.103742, loss_cps: 0.215050
[14:19:33.982] iteration 27640: total_loss: 0.230559, loss_sup: 0.007260, loss_mps: 0.077814, loss_cps: 0.145485
[14:19:34.128] iteration 27641: total_loss: 0.346098, loss_sup: 0.095603, loss_mps: 0.090600, loss_cps: 0.159895
[14:19:34.274] iteration 27642: total_loss: 0.422030, loss_sup: 0.085155, loss_mps: 0.114457, loss_cps: 0.222418
[14:19:34.420] iteration 27643: total_loss: 0.520569, loss_sup: 0.032034, loss_mps: 0.157592, loss_cps: 0.330943
[14:19:34.565] iteration 27644: total_loss: 0.249741, loss_sup: 0.010647, loss_mps: 0.085001, loss_cps: 0.154093
[14:19:34.711] iteration 27645: total_loss: 0.223373, loss_sup: 0.047724, loss_mps: 0.065091, loss_cps: 0.110558
[14:19:34.857] iteration 27646: total_loss: 0.386600, loss_sup: 0.212843, loss_mps: 0.065593, loss_cps: 0.108164
[14:19:35.003] iteration 27647: total_loss: 0.338954, loss_sup: 0.040626, loss_mps: 0.104643, loss_cps: 0.193684
[14:19:35.152] iteration 27648: total_loss: 0.364618, loss_sup: 0.046514, loss_mps: 0.096385, loss_cps: 0.221719
[14:19:35.301] iteration 27649: total_loss: 0.252253, loss_sup: 0.047953, loss_mps: 0.070276, loss_cps: 0.134024
[14:19:35.447] iteration 27650: total_loss: 0.196334, loss_sup: 0.000612, loss_mps: 0.070867, loss_cps: 0.124854
[14:19:35.593] iteration 27651: total_loss: 0.298286, loss_sup: 0.036146, loss_mps: 0.090198, loss_cps: 0.171941
[14:19:35.738] iteration 27652: total_loss: 0.285633, loss_sup: 0.014537, loss_mps: 0.091413, loss_cps: 0.179683
[14:19:35.887] iteration 27653: total_loss: 0.442689, loss_sup: 0.091306, loss_mps: 0.129316, loss_cps: 0.222066
[14:19:36.033] iteration 27654: total_loss: 0.370561, loss_sup: 0.005423, loss_mps: 0.117582, loss_cps: 0.247556
[14:19:36.178] iteration 27655: total_loss: 0.421326, loss_sup: 0.073149, loss_mps: 0.118504, loss_cps: 0.229673
[14:19:36.324] iteration 27656: total_loss: 0.305873, loss_sup: 0.017458, loss_mps: 0.098750, loss_cps: 0.189665
[14:19:36.470] iteration 27657: total_loss: 0.350384, loss_sup: 0.143317, loss_mps: 0.076968, loss_cps: 0.130099
[14:19:36.615] iteration 27658: total_loss: 0.289060, loss_sup: 0.010145, loss_mps: 0.098815, loss_cps: 0.180101
[14:19:36.761] iteration 27659: total_loss: 0.287947, loss_sup: 0.047857, loss_mps: 0.086631, loss_cps: 0.153459
[14:19:36.907] iteration 27660: total_loss: 0.210280, loss_sup: 0.014805, loss_mps: 0.072044, loss_cps: 0.123431
[14:19:37.052] iteration 27661: total_loss: 0.254679, loss_sup: 0.015071, loss_mps: 0.085650, loss_cps: 0.153957
[14:19:37.198] iteration 27662: total_loss: 0.378086, loss_sup: 0.105016, loss_mps: 0.095324, loss_cps: 0.177746
[14:19:37.344] iteration 27663: total_loss: 0.370844, loss_sup: 0.110697, loss_mps: 0.092210, loss_cps: 0.167938
[14:19:37.489] iteration 27664: total_loss: 0.433116, loss_sup: 0.010417, loss_mps: 0.133853, loss_cps: 0.288845
[14:19:37.636] iteration 27665: total_loss: 0.243151, loss_sup: 0.000744, loss_mps: 0.083164, loss_cps: 0.159243
[14:19:37.782] iteration 27666: total_loss: 0.544804, loss_sup: 0.101404, loss_mps: 0.146036, loss_cps: 0.297364
[14:19:37.927] iteration 27667: total_loss: 0.351701, loss_sup: 0.034902, loss_mps: 0.109297, loss_cps: 0.207502
[14:19:38.072] iteration 27668: total_loss: 0.649584, loss_sup: 0.192382, loss_mps: 0.152848, loss_cps: 0.304355
[14:19:38.220] iteration 27669: total_loss: 0.373127, loss_sup: 0.008346, loss_mps: 0.122458, loss_cps: 0.242323
[14:19:38.366] iteration 27670: total_loss: 0.308019, loss_sup: 0.063081, loss_mps: 0.092479, loss_cps: 0.152459
[14:19:38.511] iteration 27671: total_loss: 0.444846, loss_sup: 0.143328, loss_mps: 0.114184, loss_cps: 0.187334
[14:19:38.657] iteration 27672: total_loss: 0.417447, loss_sup: 0.038156, loss_mps: 0.126408, loss_cps: 0.252883
[14:19:38.803] iteration 27673: total_loss: 0.309386, loss_sup: 0.064229, loss_mps: 0.090992, loss_cps: 0.154165
[14:19:38.950] iteration 27674: total_loss: 0.311561, loss_sup: 0.059026, loss_mps: 0.093156, loss_cps: 0.159379
[14:19:39.096] iteration 27675: total_loss: 0.302662, loss_sup: 0.008473, loss_mps: 0.101317, loss_cps: 0.192872
[14:19:39.243] iteration 27676: total_loss: 0.194805, loss_sup: 0.023076, loss_mps: 0.065318, loss_cps: 0.106411
[14:19:39.390] iteration 27677: total_loss: 0.220597, loss_sup: 0.006939, loss_mps: 0.081182, loss_cps: 0.132476
[14:19:39.536] iteration 27678: total_loss: 0.287731, loss_sup: 0.018301, loss_mps: 0.100581, loss_cps: 0.168849
[14:19:39.683] iteration 27679: total_loss: 0.544675, loss_sup: 0.015491, loss_mps: 0.170607, loss_cps: 0.358577
[14:19:39.830] iteration 27680: total_loss: 0.227298, loss_sup: 0.026718, loss_mps: 0.074413, loss_cps: 0.126167
[14:19:39.978] iteration 27681: total_loss: 0.276787, loss_sup: 0.029009, loss_mps: 0.081276, loss_cps: 0.166501
[14:19:40.124] iteration 27682: total_loss: 0.522274, loss_sup: 0.073518, loss_mps: 0.139868, loss_cps: 0.308889
[14:19:40.270] iteration 27683: total_loss: 0.412611, loss_sup: 0.083638, loss_mps: 0.122474, loss_cps: 0.206499
[14:19:40.415] iteration 27684: total_loss: 0.365948, loss_sup: 0.076385, loss_mps: 0.104836, loss_cps: 0.184727
[14:19:40.562] iteration 27685: total_loss: 0.412773, loss_sup: 0.117894, loss_mps: 0.105274, loss_cps: 0.189604
[14:19:40.708] iteration 27686: total_loss: 0.295831, loss_sup: 0.033072, loss_mps: 0.095638, loss_cps: 0.167120
[14:19:40.854] iteration 27687: total_loss: 0.363336, loss_sup: 0.012623, loss_mps: 0.121222, loss_cps: 0.229491
[14:19:41.001] iteration 27688: total_loss: 0.297531, loss_sup: 0.046263, loss_mps: 0.091096, loss_cps: 0.160172
[14:19:41.147] iteration 27689: total_loss: 0.241063, loss_sup: 0.022461, loss_mps: 0.078857, loss_cps: 0.139745
[14:19:41.293] iteration 27690: total_loss: 0.723186, loss_sup: 0.032777, loss_mps: 0.219004, loss_cps: 0.471405
[14:19:41.438] iteration 27691: total_loss: 0.477004, loss_sup: 0.057472, loss_mps: 0.142779, loss_cps: 0.276752
[14:19:41.584] iteration 27692: total_loss: 0.546001, loss_sup: 0.063828, loss_mps: 0.157391, loss_cps: 0.324782
[14:19:41.732] iteration 27693: total_loss: 0.184158, loss_sup: 0.016041, loss_mps: 0.064646, loss_cps: 0.103471
[14:19:41.880] iteration 27694: total_loss: 0.296541, loss_sup: 0.025688, loss_mps: 0.098799, loss_cps: 0.172053
[14:19:42.026] iteration 27695: total_loss: 0.276318, loss_sup: 0.034321, loss_mps: 0.084594, loss_cps: 0.157404
[14:19:42.172] iteration 27696: total_loss: 0.315599, loss_sup: 0.052778, loss_mps: 0.090358, loss_cps: 0.172463
[14:19:42.318] iteration 27697: total_loss: 0.408375, loss_sup: 0.057186, loss_mps: 0.132257, loss_cps: 0.218933
[14:19:42.463] iteration 27698: total_loss: 0.484051, loss_sup: 0.163380, loss_mps: 0.112640, loss_cps: 0.208030
[14:19:42.610] iteration 27699: total_loss: 0.276542, loss_sup: 0.005647, loss_mps: 0.099766, loss_cps: 0.171128
[14:19:42.755] iteration 27700: total_loss: 0.333682, loss_sup: 0.039293, loss_mps: 0.105269, loss_cps: 0.189120
[14:19:42.756] Evaluation Started ==>
[14:19:54.118] ==> valid iteration 27700: unet metrics: {'dc': 0.6646392621519462, 'jc': 0.5500244894808315, 'pre': 0.8032788083159093, 'hd': 5.330984936829957}, ynet metrics: {'dc': 0.6097842319506691, 'jc': 0.49842627459952854, 'pre': 0.805573192559301, 'hd': 5.395421575970583}.
[14:19:54.119] Evaluation Finished!⏹️
[14:19:54.271] iteration 27701: total_loss: 0.251758, loss_sup: 0.017740, loss_mps: 0.083866, loss_cps: 0.150152
[14:19:54.421] iteration 27702: total_loss: 0.182368, loss_sup: 0.026463, loss_mps: 0.059662, loss_cps: 0.096243
[14:19:54.566] iteration 27703: total_loss: 0.225225, loss_sup: 0.032634, loss_mps: 0.073751, loss_cps: 0.118841
[14:19:54.711] iteration 27704: total_loss: 0.254437, loss_sup: 0.059373, loss_mps: 0.071178, loss_cps: 0.123886
[14:19:54.856] iteration 27705: total_loss: 0.420568, loss_sup: 0.213461, loss_mps: 0.079444, loss_cps: 0.127663
[14:19:55.001] iteration 27706: total_loss: 0.333784, loss_sup: 0.015476, loss_mps: 0.107024, loss_cps: 0.211284
[14:19:55.146] iteration 27707: total_loss: 0.322893, loss_sup: 0.011938, loss_mps: 0.102378, loss_cps: 0.208576
[14:19:55.291] iteration 27708: total_loss: 0.149487, loss_sup: 0.002427, loss_mps: 0.055667, loss_cps: 0.091393
[14:19:55.436] iteration 27709: total_loss: 0.578016, loss_sup: 0.180205, loss_mps: 0.132077, loss_cps: 0.265734
[14:19:55.582] iteration 27710: total_loss: 0.369904, loss_sup: 0.007779, loss_mps: 0.114044, loss_cps: 0.248081
[14:19:55.727] iteration 27711: total_loss: 0.228036, loss_sup: 0.080257, loss_mps: 0.055961, loss_cps: 0.091818
[14:19:55.872] iteration 27712: total_loss: 0.379042, loss_sup: 0.033510, loss_mps: 0.117546, loss_cps: 0.227986
[14:19:56.017] iteration 27713: total_loss: 0.282645, loss_sup: 0.046869, loss_mps: 0.087817, loss_cps: 0.147959
[14:19:56.164] iteration 27714: total_loss: 0.232032, loss_sup: 0.031064, loss_mps: 0.075565, loss_cps: 0.125403
[14:19:56.311] iteration 27715: total_loss: 0.193110, loss_sup: 0.011502, loss_mps: 0.065919, loss_cps: 0.115689
[14:19:56.457] iteration 27716: total_loss: 0.270903, loss_sup: 0.004452, loss_mps: 0.093264, loss_cps: 0.173188
[14:19:56.603] iteration 27717: total_loss: 0.248005, loss_sup: 0.021863, loss_mps: 0.082489, loss_cps: 0.143652
[14:19:56.748] iteration 27718: total_loss: 0.456872, loss_sup: 0.216925, loss_mps: 0.085821, loss_cps: 0.154126
[14:19:56.895] iteration 27719: total_loss: 0.202750, loss_sup: 0.004905, loss_mps: 0.070496, loss_cps: 0.127348
[14:19:57.043] iteration 27720: total_loss: 0.246870, loss_sup: 0.066187, loss_mps: 0.067711, loss_cps: 0.112972
[14:19:57.189] iteration 27721: total_loss: 0.534373, loss_sup: 0.026268, loss_mps: 0.162608, loss_cps: 0.345497
[14:19:57.335] iteration 27722: total_loss: 0.752697, loss_sup: 0.215819, loss_mps: 0.176040, loss_cps: 0.360838
[14:19:57.481] iteration 27723: total_loss: 0.307812, loss_sup: 0.068897, loss_mps: 0.081485, loss_cps: 0.157430
[14:19:57.629] iteration 27724: total_loss: 0.187874, loss_sup: 0.041700, loss_mps: 0.057784, loss_cps: 0.088391
[14:19:57.775] iteration 27725: total_loss: 0.387804, loss_sup: 0.042817, loss_mps: 0.116259, loss_cps: 0.228728
[14:19:57.921] iteration 27726: total_loss: 0.237865, loss_sup: 0.043203, loss_mps: 0.075391, loss_cps: 0.119272
[14:19:58.067] iteration 27727: total_loss: 0.345100, loss_sup: 0.036993, loss_mps: 0.105406, loss_cps: 0.202701
[14:19:58.212] iteration 27728: total_loss: 0.210926, loss_sup: 0.028948, loss_mps: 0.069848, loss_cps: 0.112130
[14:19:58.359] iteration 27729: total_loss: 0.210873, loss_sup: 0.001136, loss_mps: 0.074864, loss_cps: 0.134873
[14:19:58.505] iteration 27730: total_loss: 0.479301, loss_sup: 0.002126, loss_mps: 0.161010, loss_cps: 0.316165
[14:19:58.651] iteration 27731: total_loss: 0.301744, loss_sup: 0.014603, loss_mps: 0.101368, loss_cps: 0.185773
[14:19:58.798] iteration 27732: total_loss: 0.289241, loss_sup: 0.018427, loss_mps: 0.092675, loss_cps: 0.178138
[14:19:58.944] iteration 27733: total_loss: 0.352755, loss_sup: 0.033902, loss_mps: 0.109581, loss_cps: 0.209272
[14:19:59.090] iteration 27734: total_loss: 0.237329, loss_sup: 0.063802, loss_mps: 0.064594, loss_cps: 0.108932
[14:19:59.237] iteration 27735: total_loss: 0.172171, loss_sup: 0.013989, loss_mps: 0.059217, loss_cps: 0.098964
[14:19:59.383] iteration 27736: total_loss: 0.420560, loss_sup: 0.059145, loss_mps: 0.123180, loss_cps: 0.238236
[14:19:59.528] iteration 27737: total_loss: 0.262263, loss_sup: 0.017403, loss_mps: 0.087429, loss_cps: 0.157431
[14:19:59.674] iteration 27738: total_loss: 0.345230, loss_sup: 0.028816, loss_mps: 0.104348, loss_cps: 0.212066
[14:19:59.821] iteration 27739: total_loss: 0.258648, loss_sup: 0.016344, loss_mps: 0.090287, loss_cps: 0.152017
[14:19:59.968] iteration 27740: total_loss: 0.244054, loss_sup: 0.030168, loss_mps: 0.076645, loss_cps: 0.137241
[14:20:00.114] iteration 27741: total_loss: 0.455002, loss_sup: 0.103449, loss_mps: 0.123092, loss_cps: 0.228460
[14:20:00.260] iteration 27742: total_loss: 0.754453, loss_sup: 0.007356, loss_mps: 0.220415, loss_cps: 0.526682
[14:20:00.410] iteration 27743: total_loss: 0.243380, loss_sup: 0.018452, loss_mps: 0.081880, loss_cps: 0.143047
[14:20:00.555] iteration 27744: total_loss: 0.321534, loss_sup: 0.028560, loss_mps: 0.100154, loss_cps: 0.192820
[14:20:00.701] iteration 27745: total_loss: 0.265374, loss_sup: 0.001982, loss_mps: 0.091555, loss_cps: 0.171837
[14:20:00.846] iteration 27746: total_loss: 0.272379, loss_sup: 0.010297, loss_mps: 0.092194, loss_cps: 0.169889
[14:20:00.994] iteration 27747: total_loss: 0.317164, loss_sup: 0.020020, loss_mps: 0.112000, loss_cps: 0.185145
[14:20:01.142] iteration 27748: total_loss: 0.331741, loss_sup: 0.011826, loss_mps: 0.114780, loss_cps: 0.205135
[14:20:01.287] iteration 27749: total_loss: 0.560561, loss_sup: 0.265340, loss_mps: 0.099521, loss_cps: 0.195700
[14:20:01.434] iteration 27750: total_loss: 0.188543, loss_sup: 0.005200, loss_mps: 0.072149, loss_cps: 0.111193
[14:20:01.579] iteration 27751: total_loss: 0.195965, loss_sup: 0.009922, loss_mps: 0.066800, loss_cps: 0.119244
[14:20:01.725] iteration 27752: total_loss: 0.304586, loss_sup: 0.052303, loss_mps: 0.089803, loss_cps: 0.162481
[14:20:01.872] iteration 27753: total_loss: 0.294667, loss_sup: 0.043736, loss_mps: 0.086440, loss_cps: 0.164491
[14:20:02.020] iteration 27754: total_loss: 0.319696, loss_sup: 0.070039, loss_mps: 0.084371, loss_cps: 0.165285
[14:20:02.169] iteration 27755: total_loss: 0.410165, loss_sup: 0.127242, loss_mps: 0.098034, loss_cps: 0.184889
[14:20:02.315] iteration 27756: total_loss: 0.402754, loss_sup: 0.003119, loss_mps: 0.129257, loss_cps: 0.270377
[14:20:02.461] iteration 27757: total_loss: 0.436717, loss_sup: 0.009377, loss_mps: 0.131067, loss_cps: 0.296274
[14:20:02.606] iteration 27758: total_loss: 0.172653, loss_sup: 0.009654, loss_mps: 0.059529, loss_cps: 0.103470
[14:20:02.752] iteration 27759: total_loss: 0.520714, loss_sup: 0.066702, loss_mps: 0.145321, loss_cps: 0.308691
[14:20:02.900] iteration 27760: total_loss: 0.385034, loss_sup: 0.062464, loss_mps: 0.107625, loss_cps: 0.214945
[14:20:03.046] iteration 27761: total_loss: 0.782748, loss_sup: 0.098419, loss_mps: 0.214620, loss_cps: 0.469709
[14:20:03.191] iteration 27762: total_loss: 0.495376, loss_sup: 0.142842, loss_mps: 0.116644, loss_cps: 0.235890
[14:20:03.338] iteration 27763: total_loss: 0.350366, loss_sup: 0.021020, loss_mps: 0.112518, loss_cps: 0.216827
[14:20:03.485] iteration 27764: total_loss: 0.351153, loss_sup: 0.014425, loss_mps: 0.105291, loss_cps: 0.231437
[14:20:03.633] iteration 27765: total_loss: 0.400407, loss_sup: 0.216870, loss_mps: 0.066919, loss_cps: 0.116618
[14:20:03.779] iteration 27766: total_loss: 0.506066, loss_sup: 0.043874, loss_mps: 0.148871, loss_cps: 0.313320
[14:20:03.927] iteration 27767: total_loss: 0.211353, loss_sup: 0.008290, loss_mps: 0.073487, loss_cps: 0.129575
[14:20:04.074] iteration 27768: total_loss: 0.209223, loss_sup: 0.010090, loss_mps: 0.069117, loss_cps: 0.130016
[14:20:04.222] iteration 27769: total_loss: 0.228998, loss_sup: 0.002055, loss_mps: 0.080930, loss_cps: 0.146012
[14:20:04.368] iteration 27770: total_loss: 0.181523, loss_sup: 0.003120, loss_mps: 0.061514, loss_cps: 0.116889
[14:20:04.514] iteration 27771: total_loss: 0.363837, loss_sup: 0.044512, loss_mps: 0.110175, loss_cps: 0.209150
[14:20:04.660] iteration 27772: total_loss: 0.362549, loss_sup: 0.023581, loss_mps: 0.117532, loss_cps: 0.221437
[14:20:04.807] iteration 27773: total_loss: 0.303935, loss_sup: 0.016157, loss_mps: 0.100823, loss_cps: 0.186955
[14:20:04.953] iteration 27774: total_loss: 0.417874, loss_sup: 0.123238, loss_mps: 0.098089, loss_cps: 0.196547
[14:20:05.099] iteration 27775: total_loss: 0.463356, loss_sup: 0.143190, loss_mps: 0.103995, loss_cps: 0.216172
[14:20:05.246] iteration 27776: total_loss: 0.192598, loss_sup: 0.016832, loss_mps: 0.064132, loss_cps: 0.111634
[14:20:05.392] iteration 27777: total_loss: 0.261033, loss_sup: 0.017458, loss_mps: 0.089241, loss_cps: 0.154334
[14:20:05.539] iteration 27778: total_loss: 0.221395, loss_sup: 0.004098, loss_mps: 0.079042, loss_cps: 0.138255
[14:20:05.685] iteration 27779: total_loss: 0.282624, loss_sup: 0.066367, loss_mps: 0.079633, loss_cps: 0.136624
[14:20:05.831] iteration 27780: total_loss: 0.633415, loss_sup: 0.046981, loss_mps: 0.184826, loss_cps: 0.401608
[14:20:05.978] iteration 27781: total_loss: 0.276313, loss_sup: 0.021400, loss_mps: 0.092847, loss_cps: 0.162067
[14:20:06.124] iteration 27782: total_loss: 0.327748, loss_sup: 0.061114, loss_mps: 0.095987, loss_cps: 0.170647
[14:20:06.271] iteration 27783: total_loss: 0.204586, loss_sup: 0.009498, loss_mps: 0.070430, loss_cps: 0.124658
[14:20:06.421] iteration 27784: total_loss: 0.443838, loss_sup: 0.061188, loss_mps: 0.130566, loss_cps: 0.252084
[14:20:06.567] iteration 27785: total_loss: 0.299120, loss_sup: 0.031720, loss_mps: 0.091209, loss_cps: 0.176191
[14:20:06.713] iteration 27786: total_loss: 0.284276, loss_sup: 0.005409, loss_mps: 0.102899, loss_cps: 0.175968
[14:20:06.859] iteration 27787: total_loss: 0.344348, loss_sup: 0.036021, loss_mps: 0.109235, loss_cps: 0.199092
[14:20:07.005] iteration 27788: total_loss: 0.471895, loss_sup: 0.048092, loss_mps: 0.140059, loss_cps: 0.283744
[14:20:07.153] iteration 27789: total_loss: 0.336719, loss_sup: 0.007177, loss_mps: 0.116766, loss_cps: 0.212776
[14:20:07.300] iteration 27790: total_loss: 0.294154, loss_sup: 0.011223, loss_mps: 0.096680, loss_cps: 0.186250
[14:20:07.447] iteration 27791: total_loss: 0.268011, loss_sup: 0.034278, loss_mps: 0.083982, loss_cps: 0.149751
[14:20:07.594] iteration 27792: total_loss: 0.228067, loss_sup: 0.035582, loss_mps: 0.068204, loss_cps: 0.124281
[14:20:07.740] iteration 27793: total_loss: 0.257727, loss_sup: 0.102926, loss_mps: 0.059287, loss_cps: 0.095515
[14:20:07.886] iteration 27794: total_loss: 0.215804, loss_sup: 0.030230, loss_mps: 0.065408, loss_cps: 0.120166
[14:20:08.034] iteration 27795: total_loss: 0.293286, loss_sup: 0.057555, loss_mps: 0.086472, loss_cps: 0.149259
[14:20:08.179] iteration 27796: total_loss: 0.265003, loss_sup: 0.067455, loss_mps: 0.074048, loss_cps: 0.123500
[14:20:08.327] iteration 27797: total_loss: 0.438657, loss_sup: 0.075390, loss_mps: 0.117614, loss_cps: 0.245653
[14:20:08.475] iteration 27798: total_loss: 0.349156, loss_sup: 0.005598, loss_mps: 0.118908, loss_cps: 0.224650
[14:20:08.621] iteration 27799: total_loss: 0.214483, loss_sup: 0.001437, loss_mps: 0.074564, loss_cps: 0.138483
[14:20:08.768] iteration 27800: total_loss: 0.600181, loss_sup: 0.241543, loss_mps: 0.117027, loss_cps: 0.241610
[14:20:08.768] Evaluation Started ==>
[14:20:20.093] ==> valid iteration 27800: unet metrics: {'dc': 0.6641476571627837, 'jc': 0.551962732612968, 'pre': 0.7979514688681005, 'hd': 5.3168075482005515}, ynet metrics: {'dc': 0.6115120383521414, 'jc': 0.5006517473326054, 'pre': 0.7961051904953137, 'hd': 5.409492907117786}.
[14:20:20.095] Evaluation Finished!⏹️
[14:20:20.248] iteration 27801: total_loss: 0.224044, loss_sup: 0.020793, loss_mps: 0.071580, loss_cps: 0.131670
[14:20:20.398] iteration 27802: total_loss: 0.356476, loss_sup: 0.007505, loss_mps: 0.113691, loss_cps: 0.235280
[14:20:20.544] iteration 27803: total_loss: 0.350180, loss_sup: 0.037040, loss_mps: 0.113671, loss_cps: 0.199469
[14:20:20.690] iteration 27804: total_loss: 0.460908, loss_sup: 0.122119, loss_mps: 0.112923, loss_cps: 0.225867
[14:20:20.837] iteration 27805: total_loss: 0.296445, loss_sup: 0.011422, loss_mps: 0.097660, loss_cps: 0.187363
[14:20:20.983] iteration 27806: total_loss: 0.253400, loss_sup: 0.015847, loss_mps: 0.082295, loss_cps: 0.155258
[14:20:21.129] iteration 27807: total_loss: 0.354199, loss_sup: 0.063159, loss_mps: 0.107166, loss_cps: 0.183874
[14:20:21.279] iteration 27808: total_loss: 1.008708, loss_sup: 0.023421, loss_mps: 0.289440, loss_cps: 0.695848
[14:20:21.425] iteration 27809: total_loss: 0.183260, loss_sup: 0.005353, loss_mps: 0.063991, loss_cps: 0.113916
[14:20:21.570] iteration 27810: total_loss: 0.502259, loss_sup: 0.105702, loss_mps: 0.142205, loss_cps: 0.254352
[14:20:21.716] iteration 27811: total_loss: 0.205580, loss_sup: 0.011971, loss_mps: 0.070123, loss_cps: 0.123486
[14:20:21.865] iteration 27812: total_loss: 0.349338, loss_sup: 0.107597, loss_mps: 0.087851, loss_cps: 0.153890
[14:20:22.010] iteration 27813: total_loss: 0.234391, loss_sup: 0.017859, loss_mps: 0.077276, loss_cps: 0.139256
[14:20:22.156] iteration 27814: total_loss: 0.251410, loss_sup: 0.007804, loss_mps: 0.083761, loss_cps: 0.159845
[14:20:22.301] iteration 27815: total_loss: 0.404230, loss_sup: 0.023428, loss_mps: 0.130181, loss_cps: 0.250621
[14:20:22.446] iteration 27816: total_loss: 0.477994, loss_sup: 0.062087, loss_mps: 0.139928, loss_cps: 0.275979
[14:20:22.593] iteration 27817: total_loss: 0.446312, loss_sup: 0.017530, loss_mps: 0.137462, loss_cps: 0.291320
[14:20:22.739] iteration 27818: total_loss: 0.402393, loss_sup: 0.032390, loss_mps: 0.125157, loss_cps: 0.244845
[14:20:22.884] iteration 27819: total_loss: 0.541105, loss_sup: 0.026093, loss_mps: 0.169737, loss_cps: 0.345275
[14:20:23.030] iteration 27820: total_loss: 0.287432, loss_sup: 0.070023, loss_mps: 0.077109, loss_cps: 0.140299
[14:20:23.178] iteration 27821: total_loss: 0.259658, loss_sup: 0.018445, loss_mps: 0.083621, loss_cps: 0.157591
[14:20:23.324] iteration 27822: total_loss: 0.520004, loss_sup: 0.054838, loss_mps: 0.154382, loss_cps: 0.310784
[14:20:23.470] iteration 27823: total_loss: 0.377916, loss_sup: 0.002535, loss_mps: 0.120111, loss_cps: 0.255270
[14:20:23.615] iteration 27824: total_loss: 0.185579, loss_sup: 0.022929, loss_mps: 0.061713, loss_cps: 0.100937
[14:20:23.761] iteration 27825: total_loss: 0.379769, loss_sup: 0.051922, loss_mps: 0.109559, loss_cps: 0.218288
[14:20:23.907] iteration 27826: total_loss: 0.455567, loss_sup: 0.202308, loss_mps: 0.088534, loss_cps: 0.164725
[14:20:24.052] iteration 27827: total_loss: 0.481541, loss_sup: 0.027348, loss_mps: 0.144433, loss_cps: 0.309759
[14:20:24.197] iteration 27828: total_loss: 0.224403, loss_sup: 0.021549, loss_mps: 0.075206, loss_cps: 0.127649
[14:20:24.343] iteration 27829: total_loss: 0.606996, loss_sup: 0.151091, loss_mps: 0.151685, loss_cps: 0.304220
[14:20:24.491] iteration 27830: total_loss: 0.445604, loss_sup: 0.030015, loss_mps: 0.134369, loss_cps: 0.281220
[14:20:24.641] iteration 27831: total_loss: 0.666834, loss_sup: 0.204632, loss_mps: 0.139277, loss_cps: 0.322925
[14:20:24.787] iteration 27832: total_loss: 0.340475, loss_sup: 0.122722, loss_mps: 0.080672, loss_cps: 0.137081
[14:20:24.933] iteration 27833: total_loss: 0.226973, loss_sup: 0.020255, loss_mps: 0.075952, loss_cps: 0.130766
[14:20:25.078] iteration 27834: total_loss: 0.264158, loss_sup: 0.004709, loss_mps: 0.091515, loss_cps: 0.167934
[14:20:25.224] iteration 27835: total_loss: 0.283621, loss_sup: 0.052959, loss_mps: 0.081112, loss_cps: 0.149550
[14:20:25.370] iteration 27836: total_loss: 0.286593, loss_sup: 0.005644, loss_mps: 0.098306, loss_cps: 0.182644
[14:20:25.515] iteration 27837: total_loss: 0.207915, loss_sup: 0.049095, loss_mps: 0.062574, loss_cps: 0.096246
[14:20:25.664] iteration 27838: total_loss: 0.376855, loss_sup: 0.041065, loss_mps: 0.115444, loss_cps: 0.220346
[14:20:25.810] iteration 27839: total_loss: 0.329900, loss_sup: 0.077577, loss_mps: 0.092170, loss_cps: 0.160153
[14:20:25.955] iteration 27840: total_loss: 0.292399, loss_sup: 0.075104, loss_mps: 0.079606, loss_cps: 0.137690
[14:20:26.101] iteration 27841: total_loss: 0.341751, loss_sup: 0.088950, loss_mps: 0.089965, loss_cps: 0.162836
[14:20:26.247] iteration 27842: total_loss: 0.195717, loss_sup: 0.020115, loss_mps: 0.063397, loss_cps: 0.112206
[14:20:26.395] iteration 27843: total_loss: 0.256808, loss_sup: 0.029633, loss_mps: 0.081687, loss_cps: 0.145488
[14:20:26.540] iteration 27844: total_loss: 0.553210, loss_sup: 0.125764, loss_mps: 0.139644, loss_cps: 0.287802
[14:20:26.686] iteration 27845: total_loss: 0.281651, loss_sup: 0.035121, loss_mps: 0.086920, loss_cps: 0.159609
[14:20:26.831] iteration 27846: total_loss: 0.377576, loss_sup: 0.101380, loss_mps: 0.107444, loss_cps: 0.168752
[14:20:26.977] iteration 27847: total_loss: 0.247907, loss_sup: 0.048804, loss_mps: 0.074863, loss_cps: 0.124240
[14:20:27.123] iteration 27848: total_loss: 0.399475, loss_sup: 0.031909, loss_mps: 0.127133, loss_cps: 0.240434
[14:20:27.269] iteration 27849: total_loss: 0.495384, loss_sup: 0.086700, loss_mps: 0.134904, loss_cps: 0.273779
[14:20:27.415] iteration 27850: total_loss: 0.241349, loss_sup: 0.009982, loss_mps: 0.083064, loss_cps: 0.148303
[14:20:27.561] iteration 27851: total_loss: 0.370546, loss_sup: 0.074465, loss_mps: 0.106771, loss_cps: 0.189311
[14:20:27.707] iteration 27852: total_loss: 0.230661, loss_sup: 0.095926, loss_mps: 0.049956, loss_cps: 0.084779
[14:20:27.853] iteration 27853: total_loss: 0.246800, loss_sup: 0.029532, loss_mps: 0.077247, loss_cps: 0.140022
[14:20:27.998] iteration 27854: total_loss: 0.203685, loss_sup: 0.010429, loss_mps: 0.075620, loss_cps: 0.117636
[14:20:28.145] iteration 27855: total_loss: 0.429286, loss_sup: 0.002484, loss_mps: 0.143490, loss_cps: 0.283312
[14:20:28.291] iteration 27856: total_loss: 0.475336, loss_sup: 0.058823, loss_mps: 0.140857, loss_cps: 0.275656
[14:20:28.436] iteration 27857: total_loss: 0.440220, loss_sup: 0.032247, loss_mps: 0.144305, loss_cps: 0.263668
[14:20:28.582] iteration 27858: total_loss: 0.370531, loss_sup: 0.006628, loss_mps: 0.118887, loss_cps: 0.245016
[14:20:28.728] iteration 27859: total_loss: 0.195747, loss_sup: 0.019524, loss_mps: 0.070785, loss_cps: 0.105438
[14:20:28.875] iteration 27860: total_loss: 0.273897, loss_sup: 0.017305, loss_mps: 0.087700, loss_cps: 0.168891
[14:20:29.021] iteration 27861: total_loss: 0.278987, loss_sup: 0.017303, loss_mps: 0.088354, loss_cps: 0.173330
[14:20:29.169] iteration 27862: total_loss: 0.358550, loss_sup: 0.013335, loss_mps: 0.109486, loss_cps: 0.235729
[14:20:29.315] iteration 27863: total_loss: 0.353622, loss_sup: 0.043530, loss_mps: 0.107672, loss_cps: 0.202420
[14:20:29.461] iteration 27864: total_loss: 0.353617, loss_sup: 0.027370, loss_mps: 0.118024, loss_cps: 0.208224
[14:20:29.607] iteration 27865: total_loss: 0.289638, loss_sup: 0.022386, loss_mps: 0.102171, loss_cps: 0.165081
[14:20:29.752] iteration 27866: total_loss: 0.230048, loss_sup: 0.036658, loss_mps: 0.071992, loss_cps: 0.121398
[14:20:29.902] iteration 27867: total_loss: 0.357514, loss_sup: 0.047800, loss_mps: 0.112055, loss_cps: 0.197660
[14:20:30.049] iteration 27868: total_loss: 0.245537, loss_sup: 0.014274, loss_mps: 0.078477, loss_cps: 0.152785
[14:20:30.196] iteration 27869: total_loss: 0.498547, loss_sup: 0.033030, loss_mps: 0.158771, loss_cps: 0.306747
[14:20:30.341] iteration 27870: total_loss: 0.325759, loss_sup: 0.067210, loss_mps: 0.094814, loss_cps: 0.163735
[14:20:30.487] iteration 27871: total_loss: 0.443159, loss_sup: 0.133380, loss_mps: 0.108732, loss_cps: 0.201047
[14:20:30.633] iteration 27872: total_loss: 0.253043, loss_sup: 0.012605, loss_mps: 0.082694, loss_cps: 0.157744
[14:20:30.780] iteration 27873: total_loss: 0.228126, loss_sup: 0.044870, loss_mps: 0.066412, loss_cps: 0.116844
[14:20:30.927] iteration 27874: total_loss: 0.263117, loss_sup: 0.050958, loss_mps: 0.083831, loss_cps: 0.128328
[14:20:31.073] iteration 27875: total_loss: 0.705930, loss_sup: 0.271035, loss_mps: 0.143093, loss_cps: 0.291802
[14:20:31.219] iteration 27876: total_loss: 0.407255, loss_sup: 0.019402, loss_mps: 0.136366, loss_cps: 0.251486
[14:20:31.366] iteration 27877: total_loss: 0.502695, loss_sup: 0.006876, loss_mps: 0.156750, loss_cps: 0.339069
[14:20:31.512] iteration 27878: total_loss: 0.472708, loss_sup: 0.052122, loss_mps: 0.138481, loss_cps: 0.282104
[14:20:31.658] iteration 27879: total_loss: 0.229100, loss_sup: 0.060481, loss_mps: 0.063156, loss_cps: 0.105462
[14:20:31.804] iteration 27880: total_loss: 0.299742, loss_sup: 0.001476, loss_mps: 0.100486, loss_cps: 0.197781
[14:20:31.951] iteration 27881: total_loss: 0.220111, loss_sup: 0.029421, loss_mps: 0.071080, loss_cps: 0.119610
[14:20:32.097] iteration 27882: total_loss: 0.370286, loss_sup: 0.008470, loss_mps: 0.125402, loss_cps: 0.236414
[14:20:32.243] iteration 27883: total_loss: 0.410720, loss_sup: 0.070417, loss_mps: 0.116634, loss_cps: 0.223668
[14:20:32.389] iteration 27884: total_loss: 0.259825, loss_sup: 0.034127, loss_mps: 0.078481, loss_cps: 0.147216
[14:20:32.535] iteration 27885: total_loss: 0.222803, loss_sup: 0.017934, loss_mps: 0.073307, loss_cps: 0.131562
[14:20:32.681] iteration 27886: total_loss: 0.297314, loss_sup: 0.129880, loss_mps: 0.061256, loss_cps: 0.106177
[14:20:32.833] iteration 27887: total_loss: 0.184640, loss_sup: 0.014078, loss_mps: 0.061509, loss_cps: 0.109053
[14:20:32.980] iteration 27888: total_loss: 0.144972, loss_sup: 0.002651, loss_mps: 0.051638, loss_cps: 0.090683
[14:20:33.126] iteration 27889: total_loss: 0.350851, loss_sup: 0.062285, loss_mps: 0.099781, loss_cps: 0.188785
[14:20:33.273] iteration 27890: total_loss: 0.189569, loss_sup: 0.047176, loss_mps: 0.052735, loss_cps: 0.089658
[14:20:33.418] iteration 27891: total_loss: 0.648993, loss_sup: 0.066554, loss_mps: 0.189500, loss_cps: 0.392940
[14:20:33.564] iteration 27892: total_loss: 0.336207, loss_sup: 0.003050, loss_mps: 0.117052, loss_cps: 0.216106
[14:20:33.716] iteration 27893: total_loss: 0.236998, loss_sup: 0.029601, loss_mps: 0.074169, loss_cps: 0.133227
[14:20:33.862] iteration 27894: total_loss: 0.567402, loss_sup: 0.197268, loss_mps: 0.132361, loss_cps: 0.237773
[14:20:34.008] iteration 27895: total_loss: 0.220927, loss_sup: 0.012277, loss_mps: 0.076250, loss_cps: 0.132399
[14:20:34.154] iteration 27896: total_loss: 0.317709, loss_sup: 0.030388, loss_mps: 0.104402, loss_cps: 0.182919
[14:20:34.299] iteration 27897: total_loss: 0.200585, loss_sup: 0.002018, loss_mps: 0.074688, loss_cps: 0.123879
[14:20:34.445] iteration 27898: total_loss: 0.208917, loss_sup: 0.019949, loss_mps: 0.070209, loss_cps: 0.118759
[14:20:34.593] iteration 27899: total_loss: 0.515682, loss_sup: 0.063833, loss_mps: 0.153885, loss_cps: 0.297965
[14:20:34.739] iteration 27900: total_loss: 0.257226, loss_sup: 0.038060, loss_mps: 0.076844, loss_cps: 0.142322
[14:20:34.739] Evaluation Started ==>
[14:20:46.107] ==> valid iteration 27900: unet metrics: {'dc': 0.6675771360477718, 'jc': 0.5540763080158558, 'pre': 0.801282747382973, 'hd': 5.355076489986179}, ynet metrics: {'dc': 0.6096179206666252, 'jc': 0.49741751458428896, 'pre': 0.8001223101670486, 'hd': 5.362447124668778}.
[14:20:46.109] Evaluation Finished!⏹️
[14:20:46.262] iteration 27901: total_loss: 0.293155, loss_sup: 0.012364, loss_mps: 0.092775, loss_cps: 0.188017
[14:20:46.410] iteration 27902: total_loss: 0.219400, loss_sup: 0.017789, loss_mps: 0.069079, loss_cps: 0.132533
[14:20:46.557] iteration 27903: total_loss: 0.287385, loss_sup: 0.080523, loss_mps: 0.077094, loss_cps: 0.129769
[14:20:46.703] iteration 27904: total_loss: 0.263353, loss_sup: 0.006853, loss_mps: 0.094129, loss_cps: 0.162371
[14:20:46.849] iteration 27905: total_loss: 0.486541, loss_sup: 0.111055, loss_mps: 0.118310, loss_cps: 0.257176
[14:20:46.994] iteration 27906: total_loss: 0.325529, loss_sup: 0.023243, loss_mps: 0.109305, loss_cps: 0.192980
[14:20:47.141] iteration 27907: total_loss: 0.445028, loss_sup: 0.212304, loss_mps: 0.074876, loss_cps: 0.157848
[14:20:47.286] iteration 27908: total_loss: 0.254285, loss_sup: 0.028668, loss_mps: 0.086266, loss_cps: 0.139351
[14:20:47.432] iteration 27909: total_loss: 0.480896, loss_sup: 0.053905, loss_mps: 0.138842, loss_cps: 0.288149
[14:20:47.577] iteration 27910: total_loss: 0.238313, loss_sup: 0.008232, loss_mps: 0.079972, loss_cps: 0.150110
[14:20:47.723] iteration 27911: total_loss: 0.202026, loss_sup: 0.010984, loss_mps: 0.071850, loss_cps: 0.119192
[14:20:47.868] iteration 27912: total_loss: 0.225085, loss_sup: 0.002582, loss_mps: 0.079290, loss_cps: 0.143213
[14:20:48.013] iteration 27913: total_loss: 0.260391, loss_sup: 0.003338, loss_mps: 0.087250, loss_cps: 0.169804
[14:20:48.158] iteration 27914: total_loss: 0.330362, loss_sup: 0.010508, loss_mps: 0.110601, loss_cps: 0.209254
[14:20:48.306] iteration 27915: total_loss: 0.331461, loss_sup: 0.016027, loss_mps: 0.112427, loss_cps: 0.203007
[14:20:48.456] iteration 27916: total_loss: 0.209923, loss_sup: 0.009180, loss_mps: 0.076149, loss_cps: 0.124594
[14:20:48.605] iteration 27917: total_loss: 0.358482, loss_sup: 0.051284, loss_mps: 0.106945, loss_cps: 0.200253
[14:20:48.750] iteration 27918: total_loss: 0.351685, loss_sup: 0.111782, loss_mps: 0.084786, loss_cps: 0.155117
[14:20:48.896] iteration 27919: total_loss: 0.300607, loss_sup: 0.015841, loss_mps: 0.101809, loss_cps: 0.182957
[14:20:49.042] iteration 27920: total_loss: 0.413480, loss_sup: 0.016702, loss_mps: 0.127029, loss_cps: 0.269749
[14:20:49.188] iteration 27921: total_loss: 0.245176, loss_sup: 0.040274, loss_mps: 0.071670, loss_cps: 0.133232
[14:20:49.333] iteration 27922: total_loss: 0.232475, loss_sup: 0.035898, loss_mps: 0.070951, loss_cps: 0.125626
[14:20:49.479] iteration 27923: total_loss: 0.303995, loss_sup: 0.002051, loss_mps: 0.100531, loss_cps: 0.201412
[14:20:49.627] iteration 27924: total_loss: 0.338742, loss_sup: 0.044279, loss_mps: 0.110639, loss_cps: 0.183824
[14:20:49.773] iteration 27925: total_loss: 0.615062, loss_sup: 0.038820, loss_mps: 0.179680, loss_cps: 0.396562
[14:20:49.918] iteration 27926: total_loss: 0.152993, loss_sup: 0.037672, loss_mps: 0.046495, loss_cps: 0.068825
[14:20:50.064] iteration 27927: total_loss: 0.255832, loss_sup: 0.047627, loss_mps: 0.076086, loss_cps: 0.132119
[14:20:50.209] iteration 27928: total_loss: 0.361380, loss_sup: 0.130936, loss_mps: 0.084506, loss_cps: 0.145938
[14:20:50.355] iteration 27929: total_loss: 0.321067, loss_sup: 0.036651, loss_mps: 0.097341, loss_cps: 0.187075
[14:20:50.500] iteration 27930: total_loss: 0.329145, loss_sup: 0.118585, loss_mps: 0.075115, loss_cps: 0.135445
[14:20:50.646] iteration 27931: total_loss: 0.451038, loss_sup: 0.011842, loss_mps: 0.136692, loss_cps: 0.302503
[14:20:50.791] iteration 27932: total_loss: 0.309280, loss_sup: 0.017537, loss_mps: 0.103998, loss_cps: 0.187744
[14:20:50.937] iteration 27933: total_loss: 0.208191, loss_sup: 0.015491, loss_mps: 0.071072, loss_cps: 0.121629
[14:20:51.083] iteration 27934: total_loss: 0.419134, loss_sup: 0.198556, loss_mps: 0.076750, loss_cps: 0.143829
[14:20:51.229] iteration 27935: total_loss: 0.221240, loss_sup: 0.007671, loss_mps: 0.077010, loss_cps: 0.136559
[14:20:51.376] iteration 27936: total_loss: 0.462981, loss_sup: 0.106632, loss_mps: 0.125252, loss_cps: 0.231096
[14:20:51.522] iteration 27937: total_loss: 0.513463, loss_sup: 0.006680, loss_mps: 0.155866, loss_cps: 0.350917
[14:20:51.667] iteration 27938: total_loss: 0.445244, loss_sup: 0.014933, loss_mps: 0.141072, loss_cps: 0.289238
[14:20:51.813] iteration 27939: total_loss: 0.360488, loss_sup: 0.060930, loss_mps: 0.104033, loss_cps: 0.195525
[14:20:51.959] iteration 27940: total_loss: 0.431796, loss_sup: 0.104945, loss_mps: 0.114349, loss_cps: 0.212502
[14:20:52.105] iteration 27941: total_loss: 0.384223, loss_sup: 0.052799, loss_mps: 0.112990, loss_cps: 0.218434
[14:20:52.251] iteration 27942: total_loss: 0.341677, loss_sup: 0.068868, loss_mps: 0.095182, loss_cps: 0.177627
[14:20:52.397] iteration 27943: total_loss: 0.309230, loss_sup: 0.038033, loss_mps: 0.091175, loss_cps: 0.180023
[14:20:52.544] iteration 27944: total_loss: 0.443980, loss_sup: 0.148906, loss_mps: 0.105820, loss_cps: 0.189254
[14:20:52.690] iteration 27945: total_loss: 0.200822, loss_sup: 0.011415, loss_mps: 0.073270, loss_cps: 0.116137
[14:20:52.837] iteration 27946: total_loss: 0.333027, loss_sup: 0.055600, loss_mps: 0.094216, loss_cps: 0.183210
[14:20:52.983] iteration 27947: total_loss: 0.336993, loss_sup: 0.066133, loss_mps: 0.090108, loss_cps: 0.180751
[14:20:53.129] iteration 27948: total_loss: 0.255985, loss_sup: 0.009436, loss_mps: 0.082642, loss_cps: 0.163907
[14:20:53.275] iteration 27949: total_loss: 0.464895, loss_sup: 0.206384, loss_mps: 0.091925, loss_cps: 0.166586
[14:20:53.421] iteration 27950: total_loss: 0.343063, loss_sup: 0.007598, loss_mps: 0.112678, loss_cps: 0.222787
[14:20:53.567] iteration 27951: total_loss: 0.361456, loss_sup: 0.051871, loss_mps: 0.103476, loss_cps: 0.206110
[14:20:53.715] iteration 27952: total_loss: 0.307277, loss_sup: 0.001152, loss_mps: 0.103330, loss_cps: 0.202795
[14:20:53.862] iteration 27953: total_loss: 0.250152, loss_sup: 0.018959, loss_mps: 0.082482, loss_cps: 0.148712
[14:20:54.007] iteration 27954: total_loss: 0.613262, loss_sup: 0.053007, loss_mps: 0.174157, loss_cps: 0.386097
[14:20:54.153] iteration 27955: total_loss: 0.316771, loss_sup: 0.035059, loss_mps: 0.100477, loss_cps: 0.181235
[14:20:54.302] iteration 27956: total_loss: 0.232852, loss_sup: 0.017018, loss_mps: 0.083238, loss_cps: 0.132596
[14:20:54.448] iteration 27957: total_loss: 0.325210, loss_sup: 0.061042, loss_mps: 0.095952, loss_cps: 0.168215
[14:20:54.602] iteration 27958: total_loss: 0.330120, loss_sup: 0.019595, loss_mps: 0.105980, loss_cps: 0.204545
[14:20:54.748] iteration 27959: total_loss: 0.507540, loss_sup: 0.068956, loss_mps: 0.136027, loss_cps: 0.302557
[14:20:54.894] iteration 27960: total_loss: 0.308984, loss_sup: 0.103320, loss_mps: 0.073972, loss_cps: 0.131692
[14:20:55.041] iteration 27961: total_loss: 0.263420, loss_sup: 0.010599, loss_mps: 0.085572, loss_cps: 0.167249
[14:20:55.187] iteration 27962: total_loss: 0.374623, loss_sup: 0.050779, loss_mps: 0.105443, loss_cps: 0.218401
[14:20:55.334] iteration 27963: total_loss: 0.279383, loss_sup: 0.024489, loss_mps: 0.089008, loss_cps: 0.165886
[14:20:55.482] iteration 27964: total_loss: 0.336052, loss_sup: 0.022281, loss_mps: 0.116871, loss_cps: 0.196900
[14:20:55.627] iteration 27965: total_loss: 0.250882, loss_sup: 0.010748, loss_mps: 0.084933, loss_cps: 0.155201
[14:20:55.773] iteration 27966: total_loss: 0.632136, loss_sup: 0.042668, loss_mps: 0.174087, loss_cps: 0.415381
[14:20:55.919] iteration 27967: total_loss: 0.306896, loss_sup: 0.050062, loss_mps: 0.089870, loss_cps: 0.166964
[14:20:56.064] iteration 27968: total_loss: 0.296115, loss_sup: 0.036499, loss_mps: 0.099233, loss_cps: 0.160383
[14:20:56.213] iteration 27969: total_loss: 0.463383, loss_sup: 0.065254, loss_mps: 0.133240, loss_cps: 0.264889
[14:20:56.358] iteration 27970: total_loss: 0.529019, loss_sup: 0.021230, loss_mps: 0.182965, loss_cps: 0.324825
[14:20:56.506] iteration 27971: total_loss: 0.337527, loss_sup: 0.037678, loss_mps: 0.107710, loss_cps: 0.192139
[14:20:56.651] iteration 27972: total_loss: 0.276291, loss_sup: 0.020518, loss_mps: 0.094861, loss_cps: 0.160912
[14:20:56.798] iteration 27973: total_loss: 0.285986, loss_sup: 0.041146, loss_mps: 0.092381, loss_cps: 0.152458
[14:20:56.944] iteration 27974: total_loss: 0.210065, loss_sup: 0.017683, loss_mps: 0.070545, loss_cps: 0.121837
[14:20:57.091] iteration 27975: total_loss: 0.175031, loss_sup: 0.011635, loss_mps: 0.063323, loss_cps: 0.100074
[14:20:57.237] iteration 27976: total_loss: 0.113937, loss_sup: 0.004306, loss_mps: 0.042816, loss_cps: 0.066815
[14:20:57.383] iteration 27977: total_loss: 0.420330, loss_sup: 0.092241, loss_mps: 0.113619, loss_cps: 0.214470
[14:20:57.531] iteration 27978: total_loss: 0.506257, loss_sup: 0.055787, loss_mps: 0.135915, loss_cps: 0.314555
[14:20:57.676] iteration 27979: total_loss: 0.339866, loss_sup: 0.041276, loss_mps: 0.107281, loss_cps: 0.191308
[14:20:57.823] iteration 27980: total_loss: 0.240369, loss_sup: 0.046740, loss_mps: 0.069979, loss_cps: 0.123650
[14:20:57.970] iteration 27981: total_loss: 0.321314, loss_sup: 0.015628, loss_mps: 0.104403, loss_cps: 0.201283
[14:20:58.116] iteration 27982: total_loss: 0.342145, loss_sup: 0.051634, loss_mps: 0.109435, loss_cps: 0.181076
[14:20:58.266] iteration 27983: total_loss: 0.527349, loss_sup: 0.222068, loss_mps: 0.103563, loss_cps: 0.201719
[14:20:58.413] iteration 27984: total_loss: 0.287711, loss_sup: 0.038870, loss_mps: 0.083558, loss_cps: 0.165283
[14:20:58.562] iteration 27985: total_loss: 0.298672, loss_sup: 0.079280, loss_mps: 0.079251, loss_cps: 0.140141
[14:20:58.709] iteration 27986: total_loss: 0.358438, loss_sup: 0.006089, loss_mps: 0.121168, loss_cps: 0.231181
[14:20:58.855] iteration 27987: total_loss: 0.210975, loss_sup: 0.018052, loss_mps: 0.070678, loss_cps: 0.122244
[14:20:59.003] iteration 27988: total_loss: 0.451311, loss_sup: 0.220282, loss_mps: 0.082999, loss_cps: 0.148030
[14:20:59.149] iteration 27989: total_loss: 0.382884, loss_sup: 0.012730, loss_mps: 0.121930, loss_cps: 0.248224
[14:20:59.296] iteration 27990: total_loss: 0.538648, loss_sup: 0.068131, loss_mps: 0.145931, loss_cps: 0.324585
[14:20:59.443] iteration 27991: total_loss: 0.237374, loss_sup: 0.002396, loss_mps: 0.080188, loss_cps: 0.154790
[14:20:59.591] iteration 27992: total_loss: 0.216409, loss_sup: 0.007730, loss_mps: 0.078819, loss_cps: 0.129859
[14:20:59.741] iteration 27993: total_loss: 0.261754, loss_sup: 0.020863, loss_mps: 0.088934, loss_cps: 0.151956
[14:20:59.886] iteration 27994: total_loss: 0.219167, loss_sup: 0.021450, loss_mps: 0.071142, loss_cps: 0.126575
[14:21:00.032] iteration 27995: total_loss: 0.258707, loss_sup: 0.016801, loss_mps: 0.089412, loss_cps: 0.152494
[14:21:00.179] iteration 27996: total_loss: 0.246333, loss_sup: 0.059505, loss_mps: 0.070439, loss_cps: 0.116389
[14:21:00.324] iteration 27997: total_loss: 0.538006, loss_sup: 0.183976, loss_mps: 0.123991, loss_cps: 0.230039
[14:21:00.470] iteration 27998: total_loss: 0.193988, loss_sup: 0.003226, loss_mps: 0.069298, loss_cps: 0.121464
[14:21:00.617] iteration 27999: total_loss: 0.221798, loss_sup: 0.008176, loss_mps: 0.076464, loss_cps: 0.137158
[14:21:00.763] iteration 28000: total_loss: 0.221798, loss_sup: 0.002551, loss_mps: 0.082570, loss_cps: 0.136677
[14:21:00.763] Evaluation Started ==>
[14:21:12.086] ==> valid iteration 28000: unet metrics: {'dc': 0.6581730732313765, 'jc': 0.545003556159821, 'pre': 0.8021123383510048, 'hd': 5.270162872388359}, ynet metrics: {'dc': 0.5969077420106844, 'jc': 0.4856895302136255, 'pre': 0.7882757413015327, 'hd': 5.3435632399797806}.
[14:21:12.088] Evaluation Finished!⏹️
[14:21:12.240] iteration 28001: total_loss: 0.376877, loss_sup: 0.040461, loss_mps: 0.120511, loss_cps: 0.215904
[14:21:12.389] iteration 28002: total_loss: 0.368784, loss_sup: 0.071134, loss_mps: 0.107166, loss_cps: 0.190484
[14:21:12.537] iteration 28003: total_loss: 0.353974, loss_sup: 0.023696, loss_mps: 0.112189, loss_cps: 0.218088
[14:21:12.682] iteration 28004: total_loss: 0.538047, loss_sup: 0.111856, loss_mps: 0.144154, loss_cps: 0.282037
[14:21:12.827] iteration 28005: total_loss: 0.210938, loss_sup: 0.036386, loss_mps: 0.063971, loss_cps: 0.110581
[14:21:12.890] iteration 28006: total_loss: 0.214952, loss_sup: 0.001249, loss_mps: 0.075630, loss_cps: 0.138073
[14:21:14.089] iteration 28007: total_loss: 0.285108, loss_sup: 0.010806, loss_mps: 0.099221, loss_cps: 0.175081
[14:21:14.238] iteration 28008: total_loss: 0.563220, loss_sup: 0.306875, loss_mps: 0.085491, loss_cps: 0.170853
[14:21:14.385] iteration 28009: total_loss: 0.372127, loss_sup: 0.033654, loss_mps: 0.120988, loss_cps: 0.217485
[14:21:14.532] iteration 28010: total_loss: 0.408610, loss_sup: 0.070591, loss_mps: 0.115508, loss_cps: 0.222511
[14:21:14.679] iteration 28011: total_loss: 0.421174, loss_sup: 0.028263, loss_mps: 0.129452, loss_cps: 0.263460
[14:21:14.825] iteration 28012: total_loss: 0.271310, loss_sup: 0.048585, loss_mps: 0.080732, loss_cps: 0.141993
[14:21:14.972] iteration 28013: total_loss: 0.490977, loss_sup: 0.040113, loss_mps: 0.143323, loss_cps: 0.307541
[14:21:15.123] iteration 28014: total_loss: 0.197877, loss_sup: 0.001841, loss_mps: 0.072608, loss_cps: 0.123428
[14:21:15.270] iteration 28015: total_loss: 0.307790, loss_sup: 0.047692, loss_mps: 0.094683, loss_cps: 0.165414
[14:21:15.416] iteration 28016: total_loss: 0.289141, loss_sup: 0.017629, loss_mps: 0.100795, loss_cps: 0.170717
[14:21:15.563] iteration 28017: total_loss: 0.182151, loss_sup: 0.005085, loss_mps: 0.065681, loss_cps: 0.111385
[14:21:15.710] iteration 28018: total_loss: 0.274302, loss_sup: 0.033325, loss_mps: 0.085551, loss_cps: 0.155425
[14:21:15.856] iteration 28019: total_loss: 0.207709, loss_sup: 0.025678, loss_mps: 0.070955, loss_cps: 0.111076
[14:21:16.004] iteration 28020: total_loss: 0.297309, loss_sup: 0.042415, loss_mps: 0.089481, loss_cps: 0.165413
[14:21:16.151] iteration 28021: total_loss: 0.495316, loss_sup: 0.005778, loss_mps: 0.159155, loss_cps: 0.330383
[14:21:16.298] iteration 28022: total_loss: 0.202733, loss_sup: 0.003213, loss_mps: 0.072755, loss_cps: 0.126764
[14:21:16.444] iteration 28023: total_loss: 0.218335, loss_sup: 0.048162, loss_mps: 0.061080, loss_cps: 0.109094
[14:21:16.593] iteration 28024: total_loss: 0.553573, loss_sup: 0.222368, loss_mps: 0.117877, loss_cps: 0.213329
[14:21:16.739] iteration 28025: total_loss: 0.202771, loss_sup: 0.003160, loss_mps: 0.076258, loss_cps: 0.123352
[14:21:16.888] iteration 28026: total_loss: 0.429813, loss_sup: 0.113888, loss_mps: 0.107640, loss_cps: 0.208286
[14:21:17.036] iteration 28027: total_loss: 0.362745, loss_sup: 0.095690, loss_mps: 0.088456, loss_cps: 0.178600
[14:21:17.184] iteration 28028: total_loss: 0.376302, loss_sup: 0.103741, loss_mps: 0.095261, loss_cps: 0.177300
[14:21:17.331] iteration 28029: total_loss: 0.263361, loss_sup: 0.018068, loss_mps: 0.083780, loss_cps: 0.161513
[14:21:17.477] iteration 28030: total_loss: 0.272206, loss_sup: 0.040081, loss_mps: 0.082088, loss_cps: 0.150037
[14:21:17.625] iteration 28031: total_loss: 0.359319, loss_sup: 0.001359, loss_mps: 0.119347, loss_cps: 0.238613
[14:21:17.776] iteration 28032: total_loss: 0.429147, loss_sup: 0.188833, loss_mps: 0.094589, loss_cps: 0.145726
[14:21:17.922] iteration 28033: total_loss: 0.321652, loss_sup: 0.016382, loss_mps: 0.100943, loss_cps: 0.204328
[14:21:18.070] iteration 28034: total_loss: 0.196398, loss_sup: 0.016868, loss_mps: 0.066501, loss_cps: 0.113029
[14:21:18.217] iteration 28035: total_loss: 0.276068, loss_sup: 0.039644, loss_mps: 0.084538, loss_cps: 0.151886
[14:21:18.363] iteration 28036: total_loss: 0.298151, loss_sup: 0.034490, loss_mps: 0.094084, loss_cps: 0.169577
[14:21:18.510] iteration 28037: total_loss: 0.463550, loss_sup: 0.024461, loss_mps: 0.143328, loss_cps: 0.295761
[14:21:18.658] iteration 28038: total_loss: 0.384959, loss_sup: 0.006288, loss_mps: 0.131038, loss_cps: 0.247634
[14:21:18.806] iteration 28039: total_loss: 0.370392, loss_sup: 0.016005, loss_mps: 0.111558, loss_cps: 0.242829
[14:21:18.952] iteration 28040: total_loss: 0.266264, loss_sup: 0.007360, loss_mps: 0.090483, loss_cps: 0.168422
[14:21:19.099] iteration 28041: total_loss: 0.152242, loss_sup: 0.007465, loss_mps: 0.052671, loss_cps: 0.092106
[14:21:19.252] iteration 28042: total_loss: 0.443786, loss_sup: 0.034840, loss_mps: 0.134358, loss_cps: 0.274588
[14:21:19.399] iteration 28043: total_loss: 0.234485, loss_sup: 0.032331, loss_mps: 0.070035, loss_cps: 0.132119
[14:21:19.546] iteration 28044: total_loss: 0.822887, loss_sup: 0.016140, loss_mps: 0.244617, loss_cps: 0.562129
[14:21:19.693] iteration 28045: total_loss: 0.190148, loss_sup: 0.019346, loss_mps: 0.066784, loss_cps: 0.104017
[14:21:19.841] iteration 28046: total_loss: 0.471976, loss_sup: 0.061188, loss_mps: 0.135903, loss_cps: 0.274885
[14:21:19.987] iteration 28047: total_loss: 0.313652, loss_sup: 0.076821, loss_mps: 0.083871, loss_cps: 0.152960
[14:21:20.134] iteration 28048: total_loss: 0.431183, loss_sup: 0.018863, loss_mps: 0.138273, loss_cps: 0.274047
[14:21:20.280] iteration 28049: total_loss: 0.211945, loss_sup: 0.026118, loss_mps: 0.075457, loss_cps: 0.110370
[14:21:20.428] iteration 28050: total_loss: 0.301665, loss_sup: 0.021507, loss_mps: 0.098527, loss_cps: 0.181631
[14:21:20.574] iteration 28051: total_loss: 0.292373, loss_sup: 0.013780, loss_mps: 0.097741, loss_cps: 0.180852
[14:21:20.721] iteration 28052: total_loss: 0.257618, loss_sup: 0.005505, loss_mps: 0.085334, loss_cps: 0.166779
[14:21:20.868] iteration 28053: total_loss: 0.384473, loss_sup: 0.107944, loss_mps: 0.094704, loss_cps: 0.181825
[14:21:21.016] iteration 28054: total_loss: 0.614512, loss_sup: 0.180847, loss_mps: 0.149165, loss_cps: 0.284501
[14:21:21.163] iteration 28055: total_loss: 0.232685, loss_sup: 0.007039, loss_mps: 0.081952, loss_cps: 0.143694
[14:21:21.310] iteration 28056: total_loss: 0.418782, loss_sup: 0.052847, loss_mps: 0.116534, loss_cps: 0.249400
[14:21:21.458] iteration 28057: total_loss: 0.349007, loss_sup: 0.116089, loss_mps: 0.082463, loss_cps: 0.150456
[14:21:21.606] iteration 28058: total_loss: 0.196670, loss_sup: 0.001077, loss_mps: 0.069143, loss_cps: 0.126449
[14:21:21.753] iteration 28059: total_loss: 0.148110, loss_sup: 0.008648, loss_mps: 0.052092, loss_cps: 0.087369
[14:21:21.902] iteration 28060: total_loss: 0.247526, loss_sup: 0.051015, loss_mps: 0.072371, loss_cps: 0.124141
[14:21:22.049] iteration 28061: total_loss: 0.431423, loss_sup: 0.160265, loss_mps: 0.097290, loss_cps: 0.173869
[14:21:22.196] iteration 28062: total_loss: 0.277712, loss_sup: 0.045635, loss_mps: 0.089287, loss_cps: 0.142791
[14:21:22.343] iteration 28063: total_loss: 0.322253, loss_sup: 0.034119, loss_mps: 0.104244, loss_cps: 0.183890
[14:21:22.491] iteration 28064: total_loss: 0.151898, loss_sup: 0.001456, loss_mps: 0.056323, loss_cps: 0.094118
[14:21:22.639] iteration 28065: total_loss: 0.303754, loss_sup: 0.001256, loss_mps: 0.098302, loss_cps: 0.204196
[14:21:22.787] iteration 28066: total_loss: 0.189252, loss_sup: 0.021792, loss_mps: 0.060722, loss_cps: 0.106738
[14:21:22.934] iteration 28067: total_loss: 0.212827, loss_sup: 0.027312, loss_mps: 0.069129, loss_cps: 0.116385
[14:21:23.081] iteration 28068: total_loss: 0.566158, loss_sup: 0.021522, loss_mps: 0.177992, loss_cps: 0.366643
[14:21:23.229] iteration 28069: total_loss: 0.359852, loss_sup: 0.021017, loss_mps: 0.112458, loss_cps: 0.226377
[14:21:23.376] iteration 28070: total_loss: 0.294783, loss_sup: 0.012437, loss_mps: 0.100250, loss_cps: 0.182096
[14:21:23.525] iteration 28071: total_loss: 0.261455, loss_sup: 0.003228, loss_mps: 0.088501, loss_cps: 0.169726
[14:21:23.672] iteration 28072: total_loss: 0.257430, loss_sup: 0.024604, loss_mps: 0.085493, loss_cps: 0.147333
[14:21:23.819] iteration 28073: total_loss: 0.323803, loss_sup: 0.032567, loss_mps: 0.104688, loss_cps: 0.186548
[14:21:23.970] iteration 28074: total_loss: 0.361654, loss_sup: 0.035875, loss_mps: 0.113184, loss_cps: 0.212594
[14:21:24.117] iteration 28075: total_loss: 0.470088, loss_sup: 0.017537, loss_mps: 0.148798, loss_cps: 0.303753
[14:21:24.263] iteration 28076: total_loss: 0.158908, loss_sup: 0.009232, loss_mps: 0.057487, loss_cps: 0.092189
[14:21:24.411] iteration 28077: total_loss: 0.341320, loss_sup: 0.018300, loss_mps: 0.111235, loss_cps: 0.211784
[14:21:24.558] iteration 28078: total_loss: 0.426403, loss_sup: 0.035891, loss_mps: 0.135507, loss_cps: 0.255004
[14:21:24.707] iteration 28079: total_loss: 0.152530, loss_sup: 0.021278, loss_mps: 0.052914, loss_cps: 0.078337
[14:21:24.853] iteration 28080: total_loss: 0.356413, loss_sup: 0.016876, loss_mps: 0.119404, loss_cps: 0.220133
[14:21:25.000] iteration 28081: total_loss: 0.252193, loss_sup: 0.046931, loss_mps: 0.075478, loss_cps: 0.129784
[14:21:25.147] iteration 28082: total_loss: 0.488344, loss_sup: 0.141498, loss_mps: 0.123754, loss_cps: 0.223092
[14:21:25.296] iteration 28083: total_loss: 0.269514, loss_sup: 0.013461, loss_mps: 0.089810, loss_cps: 0.166243
[14:21:25.444] iteration 28084: total_loss: 0.358026, loss_sup: 0.169629, loss_mps: 0.069429, loss_cps: 0.118968
[14:21:25.591] iteration 28085: total_loss: 0.385707, loss_sup: 0.097922, loss_mps: 0.101697, loss_cps: 0.186088
[14:21:25.739] iteration 28086: total_loss: 0.151008, loss_sup: 0.002332, loss_mps: 0.053548, loss_cps: 0.095128
[14:21:25.885] iteration 28087: total_loss: 0.337425, loss_sup: 0.035887, loss_mps: 0.106821, loss_cps: 0.194717
[14:21:26.033] iteration 28088: total_loss: 0.193936, loss_sup: 0.005020, loss_mps: 0.068463, loss_cps: 0.120453
[14:21:26.181] iteration 28089: total_loss: 0.379790, loss_sup: 0.050614, loss_mps: 0.106104, loss_cps: 0.223072
[14:21:26.331] iteration 28090: total_loss: 0.311261, loss_sup: 0.090015, loss_mps: 0.082525, loss_cps: 0.138721
[14:21:26.477] iteration 28091: total_loss: 0.341252, loss_sup: 0.012268, loss_mps: 0.117809, loss_cps: 0.211175
[14:21:26.625] iteration 28092: total_loss: 0.311395, loss_sup: 0.004747, loss_mps: 0.102422, loss_cps: 0.204226
[14:21:26.771] iteration 28093: total_loss: 0.220985, loss_sup: 0.024266, loss_mps: 0.070469, loss_cps: 0.126250
[14:21:26.918] iteration 28094: total_loss: 0.960422, loss_sup: 0.094222, loss_mps: 0.286817, loss_cps: 0.579383
[14:21:27.065] iteration 28095: total_loss: 0.208971, loss_sup: 0.017742, loss_mps: 0.072169, loss_cps: 0.119060
[14:21:27.215] iteration 28096: total_loss: 0.231334, loss_sup: 0.030784, loss_mps: 0.076760, loss_cps: 0.123790
[14:21:27.362] iteration 28097: total_loss: 0.263905, loss_sup: 0.039096, loss_mps: 0.083515, loss_cps: 0.141294
[14:21:27.512] iteration 28098: total_loss: 0.485533, loss_sup: 0.070946, loss_mps: 0.134982, loss_cps: 0.279605
[14:21:27.660] iteration 28099: total_loss: 0.397281, loss_sup: 0.047123, loss_mps: 0.118814, loss_cps: 0.231345
[14:21:27.807] iteration 28100: total_loss: 0.337069, loss_sup: 0.010333, loss_mps: 0.113158, loss_cps: 0.213578
[14:21:27.807] Evaluation Started ==>
[14:21:39.171] ==> valid iteration 28100: unet metrics: {'dc': 0.6577263756463579, 'jc': 0.5421347650451487, 'pre': 0.804498707357564, 'hd': 5.399157658512608}, ynet metrics: {'dc': 0.6051401657358306, 'jc': 0.49337919725129353, 'pre': 0.7931519821187415, 'hd': 5.478819767872489}.
[14:21:39.173] Evaluation Finished!⏹️
[14:21:39.323] iteration 28101: total_loss: 0.192459, loss_sup: 0.001635, loss_mps: 0.068070, loss_cps: 0.122754
[14:21:39.471] iteration 28102: total_loss: 0.209323, loss_sup: 0.003467, loss_mps: 0.071257, loss_cps: 0.134598
[14:21:39.617] iteration 28103: total_loss: 0.358197, loss_sup: 0.039203, loss_mps: 0.112526, loss_cps: 0.206468
[14:21:39.764] iteration 28104: total_loss: 0.287598, loss_sup: 0.000567, loss_mps: 0.100608, loss_cps: 0.186422
[14:21:39.910] iteration 28105: total_loss: 0.187826, loss_sup: 0.017520, loss_mps: 0.062060, loss_cps: 0.108245
[14:21:40.058] iteration 28106: total_loss: 0.413618, loss_sup: 0.032957, loss_mps: 0.129497, loss_cps: 0.251164
[14:21:40.205] iteration 28107: total_loss: 0.397965, loss_sup: 0.006789, loss_mps: 0.127053, loss_cps: 0.264124
[14:21:40.350] iteration 28108: total_loss: 0.200901, loss_sup: 0.004404, loss_mps: 0.072457, loss_cps: 0.124040
[14:21:40.496] iteration 28109: total_loss: 0.184264, loss_sup: 0.008615, loss_mps: 0.065257, loss_cps: 0.110392
[14:21:40.642] iteration 28110: total_loss: 0.167668, loss_sup: 0.004668, loss_mps: 0.064740, loss_cps: 0.098261
[14:21:40.788] iteration 28111: total_loss: 0.192045, loss_sup: 0.012452, loss_mps: 0.063565, loss_cps: 0.116028
[14:21:40.933] iteration 28112: total_loss: 0.399422, loss_sup: 0.090658, loss_mps: 0.101570, loss_cps: 0.207194
[14:21:41.079] iteration 28113: total_loss: 0.311333, loss_sup: 0.026584, loss_mps: 0.094487, loss_cps: 0.190262
[14:21:41.225] iteration 28114: total_loss: 0.578531, loss_sup: 0.032596, loss_mps: 0.166756, loss_cps: 0.379179
[14:21:41.371] iteration 28115: total_loss: 0.179185, loss_sup: 0.003136, loss_mps: 0.064735, loss_cps: 0.111314
[14:21:41.517] iteration 28116: total_loss: 0.456367, loss_sup: 0.047362, loss_mps: 0.137632, loss_cps: 0.271372
[14:21:41.670] iteration 28117: total_loss: 0.213463, loss_sup: 0.002306, loss_mps: 0.076489, loss_cps: 0.134668
[14:21:41.817] iteration 28118: total_loss: 0.367434, loss_sup: 0.032903, loss_mps: 0.110944, loss_cps: 0.223587
[14:21:41.965] iteration 28119: total_loss: 0.255445, loss_sup: 0.007446, loss_mps: 0.089459, loss_cps: 0.158540
[14:21:42.111] iteration 28120: total_loss: 0.211878, loss_sup: 0.017122, loss_mps: 0.071827, loss_cps: 0.122930
[14:21:42.257] iteration 28121: total_loss: 0.493195, loss_sup: 0.060996, loss_mps: 0.140511, loss_cps: 0.291688
[14:21:42.403] iteration 28122: total_loss: 0.281250, loss_sup: 0.015276, loss_mps: 0.100475, loss_cps: 0.165500
[14:21:42.549] iteration 28123: total_loss: 0.567549, loss_sup: 0.123433, loss_mps: 0.154301, loss_cps: 0.289815
[14:21:42.695] iteration 28124: total_loss: 0.871082, loss_sup: 0.103236, loss_mps: 0.245022, loss_cps: 0.522824
[14:21:42.842] iteration 28125: total_loss: 0.287947, loss_sup: 0.026410, loss_mps: 0.091599, loss_cps: 0.169938
[14:21:42.989] iteration 28126: total_loss: 0.143239, loss_sup: 0.011651, loss_mps: 0.050024, loss_cps: 0.081564
[14:21:43.135] iteration 28127: total_loss: 0.354169, loss_sup: 0.083890, loss_mps: 0.095182, loss_cps: 0.175097
[14:21:43.282] iteration 28128: total_loss: 0.200325, loss_sup: 0.015190, loss_mps: 0.068123, loss_cps: 0.117012
[14:21:43.428] iteration 28129: total_loss: 0.247525, loss_sup: 0.014640, loss_mps: 0.080626, loss_cps: 0.152259
[14:21:43.573] iteration 28130: total_loss: 0.473828, loss_sup: 0.036240, loss_mps: 0.140260, loss_cps: 0.297329
[14:21:43.719] iteration 28131: total_loss: 0.137110, loss_sup: 0.005815, loss_mps: 0.053199, loss_cps: 0.078097
[14:21:43.867] iteration 28132: total_loss: 0.325578, loss_sup: 0.117747, loss_mps: 0.072012, loss_cps: 0.135820
[14:21:44.013] iteration 28133: total_loss: 0.291226, loss_sup: 0.016427, loss_mps: 0.097975, loss_cps: 0.176824
[14:21:44.158] iteration 28134: total_loss: 0.207707, loss_sup: 0.047153, loss_mps: 0.061694, loss_cps: 0.098859
[14:21:44.304] iteration 28135: total_loss: 0.162478, loss_sup: 0.003141, loss_mps: 0.059113, loss_cps: 0.100224
[14:21:44.450] iteration 28136: total_loss: 0.362291, loss_sup: 0.021724, loss_mps: 0.119688, loss_cps: 0.220879
[14:21:44.601] iteration 28137: total_loss: 0.274859, loss_sup: 0.040685, loss_mps: 0.082982, loss_cps: 0.151192
[14:21:44.748] iteration 28138: total_loss: 0.575200, loss_sup: 0.077237, loss_mps: 0.155404, loss_cps: 0.342559
[14:21:44.895] iteration 28139: total_loss: 0.160601, loss_sup: 0.015389, loss_mps: 0.055625, loss_cps: 0.089587
[14:21:45.041] iteration 28140: total_loss: 0.241147, loss_sup: 0.017545, loss_mps: 0.078161, loss_cps: 0.145440
[14:21:45.189] iteration 28141: total_loss: 0.185732, loss_sup: 0.005625, loss_mps: 0.067558, loss_cps: 0.112549
[14:21:45.335] iteration 28142: total_loss: 0.266431, loss_sup: 0.033408, loss_mps: 0.078662, loss_cps: 0.154361
[14:21:45.481] iteration 28143: total_loss: 0.319438, loss_sup: 0.100189, loss_mps: 0.082234, loss_cps: 0.137015
[14:21:45.628] iteration 28144: total_loss: 0.260269, loss_sup: 0.002350, loss_mps: 0.087486, loss_cps: 0.170433
[14:21:45.775] iteration 28145: total_loss: 0.133710, loss_sup: 0.003698, loss_mps: 0.048640, loss_cps: 0.081371
[14:21:45.921] iteration 28146: total_loss: 0.282398, loss_sup: 0.030827, loss_mps: 0.089896, loss_cps: 0.161675
[14:21:46.067] iteration 28147: total_loss: 0.519132, loss_sup: 0.051128, loss_mps: 0.156086, loss_cps: 0.311918
[14:21:46.213] iteration 28148: total_loss: 0.370581, loss_sup: 0.106036, loss_mps: 0.090588, loss_cps: 0.173957
[14:21:46.361] iteration 28149: total_loss: 0.300483, loss_sup: 0.033213, loss_mps: 0.090545, loss_cps: 0.176726
[14:21:46.513] iteration 28150: total_loss: 0.330490, loss_sup: 0.004156, loss_mps: 0.110226, loss_cps: 0.216108
[14:21:46.661] iteration 28151: total_loss: 0.797209, loss_sup: 0.477441, loss_mps: 0.112363, loss_cps: 0.207406
[14:21:46.808] iteration 28152: total_loss: 0.313571, loss_sup: 0.011812, loss_mps: 0.108701, loss_cps: 0.193058
[14:21:46.954] iteration 28153: total_loss: 0.259128, loss_sup: 0.030407, loss_mps: 0.083113, loss_cps: 0.145608
[14:21:47.100] iteration 28154: total_loss: 0.484508, loss_sup: 0.177456, loss_mps: 0.114153, loss_cps: 0.192900
[14:21:47.247] iteration 28155: total_loss: 0.247420, loss_sup: 0.037126, loss_mps: 0.075636, loss_cps: 0.134658
[14:21:47.393] iteration 28156: total_loss: 0.313351, loss_sup: 0.025266, loss_mps: 0.101358, loss_cps: 0.186727
[14:21:47.539] iteration 28157: total_loss: 0.436700, loss_sup: 0.169294, loss_mps: 0.092783, loss_cps: 0.174623
[14:21:47.686] iteration 28158: total_loss: 0.313850, loss_sup: 0.000328, loss_mps: 0.102885, loss_cps: 0.210636
[14:21:47.832] iteration 28159: total_loss: 0.239515, loss_sup: 0.028602, loss_mps: 0.078069, loss_cps: 0.132844
[14:21:47.979] iteration 28160: total_loss: 0.195092, loss_sup: 0.003268, loss_mps: 0.070951, loss_cps: 0.120873
[14:21:48.126] iteration 28161: total_loss: 0.341366, loss_sup: 0.095257, loss_mps: 0.088912, loss_cps: 0.157197
[14:21:48.273] iteration 28162: total_loss: 0.175072, loss_sup: 0.062600, loss_mps: 0.041345, loss_cps: 0.071127
[14:21:48.419] iteration 28163: total_loss: 0.205087, loss_sup: 0.001494, loss_mps: 0.074849, loss_cps: 0.128744
[14:21:48.566] iteration 28164: total_loss: 0.413222, loss_sup: 0.175314, loss_mps: 0.084244, loss_cps: 0.153663
[14:21:48.712] iteration 28165: total_loss: 0.489583, loss_sup: 0.050205, loss_mps: 0.149380, loss_cps: 0.289998
[14:21:48.859] iteration 28166: total_loss: 0.250128, loss_sup: 0.073557, loss_mps: 0.068865, loss_cps: 0.107706
[14:21:49.005] iteration 28167: total_loss: 0.322042, loss_sup: 0.059294, loss_mps: 0.090143, loss_cps: 0.172604
[14:21:49.151] iteration 28168: total_loss: 0.288567, loss_sup: 0.025361, loss_mps: 0.093897, loss_cps: 0.169308
[14:21:49.297] iteration 28169: total_loss: 0.500797, loss_sup: 0.273682, loss_mps: 0.087171, loss_cps: 0.139944
[14:21:49.443] iteration 28170: total_loss: 0.798607, loss_sup: 0.123388, loss_mps: 0.199977, loss_cps: 0.475242
[14:21:49.591] iteration 28171: total_loss: 0.166694, loss_sup: 0.003081, loss_mps: 0.060625, loss_cps: 0.102988
[14:21:49.737] iteration 28172: total_loss: 0.783136, loss_sup: 0.112307, loss_mps: 0.205501, loss_cps: 0.465328
[14:21:49.883] iteration 28173: total_loss: 0.453393, loss_sup: 0.013501, loss_mps: 0.146000, loss_cps: 0.293892
[14:21:50.030] iteration 28174: total_loss: 0.196145, loss_sup: 0.026828, loss_mps: 0.063450, loss_cps: 0.105867
[14:21:50.176] iteration 28175: total_loss: 0.134304, loss_sup: 0.002848, loss_mps: 0.050730, loss_cps: 0.080727
[14:21:50.322] iteration 28176: total_loss: 0.286454, loss_sup: 0.006583, loss_mps: 0.097218, loss_cps: 0.182653
[14:21:50.468] iteration 28177: total_loss: 0.349855, loss_sup: 0.024926, loss_mps: 0.112466, loss_cps: 0.212463
[14:21:50.614] iteration 28178: total_loss: 0.454033, loss_sup: 0.072949, loss_mps: 0.133273, loss_cps: 0.247811
[14:21:50.760] iteration 28179: total_loss: 0.332391, loss_sup: 0.030717, loss_mps: 0.109353, loss_cps: 0.192321
[14:21:50.907] iteration 28180: total_loss: 0.321872, loss_sup: 0.010919, loss_mps: 0.105539, loss_cps: 0.205414
[14:21:51.052] iteration 28181: total_loss: 0.444707, loss_sup: 0.009224, loss_mps: 0.138986, loss_cps: 0.296497
[14:21:51.199] iteration 28182: total_loss: 0.539571, loss_sup: 0.050250, loss_mps: 0.153392, loss_cps: 0.335929
[14:21:51.346] iteration 28183: total_loss: 0.315998, loss_sup: 0.076398, loss_mps: 0.084269, loss_cps: 0.155331
[14:21:51.493] iteration 28184: total_loss: 0.217466, loss_sup: 0.017680, loss_mps: 0.072714, loss_cps: 0.127072
[14:21:51.639] iteration 28185: total_loss: 0.541190, loss_sup: 0.031070, loss_mps: 0.166925, loss_cps: 0.343194
[14:21:51.787] iteration 28186: total_loss: 0.322889, loss_sup: 0.005722, loss_mps: 0.109973, loss_cps: 0.207194
[14:21:51.933] iteration 28187: total_loss: 0.185279, loss_sup: 0.003786, loss_mps: 0.067376, loss_cps: 0.114117
[14:21:52.080] iteration 28188: total_loss: 0.234859, loss_sup: 0.038403, loss_mps: 0.072317, loss_cps: 0.124139
[14:21:52.228] iteration 28189: total_loss: 0.152552, loss_sup: 0.001842, loss_mps: 0.057417, loss_cps: 0.093293
[14:21:52.374] iteration 28190: total_loss: 0.281330, loss_sup: 0.011832, loss_mps: 0.098671, loss_cps: 0.170826
[14:21:52.521] iteration 28191: total_loss: 0.486145, loss_sup: 0.022579, loss_mps: 0.144468, loss_cps: 0.319098
[14:21:52.667] iteration 28192: total_loss: 0.293794, loss_sup: 0.071893, loss_mps: 0.083396, loss_cps: 0.138505
[14:21:52.814] iteration 28193: total_loss: 0.218366, loss_sup: 0.021261, loss_mps: 0.072486, loss_cps: 0.124619
[14:21:52.961] iteration 28194: total_loss: 0.236124, loss_sup: 0.069977, loss_mps: 0.060553, loss_cps: 0.105594
[14:21:53.107] iteration 28195: total_loss: 0.330565, loss_sup: 0.023361, loss_mps: 0.103992, loss_cps: 0.203213
[14:21:53.254] iteration 28196: total_loss: 0.210446, loss_sup: 0.006888, loss_mps: 0.071504, loss_cps: 0.132054
[14:21:53.401] iteration 28197: total_loss: 0.197081, loss_sup: 0.020128, loss_mps: 0.066483, loss_cps: 0.110470
[14:21:53.548] iteration 28198: total_loss: 0.243482, loss_sup: 0.009544, loss_mps: 0.079333, loss_cps: 0.154606
[14:21:53.694] iteration 28199: total_loss: 0.264807, loss_sup: 0.014222, loss_mps: 0.089259, loss_cps: 0.161327
[14:21:53.840] iteration 28200: total_loss: 0.236669, loss_sup: 0.017715, loss_mps: 0.077612, loss_cps: 0.141342
[14:21:53.841] Evaluation Started ==>
[14:22:05.136] ==> valid iteration 28200: unet metrics: {'dc': 0.6742664450503241, 'jc': 0.5594742489426974, 'pre': 0.8068600377717957, 'hd': 5.354718605207117}, ynet metrics: {'dc': 0.627342499025254, 'jc': 0.5158081486676754, 'pre': 0.7937346673623227, 'hd': 5.448848654434362}.
[14:22:05.138] Evaluation Finished!⏹️
[14:22:05.290] iteration 28201: total_loss: 0.866662, loss_sup: 0.046173, loss_mps: 0.245756, loss_cps: 0.574733
[14:22:05.438] iteration 28202: total_loss: 0.350351, loss_sup: 0.054820, loss_mps: 0.103437, loss_cps: 0.192095
[14:22:05.584] iteration 28203: total_loss: 0.232654, loss_sup: 0.002779, loss_mps: 0.082540, loss_cps: 0.147334
[14:22:05.730] iteration 28204: total_loss: 0.282309, loss_sup: 0.071459, loss_mps: 0.074554, loss_cps: 0.136295
[14:22:05.875] iteration 28205: total_loss: 0.227661, loss_sup: 0.006375, loss_mps: 0.079482, loss_cps: 0.141803
[14:22:06.024] iteration 28206: total_loss: 0.355049, loss_sup: 0.087709, loss_mps: 0.096478, loss_cps: 0.170861
[14:22:06.170] iteration 28207: total_loss: 0.419840, loss_sup: 0.171875, loss_mps: 0.092062, loss_cps: 0.155903
[14:22:06.316] iteration 28208: total_loss: 0.221077, loss_sup: 0.019181, loss_mps: 0.077400, loss_cps: 0.124496
[14:22:06.462] iteration 28209: total_loss: 0.217367, loss_sup: 0.018978, loss_mps: 0.074296, loss_cps: 0.124093
[14:22:06.608] iteration 28210: total_loss: 0.192268, loss_sup: 0.034512, loss_mps: 0.061927, loss_cps: 0.095829
[14:22:06.755] iteration 28211: total_loss: 0.263873, loss_sup: 0.002128, loss_mps: 0.091347, loss_cps: 0.170398
[14:22:06.901] iteration 28212: total_loss: 0.314822, loss_sup: 0.029628, loss_mps: 0.097034, loss_cps: 0.188161
[14:22:07.046] iteration 28213: total_loss: 0.237664, loss_sup: 0.005702, loss_mps: 0.081376, loss_cps: 0.150585
[14:22:07.193] iteration 28214: total_loss: 0.333404, loss_sup: 0.055316, loss_mps: 0.101991, loss_cps: 0.176097
[14:22:07.339] iteration 28215: total_loss: 0.111818, loss_sup: 0.001732, loss_mps: 0.041556, loss_cps: 0.068531
[14:22:07.485] iteration 28216: total_loss: 0.362969, loss_sup: 0.012226, loss_mps: 0.113378, loss_cps: 0.237365
[14:22:07.631] iteration 28217: total_loss: 0.254424, loss_sup: 0.032060, loss_mps: 0.082668, loss_cps: 0.139696
[14:22:07.779] iteration 28218: total_loss: 0.287901, loss_sup: 0.021910, loss_mps: 0.091847, loss_cps: 0.174144
[14:22:07.926] iteration 28219: total_loss: 0.282806, loss_sup: 0.007096, loss_mps: 0.093945, loss_cps: 0.181765
[14:22:08.071] iteration 28220: total_loss: 0.618191, loss_sup: 0.113554, loss_mps: 0.170883, loss_cps: 0.333754
[14:22:08.218] iteration 28221: total_loss: 0.266895, loss_sup: 0.040087, loss_mps: 0.085512, loss_cps: 0.141296
[14:22:08.363] iteration 28222: total_loss: 0.251208, loss_sup: 0.056660, loss_mps: 0.072527, loss_cps: 0.122021
[14:22:08.509] iteration 28223: total_loss: 0.194381, loss_sup: 0.004933, loss_mps: 0.068137, loss_cps: 0.121310
[14:22:08.655] iteration 28224: total_loss: 0.233733, loss_sup: 0.007156, loss_mps: 0.078850, loss_cps: 0.147727
[14:22:08.801] iteration 28225: total_loss: 0.643177, loss_sup: 0.108850, loss_mps: 0.171677, loss_cps: 0.362650
[14:22:08.947] iteration 28226: total_loss: 0.400918, loss_sup: 0.060571, loss_mps: 0.120426, loss_cps: 0.219922
[14:22:09.093] iteration 28227: total_loss: 0.273234, loss_sup: 0.010434, loss_mps: 0.089957, loss_cps: 0.172842
[14:22:09.243] iteration 28228: total_loss: 0.220758, loss_sup: 0.022480, loss_mps: 0.076908, loss_cps: 0.121371
[14:22:09.391] iteration 28229: total_loss: 0.217639, loss_sup: 0.003344, loss_mps: 0.078777, loss_cps: 0.135518
[14:22:09.537] iteration 28230: total_loss: 0.261086, loss_sup: 0.022381, loss_mps: 0.085686, loss_cps: 0.153019
[14:22:09.684] iteration 28231: total_loss: 0.428558, loss_sup: 0.031029, loss_mps: 0.136694, loss_cps: 0.260835
[14:22:09.832] iteration 28232: total_loss: 0.193216, loss_sup: 0.006844, loss_mps: 0.062954, loss_cps: 0.123418
[14:22:09.980] iteration 28233: total_loss: 0.234544, loss_sup: 0.016369, loss_mps: 0.076554, loss_cps: 0.141621
[14:22:10.125] iteration 28234: total_loss: 0.216233, loss_sup: 0.059502, loss_mps: 0.059633, loss_cps: 0.097098
[14:22:10.272] iteration 28235: total_loss: 0.296754, loss_sup: 0.009522, loss_mps: 0.098348, loss_cps: 0.188884
[14:22:10.418] iteration 28236: total_loss: 0.190405, loss_sup: 0.006820, loss_mps: 0.063906, loss_cps: 0.119680
[14:22:10.564] iteration 28237: total_loss: 0.374487, loss_sup: 0.100150, loss_mps: 0.095943, loss_cps: 0.178394
[14:22:10.709] iteration 28238: total_loss: 0.370180, loss_sup: 0.097191, loss_mps: 0.098198, loss_cps: 0.174791
[14:22:10.858] iteration 28239: total_loss: 0.206624, loss_sup: 0.009498, loss_mps: 0.068900, loss_cps: 0.128226
[14:22:11.004] iteration 28240: total_loss: 0.337288, loss_sup: 0.013181, loss_mps: 0.111794, loss_cps: 0.212313
[14:22:11.150] iteration 28241: total_loss: 0.249761, loss_sup: 0.054364, loss_mps: 0.069499, loss_cps: 0.125899
[14:22:11.298] iteration 28242: total_loss: 0.315384, loss_sup: 0.092192, loss_mps: 0.081769, loss_cps: 0.141423
[14:22:11.444] iteration 28243: total_loss: 0.268665, loss_sup: 0.075396, loss_mps: 0.069098, loss_cps: 0.124172
[14:22:11.590] iteration 28244: total_loss: 0.267704, loss_sup: 0.062303, loss_mps: 0.073136, loss_cps: 0.132264
[14:22:11.736] iteration 28245: total_loss: 0.346068, loss_sup: 0.040796, loss_mps: 0.107343, loss_cps: 0.197929
[14:22:11.882] iteration 28246: total_loss: 0.411782, loss_sup: 0.001098, loss_mps: 0.136090, loss_cps: 0.274593
[14:22:12.028] iteration 28247: total_loss: 0.238128, loss_sup: 0.011772, loss_mps: 0.076796, loss_cps: 0.149559
[14:22:12.174] iteration 28248: total_loss: 0.367771, loss_sup: 0.081717, loss_mps: 0.094821, loss_cps: 0.191232
[14:22:12.320] iteration 28249: total_loss: 0.391975, loss_sup: 0.086882, loss_mps: 0.090729, loss_cps: 0.214364
[14:22:12.465] iteration 28250: total_loss: 0.271585, loss_sup: 0.002248, loss_mps: 0.094689, loss_cps: 0.174649
[14:22:12.611] iteration 28251: total_loss: 0.219325, loss_sup: 0.015914, loss_mps: 0.072168, loss_cps: 0.131243
[14:22:12.758] iteration 28252: total_loss: 0.274193, loss_sup: 0.001773, loss_mps: 0.097265, loss_cps: 0.175155
[14:22:12.904] iteration 28253: total_loss: 0.957358, loss_sup: 0.073267, loss_mps: 0.272357, loss_cps: 0.611734
[14:22:13.049] iteration 28254: total_loss: 0.333894, loss_sup: 0.016004, loss_mps: 0.110401, loss_cps: 0.207488
[14:22:13.195] iteration 28255: total_loss: 0.241817, loss_sup: 0.026384, loss_mps: 0.075606, loss_cps: 0.139827
[14:22:13.342] iteration 28256: total_loss: 0.370894, loss_sup: 0.002386, loss_mps: 0.120297, loss_cps: 0.248211
[14:22:13.489] iteration 28257: total_loss: 0.163074, loss_sup: 0.002136, loss_mps: 0.060156, loss_cps: 0.100781
[14:22:13.636] iteration 28258: total_loss: 0.258031, loss_sup: 0.001882, loss_mps: 0.092070, loss_cps: 0.164080
[14:22:13.781] iteration 28259: total_loss: 0.386195, loss_sup: 0.078761, loss_mps: 0.106620, loss_cps: 0.200813
[14:22:13.927] iteration 28260: total_loss: 0.269299, loss_sup: 0.035644, loss_mps: 0.080997, loss_cps: 0.152658
[14:22:14.072] iteration 28261: total_loss: 0.191930, loss_sup: 0.006431, loss_mps: 0.069061, loss_cps: 0.116438
[14:22:14.218] iteration 28262: total_loss: 0.193457, loss_sup: 0.005760, loss_mps: 0.070078, loss_cps: 0.117620
[14:22:14.364] iteration 28263: total_loss: 0.258474, loss_sup: 0.009629, loss_mps: 0.084449, loss_cps: 0.164396
[14:22:14.509] iteration 28264: total_loss: 0.300057, loss_sup: 0.022871, loss_mps: 0.094269, loss_cps: 0.182916
[14:22:14.655] iteration 28265: total_loss: 0.456402, loss_sup: 0.204152, loss_mps: 0.086617, loss_cps: 0.165633
[14:22:14.803] iteration 28266: total_loss: 0.289550, loss_sup: 0.025382, loss_mps: 0.094426, loss_cps: 0.169741
[14:22:14.950] iteration 28267: total_loss: 0.227657, loss_sup: 0.007544, loss_mps: 0.080424, loss_cps: 0.139689
[14:22:15.096] iteration 28268: total_loss: 0.159381, loss_sup: 0.002106, loss_mps: 0.057017, loss_cps: 0.100258
[14:22:15.242] iteration 28269: total_loss: 0.326410, loss_sup: 0.022116, loss_mps: 0.110587, loss_cps: 0.193707
[14:22:15.388] iteration 28270: total_loss: 0.229967, loss_sup: 0.013224, loss_mps: 0.075282, loss_cps: 0.141461
[14:22:15.534] iteration 28271: total_loss: 0.143667, loss_sup: 0.001545, loss_mps: 0.055045, loss_cps: 0.087077
[14:22:15.680] iteration 28272: total_loss: 0.161250, loss_sup: 0.009832, loss_mps: 0.057716, loss_cps: 0.093703
[14:22:15.826] iteration 28273: total_loss: 0.653245, loss_sup: 0.159541, loss_mps: 0.155550, loss_cps: 0.338153
[14:22:15.972] iteration 28274: total_loss: 0.187092, loss_sup: 0.013583, loss_mps: 0.062724, loss_cps: 0.110785
[14:22:16.118] iteration 28275: total_loss: 0.317380, loss_sup: 0.032660, loss_mps: 0.099812, loss_cps: 0.184909
[14:22:16.264] iteration 28276: total_loss: 0.341200, loss_sup: 0.067742, loss_mps: 0.093101, loss_cps: 0.180357
[14:22:16.411] iteration 28277: total_loss: 0.889940, loss_sup: 0.157745, loss_mps: 0.213882, loss_cps: 0.518312
[14:22:16.557] iteration 28278: total_loss: 0.224532, loss_sup: 0.012552, loss_mps: 0.078305, loss_cps: 0.133675
[14:22:16.703] iteration 28279: total_loss: 0.207790, loss_sup: 0.017521, loss_mps: 0.069504, loss_cps: 0.120765
[14:22:16.849] iteration 28280: total_loss: 0.291297, loss_sup: 0.023714, loss_mps: 0.094429, loss_cps: 0.173155
[14:22:16.997] iteration 28281: total_loss: 0.289070, loss_sup: 0.074536, loss_mps: 0.078104, loss_cps: 0.136431
[14:22:17.143] iteration 28282: total_loss: 0.295372, loss_sup: 0.021856, loss_mps: 0.098286, loss_cps: 0.175230
[14:22:17.288] iteration 28283: total_loss: 0.393555, loss_sup: 0.103068, loss_mps: 0.096046, loss_cps: 0.194441
[14:22:17.435] iteration 28284: total_loss: 0.197577, loss_sup: 0.022817, loss_mps: 0.064806, loss_cps: 0.109954
[14:22:17.582] iteration 28285: total_loss: 0.201443, loss_sup: 0.046195, loss_mps: 0.058352, loss_cps: 0.096896
[14:22:17.728] iteration 28286: total_loss: 0.209652, loss_sup: 0.005260, loss_mps: 0.072065, loss_cps: 0.132328
[14:22:17.876] iteration 28287: total_loss: 0.300036, loss_sup: 0.129828, loss_mps: 0.062840, loss_cps: 0.107367
[14:22:18.022] iteration 28288: total_loss: 0.974977, loss_sup: 0.133552, loss_mps: 0.265301, loss_cps: 0.576123
[14:22:18.168] iteration 28289: total_loss: 0.331643, loss_sup: 0.012077, loss_mps: 0.108750, loss_cps: 0.210816
[14:22:18.316] iteration 28290: total_loss: 0.176401, loss_sup: 0.003550, loss_mps: 0.063594, loss_cps: 0.109257
[14:22:18.463] iteration 28291: total_loss: 0.252200, loss_sup: 0.020472, loss_mps: 0.082088, loss_cps: 0.149639
[14:22:18.610] iteration 28292: total_loss: 0.211496, loss_sup: 0.001448, loss_mps: 0.076545, loss_cps: 0.133502
[14:22:18.758] iteration 28293: total_loss: 0.329121, loss_sup: 0.016719, loss_mps: 0.100876, loss_cps: 0.211526
[14:22:18.904] iteration 28294: total_loss: 0.233547, loss_sup: 0.027450, loss_mps: 0.078475, loss_cps: 0.127621
[14:22:19.053] iteration 28295: total_loss: 0.219199, loss_sup: 0.016768, loss_mps: 0.077300, loss_cps: 0.125131
[14:22:19.200] iteration 28296: total_loss: 0.181044, loss_sup: 0.004394, loss_mps: 0.066945, loss_cps: 0.109705
[14:22:19.346] iteration 28297: total_loss: 0.500335, loss_sup: 0.007044, loss_mps: 0.153201, loss_cps: 0.340091
[14:22:19.493] iteration 28298: total_loss: 0.212670, loss_sup: 0.028928, loss_mps: 0.067463, loss_cps: 0.116280
[14:22:19.641] iteration 28299: total_loss: 0.403500, loss_sup: 0.133472, loss_mps: 0.092947, loss_cps: 0.177080
[14:22:19.788] iteration 28300: total_loss: 0.254224, loss_sup: 0.038382, loss_mps: 0.076593, loss_cps: 0.139249
[14:22:19.788] Evaluation Started ==>
[14:22:31.112] ==> valid iteration 28300: unet metrics: {'dc': 0.6754088357023569, 'jc': 0.5604716452850729, 'pre': 0.8018165494975759, 'hd': 5.394664082070881}, ynet metrics: {'dc': 0.633479314583022, 'jc': 0.5223990833856826, 'pre': 0.8022664766586964, 'hd': 5.382132022326361}.
[14:22:31.114] Evaluation Finished!⏹️
[14:22:31.265] iteration 28301: total_loss: 0.315326, loss_sup: 0.006126, loss_mps: 0.105039, loss_cps: 0.204161
[14:22:31.413] iteration 28302: total_loss: 0.148611, loss_sup: 0.006140, loss_mps: 0.052769, loss_cps: 0.089702
[14:22:31.559] iteration 28303: total_loss: 0.482004, loss_sup: 0.138886, loss_mps: 0.115590, loss_cps: 0.227527
[14:22:31.704] iteration 28304: total_loss: 0.258657, loss_sup: 0.023152, loss_mps: 0.082537, loss_cps: 0.152969
[14:22:31.850] iteration 28305: total_loss: 0.271988, loss_sup: 0.075634, loss_mps: 0.072199, loss_cps: 0.124155
[14:22:31.996] iteration 28306: total_loss: 0.357629, loss_sup: 0.052141, loss_mps: 0.104901, loss_cps: 0.200587
[14:22:32.146] iteration 28307: total_loss: 0.301115, loss_sup: 0.007254, loss_mps: 0.098061, loss_cps: 0.195800
[14:22:32.292] iteration 28308: total_loss: 0.228810, loss_sup: 0.038924, loss_mps: 0.066698, loss_cps: 0.123188
[14:22:32.440] iteration 28309: total_loss: 0.261804, loss_sup: 0.003378, loss_mps: 0.086923, loss_cps: 0.171503
[14:22:32.588] iteration 28310: total_loss: 0.289399, loss_sup: 0.074536, loss_mps: 0.074980, loss_cps: 0.139883
[14:22:32.736] iteration 28311: total_loss: 0.427116, loss_sup: 0.073514, loss_mps: 0.117838, loss_cps: 0.235765
[14:22:32.881] iteration 28312: total_loss: 0.500619, loss_sup: 0.143653, loss_mps: 0.119850, loss_cps: 0.237116
[14:22:33.028] iteration 28313: total_loss: 0.296328, loss_sup: 0.007162, loss_mps: 0.098831, loss_cps: 0.190335
[14:22:33.175] iteration 28314: total_loss: 0.647224, loss_sup: 0.337898, loss_mps: 0.099642, loss_cps: 0.209683
[14:22:33.321] iteration 28315: total_loss: 0.204990, loss_sup: 0.009800, loss_mps: 0.075431, loss_cps: 0.119759
[14:22:33.467] iteration 28316: total_loss: 0.185493, loss_sup: 0.023786, loss_mps: 0.060071, loss_cps: 0.101637
[14:22:33.614] iteration 28317: total_loss: 0.285749, loss_sup: 0.055387, loss_mps: 0.082441, loss_cps: 0.147922
[14:22:33.761] iteration 28318: total_loss: 0.180654, loss_sup: 0.005345, loss_mps: 0.064237, loss_cps: 0.111072
[14:22:33.909] iteration 28319: total_loss: 0.350813, loss_sup: 0.043989, loss_mps: 0.107284, loss_cps: 0.199539
[14:22:34.055] iteration 28320: total_loss: 0.264362, loss_sup: 0.049149, loss_mps: 0.076338, loss_cps: 0.138875
[14:22:34.202] iteration 28321: total_loss: 0.219425, loss_sup: 0.074016, loss_mps: 0.053457, loss_cps: 0.091952
[14:22:34.348] iteration 28322: total_loss: 0.597094, loss_sup: 0.115367, loss_mps: 0.155450, loss_cps: 0.326276
[14:22:34.494] iteration 28323: total_loss: 0.184660, loss_sup: 0.007733, loss_mps: 0.064818, loss_cps: 0.112109
[14:22:34.641] iteration 28324: total_loss: 0.453608, loss_sup: 0.041327, loss_mps: 0.120457, loss_cps: 0.291825
[14:22:34.787] iteration 28325: total_loss: 0.299229, loss_sup: 0.035883, loss_mps: 0.091997, loss_cps: 0.171349
[14:22:34.940] iteration 28326: total_loss: 0.250957, loss_sup: 0.067812, loss_mps: 0.069537, loss_cps: 0.113609
[14:22:35.087] iteration 28327: total_loss: 0.249165, loss_sup: 0.102216, loss_mps: 0.057258, loss_cps: 0.089691
[14:22:35.233] iteration 28328: total_loss: 0.249099, loss_sup: 0.076439, loss_mps: 0.063251, loss_cps: 0.109408
[14:22:35.383] iteration 28329: total_loss: 0.231515, loss_sup: 0.068959, loss_mps: 0.060118, loss_cps: 0.102439
[14:22:35.529] iteration 28330: total_loss: 0.371657, loss_sup: 0.076002, loss_mps: 0.107622, loss_cps: 0.188032
[14:22:35.675] iteration 28331: total_loss: 0.248457, loss_sup: 0.083396, loss_mps: 0.063439, loss_cps: 0.101621
[14:22:35.821] iteration 28332: total_loss: 0.315373, loss_sup: 0.058723, loss_mps: 0.090834, loss_cps: 0.165815
[14:22:35.967] iteration 28333: total_loss: 0.310956, loss_sup: 0.044909, loss_mps: 0.092800, loss_cps: 0.173246
[14:22:36.112] iteration 28334: total_loss: 0.308271, loss_sup: 0.093637, loss_mps: 0.075112, loss_cps: 0.139522
[14:22:36.260] iteration 28335: total_loss: 0.357759, loss_sup: 0.124441, loss_mps: 0.088719, loss_cps: 0.144599
[14:22:36.406] iteration 28336: total_loss: 0.515402, loss_sup: 0.105955, loss_mps: 0.136188, loss_cps: 0.273260
[14:22:36.553] iteration 28337: total_loss: 0.568370, loss_sup: 0.109439, loss_mps: 0.151081, loss_cps: 0.307849
[14:22:36.698] iteration 28338: total_loss: 0.201514, loss_sup: 0.006882, loss_mps: 0.068068, loss_cps: 0.126565
[14:22:36.844] iteration 28339: total_loss: 0.493529, loss_sup: 0.102021, loss_mps: 0.129704, loss_cps: 0.261804
[14:22:36.991] iteration 28340: total_loss: 0.283258, loss_sup: 0.014949, loss_mps: 0.093575, loss_cps: 0.174733
[14:22:37.139] iteration 28341: total_loss: 0.318876, loss_sup: 0.047576, loss_mps: 0.097451, loss_cps: 0.173849
[14:22:37.285] iteration 28342: total_loss: 0.338767, loss_sup: 0.014807, loss_mps: 0.110226, loss_cps: 0.213734
[14:22:37.432] iteration 28343: total_loss: 0.161455, loss_sup: 0.012521, loss_mps: 0.055420, loss_cps: 0.093513
[14:22:37.578] iteration 28344: total_loss: 0.199946, loss_sup: 0.003614, loss_mps: 0.073512, loss_cps: 0.122820
[14:22:37.725] iteration 28345: total_loss: 0.201590, loss_sup: 0.021816, loss_mps: 0.064265, loss_cps: 0.115509
[14:22:37.871] iteration 28346: total_loss: 0.276619, loss_sup: 0.018105, loss_mps: 0.091157, loss_cps: 0.167356
[14:22:38.017] iteration 28347: total_loss: 0.211803, loss_sup: 0.005253, loss_mps: 0.076458, loss_cps: 0.130093
[14:22:38.164] iteration 28348: total_loss: 0.153794, loss_sup: 0.001703, loss_mps: 0.057471, loss_cps: 0.094620
[14:22:38.311] iteration 28349: total_loss: 0.326545, loss_sup: 0.020397, loss_mps: 0.105621, loss_cps: 0.200527
[14:22:38.457] iteration 28350: total_loss: 0.320356, loss_sup: 0.085603, loss_mps: 0.086190, loss_cps: 0.148563
[14:22:38.603] iteration 28351: total_loss: 0.480960, loss_sup: 0.231686, loss_mps: 0.088474, loss_cps: 0.160800
[14:22:38.750] iteration 28352: total_loss: 0.339381, loss_sup: 0.028227, loss_mps: 0.102237, loss_cps: 0.208917
[14:22:38.896] iteration 28353: total_loss: 0.468955, loss_sup: 0.009206, loss_mps: 0.146817, loss_cps: 0.312931
[14:22:39.042] iteration 28354: total_loss: 0.191986, loss_sup: 0.000698, loss_mps: 0.069793, loss_cps: 0.121495
[14:22:39.190] iteration 28355: total_loss: 0.325736, loss_sup: 0.022760, loss_mps: 0.106511, loss_cps: 0.196465
[14:22:39.337] iteration 28356: total_loss: 0.320381, loss_sup: 0.048921, loss_mps: 0.092120, loss_cps: 0.179339
[14:22:39.482] iteration 28357: total_loss: 0.336121, loss_sup: 0.094439, loss_mps: 0.085912, loss_cps: 0.155771
[14:22:39.629] iteration 28358: total_loss: 0.208887, loss_sup: 0.026281, loss_mps: 0.065312, loss_cps: 0.117293
[14:22:39.775] iteration 28359: total_loss: 0.332077, loss_sup: 0.047821, loss_mps: 0.100348, loss_cps: 0.183908
[14:22:39.921] iteration 28360: total_loss: 0.416215, loss_sup: 0.057151, loss_mps: 0.129054, loss_cps: 0.230011
[14:22:40.068] iteration 28361: total_loss: 0.148592, loss_sup: 0.000807, loss_mps: 0.055617, loss_cps: 0.092168
[14:22:40.215] iteration 28362: total_loss: 0.350380, loss_sup: 0.108804, loss_mps: 0.086238, loss_cps: 0.155339
[14:22:40.365] iteration 28363: total_loss: 0.241587, loss_sup: 0.035493, loss_mps: 0.074213, loss_cps: 0.131881
[14:22:40.512] iteration 28364: total_loss: 0.174273, loss_sup: 0.008305, loss_mps: 0.061192, loss_cps: 0.104776
[14:22:40.659] iteration 28365: total_loss: 0.156963, loss_sup: 0.003469, loss_mps: 0.058384, loss_cps: 0.095111
[14:22:40.805] iteration 28366: total_loss: 0.153773, loss_sup: 0.002735, loss_mps: 0.055010, loss_cps: 0.096028
[14:22:40.952] iteration 28367: total_loss: 0.352088, loss_sup: 0.087557, loss_mps: 0.097718, loss_cps: 0.166813
[14:22:41.098] iteration 28368: total_loss: 0.202992, loss_sup: 0.017506, loss_mps: 0.070271, loss_cps: 0.115215
[14:22:41.247] iteration 28369: total_loss: 0.262484, loss_sup: 0.045801, loss_mps: 0.079301, loss_cps: 0.137382
[14:22:41.394] iteration 28370: total_loss: 0.292869, loss_sup: 0.005260, loss_mps: 0.102434, loss_cps: 0.185175
[14:22:41.540] iteration 28371: total_loss: 0.393563, loss_sup: 0.013327, loss_mps: 0.129694, loss_cps: 0.250541
[14:22:41.686] iteration 28372: total_loss: 0.234890, loss_sup: 0.031748, loss_mps: 0.070534, loss_cps: 0.132608
[14:22:41.834] iteration 28373: total_loss: 0.249167, loss_sup: 0.019876, loss_mps: 0.080110, loss_cps: 0.149181
[14:22:41.980] iteration 28374: total_loss: 0.284752, loss_sup: 0.002886, loss_mps: 0.105145, loss_cps: 0.176721
[14:22:42.127] iteration 28375: total_loss: 0.236650, loss_sup: 0.070606, loss_mps: 0.062545, loss_cps: 0.103500
[14:22:42.273] iteration 28376: total_loss: 0.530604, loss_sup: 0.175297, loss_mps: 0.120129, loss_cps: 0.235179
[14:22:42.420] iteration 28377: total_loss: 0.212708, loss_sup: 0.021971, loss_mps: 0.070922, loss_cps: 0.119815
[14:22:42.567] iteration 28378: total_loss: 0.288131, loss_sup: 0.096598, loss_mps: 0.071245, loss_cps: 0.120288
[14:22:42.714] iteration 28379: total_loss: 0.291202, loss_sup: 0.038461, loss_mps: 0.086002, loss_cps: 0.166739
[14:22:42.860] iteration 28380: total_loss: 0.648663, loss_sup: 0.020419, loss_mps: 0.195332, loss_cps: 0.432912
[14:22:43.006] iteration 28381: total_loss: 0.312439, loss_sup: 0.013473, loss_mps: 0.103527, loss_cps: 0.195438
[14:22:43.153] iteration 28382: total_loss: 0.290421, loss_sup: 0.022814, loss_mps: 0.089487, loss_cps: 0.178120
[14:22:43.299] iteration 28383: total_loss: 0.301592, loss_sup: 0.010294, loss_mps: 0.100641, loss_cps: 0.190657
[14:22:43.446] iteration 28384: total_loss: 0.282853, loss_sup: 0.046935, loss_mps: 0.079767, loss_cps: 0.156151
[14:22:43.593] iteration 28385: total_loss: 0.220580, loss_sup: 0.013895, loss_mps: 0.073635, loss_cps: 0.133050
[14:22:43.739] iteration 28386: total_loss: 0.173710, loss_sup: 0.001286, loss_mps: 0.063770, loss_cps: 0.108654
[14:22:43.887] iteration 28387: total_loss: 0.809229, loss_sup: 0.297636, loss_mps: 0.165719, loss_cps: 0.345875
[14:22:44.034] iteration 28388: total_loss: 0.460848, loss_sup: 0.033519, loss_mps: 0.148564, loss_cps: 0.278765
[14:22:44.180] iteration 28389: total_loss: 0.492423, loss_sup: 0.008324, loss_mps: 0.164196, loss_cps: 0.319902
[14:22:44.326] iteration 28390: total_loss: 0.319380, loss_sup: 0.029600, loss_mps: 0.099387, loss_cps: 0.190393
[14:22:44.472] iteration 28391: total_loss: 0.195626, loss_sup: 0.035380, loss_mps: 0.058859, loss_cps: 0.101387
[14:22:44.620] iteration 28392: total_loss: 1.050049, loss_sup: 0.021647, loss_mps: 0.315706, loss_cps: 0.712696
[14:22:44.767] iteration 28393: total_loss: 0.546926, loss_sup: 0.175793, loss_mps: 0.130468, loss_cps: 0.240665
[14:22:44.914] iteration 28394: total_loss: 0.334783, loss_sup: 0.097307, loss_mps: 0.081186, loss_cps: 0.156291
[14:22:45.061] iteration 28395: total_loss: 0.458310, loss_sup: 0.032291, loss_mps: 0.133946, loss_cps: 0.292073
[14:22:45.211] iteration 28396: total_loss: 0.224171, loss_sup: 0.034598, loss_mps: 0.068605, loss_cps: 0.120967
[14:22:45.356] iteration 28397: total_loss: 0.306641, loss_sup: 0.053848, loss_mps: 0.097826, loss_cps: 0.154967
[14:22:45.503] iteration 28398: total_loss: 0.277911, loss_sup: 0.011061, loss_mps: 0.091263, loss_cps: 0.175587
[14:22:45.650] iteration 28399: total_loss: 0.259917, loss_sup: 0.021010, loss_mps: 0.082572, loss_cps: 0.156335
[14:22:45.797] iteration 28400: total_loss: 0.275691, loss_sup: 0.051205, loss_mps: 0.078390, loss_cps: 0.146096
[14:22:45.797] Evaluation Started ==>
[14:22:57.153] ==> valid iteration 28400: unet metrics: {'dc': 0.6692889677047986, 'jc': 0.555671375393929, 'pre': 0.807656721671641, 'hd': 5.279005875254651}, ynet metrics: {'dc': 0.6172655377929928, 'jc': 0.5066324864460197, 'pre': 0.7992030542236045, 'hd': 5.333081747854326}.
[14:22:57.155] Evaluation Finished!⏹️
[14:22:57.309] iteration 28401: total_loss: 0.265094, loss_sup: 0.031391, loss_mps: 0.086102, loss_cps: 0.147602
[14:22:57.456] iteration 28402: total_loss: 0.291229, loss_sup: 0.003456, loss_mps: 0.095765, loss_cps: 0.192008
[14:22:57.602] iteration 28403: total_loss: 0.300704, loss_sup: 0.015886, loss_mps: 0.100788, loss_cps: 0.184031
[14:22:57.748] iteration 28404: total_loss: 0.150902, loss_sup: 0.010980, loss_mps: 0.056253, loss_cps: 0.083669
[14:22:57.893] iteration 28405: total_loss: 0.246874, loss_sup: 0.002757, loss_mps: 0.082679, loss_cps: 0.161438
[14:22:58.040] iteration 28406: total_loss: 0.261351, loss_sup: 0.056599, loss_mps: 0.079479, loss_cps: 0.125273
[14:22:58.187] iteration 28407: total_loss: 0.435805, loss_sup: 0.106199, loss_mps: 0.109077, loss_cps: 0.220530
[14:22:58.334] iteration 28408: total_loss: 0.250415, loss_sup: 0.037697, loss_mps: 0.080409, loss_cps: 0.132309
[14:22:58.480] iteration 28409: total_loss: 0.330916, loss_sup: 0.044591, loss_mps: 0.106254, loss_cps: 0.180071
[14:22:58.627] iteration 28410: total_loss: 0.225653, loss_sup: 0.026644, loss_mps: 0.077353, loss_cps: 0.121657
[14:22:58.773] iteration 28411: total_loss: 0.250460, loss_sup: 0.018393, loss_mps: 0.079856, loss_cps: 0.152211
[14:22:58.920] iteration 28412: total_loss: 0.313913, loss_sup: 0.021557, loss_mps: 0.103642, loss_cps: 0.188713
[14:22:59.065] iteration 28413: total_loss: 0.314785, loss_sup: 0.115668, loss_mps: 0.075518, loss_cps: 0.123599
[14:22:59.211] iteration 28414: total_loss: 0.765924, loss_sup: 0.139557, loss_mps: 0.198096, loss_cps: 0.428271
[14:22:59.357] iteration 28415: total_loss: 0.202691, loss_sup: 0.030478, loss_mps: 0.063038, loss_cps: 0.109175
[14:22:59.504] iteration 28416: total_loss: 0.309760, loss_sup: 0.052295, loss_mps: 0.088851, loss_cps: 0.168614
[14:22:59.650] iteration 28417: total_loss: 0.232404, loss_sup: 0.001085, loss_mps: 0.083103, loss_cps: 0.148216
[14:22:59.796] iteration 28418: total_loss: 0.335678, loss_sup: 0.009659, loss_mps: 0.108270, loss_cps: 0.217749
[14:22:59.942] iteration 28419: total_loss: 0.225829, loss_sup: 0.008155, loss_mps: 0.080159, loss_cps: 0.137515
[14:23:00.088] iteration 28420: total_loss: 0.266117, loss_sup: 0.050591, loss_mps: 0.076525, loss_cps: 0.139001
[14:23:00.234] iteration 28421: total_loss: 0.616417, loss_sup: 0.191444, loss_mps: 0.146268, loss_cps: 0.278704
[14:23:00.380] iteration 28422: total_loss: 0.345287, loss_sup: 0.016564, loss_mps: 0.112323, loss_cps: 0.216400
[14:23:00.527] iteration 28423: total_loss: 0.258728, loss_sup: 0.015053, loss_mps: 0.083753, loss_cps: 0.159921
[14:23:00.595] iteration 28424: total_loss: 0.759946, loss_sup: 0.117146, loss_mps: 0.216082, loss_cps: 0.426718
[14:23:01.804] iteration 28425: total_loss: 0.481863, loss_sup: 0.117710, loss_mps: 0.115076, loss_cps: 0.249078
[14:23:01.954] iteration 28426: total_loss: 0.535680, loss_sup: 0.073901, loss_mps: 0.147661, loss_cps: 0.314119
[14:23:02.102] iteration 28427: total_loss: 0.287382, loss_sup: 0.042016, loss_mps: 0.085712, loss_cps: 0.159655
[14:23:02.249] iteration 28428: total_loss: 0.349600, loss_sup: 0.006468, loss_mps: 0.112193, loss_cps: 0.230939
[14:23:02.396] iteration 28429: total_loss: 0.249746, loss_sup: 0.040216, loss_mps: 0.072542, loss_cps: 0.136989
[14:23:02.543] iteration 28430: total_loss: 0.238166, loss_sup: 0.009175, loss_mps: 0.082498, loss_cps: 0.146493
[14:23:02.689] iteration 28431: total_loss: 0.283376, loss_sup: 0.059423, loss_mps: 0.080214, loss_cps: 0.143739
[14:23:02.840] iteration 28432: total_loss: 0.524814, loss_sup: 0.199396, loss_mps: 0.111964, loss_cps: 0.213453
[14:23:02.989] iteration 28433: total_loss: 0.183594, loss_sup: 0.004625, loss_mps: 0.064409, loss_cps: 0.114560
[14:23:03.140] iteration 28434: total_loss: 0.233963, loss_sup: 0.035531, loss_mps: 0.071038, loss_cps: 0.127394
[14:23:03.288] iteration 28435: total_loss: 0.353303, loss_sup: 0.036212, loss_mps: 0.113226, loss_cps: 0.203865
[14:23:03.435] iteration 28436: total_loss: 0.174562, loss_sup: 0.017802, loss_mps: 0.057261, loss_cps: 0.099499
[14:23:03.581] iteration 28437: total_loss: 0.222546, loss_sup: 0.015076, loss_mps: 0.075780, loss_cps: 0.131690
[14:23:03.728] iteration 28438: total_loss: 0.224674, loss_sup: 0.066421, loss_mps: 0.060667, loss_cps: 0.097586
[14:23:03.874] iteration 28439: total_loss: 0.234469, loss_sup: 0.051014, loss_mps: 0.067978, loss_cps: 0.115477
[14:23:04.023] iteration 28440: total_loss: 0.416714, loss_sup: 0.012806, loss_mps: 0.135037, loss_cps: 0.268871
[14:23:04.171] iteration 28441: total_loss: 0.264661, loss_sup: 0.012152, loss_mps: 0.093089, loss_cps: 0.159419
[14:23:04.318] iteration 28442: total_loss: 0.194657, loss_sup: 0.001232, loss_mps: 0.070700, loss_cps: 0.122725
[14:23:04.465] iteration 28443: total_loss: 0.281528, loss_sup: 0.014827, loss_mps: 0.094760, loss_cps: 0.171941
[14:23:04.611] iteration 28444: total_loss: 0.186488, loss_sup: 0.021174, loss_mps: 0.063604, loss_cps: 0.101710
[14:23:04.758] iteration 28445: total_loss: 0.280075, loss_sup: 0.010230, loss_mps: 0.094140, loss_cps: 0.175705
[14:23:04.905] iteration 28446: total_loss: 0.209850, loss_sup: 0.025617, loss_mps: 0.065422, loss_cps: 0.118811
[14:23:05.054] iteration 28447: total_loss: 0.597507, loss_sup: 0.016684, loss_mps: 0.178736, loss_cps: 0.402087
[14:23:05.201] iteration 28448: total_loss: 0.219565, loss_sup: 0.003248, loss_mps: 0.079562, loss_cps: 0.136755
[14:23:05.348] iteration 28449: total_loss: 0.240165, loss_sup: 0.007025, loss_mps: 0.083673, loss_cps: 0.149467
[14:23:05.494] iteration 28450: total_loss: 0.287851, loss_sup: 0.007381, loss_mps: 0.094410, loss_cps: 0.186060
[14:23:05.643] iteration 28451: total_loss: 0.248486, loss_sup: 0.054892, loss_mps: 0.069018, loss_cps: 0.124576
[14:23:05.790] iteration 28452: total_loss: 0.327457, loss_sup: 0.064706, loss_mps: 0.094227, loss_cps: 0.168523
[14:23:05.936] iteration 28453: total_loss: 0.263057, loss_sup: 0.003073, loss_mps: 0.089799, loss_cps: 0.170185
[14:23:06.083] iteration 28454: total_loss: 0.155800, loss_sup: 0.009206, loss_mps: 0.054777, loss_cps: 0.091817
[14:23:06.229] iteration 28455: total_loss: 0.293558, loss_sup: 0.010491, loss_mps: 0.099064, loss_cps: 0.184003
[14:23:06.375] iteration 28456: total_loss: 0.298040, loss_sup: 0.101837, loss_mps: 0.075197, loss_cps: 0.121006
[14:23:06.522] iteration 28457: total_loss: 0.329508, loss_sup: 0.008663, loss_mps: 0.110373, loss_cps: 0.210472
[14:23:06.668] iteration 28458: total_loss: 0.283653, loss_sup: 0.009978, loss_mps: 0.100481, loss_cps: 0.173194
[14:23:06.815] iteration 28459: total_loss: 0.328948, loss_sup: 0.010827, loss_mps: 0.107009, loss_cps: 0.211113
[14:23:06.961] iteration 28460: total_loss: 0.160238, loss_sup: 0.005414, loss_mps: 0.057642, loss_cps: 0.097183
[14:23:07.107] iteration 28461: total_loss: 0.289137, loss_sup: 0.036792, loss_mps: 0.095322, loss_cps: 0.157022
[14:23:07.255] iteration 28462: total_loss: 0.230567, loss_sup: 0.002429, loss_mps: 0.085600, loss_cps: 0.142538
[14:23:07.401] iteration 28463: total_loss: 0.285818, loss_sup: 0.006089, loss_mps: 0.095549, loss_cps: 0.184180
[14:23:07.547] iteration 28464: total_loss: 0.469216, loss_sup: 0.076450, loss_mps: 0.133016, loss_cps: 0.259750
[14:23:07.694] iteration 28465: total_loss: 0.258414, loss_sup: 0.034747, loss_mps: 0.083146, loss_cps: 0.140521
[14:23:07.846] iteration 28466: total_loss: 0.208094, loss_sup: 0.001316, loss_mps: 0.073789, loss_cps: 0.132989
[14:23:07.992] iteration 28467: total_loss: 0.336164, loss_sup: 0.015782, loss_mps: 0.110803, loss_cps: 0.209579
[14:23:08.139] iteration 28468: total_loss: 0.455717, loss_sup: 0.077632, loss_mps: 0.131904, loss_cps: 0.246182
[14:23:08.285] iteration 28469: total_loss: 0.184288, loss_sup: 0.017017, loss_mps: 0.061811, loss_cps: 0.105459
[14:23:08.435] iteration 28470: total_loss: 0.239896, loss_sup: 0.020151, loss_mps: 0.082259, loss_cps: 0.137487
[14:23:08.582] iteration 28471: total_loss: 0.268987, loss_sup: 0.003463, loss_mps: 0.092996, loss_cps: 0.172528
[14:23:08.729] iteration 28472: total_loss: 0.662582, loss_sup: 0.255091, loss_mps: 0.133848, loss_cps: 0.273644
[14:23:08.878] iteration 28473: total_loss: 0.548971, loss_sup: 0.220039, loss_mps: 0.107819, loss_cps: 0.221113
[14:23:09.025] iteration 28474: total_loss: 0.187191, loss_sup: 0.014833, loss_mps: 0.067876, loss_cps: 0.104482
[14:23:09.174] iteration 28475: total_loss: 0.265373, loss_sup: 0.043423, loss_mps: 0.078192, loss_cps: 0.143758
[14:23:09.321] iteration 28476: total_loss: 0.174465, loss_sup: 0.001153, loss_mps: 0.061101, loss_cps: 0.112211
[14:23:09.468] iteration 28477: total_loss: 0.267154, loss_sup: 0.010961, loss_mps: 0.086721, loss_cps: 0.169472
[14:23:09.614] iteration 28478: total_loss: 0.323217, loss_sup: 0.145413, loss_mps: 0.064280, loss_cps: 0.113524
[14:23:09.765] iteration 28479: total_loss: 0.337911, loss_sup: 0.018987, loss_mps: 0.107543, loss_cps: 0.211381
[14:23:09.911] iteration 28480: total_loss: 0.492660, loss_sup: 0.064157, loss_mps: 0.143388, loss_cps: 0.285114
[14:23:10.058] iteration 28481: total_loss: 0.271003, loss_sup: 0.067169, loss_mps: 0.076197, loss_cps: 0.127637
[14:23:10.211] iteration 28482: total_loss: 0.415393, loss_sup: 0.060224, loss_mps: 0.116357, loss_cps: 0.238812
[14:23:10.357] iteration 28483: total_loss: 0.520287, loss_sup: 0.056860, loss_mps: 0.150823, loss_cps: 0.312603
[14:23:10.504] iteration 28484: total_loss: 0.186832, loss_sup: 0.004824, loss_mps: 0.068198, loss_cps: 0.113810
[14:23:10.652] iteration 28485: total_loss: 0.194796, loss_sup: 0.004448, loss_mps: 0.068115, loss_cps: 0.122233
[14:23:10.801] iteration 28486: total_loss: 0.209275, loss_sup: 0.001451, loss_mps: 0.073785, loss_cps: 0.134038
[14:23:10.947] iteration 28487: total_loss: 0.442933, loss_sup: 0.030879, loss_mps: 0.141594, loss_cps: 0.270460
[14:23:11.095] iteration 28488: total_loss: 0.230080, loss_sup: 0.005484, loss_mps: 0.078740, loss_cps: 0.145856
[14:23:11.243] iteration 28489: total_loss: 0.435202, loss_sup: 0.013550, loss_mps: 0.130405, loss_cps: 0.291248
[14:23:11.389] iteration 28490: total_loss: 0.243554, loss_sup: 0.020184, loss_mps: 0.078097, loss_cps: 0.145273
[14:23:11.536] iteration 28491: total_loss: 0.320129, loss_sup: 0.022476, loss_mps: 0.097146, loss_cps: 0.200508
[14:23:11.682] iteration 28492: total_loss: 0.178384, loss_sup: 0.010448, loss_mps: 0.062199, loss_cps: 0.105737
[14:23:11.830] iteration 28493: total_loss: 0.807389, loss_sup: 0.118216, loss_mps: 0.217196, loss_cps: 0.471976
[14:23:11.977] iteration 28494: total_loss: 0.164173, loss_sup: 0.027711, loss_mps: 0.054496, loss_cps: 0.081966
[14:23:12.127] iteration 28495: total_loss: 0.454006, loss_sup: 0.076823, loss_mps: 0.123519, loss_cps: 0.253663
[14:23:12.274] iteration 28496: total_loss: 0.248404, loss_sup: 0.013025, loss_mps: 0.082312, loss_cps: 0.153066
[14:23:12.421] iteration 28497: total_loss: 0.119785, loss_sup: 0.007842, loss_mps: 0.042811, loss_cps: 0.069133
[14:23:12.569] iteration 28498: total_loss: 0.172225, loss_sup: 0.054344, loss_mps: 0.043346, loss_cps: 0.074534
[14:23:12.716] iteration 28499: total_loss: 0.257981, loss_sup: 0.016977, loss_mps: 0.087070, loss_cps: 0.153934
[14:23:12.863] iteration 28500: total_loss: 0.541119, loss_sup: 0.072633, loss_mps: 0.141846, loss_cps: 0.326639
[14:23:12.863] Evaluation Started ==>
[14:23:24.161] ==> valid iteration 28500: unet metrics: {'dc': 0.6670734239044557, 'jc': 0.5526911473698396, 'pre': 0.8086371712044355, 'hd': 5.299256417148675}, ynet metrics: {'dc': 0.6400080976058631, 'jc': 0.528976179718741, 'pre': 0.8053869508311103, 'hd': 5.267358179958571}.
[14:23:24.163] Evaluation Finished!⏹️
[14:23:24.314] iteration 28501: total_loss: 0.321567, loss_sup: 0.009415, loss_mps: 0.111583, loss_cps: 0.200569
[14:23:24.463] iteration 28502: total_loss: 0.222874, loss_sup: 0.012017, loss_mps: 0.075014, loss_cps: 0.135844
[14:23:24.610] iteration 28503: total_loss: 0.997540, loss_sup: 0.128571, loss_mps: 0.267530, loss_cps: 0.601440
[14:23:24.758] iteration 28504: total_loss: 0.196884, loss_sup: 0.006412, loss_mps: 0.068405, loss_cps: 0.122067
[14:23:24.904] iteration 28505: total_loss: 0.588659, loss_sup: 0.187057, loss_mps: 0.127030, loss_cps: 0.274572
[14:23:25.050] iteration 28506: total_loss: 0.177983, loss_sup: 0.011841, loss_mps: 0.059848, loss_cps: 0.106294
[14:23:25.198] iteration 28507: total_loss: 0.183060, loss_sup: 0.004066, loss_mps: 0.066442, loss_cps: 0.112552
[14:23:25.346] iteration 28508: total_loss: 0.259392, loss_sup: 0.059147, loss_mps: 0.068959, loss_cps: 0.131285
[14:23:25.492] iteration 28509: total_loss: 0.239560, loss_sup: 0.111037, loss_mps: 0.049076, loss_cps: 0.079447
[14:23:25.639] iteration 28510: total_loss: 0.403873, loss_sup: 0.001109, loss_mps: 0.125294, loss_cps: 0.277470
[14:23:25.784] iteration 28511: total_loss: 0.307265, loss_sup: 0.014456, loss_mps: 0.097618, loss_cps: 0.195190
[14:23:25.930] iteration 28512: total_loss: 0.262291, loss_sup: 0.055784, loss_mps: 0.075117, loss_cps: 0.131390
[14:23:26.076] iteration 28513: total_loss: 0.275588, loss_sup: 0.024422, loss_mps: 0.086858, loss_cps: 0.164308
[14:23:26.224] iteration 28514: total_loss: 0.332727, loss_sup: 0.004740, loss_mps: 0.099405, loss_cps: 0.228581
[14:23:26.370] iteration 28515: total_loss: 0.340664, loss_sup: 0.005592, loss_mps: 0.109451, loss_cps: 0.225622
[14:23:26.517] iteration 28516: total_loss: 0.392018, loss_sup: 0.067586, loss_mps: 0.112535, loss_cps: 0.211897
[14:23:26.664] iteration 28517: total_loss: 0.187856, loss_sup: 0.015976, loss_mps: 0.063958, loss_cps: 0.107922
[14:23:26.811] iteration 28518: total_loss: 0.296388, loss_sup: 0.057613, loss_mps: 0.082578, loss_cps: 0.156196
[14:23:26.957] iteration 28519: total_loss: 0.275530, loss_sup: 0.048253, loss_mps: 0.080800, loss_cps: 0.146477
[14:23:27.106] iteration 28520: total_loss: 0.172043, loss_sup: 0.006573, loss_mps: 0.060055, loss_cps: 0.105414
[14:23:27.254] iteration 28521: total_loss: 0.200054, loss_sup: 0.003721, loss_mps: 0.075862, loss_cps: 0.120470
[14:23:27.399] iteration 28522: total_loss: 0.478632, loss_sup: 0.006312, loss_mps: 0.150512, loss_cps: 0.321807
[14:23:27.545] iteration 28523: total_loss: 0.223485, loss_sup: 0.027452, loss_mps: 0.070844, loss_cps: 0.125189
[14:23:27.692] iteration 28524: total_loss: 0.271538, loss_sup: 0.040773, loss_mps: 0.081357, loss_cps: 0.149407
[14:23:27.839] iteration 28525: total_loss: 0.562001, loss_sup: 0.060645, loss_mps: 0.161315, loss_cps: 0.340042
[14:23:27.985] iteration 28526: total_loss: 0.169308, loss_sup: 0.004134, loss_mps: 0.062594, loss_cps: 0.102579
[14:23:28.131] iteration 28527: total_loss: 0.213933, loss_sup: 0.026333, loss_mps: 0.071335, loss_cps: 0.116265
[14:23:28.277] iteration 28528: total_loss: 0.188851, loss_sup: 0.055722, loss_mps: 0.048882, loss_cps: 0.084247
[14:23:28.425] iteration 28529: total_loss: 0.145909, loss_sup: 0.001009, loss_mps: 0.055607, loss_cps: 0.089292
[14:23:28.571] iteration 28530: total_loss: 0.228411, loss_sup: 0.027281, loss_mps: 0.067985, loss_cps: 0.133145
[14:23:28.716] iteration 28531: total_loss: 0.273084, loss_sup: 0.006768, loss_mps: 0.097065, loss_cps: 0.169251
[14:23:28.862] iteration 28532: total_loss: 0.302491, loss_sup: 0.008943, loss_mps: 0.100581, loss_cps: 0.192967
[14:23:29.008] iteration 28533: total_loss: 0.405945, loss_sup: 0.111687, loss_mps: 0.101314, loss_cps: 0.192944
[14:23:29.154] iteration 28534: total_loss: 0.442436, loss_sup: 0.019931, loss_mps: 0.137112, loss_cps: 0.285393
[14:23:29.299] iteration 28535: total_loss: 0.210340, loss_sup: 0.032815, loss_mps: 0.063291, loss_cps: 0.114234
[14:23:29.446] iteration 28536: total_loss: 0.289417, loss_sup: 0.012610, loss_mps: 0.090568, loss_cps: 0.186238
[14:23:29.592] iteration 28537: total_loss: 0.254995, loss_sup: 0.066872, loss_mps: 0.071971, loss_cps: 0.116152
[14:23:29.738] iteration 28538: total_loss: 0.185090, loss_sup: 0.028145, loss_mps: 0.060369, loss_cps: 0.096575
[14:23:29.883] iteration 28539: total_loss: 0.347972, loss_sup: 0.050785, loss_mps: 0.107417, loss_cps: 0.189770
[14:23:30.029] iteration 28540: total_loss: 0.266622, loss_sup: 0.023355, loss_mps: 0.087637, loss_cps: 0.155630
[14:23:30.175] iteration 28541: total_loss: 0.426464, loss_sup: 0.040803, loss_mps: 0.130072, loss_cps: 0.255589
[14:23:30.320] iteration 28542: total_loss: 0.591559, loss_sup: 0.076694, loss_mps: 0.165461, loss_cps: 0.349404
[14:23:30.466] iteration 28543: total_loss: 0.342009, loss_sup: 0.015536, loss_mps: 0.116024, loss_cps: 0.210449
[14:23:30.612] iteration 28544: total_loss: 0.397363, loss_sup: 0.024200, loss_mps: 0.132861, loss_cps: 0.240302
[14:23:30.757] iteration 28545: total_loss: 0.350509, loss_sup: 0.035093, loss_mps: 0.106457, loss_cps: 0.208960
[14:23:30.902] iteration 28546: total_loss: 0.175566, loss_sup: 0.007649, loss_mps: 0.063520, loss_cps: 0.104397
[14:23:31.048] iteration 28547: total_loss: 0.324176, loss_sup: 0.055996, loss_mps: 0.098359, loss_cps: 0.169821
[14:23:31.195] iteration 28548: total_loss: 0.421340, loss_sup: 0.191241, loss_mps: 0.081920, loss_cps: 0.148180
[14:23:31.340] iteration 28549: total_loss: 0.494058, loss_sup: 0.070835, loss_mps: 0.135768, loss_cps: 0.287454
[14:23:31.486] iteration 28550: total_loss: 0.368968, loss_sup: 0.006654, loss_mps: 0.124814, loss_cps: 0.237501
[14:23:31.631] iteration 28551: total_loss: 0.503707, loss_sup: 0.064287, loss_mps: 0.142972, loss_cps: 0.296448
[14:23:31.778] iteration 28552: total_loss: 0.191353, loss_sup: 0.001205, loss_mps: 0.068366, loss_cps: 0.121781
[14:23:31.924] iteration 28553: total_loss: 0.240019, loss_sup: 0.056807, loss_mps: 0.073244, loss_cps: 0.109967
[14:23:32.070] iteration 28554: total_loss: 0.318806, loss_sup: 0.084507, loss_mps: 0.080704, loss_cps: 0.153595
[14:23:32.216] iteration 28555: total_loss: 0.405547, loss_sup: 0.016367, loss_mps: 0.131389, loss_cps: 0.257790
[14:23:32.362] iteration 28556: total_loss: 0.270046, loss_sup: 0.008896, loss_mps: 0.093820, loss_cps: 0.167330
[14:23:32.508] iteration 28557: total_loss: 0.267126, loss_sup: 0.032781, loss_mps: 0.083688, loss_cps: 0.150658
[14:23:32.659] iteration 28558: total_loss: 0.315665, loss_sup: 0.025771, loss_mps: 0.099138, loss_cps: 0.190755
[14:23:32.806] iteration 28559: total_loss: 0.289372, loss_sup: 0.150641, loss_mps: 0.052099, loss_cps: 0.086632
[14:23:32.952] iteration 28560: total_loss: 0.208339, loss_sup: 0.009960, loss_mps: 0.071894, loss_cps: 0.126485
[14:23:33.097] iteration 28561: total_loss: 0.451398, loss_sup: 0.103497, loss_mps: 0.123133, loss_cps: 0.224769
[14:23:33.243] iteration 28562: total_loss: 0.432896, loss_sup: 0.047800, loss_mps: 0.135907, loss_cps: 0.249189
[14:23:33.389] iteration 28563: total_loss: 0.140015, loss_sup: 0.002612, loss_mps: 0.052302, loss_cps: 0.085101
[14:23:33.535] iteration 28564: total_loss: 0.408122, loss_sup: 0.028372, loss_mps: 0.115112, loss_cps: 0.264639
[14:23:33.684] iteration 28565: total_loss: 0.367222, loss_sup: 0.065257, loss_mps: 0.105776, loss_cps: 0.196190
[14:23:33.834] iteration 28566: total_loss: 0.188630, loss_sup: 0.021205, loss_mps: 0.064333, loss_cps: 0.103092
[14:23:33.980] iteration 28567: total_loss: 0.283680, loss_sup: 0.026813, loss_mps: 0.090965, loss_cps: 0.165902
[14:23:34.126] iteration 28568: total_loss: 0.242082, loss_sup: 0.052509, loss_mps: 0.068396, loss_cps: 0.121178
[14:23:34.274] iteration 28569: total_loss: 0.370279, loss_sup: 0.192177, loss_mps: 0.065440, loss_cps: 0.112662
[14:23:34.420] iteration 28570: total_loss: 0.180168, loss_sup: 0.004119, loss_mps: 0.062927, loss_cps: 0.113123
[14:23:34.571] iteration 28571: total_loss: 0.240552, loss_sup: 0.022775, loss_mps: 0.081513, loss_cps: 0.136264
[14:23:34.719] iteration 28572: total_loss: 0.369856, loss_sup: 0.018280, loss_mps: 0.129653, loss_cps: 0.221923
[14:23:34.868] iteration 28573: total_loss: 0.381044, loss_sup: 0.009162, loss_mps: 0.130326, loss_cps: 0.241556
[14:23:35.014] iteration 28574: total_loss: 0.269374, loss_sup: 0.014804, loss_mps: 0.094826, loss_cps: 0.159744
[14:23:35.160] iteration 28575: total_loss: 0.232316, loss_sup: 0.033071, loss_mps: 0.071010, loss_cps: 0.128235
[14:23:35.306] iteration 28576: total_loss: 0.266920, loss_sup: 0.018839, loss_mps: 0.085339, loss_cps: 0.162742
[14:23:35.453] iteration 28577: total_loss: 0.197166, loss_sup: 0.016464, loss_mps: 0.063955, loss_cps: 0.116747
[14:23:35.599] iteration 28578: total_loss: 0.298500, loss_sup: 0.030194, loss_mps: 0.098856, loss_cps: 0.169450
[14:23:35.745] iteration 28579: total_loss: 0.340440, loss_sup: 0.103998, loss_mps: 0.087035, loss_cps: 0.149406
[14:23:35.895] iteration 28580: total_loss: 0.585960, loss_sup: 0.086905, loss_mps: 0.158191, loss_cps: 0.340863
[14:23:36.042] iteration 28581: total_loss: 0.201239, loss_sup: 0.005292, loss_mps: 0.074792, loss_cps: 0.121156
[14:23:36.189] iteration 28582: total_loss: 0.351818, loss_sup: 0.086054, loss_mps: 0.093482, loss_cps: 0.172281
[14:23:36.335] iteration 28583: total_loss: 0.275463, loss_sup: 0.008108, loss_mps: 0.090983, loss_cps: 0.176371
[14:23:36.480] iteration 28584: total_loss: 0.261863, loss_sup: 0.008882, loss_mps: 0.089143, loss_cps: 0.163837
[14:23:36.628] iteration 28585: total_loss: 0.152802, loss_sup: 0.002497, loss_mps: 0.057923, loss_cps: 0.092381
[14:23:36.777] iteration 28586: total_loss: 0.227974, loss_sup: 0.001484, loss_mps: 0.082805, loss_cps: 0.143684
[14:23:36.923] iteration 28587: total_loss: 0.228268, loss_sup: 0.095041, loss_mps: 0.050902, loss_cps: 0.082325
[14:23:37.072] iteration 28588: total_loss: 0.206794, loss_sup: 0.016456, loss_mps: 0.070273, loss_cps: 0.120066
[14:23:37.220] iteration 28589: total_loss: 0.303579, loss_sup: 0.041053, loss_mps: 0.097274, loss_cps: 0.165252
[14:23:37.371] iteration 28590: total_loss: 0.237814, loss_sup: 0.020348, loss_mps: 0.076263, loss_cps: 0.141203
[14:23:37.520] iteration 28591: total_loss: 0.185262, loss_sup: 0.022190, loss_mps: 0.059359, loss_cps: 0.103714
[14:23:37.666] iteration 28592: total_loss: 0.233610, loss_sup: 0.025868, loss_mps: 0.075961, loss_cps: 0.131782
[14:23:37.812] iteration 28593: total_loss: 0.325990, loss_sup: 0.047490, loss_mps: 0.100614, loss_cps: 0.177886
[14:23:37.960] iteration 28594: total_loss: 0.303980, loss_sup: 0.026885, loss_mps: 0.095229, loss_cps: 0.181866
[14:23:38.107] iteration 28595: total_loss: 0.287683, loss_sup: 0.036237, loss_mps: 0.085358, loss_cps: 0.166089
[14:23:38.255] iteration 28596: total_loss: 0.306631, loss_sup: 0.122941, loss_mps: 0.070347, loss_cps: 0.113343
[14:23:38.403] iteration 28597: total_loss: 0.395490, loss_sup: 0.024032, loss_mps: 0.123376, loss_cps: 0.248082
[14:23:38.552] iteration 28598: total_loss: 0.298441, loss_sup: 0.017545, loss_mps: 0.096127, loss_cps: 0.184768
[14:23:38.700] iteration 28599: total_loss: 0.262771, loss_sup: 0.003036, loss_mps: 0.091381, loss_cps: 0.168354
[14:23:38.849] iteration 28600: total_loss: 0.384175, loss_sup: 0.032379, loss_mps: 0.112345, loss_cps: 0.239451
[14:23:38.849] Evaluation Started ==>
[14:23:50.155] ==> valid iteration 28600: unet metrics: {'dc': 0.6767969236034929, 'jc': 0.561836704669674, 'pre': 0.8159144324426707, 'hd': 5.299339569408791}, ynet metrics: {'dc': 0.620741343860745, 'jc': 0.5099810779997183, 'pre': 0.7946979518753918, 'hd': 5.348310783817902}.
[14:23:50.159] Evaluation Finished!⏹️
[14:23:50.311] iteration 28601: total_loss: 0.177873, loss_sup: 0.022353, loss_mps: 0.058100, loss_cps: 0.097421
[14:23:50.459] iteration 28602: total_loss: 0.424012, loss_sup: 0.051189, loss_mps: 0.123719, loss_cps: 0.249104
[14:23:50.607] iteration 28603: total_loss: 0.260154, loss_sup: 0.031561, loss_mps: 0.083882, loss_cps: 0.144711
[14:23:50.753] iteration 28604: total_loss: 0.391232, loss_sup: 0.016983, loss_mps: 0.125968, loss_cps: 0.248281
[14:23:50.898] iteration 28605: total_loss: 0.270648, loss_sup: 0.018634, loss_mps: 0.089584, loss_cps: 0.162430
[14:23:51.044] iteration 28606: total_loss: 0.233216, loss_sup: 0.043582, loss_mps: 0.068527, loss_cps: 0.121106
[14:23:51.190] iteration 28607: total_loss: 0.160764, loss_sup: 0.001062, loss_mps: 0.060744, loss_cps: 0.098958
[14:23:51.342] iteration 28608: total_loss: 0.271938, loss_sup: 0.041804, loss_mps: 0.079869, loss_cps: 0.150266
[14:23:51.491] iteration 28609: total_loss: 0.244751, loss_sup: 0.006289, loss_mps: 0.084820, loss_cps: 0.153641
[14:23:51.638] iteration 28610: total_loss: 0.569365, loss_sup: 0.127160, loss_mps: 0.139376, loss_cps: 0.302829
[14:23:51.785] iteration 28611: total_loss: 0.355941, loss_sup: 0.067176, loss_mps: 0.100964, loss_cps: 0.187801
[14:23:51.931] iteration 28612: total_loss: 0.334952, loss_sup: 0.058304, loss_mps: 0.095634, loss_cps: 0.181014
[14:23:52.079] iteration 28613: total_loss: 0.174671, loss_sup: 0.003956, loss_mps: 0.065341, loss_cps: 0.105374
[14:23:52.225] iteration 28614: total_loss: 0.299881, loss_sup: 0.009809, loss_mps: 0.098018, loss_cps: 0.192054
[14:23:52.372] iteration 28615: total_loss: 0.165568, loss_sup: 0.000152, loss_mps: 0.058430, loss_cps: 0.106986
[14:23:52.518] iteration 28616: total_loss: 0.367371, loss_sup: 0.144427, loss_mps: 0.087380, loss_cps: 0.135564
[14:23:52.664] iteration 28617: total_loss: 0.161278, loss_sup: 0.006504, loss_mps: 0.056176, loss_cps: 0.098598
[14:23:52.813] iteration 28618: total_loss: 0.322270, loss_sup: 0.030746, loss_mps: 0.098712, loss_cps: 0.192811
[14:23:52.960] iteration 28619: total_loss: 0.273515, loss_sup: 0.044769, loss_mps: 0.086362, loss_cps: 0.142383
[14:23:53.108] iteration 28620: total_loss: 0.180352, loss_sup: 0.000162, loss_mps: 0.065325, loss_cps: 0.114865
[14:23:53.254] iteration 28621: total_loss: 0.374207, loss_sup: 0.102739, loss_mps: 0.092018, loss_cps: 0.179451
[14:23:53.402] iteration 28622: total_loss: 0.208225, loss_sup: 0.002569, loss_mps: 0.071710, loss_cps: 0.133946
[14:23:53.549] iteration 28623: total_loss: 0.277700, loss_sup: 0.079818, loss_mps: 0.079815, loss_cps: 0.118068
[14:23:53.695] iteration 28624: total_loss: 0.199697, loss_sup: 0.023233, loss_mps: 0.066891, loss_cps: 0.109572
[14:23:53.842] iteration 28625: total_loss: 0.223072, loss_sup: 0.029095, loss_mps: 0.069899, loss_cps: 0.124078
[14:23:53.988] iteration 28626: total_loss: 0.356605, loss_sup: 0.068427, loss_mps: 0.094581, loss_cps: 0.193598
[14:23:54.134] iteration 28627: total_loss: 0.197129, loss_sup: 0.006128, loss_mps: 0.069146, loss_cps: 0.121856
[14:23:54.280] iteration 28628: total_loss: 0.199832, loss_sup: 0.034217, loss_mps: 0.059582, loss_cps: 0.106034
[14:23:54.427] iteration 28629: total_loss: 0.369740, loss_sup: 0.015452, loss_mps: 0.119393, loss_cps: 0.234895
[14:23:54.573] iteration 28630: total_loss: 0.319859, loss_sup: 0.057937, loss_mps: 0.093748, loss_cps: 0.168174
[14:23:54.719] iteration 28631: total_loss: 0.355896, loss_sup: 0.025592, loss_mps: 0.113300, loss_cps: 0.217003
[14:23:54.865] iteration 28632: total_loss: 0.244160, loss_sup: 0.029017, loss_mps: 0.078308, loss_cps: 0.136835
[14:23:55.010] iteration 28633: total_loss: 0.292597, loss_sup: 0.080177, loss_mps: 0.077626, loss_cps: 0.134794
[14:23:55.156] iteration 28634: total_loss: 0.283130, loss_sup: 0.020100, loss_mps: 0.098825, loss_cps: 0.164205
[14:23:55.304] iteration 28635: total_loss: 0.348237, loss_sup: 0.043001, loss_mps: 0.103456, loss_cps: 0.201780
[14:23:55.450] iteration 28636: total_loss: 0.342317, loss_sup: 0.034712, loss_mps: 0.105431, loss_cps: 0.202174
[14:23:55.595] iteration 28637: total_loss: 0.294513, loss_sup: 0.003896, loss_mps: 0.103745, loss_cps: 0.186872
[14:23:55.742] iteration 28638: total_loss: 0.295108, loss_sup: 0.014561, loss_mps: 0.096481, loss_cps: 0.184067
[14:23:55.888] iteration 28639: total_loss: 0.287781, loss_sup: 0.009001, loss_mps: 0.098993, loss_cps: 0.179788
[14:23:56.033] iteration 28640: total_loss: 0.341590, loss_sup: 0.028540, loss_mps: 0.105801, loss_cps: 0.207249
[14:23:56.180] iteration 28641: total_loss: 0.330281, loss_sup: 0.106905, loss_mps: 0.080370, loss_cps: 0.143006
[14:23:56.326] iteration 28642: total_loss: 0.262421, loss_sup: 0.033199, loss_mps: 0.079516, loss_cps: 0.149705
[14:23:56.472] iteration 28643: total_loss: 0.378469, loss_sup: 0.080954, loss_mps: 0.110408, loss_cps: 0.187108
[14:23:56.618] iteration 28644: total_loss: 0.565514, loss_sup: 0.282512, loss_mps: 0.099548, loss_cps: 0.183453
[14:23:56.764] iteration 28645: total_loss: 0.213313, loss_sup: 0.010584, loss_mps: 0.075046, loss_cps: 0.127683
[14:23:56.910] iteration 28646: total_loss: 0.633232, loss_sup: 0.090603, loss_mps: 0.169462, loss_cps: 0.373168
[14:23:57.056] iteration 28647: total_loss: 0.337964, loss_sup: 0.053111, loss_mps: 0.099672, loss_cps: 0.185180
[14:23:57.202] iteration 28648: total_loss: 0.291707, loss_sup: 0.037659, loss_mps: 0.088287, loss_cps: 0.165760
[14:23:57.348] iteration 28649: total_loss: 0.232775, loss_sup: 0.079408, loss_mps: 0.055937, loss_cps: 0.097429
[14:23:57.494] iteration 28650: total_loss: 0.258934, loss_sup: 0.020875, loss_mps: 0.082587, loss_cps: 0.155472
[14:23:57.640] iteration 28651: total_loss: 0.556406, loss_sup: 0.066679, loss_mps: 0.161003, loss_cps: 0.328724
[14:23:57.786] iteration 28652: total_loss: 0.289603, loss_sup: 0.050880, loss_mps: 0.084792, loss_cps: 0.153931
[14:23:57.932] iteration 28653: total_loss: 0.322105, loss_sup: 0.048663, loss_mps: 0.094172, loss_cps: 0.179269
[14:23:58.079] iteration 28654: total_loss: 0.285186, loss_sup: 0.092041, loss_mps: 0.069511, loss_cps: 0.123633
[14:23:58.225] iteration 28655: total_loss: 0.305691, loss_sup: 0.064302, loss_mps: 0.084819, loss_cps: 0.156570
[14:23:58.371] iteration 28656: total_loss: 0.228464, loss_sup: 0.020969, loss_mps: 0.076263, loss_cps: 0.131231
[14:23:58.517] iteration 28657: total_loss: 0.406056, loss_sup: 0.020116, loss_mps: 0.126992, loss_cps: 0.258948
[14:23:58.663] iteration 28658: total_loss: 0.358351, loss_sup: 0.033897, loss_mps: 0.110474, loss_cps: 0.213979
[14:23:58.809] iteration 28659: total_loss: 0.250650, loss_sup: 0.083242, loss_mps: 0.063715, loss_cps: 0.103693
[14:23:58.955] iteration 28660: total_loss: 0.158292, loss_sup: 0.010552, loss_mps: 0.054430, loss_cps: 0.093310
[14:23:59.102] iteration 28661: total_loss: 0.270961, loss_sup: 0.036280, loss_mps: 0.085456, loss_cps: 0.149225
[14:23:59.248] iteration 28662: total_loss: 0.214561, loss_sup: 0.033004, loss_mps: 0.070386, loss_cps: 0.111172
[14:23:59.394] iteration 28663: total_loss: 0.267858, loss_sup: 0.029773, loss_mps: 0.082298, loss_cps: 0.155788
[14:23:59.540] iteration 28664: total_loss: 0.525617, loss_sup: 0.078023, loss_mps: 0.152433, loss_cps: 0.295160
[14:23:59.687] iteration 28665: total_loss: 0.269958, loss_sup: 0.036269, loss_mps: 0.082696, loss_cps: 0.150993
[14:23:59.832] iteration 28666: total_loss: 0.199775, loss_sup: 0.000337, loss_mps: 0.071015, loss_cps: 0.128423
[14:23:59.979] iteration 28667: total_loss: 0.220136, loss_sup: 0.018575, loss_mps: 0.071742, loss_cps: 0.129818
[14:24:00.126] iteration 28668: total_loss: 0.304372, loss_sup: 0.011970, loss_mps: 0.106064, loss_cps: 0.186338
[14:24:00.272] iteration 28669: total_loss: 0.198296, loss_sup: 0.033157, loss_mps: 0.062270, loss_cps: 0.102869
[14:24:00.418] iteration 28670: total_loss: 0.539502, loss_sup: 0.043735, loss_mps: 0.163184, loss_cps: 0.332583
[14:24:00.565] iteration 28671: total_loss: 0.168502, loss_sup: 0.025761, loss_mps: 0.053511, loss_cps: 0.089230
[14:24:00.711] iteration 28672: total_loss: 0.252712, loss_sup: 0.043100, loss_mps: 0.076848, loss_cps: 0.132765
[14:24:00.857] iteration 28673: total_loss: 0.234171, loss_sup: 0.027533, loss_mps: 0.074322, loss_cps: 0.132316
[14:24:01.005] iteration 28674: total_loss: 0.299707, loss_sup: 0.032738, loss_mps: 0.093475, loss_cps: 0.173494
[14:24:01.152] iteration 28675: total_loss: 0.350311, loss_sup: 0.028823, loss_mps: 0.119841, loss_cps: 0.201646
[14:24:01.299] iteration 28676: total_loss: 0.188095, loss_sup: 0.020634, loss_mps: 0.059352, loss_cps: 0.108108
[14:24:01.445] iteration 28677: total_loss: 0.298246, loss_sup: 0.036445, loss_mps: 0.091232, loss_cps: 0.170569
[14:24:01.591] iteration 28678: total_loss: 0.141646, loss_sup: 0.008107, loss_mps: 0.050830, loss_cps: 0.082709
[14:24:01.739] iteration 28679: total_loss: 0.292767, loss_sup: 0.038971, loss_mps: 0.087057, loss_cps: 0.166739
[14:24:01.887] iteration 28680: total_loss: 0.160382, loss_sup: 0.014723, loss_mps: 0.054557, loss_cps: 0.091102
[14:24:02.035] iteration 28681: total_loss: 0.152303, loss_sup: 0.025145, loss_mps: 0.049185, loss_cps: 0.077973
[14:24:02.182] iteration 28682: total_loss: 0.249980, loss_sup: 0.004238, loss_mps: 0.085008, loss_cps: 0.160734
[14:24:02.328] iteration 28683: total_loss: 0.213583, loss_sup: 0.003362, loss_mps: 0.073432, loss_cps: 0.136789
[14:24:02.475] iteration 28684: total_loss: 0.229132, loss_sup: 0.057862, loss_mps: 0.065238, loss_cps: 0.106031
[14:24:02.621] iteration 28685: total_loss: 0.401344, loss_sup: 0.097869, loss_mps: 0.103332, loss_cps: 0.200144
[14:24:02.767] iteration 28686: total_loss: 0.295472, loss_sup: 0.105306, loss_mps: 0.069852, loss_cps: 0.120315
[14:24:02.913] iteration 28687: total_loss: 0.177740, loss_sup: 0.000200, loss_mps: 0.067652, loss_cps: 0.109888
[14:24:03.061] iteration 28688: total_loss: 0.393624, loss_sup: 0.026302, loss_mps: 0.121585, loss_cps: 0.245737
[14:24:03.209] iteration 28689: total_loss: 0.302604, loss_sup: 0.006052, loss_mps: 0.103404, loss_cps: 0.193148
[14:24:03.356] iteration 28690: total_loss: 0.416345, loss_sup: 0.022887, loss_mps: 0.127048, loss_cps: 0.266410
[14:24:03.503] iteration 28691: total_loss: 0.399918, loss_sup: 0.090107, loss_mps: 0.104962, loss_cps: 0.204850
[14:24:03.649] iteration 28692: total_loss: 0.375077, loss_sup: 0.102448, loss_mps: 0.105003, loss_cps: 0.167626
[14:24:03.796] iteration 28693: total_loss: 0.646443, loss_sup: 0.291612, loss_mps: 0.120596, loss_cps: 0.234235
[14:24:03.948] iteration 28694: total_loss: 0.203516, loss_sup: 0.003996, loss_mps: 0.070508, loss_cps: 0.129012
[14:24:04.094] iteration 28695: total_loss: 0.282513, loss_sup: 0.013127, loss_mps: 0.096413, loss_cps: 0.172972
[14:24:04.241] iteration 28696: total_loss: 0.254200, loss_sup: 0.050512, loss_mps: 0.072926, loss_cps: 0.130762
[14:24:04.388] iteration 28697: total_loss: 0.279656, loss_sup: 0.047182, loss_mps: 0.086712, loss_cps: 0.145762
[14:24:04.535] iteration 28698: total_loss: 0.310023, loss_sup: 0.091787, loss_mps: 0.076322, loss_cps: 0.141914
[14:24:04.682] iteration 28699: total_loss: 0.211115, loss_sup: 0.005536, loss_mps: 0.075668, loss_cps: 0.129911
[14:24:04.829] iteration 28700: total_loss: 0.254325, loss_sup: 0.023199, loss_mps: 0.084965, loss_cps: 0.146161
[14:24:04.829] Evaluation Started ==>
[14:24:16.210] ==> valid iteration 28700: unet metrics: {'dc': 0.6578314929234527, 'jc': 0.5428890209415588, 'pre': 0.8128074335710412, 'hd': 5.3180114278092585}, ynet metrics: {'dc': 0.6089309770880238, 'jc': 0.49806251608370145, 'pre': 0.7992851844752428, 'hd': 5.395283881798097}.
[14:24:16.212] Evaluation Finished!⏹️
[14:24:16.362] iteration 28701: total_loss: 0.478332, loss_sup: 0.231639, loss_mps: 0.094892, loss_cps: 0.151801
[14:24:16.509] iteration 28702: total_loss: 0.182078, loss_sup: 0.006835, loss_mps: 0.063863, loss_cps: 0.111380
[14:24:16.656] iteration 28703: total_loss: 0.308562, loss_sup: 0.025651, loss_mps: 0.102527, loss_cps: 0.180384
[14:24:16.802] iteration 28704: total_loss: 0.390314, loss_sup: 0.148436, loss_mps: 0.086258, loss_cps: 0.155620
[14:24:16.948] iteration 28705: total_loss: 0.287905, loss_sup: 0.037980, loss_mps: 0.093177, loss_cps: 0.156748
[14:24:17.094] iteration 28706: total_loss: 0.462624, loss_sup: 0.117965, loss_mps: 0.120543, loss_cps: 0.224116
[14:24:17.240] iteration 28707: total_loss: 0.447801, loss_sup: 0.104543, loss_mps: 0.113568, loss_cps: 0.229690
[14:24:17.386] iteration 28708: total_loss: 0.460712, loss_sup: 0.004628, loss_mps: 0.137401, loss_cps: 0.318682
[14:24:17.533] iteration 28709: total_loss: 0.227758, loss_sup: 0.005208, loss_mps: 0.082420, loss_cps: 0.140130
[14:24:17.680] iteration 28710: total_loss: 0.274200, loss_sup: 0.017577, loss_mps: 0.088430, loss_cps: 0.168193
[14:24:17.827] iteration 28711: total_loss: 0.277597, loss_sup: 0.005098, loss_mps: 0.092217, loss_cps: 0.180282
[14:24:17.974] iteration 28712: total_loss: 0.326928, loss_sup: 0.098394, loss_mps: 0.082870, loss_cps: 0.145664
[14:24:18.120] iteration 28713: total_loss: 0.247238, loss_sup: 0.008037, loss_mps: 0.083037, loss_cps: 0.156164
[14:24:18.266] iteration 28714: total_loss: 0.225100, loss_sup: 0.003428, loss_mps: 0.077954, loss_cps: 0.143719
[14:24:18.412] iteration 28715: total_loss: 0.199890, loss_sup: 0.006290, loss_mps: 0.067378, loss_cps: 0.126222
[14:24:18.558] iteration 28716: total_loss: 0.436854, loss_sup: 0.045402, loss_mps: 0.123452, loss_cps: 0.268000
[14:24:18.704] iteration 28717: total_loss: 0.442079, loss_sup: 0.291971, loss_mps: 0.054034, loss_cps: 0.096074
[14:24:18.852] iteration 28718: total_loss: 0.106630, loss_sup: 0.001069, loss_mps: 0.041770, loss_cps: 0.063791
[14:24:18.997] iteration 28719: total_loss: 0.287704, loss_sup: 0.018290, loss_mps: 0.095084, loss_cps: 0.174331
[14:24:19.143] iteration 28720: total_loss: 0.315425, loss_sup: 0.006041, loss_mps: 0.105584, loss_cps: 0.203799
[14:24:19.288] iteration 28721: total_loss: 0.325551, loss_sup: 0.026277, loss_mps: 0.099888, loss_cps: 0.199387
[14:24:19.438] iteration 28722: total_loss: 0.299757, loss_sup: 0.018783, loss_mps: 0.096966, loss_cps: 0.184008
[14:24:19.584] iteration 28723: total_loss: 0.406025, loss_sup: 0.033179, loss_mps: 0.124125, loss_cps: 0.248721
[14:24:19.730] iteration 28724: total_loss: 0.167440, loss_sup: 0.005484, loss_mps: 0.058876, loss_cps: 0.103080
[14:24:19.879] iteration 28725: total_loss: 0.158483, loss_sup: 0.007467, loss_mps: 0.057503, loss_cps: 0.093513
[14:24:20.024] iteration 28726: total_loss: 0.323778, loss_sup: 0.087023, loss_mps: 0.074470, loss_cps: 0.162285
[14:24:20.173] iteration 28727: total_loss: 0.302849, loss_sup: 0.085251, loss_mps: 0.082221, loss_cps: 0.135377
[14:24:20.321] iteration 28728: total_loss: 0.214133, loss_sup: 0.014150, loss_mps: 0.073664, loss_cps: 0.126319
[14:24:20.466] iteration 28729: total_loss: 0.282488, loss_sup: 0.045014, loss_mps: 0.080619, loss_cps: 0.156855
[14:24:20.612] iteration 28730: total_loss: 0.212551, loss_sup: 0.010540, loss_mps: 0.070150, loss_cps: 0.131860
[14:24:20.759] iteration 28731: total_loss: 0.228728, loss_sup: 0.004060, loss_mps: 0.077174, loss_cps: 0.147494
[14:24:20.904] iteration 28732: total_loss: 0.271687, loss_sup: 0.008377, loss_mps: 0.091824, loss_cps: 0.171486
[14:24:21.050] iteration 28733: total_loss: 0.347745, loss_sup: 0.073963, loss_mps: 0.088516, loss_cps: 0.185266
[14:24:21.196] iteration 28734: total_loss: 0.395553, loss_sup: 0.072997, loss_mps: 0.107184, loss_cps: 0.215372
[14:24:21.342] iteration 28735: total_loss: 0.257265, loss_sup: 0.042341, loss_mps: 0.072269, loss_cps: 0.142655
[14:24:21.498] iteration 28736: total_loss: 0.196570, loss_sup: 0.009044, loss_mps: 0.072107, loss_cps: 0.115419
[14:24:21.643] iteration 28737: total_loss: 0.303422, loss_sup: 0.001230, loss_mps: 0.101984, loss_cps: 0.200208
[14:24:21.790] iteration 28738: total_loss: 0.332522, loss_sup: 0.070440, loss_mps: 0.090871, loss_cps: 0.171211
[14:24:21.936] iteration 28739: total_loss: 0.272075, loss_sup: 0.042282, loss_mps: 0.082106, loss_cps: 0.147687
[14:24:22.083] iteration 28740: total_loss: 0.790724, loss_sup: 0.088787, loss_mps: 0.211522, loss_cps: 0.490415
[14:24:22.229] iteration 28741: total_loss: 0.448745, loss_sup: 0.003101, loss_mps: 0.146955, loss_cps: 0.298689
[14:24:22.377] iteration 28742: total_loss: 0.174794, loss_sup: 0.048475, loss_mps: 0.052421, loss_cps: 0.073898
[14:24:22.524] iteration 28743: total_loss: 0.313932, loss_sup: 0.014283, loss_mps: 0.103553, loss_cps: 0.196096
[14:24:22.671] iteration 28744: total_loss: 0.362689, loss_sup: 0.075906, loss_mps: 0.099063, loss_cps: 0.187720
[14:24:22.818] iteration 28745: total_loss: 0.259903, loss_sup: 0.000655, loss_mps: 0.090443, loss_cps: 0.168806
[14:24:22.964] iteration 28746: total_loss: 0.225057, loss_sup: 0.065044, loss_mps: 0.059099, loss_cps: 0.100915
[14:24:23.111] iteration 28747: total_loss: 0.679038, loss_sup: 0.205935, loss_mps: 0.160466, loss_cps: 0.312637
[14:24:23.258] iteration 28748: total_loss: 0.182582, loss_sup: 0.017030, loss_mps: 0.061835, loss_cps: 0.103717
[14:24:23.405] iteration 28749: total_loss: 0.610835, loss_sup: 0.160973, loss_mps: 0.152588, loss_cps: 0.297274
[14:24:23.551] iteration 28750: total_loss: 0.227890, loss_sup: 0.003465, loss_mps: 0.078549, loss_cps: 0.145875
[14:24:23.697] iteration 28751: total_loss: 0.312166, loss_sup: 0.030369, loss_mps: 0.099166, loss_cps: 0.182632
[14:24:23.850] iteration 28752: total_loss: 0.162759, loss_sup: 0.020341, loss_mps: 0.054146, loss_cps: 0.088272
[14:24:23.996] iteration 28753: total_loss: 0.165520, loss_sup: 0.002582, loss_mps: 0.060274, loss_cps: 0.102663
[14:24:24.142] iteration 28754: total_loss: 0.447099, loss_sup: 0.030038, loss_mps: 0.136172, loss_cps: 0.280888
[14:24:24.288] iteration 28755: total_loss: 0.221881, loss_sup: 0.016970, loss_mps: 0.079827, loss_cps: 0.125084
[14:24:24.434] iteration 28756: total_loss: 0.205277, loss_sup: 0.008000, loss_mps: 0.073827, loss_cps: 0.123451
[14:24:24.580] iteration 28757: total_loss: 0.204706, loss_sup: 0.044167, loss_mps: 0.058688, loss_cps: 0.101851
[14:24:24.726] iteration 28758: total_loss: 0.359258, loss_sup: 0.129412, loss_mps: 0.081638, loss_cps: 0.148208
[14:24:24.872] iteration 28759: total_loss: 0.218981, loss_sup: 0.008487, loss_mps: 0.073628, loss_cps: 0.136866
[14:24:25.021] iteration 28760: total_loss: 0.147290, loss_sup: 0.000186, loss_mps: 0.055952, loss_cps: 0.091151
[14:24:25.167] iteration 28761: total_loss: 0.301796, loss_sup: 0.105611, loss_mps: 0.072445, loss_cps: 0.123740
[14:24:25.316] iteration 28762: total_loss: 0.733423, loss_sup: 0.023939, loss_mps: 0.218109, loss_cps: 0.491375
[14:24:25.467] iteration 28763: total_loss: 0.326312, loss_sup: 0.027687, loss_mps: 0.103574, loss_cps: 0.195051
[14:24:25.614] iteration 28764: total_loss: 0.143585, loss_sup: 0.001983, loss_mps: 0.055560, loss_cps: 0.086042
[14:24:25.761] iteration 28765: total_loss: 0.181725, loss_sup: 0.006209, loss_mps: 0.068652, loss_cps: 0.106864
[14:24:25.906] iteration 28766: total_loss: 0.192184, loss_sup: 0.001504, loss_mps: 0.071578, loss_cps: 0.119103
[14:24:26.054] iteration 28767: total_loss: 0.139129, loss_sup: 0.003838, loss_mps: 0.049725, loss_cps: 0.085565
[14:24:26.200] iteration 28768: total_loss: 0.837735, loss_sup: 0.300924, loss_mps: 0.178233, loss_cps: 0.358579
[14:24:26.345] iteration 28769: total_loss: 0.507170, loss_sup: 0.087621, loss_mps: 0.139702, loss_cps: 0.279847
[14:24:26.492] iteration 28770: total_loss: 0.294724, loss_sup: 0.018155, loss_mps: 0.098646, loss_cps: 0.177922
[14:24:26.638] iteration 28771: total_loss: 0.196765, loss_sup: 0.021302, loss_mps: 0.065050, loss_cps: 0.110413
[14:24:26.784] iteration 28772: total_loss: 0.348997, loss_sup: 0.030906, loss_mps: 0.108191, loss_cps: 0.209900
[14:24:26.930] iteration 28773: total_loss: 0.201620, loss_sup: 0.075861, loss_mps: 0.046767, loss_cps: 0.078992
[14:24:27.079] iteration 28774: total_loss: 0.481292, loss_sup: 0.108646, loss_mps: 0.125190, loss_cps: 0.247455
[14:24:27.225] iteration 28775: total_loss: 0.383701, loss_sup: 0.015756, loss_mps: 0.121276, loss_cps: 0.246670
[14:24:27.371] iteration 28776: total_loss: 0.349317, loss_sup: 0.027432, loss_mps: 0.113475, loss_cps: 0.208410
[14:24:27.518] iteration 28777: total_loss: 0.564979, loss_sup: 0.319622, loss_mps: 0.083121, loss_cps: 0.162236
[14:24:27.664] iteration 28778: total_loss: 0.317526, loss_sup: 0.051054, loss_mps: 0.098689, loss_cps: 0.167783
[14:24:27.810] iteration 28779: total_loss: 0.187001, loss_sup: 0.012128, loss_mps: 0.064555, loss_cps: 0.110318
[14:24:27.956] iteration 28780: total_loss: 0.407915, loss_sup: 0.009173, loss_mps: 0.140625, loss_cps: 0.258117
[14:24:28.103] iteration 28781: total_loss: 0.417806, loss_sup: 0.078242, loss_mps: 0.113645, loss_cps: 0.225918
[14:24:28.249] iteration 28782: total_loss: 0.320604, loss_sup: 0.091216, loss_mps: 0.079424, loss_cps: 0.149965
[14:24:28.398] iteration 28783: total_loss: 0.199713, loss_sup: 0.041642, loss_mps: 0.056354, loss_cps: 0.101717
[14:24:28.544] iteration 28784: total_loss: 0.352853, loss_sup: 0.087298, loss_mps: 0.084914, loss_cps: 0.180642
[14:24:28.690] iteration 28785: total_loss: 0.162810, loss_sup: 0.021767, loss_mps: 0.052704, loss_cps: 0.088340
[14:24:28.836] iteration 28786: total_loss: 0.261864, loss_sup: 0.109839, loss_mps: 0.057902, loss_cps: 0.094123
[14:24:28.983] iteration 28787: total_loss: 0.254318, loss_sup: 0.021083, loss_mps: 0.084048, loss_cps: 0.149186
[14:24:29.130] iteration 28788: total_loss: 0.335025, loss_sup: 0.075499, loss_mps: 0.092571, loss_cps: 0.166955
[14:24:29.276] iteration 28789: total_loss: 0.184679, loss_sup: 0.000414, loss_mps: 0.065727, loss_cps: 0.118538
[14:24:29.424] iteration 28790: total_loss: 0.184025, loss_sup: 0.044170, loss_mps: 0.051941, loss_cps: 0.087915
[14:24:29.571] iteration 28791: total_loss: 0.532564, loss_sup: 0.166321, loss_mps: 0.129638, loss_cps: 0.236605
[14:24:29.717] iteration 28792: total_loss: 0.386920, loss_sup: 0.174198, loss_mps: 0.076049, loss_cps: 0.136673
[14:24:29.864] iteration 28793: total_loss: 0.296568, loss_sup: 0.011070, loss_mps: 0.099585, loss_cps: 0.185913
[14:24:30.013] iteration 28794: total_loss: 0.240000, loss_sup: 0.018494, loss_mps: 0.079921, loss_cps: 0.141585
[14:24:30.160] iteration 28795: total_loss: 0.371724, loss_sup: 0.011840, loss_mps: 0.124520, loss_cps: 0.235365
[14:24:30.306] iteration 28796: total_loss: 0.493915, loss_sup: 0.118969, loss_mps: 0.120758, loss_cps: 0.254189
[14:24:30.453] iteration 28797: total_loss: 0.194448, loss_sup: 0.011819, loss_mps: 0.066738, loss_cps: 0.115891
[14:24:30.599] iteration 28798: total_loss: 0.245995, loss_sup: 0.083773, loss_mps: 0.062113, loss_cps: 0.100109
[14:24:30.745] iteration 28799: total_loss: 0.218467, loss_sup: 0.022448, loss_mps: 0.074839, loss_cps: 0.121181
[14:24:30.891] iteration 28800: total_loss: 0.397536, loss_sup: 0.042536, loss_mps: 0.118423, loss_cps: 0.236577
[14:24:30.891] Evaluation Started ==>
[14:24:42.342] ==> valid iteration 28800: unet metrics: {'dc': 0.67763702862773, 'jc': 0.5641191301562604, 'pre': 0.8135658833784194, 'hd': 5.284654690699709}, ynet metrics: {'dc': 0.625607546396831, 'jc': 0.5139650661458877, 'pre': 0.8027779845143662, 'hd': 5.382276121708846}.
[14:24:42.344] Evaluation Finished!⏹️
[14:24:42.495] iteration 28801: total_loss: 0.223220, loss_sup: 0.011466, loss_mps: 0.077899, loss_cps: 0.133855
[14:24:42.642] iteration 28802: total_loss: 0.448671, loss_sup: 0.062673, loss_mps: 0.132977, loss_cps: 0.253022
[14:24:42.788] iteration 28803: total_loss: 0.205096, loss_sup: 0.037930, loss_mps: 0.062637, loss_cps: 0.104530
[14:24:42.933] iteration 28804: total_loss: 0.423938, loss_sup: 0.068121, loss_mps: 0.121720, loss_cps: 0.234096
[14:24:43.079] iteration 28805: total_loss: 0.365475, loss_sup: 0.124359, loss_mps: 0.089036, loss_cps: 0.152081
[14:24:43.227] iteration 28806: total_loss: 0.323481, loss_sup: 0.027676, loss_mps: 0.099197, loss_cps: 0.196608
[14:24:43.374] iteration 28807: total_loss: 0.200792, loss_sup: 0.000878, loss_mps: 0.071919, loss_cps: 0.127995
[14:24:43.521] iteration 28808: total_loss: 0.438570, loss_sup: 0.020832, loss_mps: 0.140821, loss_cps: 0.276918
[14:24:43.668] iteration 28809: total_loss: 0.205269, loss_sup: 0.011919, loss_mps: 0.071502, loss_cps: 0.121848
[14:24:43.813] iteration 28810: total_loss: 0.593637, loss_sup: 0.109214, loss_mps: 0.144103, loss_cps: 0.340320
[14:24:43.959] iteration 28811: total_loss: 0.525925, loss_sup: 0.049061, loss_mps: 0.150506, loss_cps: 0.326357
[14:24:44.106] iteration 28812: total_loss: 0.349540, loss_sup: 0.036188, loss_mps: 0.101920, loss_cps: 0.211432
[14:24:44.254] iteration 28813: total_loss: 0.229782, loss_sup: 0.006221, loss_mps: 0.078211, loss_cps: 0.145350
[14:24:44.400] iteration 28814: total_loss: 0.243207, loss_sup: 0.015138, loss_mps: 0.081318, loss_cps: 0.146752
[14:24:44.546] iteration 28815: total_loss: 0.435777, loss_sup: 0.036648, loss_mps: 0.133237, loss_cps: 0.265892
[14:24:44.692] iteration 28816: total_loss: 0.175524, loss_sup: 0.001343, loss_mps: 0.063436, loss_cps: 0.110745
[14:24:44.838] iteration 28817: total_loss: 0.132046, loss_sup: 0.002871, loss_mps: 0.048573, loss_cps: 0.080602
[14:24:44.984] iteration 28818: total_loss: 0.194463, loss_sup: 0.007624, loss_mps: 0.069618, loss_cps: 0.117221
[14:24:45.130] iteration 28819: total_loss: 0.238642, loss_sup: 0.003540, loss_mps: 0.081800, loss_cps: 0.153302
[14:24:45.276] iteration 28820: total_loss: 0.567398, loss_sup: 0.294048, loss_mps: 0.097589, loss_cps: 0.175761
[14:24:45.422] iteration 28821: total_loss: 0.195281, loss_sup: 0.013482, loss_mps: 0.067323, loss_cps: 0.114476
[14:24:45.569] iteration 28822: total_loss: 0.282121, loss_sup: 0.015344, loss_mps: 0.090898, loss_cps: 0.175879
[14:24:45.715] iteration 28823: total_loss: 0.188059, loss_sup: 0.002977, loss_mps: 0.067525, loss_cps: 0.117558
[14:24:45.860] iteration 28824: total_loss: 0.361836, loss_sup: 0.021002, loss_mps: 0.115580, loss_cps: 0.225254
[14:24:46.006] iteration 28825: total_loss: 0.230203, loss_sup: 0.017982, loss_mps: 0.075338, loss_cps: 0.136882
[14:24:46.153] iteration 28826: total_loss: 0.347250, loss_sup: 0.048867, loss_mps: 0.101328, loss_cps: 0.197055
[14:24:46.298] iteration 28827: total_loss: 0.304776, loss_sup: 0.008997, loss_mps: 0.105466, loss_cps: 0.190313
[14:24:46.445] iteration 28828: total_loss: 0.248890, loss_sup: 0.005972, loss_mps: 0.085341, loss_cps: 0.157578
[14:24:46.591] iteration 28829: total_loss: 0.256210, loss_sup: 0.017609, loss_mps: 0.089884, loss_cps: 0.148716
[14:24:46.737] iteration 28830: total_loss: 0.257431, loss_sup: 0.057140, loss_mps: 0.070796, loss_cps: 0.129496
[14:24:46.883] iteration 28831: total_loss: 0.385934, loss_sup: 0.127488, loss_mps: 0.092738, loss_cps: 0.165708
[14:24:47.028] iteration 28832: total_loss: 0.448471, loss_sup: 0.028066, loss_mps: 0.127071, loss_cps: 0.293334
[14:24:47.175] iteration 28833: total_loss: 0.224617, loss_sup: 0.002129, loss_mps: 0.078062, loss_cps: 0.144426
[14:24:47.320] iteration 28834: total_loss: 0.475734, loss_sup: 0.113913, loss_mps: 0.115143, loss_cps: 0.246678
[14:24:47.466] iteration 28835: total_loss: 0.505735, loss_sup: 0.076698, loss_mps: 0.142628, loss_cps: 0.286409
[14:24:47.613] iteration 28836: total_loss: 0.631541, loss_sup: 0.083821, loss_mps: 0.176880, loss_cps: 0.370840
[14:24:47.758] iteration 28837: total_loss: 0.258185, loss_sup: 0.068163, loss_mps: 0.068577, loss_cps: 0.121445
[14:24:47.904] iteration 28838: total_loss: 0.312114, loss_sup: 0.020108, loss_mps: 0.099848, loss_cps: 0.192157
[14:24:48.057] iteration 28839: total_loss: 0.247210, loss_sup: 0.015163, loss_mps: 0.077568, loss_cps: 0.154479
[14:24:48.203] iteration 28840: total_loss: 0.294968, loss_sup: 0.091340, loss_mps: 0.079699, loss_cps: 0.123928
[14:24:48.349] iteration 28841: total_loss: 0.431626, loss_sup: 0.050006, loss_mps: 0.126976, loss_cps: 0.254643
[14:24:48.410] iteration 28842: total_loss: 0.427756, loss_sup: 0.000774, loss_mps: 0.129030, loss_cps: 0.297953
[14:24:49.655] iteration 28843: total_loss: 0.515975, loss_sup: 0.211134, loss_mps: 0.111657, loss_cps: 0.193184
[14:24:49.803] iteration 28844: total_loss: 0.388764, loss_sup: 0.060263, loss_mps: 0.112151, loss_cps: 0.216350
[14:24:49.950] iteration 28845: total_loss: 0.322819, loss_sup: 0.057480, loss_mps: 0.095588, loss_cps: 0.169751
[14:24:50.099] iteration 28846: total_loss: 0.257869, loss_sup: 0.012503, loss_mps: 0.089251, loss_cps: 0.156115
[14:24:50.245] iteration 28847: total_loss: 0.425955, loss_sup: 0.036810, loss_mps: 0.129139, loss_cps: 0.260006
[14:24:50.391] iteration 28848: total_loss: 0.310242, loss_sup: 0.048191, loss_mps: 0.089011, loss_cps: 0.173039
[14:24:50.537] iteration 28849: total_loss: 0.484451, loss_sup: 0.027203, loss_mps: 0.141319, loss_cps: 0.315928
[14:24:50.690] iteration 28850: total_loss: 0.334111, loss_sup: 0.125207, loss_mps: 0.080031, loss_cps: 0.128873
[14:24:50.836] iteration 28851: total_loss: 0.325130, loss_sup: 0.018360, loss_mps: 0.109239, loss_cps: 0.197532
[14:24:50.983] iteration 28852: total_loss: 0.258361, loss_sup: 0.000423, loss_mps: 0.090251, loss_cps: 0.167687
[14:24:51.130] iteration 28853: total_loss: 0.247026, loss_sup: 0.017594, loss_mps: 0.086544, loss_cps: 0.142887
[14:24:51.277] iteration 28854: total_loss: 0.247408, loss_sup: 0.035153, loss_mps: 0.076926, loss_cps: 0.135328
[14:24:51.423] iteration 28855: total_loss: 0.378022, loss_sup: 0.211408, loss_mps: 0.062884, loss_cps: 0.103730
[14:24:51.569] iteration 28856: total_loss: 0.262801, loss_sup: 0.012272, loss_mps: 0.084405, loss_cps: 0.166124
[14:24:51.717] iteration 28857: total_loss: 0.316950, loss_sup: 0.037616, loss_mps: 0.098878, loss_cps: 0.180456
[14:24:51.871] iteration 28858: total_loss: 0.279061, loss_sup: 0.045028, loss_mps: 0.080368, loss_cps: 0.153665
[14:24:52.020] iteration 28859: total_loss: 0.283697, loss_sup: 0.037605, loss_mps: 0.088460, loss_cps: 0.157632
[14:24:52.167] iteration 28860: total_loss: 0.267511, loss_sup: 0.043789, loss_mps: 0.084570, loss_cps: 0.139152
[14:24:52.315] iteration 28861: total_loss: 0.172897, loss_sup: 0.000866, loss_mps: 0.063935, loss_cps: 0.108096
[14:24:52.461] iteration 28862: total_loss: 0.201520, loss_sup: 0.062698, loss_mps: 0.053619, loss_cps: 0.085203
[14:24:52.608] iteration 28863: total_loss: 0.400524, loss_sup: 0.009437, loss_mps: 0.125513, loss_cps: 0.265574
[14:24:52.755] iteration 28864: total_loss: 0.247570, loss_sup: 0.010896, loss_mps: 0.084953, loss_cps: 0.151721
[14:24:52.902] iteration 28865: total_loss: 0.319937, loss_sup: 0.015571, loss_mps: 0.110164, loss_cps: 0.194202
[14:24:53.049] iteration 28866: total_loss: 0.271420, loss_sup: 0.012966, loss_mps: 0.090717, loss_cps: 0.167737
[14:24:53.196] iteration 28867: total_loss: 0.323961, loss_sup: 0.070243, loss_mps: 0.087333, loss_cps: 0.166385
[14:24:53.342] iteration 28868: total_loss: 0.228213, loss_sup: 0.004698, loss_mps: 0.082990, loss_cps: 0.140524
[14:24:53.491] iteration 28869: total_loss: 0.256178, loss_sup: 0.018471, loss_mps: 0.085628, loss_cps: 0.152080
[14:24:53.638] iteration 28870: total_loss: 0.250135, loss_sup: 0.007397, loss_mps: 0.089310, loss_cps: 0.153428
[14:24:53.785] iteration 28871: total_loss: 0.215774, loss_sup: 0.037845, loss_mps: 0.067322, loss_cps: 0.110607
[14:24:53.932] iteration 28872: total_loss: 0.243773, loss_sup: 0.007619, loss_mps: 0.080615, loss_cps: 0.155539
[14:24:54.079] iteration 28873: total_loss: 0.488562, loss_sup: 0.090391, loss_mps: 0.136264, loss_cps: 0.261907
[14:24:54.225] iteration 28874: total_loss: 0.203432, loss_sup: 0.022202, loss_mps: 0.067612, loss_cps: 0.113618
[14:24:54.372] iteration 28875: total_loss: 0.418276, loss_sup: 0.033446, loss_mps: 0.125024, loss_cps: 0.259806
[14:24:54.518] iteration 28876: total_loss: 0.264978, loss_sup: 0.013914, loss_mps: 0.089140, loss_cps: 0.161924
[14:24:54.665] iteration 28877: total_loss: 0.319694, loss_sup: 0.090296, loss_mps: 0.082737, loss_cps: 0.146661
[14:24:54.812] iteration 28878: total_loss: 0.282804, loss_sup: 0.021946, loss_mps: 0.094958, loss_cps: 0.165900
[14:24:54.959] iteration 28879: total_loss: 0.310459, loss_sup: 0.006350, loss_mps: 0.103000, loss_cps: 0.201109
[14:24:55.107] iteration 28880: total_loss: 0.447302, loss_sup: 0.041755, loss_mps: 0.145287, loss_cps: 0.260259
[14:24:55.254] iteration 28881: total_loss: 0.262768, loss_sup: 0.005693, loss_mps: 0.090040, loss_cps: 0.167035
[14:24:55.401] iteration 28882: total_loss: 0.371255, loss_sup: 0.151886, loss_mps: 0.080250, loss_cps: 0.139119
[14:24:55.547] iteration 28883: total_loss: 0.429913, loss_sup: 0.050589, loss_mps: 0.120962, loss_cps: 0.258362
[14:24:55.694] iteration 28884: total_loss: 0.397221, loss_sup: 0.126275, loss_mps: 0.094972, loss_cps: 0.175975
[14:24:55.842] iteration 28885: total_loss: 0.315857, loss_sup: 0.079794, loss_mps: 0.085252, loss_cps: 0.150812
[14:24:55.989] iteration 28886: total_loss: 0.372515, loss_sup: 0.074168, loss_mps: 0.100696, loss_cps: 0.197651
[14:24:56.135] iteration 28887: total_loss: 0.129916, loss_sup: 0.002318, loss_mps: 0.047680, loss_cps: 0.079917
[14:24:56.284] iteration 28888: total_loss: 0.244975, loss_sup: 0.012371, loss_mps: 0.088001, loss_cps: 0.144603
[14:24:56.431] iteration 28889: total_loss: 0.206124, loss_sup: 0.016892, loss_mps: 0.071870, loss_cps: 0.117362
[14:24:56.578] iteration 28890: total_loss: 0.205104, loss_sup: 0.014032, loss_mps: 0.068130, loss_cps: 0.122942
[14:24:56.726] iteration 28891: total_loss: 0.259013, loss_sup: 0.106218, loss_mps: 0.053560, loss_cps: 0.099235
[14:24:56.873] iteration 28892: total_loss: 0.295228, loss_sup: 0.018497, loss_mps: 0.092554, loss_cps: 0.184176
[14:24:57.020] iteration 28893: total_loss: 0.234322, loss_sup: 0.050916, loss_mps: 0.065988, loss_cps: 0.117419
[14:24:57.168] iteration 28894: total_loss: 0.283185, loss_sup: 0.021638, loss_mps: 0.089613, loss_cps: 0.171935
[14:24:57.317] iteration 28895: total_loss: 0.488829, loss_sup: 0.111637, loss_mps: 0.122702, loss_cps: 0.254490
[14:24:57.465] iteration 28896: total_loss: 0.267510, loss_sup: 0.013059, loss_mps: 0.085281, loss_cps: 0.169170
[14:24:57.615] iteration 28897: total_loss: 0.455621, loss_sup: 0.106276, loss_mps: 0.127895, loss_cps: 0.221450
[14:24:57.763] iteration 28898: total_loss: 0.203993, loss_sup: 0.001439, loss_mps: 0.073779, loss_cps: 0.128775
[14:24:57.912] iteration 28899: total_loss: 0.174927, loss_sup: 0.015686, loss_mps: 0.059460, loss_cps: 0.099781
[14:24:58.060] iteration 28900: total_loss: 0.215897, loss_sup: 0.000754, loss_mps: 0.076501, loss_cps: 0.138641
[14:24:58.060] Evaluation Started ==>
[14:25:09.429] ==> valid iteration 28900: unet metrics: {'dc': 0.6718917986151614, 'jc': 0.5577390349941219, 'pre': 0.8071634271465602, 'hd': 5.334922891873967}, ynet metrics: {'dc': 0.6270061206504886, 'jc': 0.5156165793997384, 'pre': 0.8003404430874971, 'hd': 5.37089327296864}.
[14:25:09.431] Evaluation Finished!⏹️
[14:25:09.583] iteration 28901: total_loss: 0.195312, loss_sup: 0.009363, loss_mps: 0.066006, loss_cps: 0.119943
[14:25:09.732] iteration 28902: total_loss: 0.247577, loss_sup: 0.016956, loss_mps: 0.083907, loss_cps: 0.146714
[14:25:09.878] iteration 28903: total_loss: 0.655730, loss_sup: 0.300743, loss_mps: 0.119383, loss_cps: 0.235604
[14:25:10.024] iteration 28904: total_loss: 0.304535, loss_sup: 0.007612, loss_mps: 0.099781, loss_cps: 0.197142
[14:25:10.171] iteration 28905: total_loss: 0.231077, loss_sup: 0.029649, loss_mps: 0.074411, loss_cps: 0.127016
[14:25:10.316] iteration 28906: total_loss: 0.306577, loss_sup: 0.071415, loss_mps: 0.084745, loss_cps: 0.150417
[14:25:10.463] iteration 28907: total_loss: 0.183241, loss_sup: 0.000536, loss_mps: 0.065835, loss_cps: 0.116870
[14:25:10.609] iteration 28908: total_loss: 0.278988, loss_sup: 0.014845, loss_mps: 0.096414, loss_cps: 0.167729
[14:25:10.756] iteration 28909: total_loss: 0.311966, loss_sup: 0.087020, loss_mps: 0.085561, loss_cps: 0.139385
[14:25:10.902] iteration 28910: total_loss: 0.225703, loss_sup: 0.000938, loss_mps: 0.081509, loss_cps: 0.143256
[14:25:11.048] iteration 28911: total_loss: 0.337571, loss_sup: 0.033307, loss_mps: 0.110025, loss_cps: 0.194239
[14:25:11.197] iteration 28912: total_loss: 0.363749, loss_sup: 0.083031, loss_mps: 0.095104, loss_cps: 0.185614
[14:25:11.347] iteration 28913: total_loss: 0.444259, loss_sup: 0.065827, loss_mps: 0.129530, loss_cps: 0.248902
[14:25:11.493] iteration 28914: total_loss: 0.629683, loss_sup: 0.342028, loss_mps: 0.095907, loss_cps: 0.191748
[14:25:11.639] iteration 28915: total_loss: 0.304663, loss_sup: 0.007507, loss_mps: 0.103541, loss_cps: 0.193614
[14:25:11.785] iteration 28916: total_loss: 0.385206, loss_sup: 0.045319, loss_mps: 0.123437, loss_cps: 0.216449
[14:25:11.932] iteration 28917: total_loss: 0.398489, loss_sup: 0.143738, loss_mps: 0.088728, loss_cps: 0.166023
[14:25:12.078] iteration 28918: total_loss: 0.210033, loss_sup: 0.008526, loss_mps: 0.073837, loss_cps: 0.127670
[14:25:12.223] iteration 28919: total_loss: 0.489717, loss_sup: 0.028121, loss_mps: 0.151655, loss_cps: 0.309941
[14:25:12.371] iteration 28920: total_loss: 0.535355, loss_sup: 0.179825, loss_mps: 0.122897, loss_cps: 0.232633
[14:25:12.517] iteration 28921: total_loss: 0.838109, loss_sup: 0.050231, loss_mps: 0.254942, loss_cps: 0.532935
[14:25:12.663] iteration 28922: total_loss: 0.221403, loss_sup: 0.007890, loss_mps: 0.076430, loss_cps: 0.137083
[14:25:12.809] iteration 28923: total_loss: 0.293735, loss_sup: 0.002895, loss_mps: 0.101619, loss_cps: 0.189221
[14:25:12.956] iteration 28924: total_loss: 0.275243, loss_sup: 0.016660, loss_mps: 0.088749, loss_cps: 0.169834
[14:25:13.104] iteration 28925: total_loss: 0.308764, loss_sup: 0.065449, loss_mps: 0.087089, loss_cps: 0.156226
[14:25:13.250] iteration 28926: total_loss: 0.179363, loss_sup: 0.008062, loss_mps: 0.064705, loss_cps: 0.106596
[14:25:13.396] iteration 28927: total_loss: 0.310747, loss_sup: 0.071075, loss_mps: 0.085885, loss_cps: 0.153788
[14:25:13.542] iteration 28928: total_loss: 0.174903, loss_sup: 0.008205, loss_mps: 0.058883, loss_cps: 0.107815
[14:25:13.690] iteration 28929: total_loss: 0.250313, loss_sup: 0.006735, loss_mps: 0.083579, loss_cps: 0.160000
[14:25:13.838] iteration 28930: total_loss: 0.596755, loss_sup: 0.354304, loss_mps: 0.082265, loss_cps: 0.160186
[14:25:13.985] iteration 28931: total_loss: 0.311310, loss_sup: 0.056182, loss_mps: 0.086233, loss_cps: 0.168894
[14:25:14.131] iteration 28932: total_loss: 0.162406, loss_sup: 0.013612, loss_mps: 0.055091, loss_cps: 0.093702
[14:25:14.277] iteration 28933: total_loss: 0.163766, loss_sup: 0.009938, loss_mps: 0.057269, loss_cps: 0.096559
[14:25:14.424] iteration 28934: total_loss: 0.279062, loss_sup: 0.035289, loss_mps: 0.085437, loss_cps: 0.158336
[14:25:14.570] iteration 28935: total_loss: 0.378417, loss_sup: 0.023839, loss_mps: 0.121090, loss_cps: 0.233488
[14:25:14.717] iteration 28936: total_loss: 0.212757, loss_sup: 0.008436, loss_mps: 0.074576, loss_cps: 0.129746
[14:25:14.863] iteration 28937: total_loss: 0.242035, loss_sup: 0.035185, loss_mps: 0.072144, loss_cps: 0.134706
[14:25:15.009] iteration 28938: total_loss: 0.512573, loss_sup: 0.055366, loss_mps: 0.147437, loss_cps: 0.309770
[14:25:15.155] iteration 28939: total_loss: 0.154387, loss_sup: 0.001055, loss_mps: 0.057772, loss_cps: 0.095560
[14:25:15.301] iteration 28940: total_loss: 0.180828, loss_sup: 0.021392, loss_mps: 0.055897, loss_cps: 0.103539
[14:25:15.447] iteration 28941: total_loss: 0.482896, loss_sup: 0.054723, loss_mps: 0.145566, loss_cps: 0.282606
[14:25:15.593] iteration 28942: total_loss: 0.373215, loss_sup: 0.058284, loss_mps: 0.108914, loss_cps: 0.206017
[14:25:15.740] iteration 28943: total_loss: 0.247953, loss_sup: 0.074183, loss_mps: 0.061916, loss_cps: 0.111854
[14:25:15.887] iteration 28944: total_loss: 0.352244, loss_sup: 0.088559, loss_mps: 0.096915, loss_cps: 0.166770
[14:25:16.034] iteration 28945: total_loss: 0.168754, loss_sup: 0.011089, loss_mps: 0.058723, loss_cps: 0.098942
[14:25:16.181] iteration 28946: total_loss: 0.425394, loss_sup: 0.080792, loss_mps: 0.114298, loss_cps: 0.230304
[14:25:16.328] iteration 28947: total_loss: 0.229491, loss_sup: 0.025162, loss_mps: 0.077153, loss_cps: 0.127175
[14:25:16.474] iteration 28948: total_loss: 0.270969, loss_sup: 0.018792, loss_mps: 0.089210, loss_cps: 0.162967
[14:25:16.622] iteration 28949: total_loss: 0.392054, loss_sup: 0.082289, loss_mps: 0.107196, loss_cps: 0.202569
[14:25:16.770] iteration 28950: total_loss: 0.374217, loss_sup: 0.148990, loss_mps: 0.085379, loss_cps: 0.139849
[14:25:16.916] iteration 28951: total_loss: 0.248414, loss_sup: 0.007150, loss_mps: 0.083058, loss_cps: 0.158207
[14:25:17.062] iteration 28952: total_loss: 0.197948, loss_sup: 0.006697, loss_mps: 0.072869, loss_cps: 0.118382
[14:25:17.208] iteration 28953: total_loss: 0.255007, loss_sup: 0.046342, loss_mps: 0.077054, loss_cps: 0.131611
[14:25:17.354] iteration 28954: total_loss: 0.267965, loss_sup: 0.008408, loss_mps: 0.083805, loss_cps: 0.175752
[14:25:17.500] iteration 28955: total_loss: 0.276625, loss_sup: 0.032654, loss_mps: 0.084584, loss_cps: 0.159388
[14:25:17.647] iteration 28956: total_loss: 0.468518, loss_sup: 0.061807, loss_mps: 0.139630, loss_cps: 0.267080
[14:25:17.795] iteration 28957: total_loss: 0.245432, loss_sup: 0.016235, loss_mps: 0.081668, loss_cps: 0.147529
[14:25:17.941] iteration 28958: total_loss: 0.423204, loss_sup: 0.042178, loss_mps: 0.125410, loss_cps: 0.255616
[14:25:18.087] iteration 28959: total_loss: 0.376687, loss_sup: 0.006005, loss_mps: 0.118440, loss_cps: 0.252242
[14:25:18.232] iteration 28960: total_loss: 0.528748, loss_sup: 0.249447, loss_mps: 0.087390, loss_cps: 0.191910
[14:25:18.379] iteration 28961: total_loss: 0.379368, loss_sup: 0.143447, loss_mps: 0.082623, loss_cps: 0.153298
[14:25:18.524] iteration 28962: total_loss: 0.177486, loss_sup: 0.004858, loss_mps: 0.060071, loss_cps: 0.112557
[14:25:18.670] iteration 28963: total_loss: 0.251601, loss_sup: 0.066351, loss_mps: 0.070560, loss_cps: 0.114689
[14:25:18.817] iteration 28964: total_loss: 0.285911, loss_sup: 0.022495, loss_mps: 0.092090, loss_cps: 0.171327
[14:25:18.963] iteration 28965: total_loss: 0.485266, loss_sup: 0.000684, loss_mps: 0.151955, loss_cps: 0.332628
[14:25:19.109] iteration 28966: total_loss: 0.319032, loss_sup: 0.041004, loss_mps: 0.095461, loss_cps: 0.182567
[14:25:19.255] iteration 28967: total_loss: 0.302703, loss_sup: 0.003234, loss_mps: 0.095830, loss_cps: 0.203639
[14:25:19.401] iteration 28968: total_loss: 0.245899, loss_sup: 0.017721, loss_mps: 0.080019, loss_cps: 0.148159
[14:25:19.547] iteration 28969: total_loss: 0.306458, loss_sup: 0.104199, loss_mps: 0.076278, loss_cps: 0.125980
[14:25:19.693] iteration 28970: total_loss: 0.306345, loss_sup: 0.027962, loss_mps: 0.102017, loss_cps: 0.176366
[14:25:19.840] iteration 28971: total_loss: 0.271008, loss_sup: 0.038453, loss_mps: 0.084518, loss_cps: 0.148037
[14:25:19.986] iteration 28972: total_loss: 0.628408, loss_sup: 0.057020, loss_mps: 0.179281, loss_cps: 0.392107
[14:25:20.133] iteration 28973: total_loss: 0.456633, loss_sup: 0.112815, loss_mps: 0.112064, loss_cps: 0.231754
[14:25:20.279] iteration 28974: total_loss: 0.147440, loss_sup: 0.011700, loss_mps: 0.052489, loss_cps: 0.083251
[14:25:20.425] iteration 28975: total_loss: 0.357590, loss_sup: 0.046009, loss_mps: 0.106542, loss_cps: 0.205039
[14:25:20.571] iteration 28976: total_loss: 0.501750, loss_sup: 0.100993, loss_mps: 0.143553, loss_cps: 0.257204
[14:25:20.719] iteration 28977: total_loss: 0.391063, loss_sup: 0.055168, loss_mps: 0.115383, loss_cps: 0.220512
[14:25:20.866] iteration 28978: total_loss: 0.171810, loss_sup: 0.005070, loss_mps: 0.062076, loss_cps: 0.104664
[14:25:21.015] iteration 28979: total_loss: 0.278485, loss_sup: 0.022252, loss_mps: 0.093645, loss_cps: 0.162588
[14:25:21.160] iteration 28980: total_loss: 0.365853, loss_sup: 0.032510, loss_mps: 0.104772, loss_cps: 0.228572
[14:25:21.307] iteration 28981: total_loss: 0.313620, loss_sup: 0.075062, loss_mps: 0.084531, loss_cps: 0.154027
[14:25:21.455] iteration 28982: total_loss: 0.199281, loss_sup: 0.029150, loss_mps: 0.062630, loss_cps: 0.107501
[14:25:21.602] iteration 28983: total_loss: 0.160694, loss_sup: 0.000611, loss_mps: 0.060013, loss_cps: 0.100071
[14:25:21.749] iteration 28984: total_loss: 0.191753, loss_sup: 0.001630, loss_mps: 0.068726, loss_cps: 0.121398
[14:25:21.896] iteration 28985: total_loss: 0.611309, loss_sup: 0.110145, loss_mps: 0.161937, loss_cps: 0.339228
[14:25:22.043] iteration 28986: total_loss: 0.322127, loss_sup: 0.020286, loss_mps: 0.104937, loss_cps: 0.196905
[14:25:22.189] iteration 28987: total_loss: 0.381415, loss_sup: 0.001620, loss_mps: 0.126921, loss_cps: 0.252874
[14:25:22.336] iteration 28988: total_loss: 0.542797, loss_sup: 0.062901, loss_mps: 0.156353, loss_cps: 0.323543
[14:25:22.486] iteration 28989: total_loss: 0.160115, loss_sup: 0.007568, loss_mps: 0.055695, loss_cps: 0.096851
[14:25:22.632] iteration 28990: total_loss: 0.299839, loss_sup: 0.042534, loss_mps: 0.090250, loss_cps: 0.167055
[14:25:22.780] iteration 28991: total_loss: 0.182732, loss_sup: 0.016217, loss_mps: 0.059981, loss_cps: 0.106534
[14:25:22.928] iteration 28992: total_loss: 0.448809, loss_sup: 0.084707, loss_mps: 0.121797, loss_cps: 0.242305
[14:25:23.075] iteration 28993: total_loss: 0.316482, loss_sup: 0.042862, loss_mps: 0.094865, loss_cps: 0.178755
[14:25:23.224] iteration 28994: total_loss: 0.146969, loss_sup: 0.015760, loss_mps: 0.051633, loss_cps: 0.079577
[14:25:23.372] iteration 28995: total_loss: 0.294342, loss_sup: 0.057382, loss_mps: 0.083727, loss_cps: 0.153234
[14:25:23.519] iteration 28996: total_loss: 0.517015, loss_sup: 0.233625, loss_mps: 0.099200, loss_cps: 0.184190
[14:25:23.666] iteration 28997: total_loss: 0.216494, loss_sup: 0.019498, loss_mps: 0.070487, loss_cps: 0.126510
[14:25:23.814] iteration 28998: total_loss: 0.196873, loss_sup: 0.050126, loss_mps: 0.055967, loss_cps: 0.090779
[14:25:23.961] iteration 28999: total_loss: 0.228802, loss_sup: 0.002390, loss_mps: 0.082271, loss_cps: 0.144140
[14:25:24.107] iteration 29000: total_loss: 0.194477, loss_sup: 0.001479, loss_mps: 0.070634, loss_cps: 0.122364
[14:25:24.107] Evaluation Started ==>
[14:25:35.447] ==> valid iteration 29000: unet metrics: {'dc': 0.6682269383150917, 'jc': 0.5556616552099239, 'pre': 0.8034904965287608, 'hd': 5.3103203494255995}, ynet metrics: {'dc': 0.6135868195541365, 'jc': 0.5026891879707701, 'pre': 0.7994266872839483, 'hd': 5.334525777722186}.
[14:25:35.449] Evaluation Finished!⏹️
[14:25:35.599] iteration 29001: total_loss: 0.189174, loss_sup: 0.018130, loss_mps: 0.064010, loss_cps: 0.107034
[14:25:35.747] iteration 29002: total_loss: 0.181458, loss_sup: 0.008463, loss_mps: 0.067297, loss_cps: 0.105698
[14:25:35.893] iteration 29003: total_loss: 0.177981, loss_sup: 0.010356, loss_mps: 0.064813, loss_cps: 0.102813
[14:25:36.042] iteration 29004: total_loss: 0.297115, loss_sup: 0.050983, loss_mps: 0.088797, loss_cps: 0.157335
[14:25:36.188] iteration 29005: total_loss: 0.420463, loss_sup: 0.233562, loss_mps: 0.066078, loss_cps: 0.120823
[14:25:36.335] iteration 29006: total_loss: 0.427147, loss_sup: 0.050660, loss_mps: 0.125500, loss_cps: 0.250987
[14:25:36.481] iteration 29007: total_loss: 0.365146, loss_sup: 0.112270, loss_mps: 0.090531, loss_cps: 0.162344
[14:25:36.628] iteration 29008: total_loss: 0.243017, loss_sup: 0.059694, loss_mps: 0.065508, loss_cps: 0.117815
[14:25:36.774] iteration 29009: total_loss: 0.237420, loss_sup: 0.014759, loss_mps: 0.076827, loss_cps: 0.145834
[14:25:36.920] iteration 29010: total_loss: 0.319234, loss_sup: 0.007564, loss_mps: 0.105213, loss_cps: 0.206458
[14:25:37.065] iteration 29011: total_loss: 0.329319, loss_sup: 0.107335, loss_mps: 0.082302, loss_cps: 0.139682
[14:25:37.212] iteration 29012: total_loss: 0.263645, loss_sup: 0.001174, loss_mps: 0.087562, loss_cps: 0.174909
[14:25:37.358] iteration 29013: total_loss: 0.545101, loss_sup: 0.062411, loss_mps: 0.151978, loss_cps: 0.330712
[14:25:37.504] iteration 29014: total_loss: 0.397340, loss_sup: 0.050285, loss_mps: 0.115334, loss_cps: 0.231720
[14:25:37.652] iteration 29015: total_loss: 0.591805, loss_sup: 0.161057, loss_mps: 0.147549, loss_cps: 0.283199
[14:25:37.799] iteration 29016: total_loss: 0.338315, loss_sup: 0.057688, loss_mps: 0.094691, loss_cps: 0.185935
[14:25:37.945] iteration 29017: total_loss: 0.303058, loss_sup: 0.086366, loss_mps: 0.076817, loss_cps: 0.139875
[14:25:38.090] iteration 29018: total_loss: 0.332950, loss_sup: 0.069307, loss_mps: 0.096542, loss_cps: 0.167100
[14:25:38.236] iteration 29019: total_loss: 0.207488, loss_sup: 0.004412, loss_mps: 0.076046, loss_cps: 0.127029
[14:25:38.382] iteration 29020: total_loss: 0.245970, loss_sup: 0.027313, loss_mps: 0.080825, loss_cps: 0.137832
[14:25:38.528] iteration 29021: total_loss: 0.221256, loss_sup: 0.019897, loss_mps: 0.076012, loss_cps: 0.125347
[14:25:38.674] iteration 29022: total_loss: 0.246067, loss_sup: 0.005087, loss_mps: 0.088516, loss_cps: 0.152463
[14:25:38.820] iteration 29023: total_loss: 0.215383, loss_sup: 0.008311, loss_mps: 0.070548, loss_cps: 0.136525
[14:25:38.967] iteration 29024: total_loss: 0.316191, loss_sup: 0.029879, loss_mps: 0.107206, loss_cps: 0.179106
[14:25:39.113] iteration 29025: total_loss: 0.177482, loss_sup: 0.001456, loss_mps: 0.060532, loss_cps: 0.115494
[14:25:39.259] iteration 29026: total_loss: 0.218354, loss_sup: 0.027927, loss_mps: 0.070331, loss_cps: 0.120096
[14:25:39.405] iteration 29027: total_loss: 0.391732, loss_sup: 0.118194, loss_mps: 0.095692, loss_cps: 0.177846
[14:25:39.551] iteration 29028: total_loss: 0.176942, loss_sup: 0.013409, loss_mps: 0.060362, loss_cps: 0.103171
[14:25:39.697] iteration 29029: total_loss: 0.428081, loss_sup: 0.011857, loss_mps: 0.131508, loss_cps: 0.284716
[14:25:39.845] iteration 29030: total_loss: 0.151032, loss_sup: 0.021102, loss_mps: 0.050687, loss_cps: 0.079243
[14:25:39.991] iteration 29031: total_loss: 0.299364, loss_sup: 0.024748, loss_mps: 0.087246, loss_cps: 0.187370
[14:25:40.136] iteration 29032: total_loss: 0.185930, loss_sup: 0.004897, loss_mps: 0.068634, loss_cps: 0.112399
[14:25:40.282] iteration 29033: total_loss: 0.226140, loss_sup: 0.005039, loss_mps: 0.078719, loss_cps: 0.142383
[14:25:40.428] iteration 29034: total_loss: 0.259980, loss_sup: 0.031542, loss_mps: 0.078274, loss_cps: 0.150163
[14:25:40.575] iteration 29035: total_loss: 0.437041, loss_sup: 0.086109, loss_mps: 0.124007, loss_cps: 0.226925
[14:25:40.720] iteration 29036: total_loss: 0.331876, loss_sup: 0.074690, loss_mps: 0.087481, loss_cps: 0.169705
[14:25:40.866] iteration 29037: total_loss: 0.190804, loss_sup: 0.004768, loss_mps: 0.065985, loss_cps: 0.120051
[14:25:41.013] iteration 29038: total_loss: 0.240980, loss_sup: 0.023866, loss_mps: 0.074293, loss_cps: 0.142821
[14:25:41.158] iteration 29039: total_loss: 0.151820, loss_sup: 0.013938, loss_mps: 0.051227, loss_cps: 0.086655
[14:25:41.304] iteration 29040: total_loss: 0.341157, loss_sup: 0.021054, loss_mps: 0.113580, loss_cps: 0.206522
[14:25:41.450] iteration 29041: total_loss: 0.475681, loss_sup: 0.211906, loss_mps: 0.090858, loss_cps: 0.172917
[14:25:41.596] iteration 29042: total_loss: 0.237390, loss_sup: 0.038599, loss_mps: 0.074168, loss_cps: 0.124623
[14:25:41.742] iteration 29043: total_loss: 0.320782, loss_sup: 0.052430, loss_mps: 0.097911, loss_cps: 0.170441
[14:25:41.889] iteration 29044: total_loss: 0.244184, loss_sup: 0.000268, loss_mps: 0.086220, loss_cps: 0.157696
[14:25:42.036] iteration 29045: total_loss: 0.243629, loss_sup: 0.029303, loss_mps: 0.077674, loss_cps: 0.136653
[14:25:42.181] iteration 29046: total_loss: 0.214517, loss_sup: 0.055585, loss_mps: 0.060142, loss_cps: 0.098789
[14:25:42.328] iteration 29047: total_loss: 0.495196, loss_sup: 0.159873, loss_mps: 0.116616, loss_cps: 0.218708
[14:25:42.475] iteration 29048: total_loss: 0.547775, loss_sup: 0.106863, loss_mps: 0.153622, loss_cps: 0.287290
[14:25:42.621] iteration 29049: total_loss: 0.247622, loss_sup: 0.011164, loss_mps: 0.080796, loss_cps: 0.155661
[14:25:42.768] iteration 29050: total_loss: 0.329534, loss_sup: 0.031784, loss_mps: 0.107007, loss_cps: 0.190742
[14:25:42.914] iteration 29051: total_loss: 0.142905, loss_sup: 0.015677, loss_mps: 0.047891, loss_cps: 0.079337
[14:25:43.060] iteration 29052: total_loss: 0.183734, loss_sup: 0.028573, loss_mps: 0.057123, loss_cps: 0.098037
[14:25:43.206] iteration 29053: total_loss: 0.217988, loss_sup: 0.059035, loss_mps: 0.061429, loss_cps: 0.097523
[14:25:43.353] iteration 29054: total_loss: 0.224918, loss_sup: 0.000997, loss_mps: 0.079821, loss_cps: 0.144100
[14:25:43.499] iteration 29055: total_loss: 0.321212, loss_sup: 0.035618, loss_mps: 0.096743, loss_cps: 0.188852
[14:25:43.645] iteration 29056: total_loss: 0.397901, loss_sup: 0.024015, loss_mps: 0.123516, loss_cps: 0.250370
[14:25:43.791] iteration 29057: total_loss: 0.201362, loss_sup: 0.020291, loss_mps: 0.066578, loss_cps: 0.114493
[14:25:43.937] iteration 29058: total_loss: 0.723535, loss_sup: 0.038291, loss_mps: 0.219308, loss_cps: 0.465936
[14:25:44.085] iteration 29059: total_loss: 0.198512, loss_sup: 0.023614, loss_mps: 0.065341, loss_cps: 0.109557
[14:25:44.231] iteration 29060: total_loss: 0.205604, loss_sup: 0.024671, loss_mps: 0.068255, loss_cps: 0.112677
[14:25:44.377] iteration 29061: total_loss: 0.240537, loss_sup: 0.017580, loss_mps: 0.075579, loss_cps: 0.147377
[14:25:44.523] iteration 29062: total_loss: 0.179455, loss_sup: 0.037264, loss_mps: 0.053333, loss_cps: 0.088857
[14:25:44.671] iteration 29063: total_loss: 0.253309, loss_sup: 0.003485, loss_mps: 0.087143, loss_cps: 0.162681
[14:25:44.817] iteration 29064: total_loss: 0.388054, loss_sup: 0.049998, loss_mps: 0.119590, loss_cps: 0.218467
[14:25:44.963] iteration 29065: total_loss: 0.206771, loss_sup: 0.039277, loss_mps: 0.064086, loss_cps: 0.103408
[14:25:45.110] iteration 29066: total_loss: 0.248980, loss_sup: 0.061987, loss_mps: 0.070063, loss_cps: 0.116930
[14:25:45.256] iteration 29067: total_loss: 0.231970, loss_sup: 0.037806, loss_mps: 0.069395, loss_cps: 0.124770
[14:25:45.402] iteration 29068: total_loss: 0.230428, loss_sup: 0.016758, loss_mps: 0.076031, loss_cps: 0.137639
[14:25:45.552] iteration 29069: total_loss: 0.794102, loss_sup: 0.229251, loss_mps: 0.184749, loss_cps: 0.380102
[14:25:45.698] iteration 29070: total_loss: 0.463310, loss_sup: 0.163521, loss_mps: 0.103245, loss_cps: 0.196544
[14:25:45.844] iteration 29071: total_loss: 0.165272, loss_sup: 0.002566, loss_mps: 0.057759, loss_cps: 0.104947
[14:25:45.992] iteration 29072: total_loss: 0.137887, loss_sup: 0.014003, loss_mps: 0.047708, loss_cps: 0.076176
[14:25:46.138] iteration 29073: total_loss: 0.258507, loss_sup: 0.094828, loss_mps: 0.060630, loss_cps: 0.103049
[14:25:46.285] iteration 29074: total_loss: 0.408582, loss_sup: 0.031599, loss_mps: 0.127113, loss_cps: 0.249870
[14:25:46.432] iteration 29075: total_loss: 0.236657, loss_sup: 0.019908, loss_mps: 0.077500, loss_cps: 0.139248
[14:25:46.578] iteration 29076: total_loss: 0.222097, loss_sup: 0.014845, loss_mps: 0.071049, loss_cps: 0.136203
[14:25:46.725] iteration 29077: total_loss: 0.313901, loss_sup: 0.049961, loss_mps: 0.092902, loss_cps: 0.171038
[14:25:46.873] iteration 29078: total_loss: 0.246955, loss_sup: 0.002175, loss_mps: 0.082331, loss_cps: 0.162448
[14:25:47.019] iteration 29079: total_loss: 0.199759, loss_sup: 0.007748, loss_mps: 0.072945, loss_cps: 0.119065
[14:25:47.165] iteration 29080: total_loss: 0.479078, loss_sup: 0.069226, loss_mps: 0.132920, loss_cps: 0.276933
[14:25:47.312] iteration 29081: total_loss: 0.291425, loss_sup: 0.021152, loss_mps: 0.095210, loss_cps: 0.175063
[14:25:47.460] iteration 29082: total_loss: 0.207926, loss_sup: 0.005892, loss_mps: 0.069075, loss_cps: 0.132959
[14:25:47.607] iteration 29083: total_loss: 0.206659, loss_sup: 0.009835, loss_mps: 0.072544, loss_cps: 0.124280
[14:25:47.754] iteration 29084: total_loss: 0.596923, loss_sup: 0.058159, loss_mps: 0.175139, loss_cps: 0.363625
[14:25:47.900] iteration 29085: total_loss: 0.520522, loss_sup: 0.059631, loss_mps: 0.149347, loss_cps: 0.311544
[14:25:48.050] iteration 29086: total_loss: 0.256186, loss_sup: 0.012505, loss_mps: 0.088312, loss_cps: 0.155368
[14:25:48.197] iteration 29087: total_loss: 0.191186, loss_sup: 0.006550, loss_mps: 0.064900, loss_cps: 0.119736
[14:25:48.344] iteration 29088: total_loss: 0.819838, loss_sup: 0.154331, loss_mps: 0.206552, loss_cps: 0.458955
[14:25:48.490] iteration 29089: total_loss: 0.340571, loss_sup: 0.020686, loss_mps: 0.110653, loss_cps: 0.209232
[14:25:48.638] iteration 29090: total_loss: 0.184465, loss_sup: 0.001882, loss_mps: 0.067279, loss_cps: 0.115304
[14:25:48.784] iteration 29091: total_loss: 0.254338, loss_sup: 0.001961, loss_mps: 0.084209, loss_cps: 0.168168
[14:25:48.932] iteration 29092: total_loss: 0.222984, loss_sup: 0.017991, loss_mps: 0.078547, loss_cps: 0.126446
[14:25:49.078] iteration 29093: total_loss: 0.309570, loss_sup: 0.024036, loss_mps: 0.092634, loss_cps: 0.192900
[14:25:49.227] iteration 29094: total_loss: 0.234211, loss_sup: 0.008665, loss_mps: 0.082607, loss_cps: 0.142940
[14:25:49.373] iteration 29095: total_loss: 0.509142, loss_sup: 0.195039, loss_mps: 0.115919, loss_cps: 0.198184
[14:25:49.520] iteration 29096: total_loss: 0.292288, loss_sup: 0.056176, loss_mps: 0.092321, loss_cps: 0.143792
[14:25:49.666] iteration 29097: total_loss: 0.279804, loss_sup: 0.070375, loss_mps: 0.074624, loss_cps: 0.134805
[14:25:49.814] iteration 29098: total_loss: 0.221509, loss_sup: 0.011284, loss_mps: 0.073184, loss_cps: 0.137042
[14:25:49.961] iteration 29099: total_loss: 0.223926, loss_sup: 0.042553, loss_mps: 0.067061, loss_cps: 0.114312
[14:25:50.110] iteration 29100: total_loss: 0.325484, loss_sup: 0.068993, loss_mps: 0.084846, loss_cps: 0.171645
[14:25:50.110] Evaluation Started ==>
[14:26:01.413] ==> valid iteration 29100: unet metrics: {'dc': 0.6660344953742524, 'jc': 0.5533107100389829, 'pre': 0.803767282949315, 'hd': 5.294412037404854}, ynet metrics: {'dc': 0.6102545933926574, 'jc': 0.5005380151341707, 'pre': 0.7989977982739659, 'hd': 5.347937945207786}.
[14:26:01.415] Evaluation Finished!⏹️
[14:26:01.565] iteration 29101: total_loss: 0.262084, loss_sup: 0.003278, loss_mps: 0.092407, loss_cps: 0.166399
[14:26:01.712] iteration 29102: total_loss: 0.224280, loss_sup: 0.014065, loss_mps: 0.070963, loss_cps: 0.139252
[14:26:01.858] iteration 29103: total_loss: 0.172630, loss_sup: 0.007778, loss_mps: 0.064190, loss_cps: 0.100663
[14:26:02.005] iteration 29104: total_loss: 0.300413, loss_sup: 0.017485, loss_mps: 0.100402, loss_cps: 0.182526
[14:26:02.150] iteration 29105: total_loss: 0.239926, loss_sup: 0.002450, loss_mps: 0.085793, loss_cps: 0.151684
[14:26:02.296] iteration 29106: total_loss: 0.191696, loss_sup: 0.027677, loss_mps: 0.060072, loss_cps: 0.103947
[14:26:02.441] iteration 29107: total_loss: 0.282778, loss_sup: 0.012707, loss_mps: 0.093538, loss_cps: 0.176533
[14:26:02.587] iteration 29108: total_loss: 0.215690, loss_sup: 0.010537, loss_mps: 0.074998, loss_cps: 0.130154
[14:26:02.732] iteration 29109: total_loss: 0.206112, loss_sup: 0.004660, loss_mps: 0.074440, loss_cps: 0.127012
[14:26:02.878] iteration 29110: total_loss: 0.247013, loss_sup: 0.009919, loss_mps: 0.085820, loss_cps: 0.151274
[14:26:03.029] iteration 29111: total_loss: 0.262209, loss_sup: 0.011693, loss_mps: 0.088381, loss_cps: 0.162135
[14:26:03.174] iteration 29112: total_loss: 0.293144, loss_sup: 0.003728, loss_mps: 0.103444, loss_cps: 0.185973
[14:26:03.320] iteration 29113: total_loss: 0.300568, loss_sup: 0.008773, loss_mps: 0.100832, loss_cps: 0.190963
[14:26:03.466] iteration 29114: total_loss: 0.266424, loss_sup: 0.025340, loss_mps: 0.083948, loss_cps: 0.157136
[14:26:03.612] iteration 29115: total_loss: 0.334504, loss_sup: 0.003601, loss_mps: 0.112266, loss_cps: 0.218637
[14:26:03.757] iteration 29116: total_loss: 0.219662, loss_sup: 0.006383, loss_mps: 0.077899, loss_cps: 0.135380
[14:26:03.905] iteration 29117: total_loss: 0.332953, loss_sup: 0.045166, loss_mps: 0.104497, loss_cps: 0.183289
[14:26:04.050] iteration 29118: total_loss: 0.505642, loss_sup: 0.062502, loss_mps: 0.142778, loss_cps: 0.300361
[14:26:04.197] iteration 29119: total_loss: 0.209576, loss_sup: 0.023550, loss_mps: 0.068551, loss_cps: 0.117474
[14:26:04.345] iteration 29120: total_loss: 0.180839, loss_sup: 0.001160, loss_mps: 0.064469, loss_cps: 0.115210
[14:26:04.490] iteration 29121: total_loss: 0.245981, loss_sup: 0.022012, loss_mps: 0.077458, loss_cps: 0.146511
[14:26:04.636] iteration 29122: total_loss: 0.349260, loss_sup: 0.045339, loss_mps: 0.102473, loss_cps: 0.201448
[14:26:04.781] iteration 29123: total_loss: 0.354313, loss_sup: 0.067218, loss_mps: 0.102537, loss_cps: 0.184558
[14:26:04.929] iteration 29124: total_loss: 0.158907, loss_sup: 0.025553, loss_mps: 0.050804, loss_cps: 0.082551
[14:26:05.076] iteration 29125: total_loss: 0.277805, loss_sup: 0.047909, loss_mps: 0.083673, loss_cps: 0.146223
[14:26:05.222] iteration 29126: total_loss: 0.378872, loss_sup: 0.122391, loss_mps: 0.088325, loss_cps: 0.168157
[14:26:05.369] iteration 29127: total_loss: 0.318478, loss_sup: 0.068856, loss_mps: 0.085502, loss_cps: 0.164120
[14:26:05.514] iteration 29128: total_loss: 0.182327, loss_sup: 0.002817, loss_mps: 0.064235, loss_cps: 0.115275
[14:26:05.660] iteration 29129: total_loss: 0.228333, loss_sup: 0.005313, loss_mps: 0.076308, loss_cps: 0.146712
[14:26:05.806] iteration 29130: total_loss: 0.190795, loss_sup: 0.001563, loss_mps: 0.067857, loss_cps: 0.121375
[14:26:05.953] iteration 29131: total_loss: 0.207795, loss_sup: 0.003890, loss_mps: 0.074181, loss_cps: 0.129725
[14:26:06.100] iteration 29132: total_loss: 0.315702, loss_sup: 0.083251, loss_mps: 0.083835, loss_cps: 0.148617
[14:26:06.247] iteration 29133: total_loss: 0.178517, loss_sup: 0.015568, loss_mps: 0.058701, loss_cps: 0.104248
[14:26:06.393] iteration 29134: total_loss: 0.363272, loss_sup: 0.064157, loss_mps: 0.103717, loss_cps: 0.195397
[14:26:06.539] iteration 29135: total_loss: 0.228041, loss_sup: 0.031644, loss_mps: 0.072804, loss_cps: 0.123592
[14:26:06.688] iteration 29136: total_loss: 0.554731, loss_sup: 0.113677, loss_mps: 0.141449, loss_cps: 0.299605
[14:26:06.833] iteration 29137: total_loss: 0.440256, loss_sup: 0.041349, loss_mps: 0.128441, loss_cps: 0.270466
[14:26:06.982] iteration 29138: total_loss: 0.328048, loss_sup: 0.021143, loss_mps: 0.106552, loss_cps: 0.200352
[14:26:07.128] iteration 29139: total_loss: 0.247862, loss_sup: 0.003508, loss_mps: 0.085314, loss_cps: 0.159040
[14:26:07.278] iteration 29140: total_loss: 0.324611, loss_sup: 0.006222, loss_mps: 0.112456, loss_cps: 0.205933
[14:26:07.425] iteration 29141: total_loss: 0.237488, loss_sup: 0.033484, loss_mps: 0.072260, loss_cps: 0.131743
[14:26:07.571] iteration 29142: total_loss: 0.260603, loss_sup: 0.010131, loss_mps: 0.086604, loss_cps: 0.163868
[14:26:07.719] iteration 29143: total_loss: 0.235235, loss_sup: 0.004700, loss_mps: 0.084680, loss_cps: 0.145855
[14:26:07.866] iteration 29144: total_loss: 0.169892, loss_sup: 0.002009, loss_mps: 0.063103, loss_cps: 0.104779
[14:26:08.012] iteration 29145: total_loss: 0.150096, loss_sup: 0.009792, loss_mps: 0.054129, loss_cps: 0.086176
[14:26:08.158] iteration 29146: total_loss: 0.173091, loss_sup: 0.001211, loss_mps: 0.064518, loss_cps: 0.107362
[14:26:08.304] iteration 29147: total_loss: 0.163357, loss_sup: 0.002095, loss_mps: 0.058157, loss_cps: 0.103106
[14:26:08.450] iteration 29148: total_loss: 0.308102, loss_sup: 0.112381, loss_mps: 0.073179, loss_cps: 0.122543
[14:26:08.597] iteration 29149: total_loss: 0.385309, loss_sup: 0.028505, loss_mps: 0.112798, loss_cps: 0.244006
[14:26:08.744] iteration 29150: total_loss: 0.414378, loss_sup: 0.015472, loss_mps: 0.133813, loss_cps: 0.265093
[14:26:08.890] iteration 29151: total_loss: 0.259647, loss_sup: 0.044295, loss_mps: 0.076064, loss_cps: 0.139287
[14:26:09.037] iteration 29152: total_loss: 0.501094, loss_sup: 0.064496, loss_mps: 0.139238, loss_cps: 0.297360
[14:26:09.184] iteration 29153: total_loss: 0.284121, loss_sup: 0.011021, loss_mps: 0.099172, loss_cps: 0.173928
[14:26:09.332] iteration 29154: total_loss: 0.351470, loss_sup: 0.033399, loss_mps: 0.111259, loss_cps: 0.206812
[14:26:09.478] iteration 29155: total_loss: 0.172854, loss_sup: 0.015721, loss_mps: 0.057126, loss_cps: 0.100007
[14:26:09.625] iteration 29156: total_loss: 0.195615, loss_sup: 0.000480, loss_mps: 0.073761, loss_cps: 0.121374
[14:26:09.770] iteration 29157: total_loss: 0.199490, loss_sup: 0.006447, loss_mps: 0.068680, loss_cps: 0.124363
[14:26:09.916] iteration 29158: total_loss: 0.491993, loss_sup: 0.170158, loss_mps: 0.107182, loss_cps: 0.214654
[14:26:10.064] iteration 29159: total_loss: 0.488240, loss_sup: 0.047492, loss_mps: 0.142281, loss_cps: 0.298467
[14:26:10.210] iteration 29160: total_loss: 0.301672, loss_sup: 0.010990, loss_mps: 0.103846, loss_cps: 0.186835
[14:26:10.356] iteration 29161: total_loss: 0.355618, loss_sup: 0.023064, loss_mps: 0.112518, loss_cps: 0.220036
[14:26:10.502] iteration 29162: total_loss: 0.334827, loss_sup: 0.040390, loss_mps: 0.095599, loss_cps: 0.198838
[14:26:10.649] iteration 29163: total_loss: 0.623203, loss_sup: 0.049082, loss_mps: 0.175940, loss_cps: 0.398181
[14:26:10.794] iteration 29164: total_loss: 0.202217, loss_sup: 0.016462, loss_mps: 0.065273, loss_cps: 0.120481
[14:26:10.940] iteration 29165: total_loss: 0.312455, loss_sup: 0.028500, loss_mps: 0.099062, loss_cps: 0.184893
[14:26:11.087] iteration 29166: total_loss: 0.195930, loss_sup: 0.015831, loss_mps: 0.063213, loss_cps: 0.116885
[14:26:11.234] iteration 29167: total_loss: 0.163780, loss_sup: 0.012531, loss_mps: 0.054451, loss_cps: 0.096798
[14:26:11.379] iteration 29168: total_loss: 0.406812, loss_sup: 0.175950, loss_mps: 0.086536, loss_cps: 0.144326
[14:26:11.525] iteration 29169: total_loss: 0.413039, loss_sup: 0.002146, loss_mps: 0.134764, loss_cps: 0.276130
[14:26:11.670] iteration 29170: total_loss: 0.385312, loss_sup: 0.044186, loss_mps: 0.116473, loss_cps: 0.224653
[14:26:11.816] iteration 29171: total_loss: 0.178192, loss_sup: 0.004011, loss_mps: 0.061640, loss_cps: 0.112541
[14:26:11.962] iteration 29172: total_loss: 0.246945, loss_sup: 0.034240, loss_mps: 0.076443, loss_cps: 0.136262
[14:26:12.108] iteration 29173: total_loss: 0.150746, loss_sup: 0.003713, loss_mps: 0.053935, loss_cps: 0.093098
[14:26:12.253] iteration 29174: total_loss: 0.188321, loss_sup: 0.042334, loss_mps: 0.053750, loss_cps: 0.092236
[14:26:12.400] iteration 29175: total_loss: 0.292820, loss_sup: 0.022561, loss_mps: 0.090744, loss_cps: 0.179515
[14:26:12.547] iteration 29176: total_loss: 0.371574, loss_sup: 0.120185, loss_mps: 0.090104, loss_cps: 0.161285
[14:26:12.693] iteration 29177: total_loss: 0.195488, loss_sup: 0.010883, loss_mps: 0.066845, loss_cps: 0.117761
[14:26:12.841] iteration 29178: total_loss: 0.212574, loss_sup: 0.019796, loss_mps: 0.069443, loss_cps: 0.123335
[14:26:12.989] iteration 29179: total_loss: 0.221588, loss_sup: 0.023245, loss_mps: 0.069429, loss_cps: 0.128915
[14:26:13.136] iteration 29180: total_loss: 0.440628, loss_sup: 0.000813, loss_mps: 0.139204, loss_cps: 0.300611
[14:26:13.283] iteration 29181: total_loss: 0.452917, loss_sup: 0.005924, loss_mps: 0.138594, loss_cps: 0.308398
[14:26:13.429] iteration 29182: total_loss: 0.169830, loss_sup: 0.011672, loss_mps: 0.061826, loss_cps: 0.096332
[14:26:13.575] iteration 29183: total_loss: 0.201591, loss_sup: 0.026458, loss_mps: 0.066150, loss_cps: 0.108983
[14:26:13.721] iteration 29184: total_loss: 0.347887, loss_sup: 0.092299, loss_mps: 0.090873, loss_cps: 0.164715
[14:26:13.868] iteration 29185: total_loss: 0.220339, loss_sup: 0.035444, loss_mps: 0.067499, loss_cps: 0.117396
[14:26:14.014] iteration 29186: total_loss: 0.194865, loss_sup: 0.022331, loss_mps: 0.061220, loss_cps: 0.111313
[14:26:14.162] iteration 29187: total_loss: 0.397740, loss_sup: 0.050584, loss_mps: 0.112926, loss_cps: 0.234229
[14:26:14.308] iteration 29188: total_loss: 0.232455, loss_sup: 0.052021, loss_mps: 0.064185, loss_cps: 0.116249
[14:26:14.454] iteration 29189: total_loss: 0.221938, loss_sup: 0.029519, loss_mps: 0.072344, loss_cps: 0.120075
[14:26:14.600] iteration 29190: total_loss: 0.438071, loss_sup: 0.134963, loss_mps: 0.101821, loss_cps: 0.201287
[14:26:14.748] iteration 29191: total_loss: 0.316618, loss_sup: 0.033188, loss_mps: 0.104553, loss_cps: 0.178877
[14:26:14.895] iteration 29192: total_loss: 0.450659, loss_sup: 0.169582, loss_mps: 0.099104, loss_cps: 0.181973
[14:26:15.041] iteration 29193: total_loss: 0.352454, loss_sup: 0.042795, loss_mps: 0.100952, loss_cps: 0.208707
[14:26:15.188] iteration 29194: total_loss: 0.152918, loss_sup: 0.000966, loss_mps: 0.055105, loss_cps: 0.096847
[14:26:15.335] iteration 29195: total_loss: 0.169926, loss_sup: 0.019151, loss_mps: 0.051853, loss_cps: 0.098921
[14:26:15.481] iteration 29196: total_loss: 0.514987, loss_sup: 0.064084, loss_mps: 0.143949, loss_cps: 0.306953
[14:26:15.627] iteration 29197: total_loss: 0.263459, loss_sup: 0.008337, loss_mps: 0.093268, loss_cps: 0.161854
[14:26:15.776] iteration 29198: total_loss: 0.232352, loss_sup: 0.009894, loss_mps: 0.077498, loss_cps: 0.144960
[14:26:15.922] iteration 29199: total_loss: 0.262112, loss_sup: 0.052514, loss_mps: 0.079176, loss_cps: 0.130422
[14:26:16.068] iteration 29200: total_loss: 0.197479, loss_sup: 0.018657, loss_mps: 0.069715, loss_cps: 0.109106
[14:26:16.068] Evaluation Started ==>
[14:26:27.378] ==> valid iteration 29200: unet metrics: {'dc': 0.6723245753756774, 'jc': 0.5570084785324506, 'pre': 0.8047577622014765, 'hd': 5.383168707344751}, ynet metrics: {'dc': 0.6134808736708791, 'jc': 0.5021682883190538, 'pre': 0.7989108636971336, 'hd': 5.350617270910524}.
[14:26:27.379] Evaluation Finished!⏹️
[14:26:27.532] iteration 29201: total_loss: 0.399477, loss_sup: 0.018409, loss_mps: 0.125320, loss_cps: 0.255748
[14:26:27.682] iteration 29202: total_loss: 0.241447, loss_sup: 0.056748, loss_mps: 0.068212, loss_cps: 0.116488
[14:26:27.827] iteration 29203: total_loss: 0.321089, loss_sup: 0.050473, loss_mps: 0.104551, loss_cps: 0.166065
[14:26:27.973] iteration 29204: total_loss: 0.317044, loss_sup: 0.025188, loss_mps: 0.099112, loss_cps: 0.192745
[14:26:28.120] iteration 29205: total_loss: 0.318358, loss_sup: 0.141344, loss_mps: 0.062671, loss_cps: 0.114343
[14:26:28.266] iteration 29206: total_loss: 0.128815, loss_sup: 0.000504, loss_mps: 0.047328, loss_cps: 0.080983
[14:26:28.413] iteration 29207: total_loss: 0.135792, loss_sup: 0.001074, loss_mps: 0.051133, loss_cps: 0.083586
[14:26:28.558] iteration 29208: total_loss: 0.264910, loss_sup: 0.029476, loss_mps: 0.083687, loss_cps: 0.151746
[14:26:28.704] iteration 29209: total_loss: 0.458168, loss_sup: 0.102632, loss_mps: 0.117224, loss_cps: 0.238313
[14:26:28.849] iteration 29210: total_loss: 0.296637, loss_sup: 0.009953, loss_mps: 0.091253, loss_cps: 0.195431
[14:26:28.994] iteration 29211: total_loss: 0.288269, loss_sup: 0.032800, loss_mps: 0.096588, loss_cps: 0.158881
[14:26:29.140] iteration 29212: total_loss: 0.331796, loss_sup: 0.004528, loss_mps: 0.112123, loss_cps: 0.215145
[14:26:29.285] iteration 29213: total_loss: 0.196199, loss_sup: 0.045073, loss_mps: 0.056830, loss_cps: 0.094297
[14:26:29.433] iteration 29214: total_loss: 0.263698, loss_sup: 0.030374, loss_mps: 0.085247, loss_cps: 0.148077
[14:26:29.579] iteration 29215: total_loss: 0.179046, loss_sup: 0.005305, loss_mps: 0.061844, loss_cps: 0.111896
[14:26:29.725] iteration 29216: total_loss: 0.357192, loss_sup: 0.010969, loss_mps: 0.110677, loss_cps: 0.235547
[14:26:29.871] iteration 29217: total_loss: 0.311747, loss_sup: 0.032893, loss_mps: 0.100804, loss_cps: 0.178049
[14:26:30.018] iteration 29218: total_loss: 0.111529, loss_sup: 0.005586, loss_mps: 0.039919, loss_cps: 0.066024
[14:26:30.164] iteration 29219: total_loss: 0.237883, loss_sup: 0.006869, loss_mps: 0.076804, loss_cps: 0.154209
[14:26:30.310] iteration 29220: total_loss: 0.190434, loss_sup: 0.035590, loss_mps: 0.056727, loss_cps: 0.098117
[14:26:30.455] iteration 29221: total_loss: 0.159777, loss_sup: 0.003277, loss_mps: 0.057340, loss_cps: 0.099160
[14:26:30.601] iteration 29222: total_loss: 0.204721, loss_sup: 0.013872, loss_mps: 0.070677, loss_cps: 0.120172
[14:26:30.747] iteration 29223: total_loss: 0.248828, loss_sup: 0.077165, loss_mps: 0.062821, loss_cps: 0.108842
[14:26:30.892] iteration 29224: total_loss: 0.275198, loss_sup: 0.034100, loss_mps: 0.083762, loss_cps: 0.157335
[14:26:31.038] iteration 29225: total_loss: 0.296213, loss_sup: 0.082374, loss_mps: 0.078176, loss_cps: 0.135663
[14:26:31.185] iteration 29226: total_loss: 0.325636, loss_sup: 0.025746, loss_mps: 0.100622, loss_cps: 0.199268
[14:26:31.331] iteration 29227: total_loss: 0.591835, loss_sup: 0.163703, loss_mps: 0.132743, loss_cps: 0.295390
[14:26:31.476] iteration 29228: total_loss: 0.444988, loss_sup: 0.116164, loss_mps: 0.102484, loss_cps: 0.226340
[14:26:31.623] iteration 29229: total_loss: 0.301800, loss_sup: 0.004970, loss_mps: 0.101157, loss_cps: 0.195673
[14:26:31.769] iteration 29230: total_loss: 0.161463, loss_sup: 0.003257, loss_mps: 0.060554, loss_cps: 0.097652
[14:26:31.915] iteration 29231: total_loss: 0.582524, loss_sup: 0.228229, loss_mps: 0.117008, loss_cps: 0.237287
[14:26:32.061] iteration 29232: total_loss: 0.406221, loss_sup: 0.024734, loss_mps: 0.122629, loss_cps: 0.258858
[14:26:32.206] iteration 29233: total_loss: 0.234667, loss_sup: 0.004085, loss_mps: 0.084414, loss_cps: 0.146168
[14:26:32.352] iteration 29234: total_loss: 0.183451, loss_sup: 0.013757, loss_mps: 0.062663, loss_cps: 0.107031
[14:26:32.498] iteration 29235: total_loss: 0.272089, loss_sup: 0.007721, loss_mps: 0.091769, loss_cps: 0.172599
[14:26:32.644] iteration 29236: total_loss: 0.343079, loss_sup: 0.036650, loss_mps: 0.099582, loss_cps: 0.206847
[14:26:32.790] iteration 29237: total_loss: 0.314964, loss_sup: 0.056640, loss_mps: 0.089218, loss_cps: 0.169106
[14:26:32.938] iteration 29238: total_loss: 0.188202, loss_sup: 0.021087, loss_mps: 0.063403, loss_cps: 0.103712
[14:26:33.085] iteration 29239: total_loss: 0.142986, loss_sup: 0.003835, loss_mps: 0.053582, loss_cps: 0.085568
[14:26:33.231] iteration 29240: total_loss: 0.184567, loss_sup: 0.001386, loss_mps: 0.064642, loss_cps: 0.118539
[14:26:33.376] iteration 29241: total_loss: 0.273238, loss_sup: 0.005638, loss_mps: 0.096099, loss_cps: 0.171500
[14:26:33.523] iteration 29242: total_loss: 0.262398, loss_sup: 0.049086, loss_mps: 0.079494, loss_cps: 0.133818
[14:26:33.669] iteration 29243: total_loss: 0.349504, loss_sup: 0.037466, loss_mps: 0.102964, loss_cps: 0.209074
[14:26:33.815] iteration 29244: total_loss: 0.299044, loss_sup: 0.012961, loss_mps: 0.097491, loss_cps: 0.188592
[14:26:33.963] iteration 29245: total_loss: 0.352708, loss_sup: 0.005277, loss_mps: 0.113913, loss_cps: 0.233519
[14:26:34.108] iteration 29246: total_loss: 0.137265, loss_sup: 0.000895, loss_mps: 0.052542, loss_cps: 0.083828
[14:26:34.254] iteration 29247: total_loss: 0.400196, loss_sup: 0.087033, loss_mps: 0.103140, loss_cps: 0.210023
[14:26:34.399] iteration 29248: total_loss: 0.180741, loss_sup: 0.007037, loss_mps: 0.067055, loss_cps: 0.106649
[14:26:34.545] iteration 29249: total_loss: 0.280827, loss_sup: 0.020559, loss_mps: 0.094014, loss_cps: 0.166253
[14:26:34.692] iteration 29250: total_loss: 0.294373, loss_sup: 0.059275, loss_mps: 0.085198, loss_cps: 0.149900
[14:26:34.838] iteration 29251: total_loss: 0.214406, loss_sup: 0.012281, loss_mps: 0.073971, loss_cps: 0.128154
[14:26:34.983] iteration 29252: total_loss: 0.720642, loss_sup: 0.257934, loss_mps: 0.151684, loss_cps: 0.311023
[14:26:35.129] iteration 29253: total_loss: 0.282259, loss_sup: 0.066915, loss_mps: 0.078883, loss_cps: 0.136461
[14:26:35.275] iteration 29254: total_loss: 0.136780, loss_sup: 0.006206, loss_mps: 0.048312, loss_cps: 0.082262
[14:26:35.420] iteration 29255: total_loss: 0.210190, loss_sup: 0.026646, loss_mps: 0.066223, loss_cps: 0.117321
[14:26:35.566] iteration 29256: total_loss: 0.309670, loss_sup: 0.039046, loss_mps: 0.088142, loss_cps: 0.182482
[14:26:35.714] iteration 29257: total_loss: 0.173554, loss_sup: 0.014372, loss_mps: 0.059364, loss_cps: 0.099819
[14:26:35.860] iteration 29258: total_loss: 0.222656, loss_sup: 0.036044, loss_mps: 0.070007, loss_cps: 0.116605
[14:26:36.005] iteration 29259: total_loss: 0.154623, loss_sup: 0.015395, loss_mps: 0.054511, loss_cps: 0.084717
[14:26:36.066] iteration 29260: total_loss: 0.228242, loss_sup: 0.000474, loss_mps: 0.079639, loss_cps: 0.148130
[14:26:37.272] iteration 29261: total_loss: 0.256935, loss_sup: 0.004423, loss_mps: 0.090001, loss_cps: 0.162511
[14:26:37.420] iteration 29262: total_loss: 0.453028, loss_sup: 0.050686, loss_mps: 0.137174, loss_cps: 0.265169
[14:26:37.566] iteration 29263: total_loss: 0.176374, loss_sup: 0.000807, loss_mps: 0.064970, loss_cps: 0.110596
[14:26:37.713] iteration 29264: total_loss: 0.375526, loss_sup: 0.001831, loss_mps: 0.124747, loss_cps: 0.248948
[14:26:37.859] iteration 29265: total_loss: 0.691166, loss_sup: 0.188800, loss_mps: 0.158345, loss_cps: 0.344022
[14:26:38.005] iteration 29266: total_loss: 0.187952, loss_sup: 0.002018, loss_mps: 0.066398, loss_cps: 0.119537
[14:26:38.152] iteration 29267: total_loss: 0.240274, loss_sup: 0.039308, loss_mps: 0.070212, loss_cps: 0.130754
[14:26:38.302] iteration 29268: total_loss: 0.283798, loss_sup: 0.037582, loss_mps: 0.086866, loss_cps: 0.159349
[14:26:38.451] iteration 29269: total_loss: 0.148587, loss_sup: 0.003120, loss_mps: 0.057871, loss_cps: 0.087596
[14:26:38.598] iteration 29270: total_loss: 0.325111, loss_sup: 0.130380, loss_mps: 0.073259, loss_cps: 0.121471
[14:26:38.745] iteration 29271: total_loss: 0.273084, loss_sup: 0.003942, loss_mps: 0.093666, loss_cps: 0.175476
[14:26:38.891] iteration 29272: total_loss: 0.370728, loss_sup: 0.033889, loss_mps: 0.114982, loss_cps: 0.221857
[14:26:39.037] iteration 29273: total_loss: 0.189335, loss_sup: 0.025238, loss_mps: 0.060224, loss_cps: 0.103873
[14:26:39.184] iteration 29274: total_loss: 0.779118, loss_sup: 0.199748, loss_mps: 0.183820, loss_cps: 0.395550
[14:26:39.330] iteration 29275: total_loss: 0.231022, loss_sup: 0.008490, loss_mps: 0.088286, loss_cps: 0.134246
[14:26:39.476] iteration 29276: total_loss: 0.550282, loss_sup: 0.063708, loss_mps: 0.151346, loss_cps: 0.335228
[14:26:39.622] iteration 29277: total_loss: 0.186486, loss_sup: 0.014232, loss_mps: 0.063649, loss_cps: 0.108605
[14:26:39.771] iteration 29278: total_loss: 0.237557, loss_sup: 0.005681, loss_mps: 0.083460, loss_cps: 0.148416
[14:26:39.917] iteration 29279: total_loss: 0.274148, loss_sup: 0.002423, loss_mps: 0.098041, loss_cps: 0.173684
[14:26:40.063] iteration 29280: total_loss: 0.192908, loss_sup: 0.024860, loss_mps: 0.065255, loss_cps: 0.102793
[14:26:40.209] iteration 29281: total_loss: 0.189094, loss_sup: 0.008654, loss_mps: 0.067499, loss_cps: 0.112941
[14:26:40.355] iteration 29282: total_loss: 0.498785, loss_sup: 0.028558, loss_mps: 0.148465, loss_cps: 0.321761
[14:26:40.501] iteration 29283: total_loss: 0.217395, loss_sup: 0.003204, loss_mps: 0.076653, loss_cps: 0.137537
[14:26:40.647] iteration 29284: total_loss: 0.206351, loss_sup: 0.006222, loss_mps: 0.070979, loss_cps: 0.129149
[14:26:40.794] iteration 29285: total_loss: 0.391261, loss_sup: 0.112044, loss_mps: 0.104162, loss_cps: 0.175055
[14:26:40.941] iteration 29286: total_loss: 0.134392, loss_sup: 0.010523, loss_mps: 0.046323, loss_cps: 0.077546
[14:26:41.087] iteration 29287: total_loss: 0.598636, loss_sup: 0.119777, loss_mps: 0.142372, loss_cps: 0.336487
[14:26:41.234] iteration 29288: total_loss: 0.171551, loss_sup: 0.002197, loss_mps: 0.063258, loss_cps: 0.106096
[14:26:41.380] iteration 29289: total_loss: 0.295903, loss_sup: 0.015763, loss_mps: 0.094363, loss_cps: 0.185777
[14:26:41.526] iteration 29290: total_loss: 0.136937, loss_sup: 0.000685, loss_mps: 0.051371, loss_cps: 0.084881
[14:26:41.673] iteration 29291: total_loss: 0.404902, loss_sup: 0.053105, loss_mps: 0.116735, loss_cps: 0.235062
[14:26:41.821] iteration 29292: total_loss: 0.431960, loss_sup: 0.164657, loss_mps: 0.095973, loss_cps: 0.171329
[14:26:41.967] iteration 29293: total_loss: 0.237619, loss_sup: 0.034415, loss_mps: 0.070939, loss_cps: 0.132265
[14:26:42.113] iteration 29294: total_loss: 0.336585, loss_sup: 0.147896, loss_mps: 0.071360, loss_cps: 0.117328
[14:26:42.260] iteration 29295: total_loss: 0.183421, loss_sup: 0.003325, loss_mps: 0.068778, loss_cps: 0.111318
[14:26:42.406] iteration 29296: total_loss: 0.461262, loss_sup: 0.084528, loss_mps: 0.126176, loss_cps: 0.250557
[14:26:42.552] iteration 29297: total_loss: 0.246930, loss_sup: 0.032330, loss_mps: 0.077033, loss_cps: 0.137566
[14:26:42.698] iteration 29298: total_loss: 0.283610, loss_sup: 0.012057, loss_mps: 0.092502, loss_cps: 0.179051
[14:26:42.845] iteration 29299: total_loss: 0.280658, loss_sup: 0.034262, loss_mps: 0.086046, loss_cps: 0.160350
[14:26:42.993] iteration 29300: total_loss: 0.334480, loss_sup: 0.062832, loss_mps: 0.091999, loss_cps: 0.179649
[14:26:42.993] Evaluation Started ==>
[14:26:54.362] ==> valid iteration 29300: unet metrics: {'dc': 0.6773714734663376, 'jc': 0.5640591833988619, 'pre': 0.8037585113047472, 'hd': 5.312817362842784}, ynet metrics: {'dc': 0.6226166091157201, 'jc': 0.5115591718081774, 'pre': 0.7982633325326491, 'hd': 5.368231204841641}.
[14:26:54.364] Evaluation Finished!⏹️
[14:26:54.518] iteration 29301: total_loss: 0.220948, loss_sup: 0.015929, loss_mps: 0.076200, loss_cps: 0.128819
[14:26:54.668] iteration 29302: total_loss: 0.158218, loss_sup: 0.015581, loss_mps: 0.054328, loss_cps: 0.088309
[14:26:54.813] iteration 29303: total_loss: 0.252867, loss_sup: 0.063959, loss_mps: 0.070250, loss_cps: 0.118658
[14:26:54.958] iteration 29304: total_loss: 0.252076, loss_sup: 0.055924, loss_mps: 0.074084, loss_cps: 0.122067
[14:26:55.104] iteration 29305: total_loss: 0.191808, loss_sup: 0.009266, loss_mps: 0.065247, loss_cps: 0.117295
[14:26:55.249] iteration 29306: total_loss: 0.282055, loss_sup: 0.076309, loss_mps: 0.078686, loss_cps: 0.127060
[14:26:55.394] iteration 29307: total_loss: 0.131504, loss_sup: 0.012851, loss_mps: 0.045303, loss_cps: 0.073349
[14:26:55.542] iteration 29308: total_loss: 0.165956, loss_sup: 0.006276, loss_mps: 0.062484, loss_cps: 0.097195
[14:26:55.688] iteration 29309: total_loss: 0.524103, loss_sup: 0.099138, loss_mps: 0.135538, loss_cps: 0.289427
[14:26:55.833] iteration 29310: total_loss: 0.276109, loss_sup: 0.007488, loss_mps: 0.094299, loss_cps: 0.174322
[14:26:55.979] iteration 29311: total_loss: 0.348958, loss_sup: 0.102542, loss_mps: 0.080996, loss_cps: 0.165420
[14:26:56.124] iteration 29312: total_loss: 0.256438, loss_sup: 0.019687, loss_mps: 0.088256, loss_cps: 0.148495
[14:26:56.269] iteration 29313: total_loss: 0.345863, loss_sup: 0.063578, loss_mps: 0.104463, loss_cps: 0.177822
[14:26:56.414] iteration 29314: total_loss: 0.262376, loss_sup: 0.045389, loss_mps: 0.077026, loss_cps: 0.139960
[14:26:56.562] iteration 29315: total_loss: 0.270449, loss_sup: 0.035284, loss_mps: 0.078377, loss_cps: 0.156788
[14:26:56.707] iteration 29316: total_loss: 0.341468, loss_sup: 0.061448, loss_mps: 0.097870, loss_cps: 0.182150
[14:26:56.853] iteration 29317: total_loss: 0.251856, loss_sup: 0.019974, loss_mps: 0.079858, loss_cps: 0.152023
[14:26:56.999] iteration 29318: total_loss: 0.214382, loss_sup: 0.032217, loss_mps: 0.066113, loss_cps: 0.116052
[14:26:57.144] iteration 29319: total_loss: 0.389759, loss_sup: 0.119056, loss_mps: 0.090697, loss_cps: 0.180006
[14:26:57.291] iteration 29320: total_loss: 0.234309, loss_sup: 0.086372, loss_mps: 0.054770, loss_cps: 0.093168
[14:26:57.437] iteration 29321: total_loss: 0.492246, loss_sup: 0.116691, loss_mps: 0.124560, loss_cps: 0.250995
[14:26:57.583] iteration 29322: total_loss: 0.443588, loss_sup: 0.035646, loss_mps: 0.135039, loss_cps: 0.272904
[14:26:57.728] iteration 29323: total_loss: 0.172670, loss_sup: 0.008947, loss_mps: 0.058815, loss_cps: 0.104908
[14:26:57.874] iteration 29324: total_loss: 0.209694, loss_sup: 0.006495, loss_mps: 0.070549, loss_cps: 0.132651
[14:26:58.020] iteration 29325: total_loss: 0.227443, loss_sup: 0.012849, loss_mps: 0.080309, loss_cps: 0.134286
[14:26:58.166] iteration 29326: total_loss: 0.214336, loss_sup: 0.009912, loss_mps: 0.072113, loss_cps: 0.132312
[14:26:58.312] iteration 29327: total_loss: 0.239146, loss_sup: 0.061132, loss_mps: 0.067506, loss_cps: 0.110508
[14:26:58.459] iteration 29328: total_loss: 0.171621, loss_sup: 0.005938, loss_mps: 0.060291, loss_cps: 0.105393
[14:26:58.605] iteration 29329: total_loss: 0.333365, loss_sup: 0.048945, loss_mps: 0.099351, loss_cps: 0.185069
[14:26:58.751] iteration 29330: total_loss: 0.203125, loss_sup: 0.014448, loss_mps: 0.069643, loss_cps: 0.119034
[14:26:58.896] iteration 29331: total_loss: 0.502002, loss_sup: 0.024066, loss_mps: 0.153679, loss_cps: 0.324257
[14:26:59.042] iteration 29332: total_loss: 0.188764, loss_sup: 0.017743, loss_mps: 0.061279, loss_cps: 0.109742
[14:26:59.196] iteration 29333: total_loss: 0.199592, loss_sup: 0.010854, loss_mps: 0.070091, loss_cps: 0.118647
[14:26:59.342] iteration 29334: total_loss: 0.393854, loss_sup: 0.151296, loss_mps: 0.085754, loss_cps: 0.156804
[14:26:59.487] iteration 29335: total_loss: 0.303632, loss_sup: 0.010150, loss_mps: 0.100938, loss_cps: 0.192544
[14:26:59.633] iteration 29336: total_loss: 0.201076, loss_sup: 0.004323, loss_mps: 0.070564, loss_cps: 0.126189
[14:26:59.780] iteration 29337: total_loss: 0.313848, loss_sup: 0.002430, loss_mps: 0.103064, loss_cps: 0.208354
[14:26:59.926] iteration 29338: total_loss: 0.147771, loss_sup: 0.000810, loss_mps: 0.055771, loss_cps: 0.091190
[14:27:00.074] iteration 29339: total_loss: 0.406637, loss_sup: 0.076073, loss_mps: 0.117347, loss_cps: 0.213217
[14:27:00.221] iteration 29340: total_loss: 0.190507, loss_sup: 0.012458, loss_mps: 0.064228, loss_cps: 0.113821
[14:27:00.367] iteration 29341: total_loss: 0.324340, loss_sup: 0.051669, loss_mps: 0.095952, loss_cps: 0.176719
[14:27:00.513] iteration 29342: total_loss: 0.198945, loss_sup: 0.002545, loss_mps: 0.070741, loss_cps: 0.125659
[14:27:00.659] iteration 29343: total_loss: 0.515438, loss_sup: 0.174680, loss_mps: 0.106736, loss_cps: 0.234022
[14:27:00.807] iteration 29344: total_loss: 0.149226, loss_sup: 0.024602, loss_mps: 0.047007, loss_cps: 0.077617
[14:27:00.953] iteration 29345: total_loss: 0.307518, loss_sup: 0.021814, loss_mps: 0.101190, loss_cps: 0.184514
[14:27:01.099] iteration 29346: total_loss: 0.216240, loss_sup: 0.030624, loss_mps: 0.066410, loss_cps: 0.119207
[14:27:01.245] iteration 29347: total_loss: 0.209634, loss_sup: 0.050992, loss_mps: 0.061449, loss_cps: 0.097193
[14:27:01.390] iteration 29348: total_loss: 0.364981, loss_sup: 0.048869, loss_mps: 0.110359, loss_cps: 0.205753
[14:27:01.535] iteration 29349: total_loss: 0.275364, loss_sup: 0.027048, loss_mps: 0.089913, loss_cps: 0.158403
[14:27:01.684] iteration 29350: total_loss: 0.147444, loss_sup: 0.003622, loss_mps: 0.055856, loss_cps: 0.087966
[14:27:01.833] iteration 29351: total_loss: 0.607745, loss_sup: 0.138286, loss_mps: 0.153775, loss_cps: 0.315684
[14:27:01.981] iteration 29352: total_loss: 0.348874, loss_sup: 0.019069, loss_mps: 0.114847, loss_cps: 0.214959
[14:27:02.127] iteration 29353: total_loss: 0.326980, loss_sup: 0.059674, loss_mps: 0.095627, loss_cps: 0.171679
[14:27:02.272] iteration 29354: total_loss: 0.348646, loss_sup: 0.029721, loss_mps: 0.109539, loss_cps: 0.209385
[14:27:02.418] iteration 29355: total_loss: 0.276066, loss_sup: 0.053617, loss_mps: 0.080071, loss_cps: 0.142378
[14:27:02.568] iteration 29356: total_loss: 0.211990, loss_sup: 0.011628, loss_mps: 0.071272, loss_cps: 0.129090
[14:27:02.715] iteration 29357: total_loss: 0.361524, loss_sup: 0.112323, loss_mps: 0.094071, loss_cps: 0.155129
[14:27:02.861] iteration 29358: total_loss: 0.231674, loss_sup: 0.012586, loss_mps: 0.075422, loss_cps: 0.143666
[14:27:03.007] iteration 29359: total_loss: 0.327294, loss_sup: 0.098222, loss_mps: 0.080693, loss_cps: 0.148380
[14:27:03.153] iteration 29360: total_loss: 0.151270, loss_sup: 0.006730, loss_mps: 0.056762, loss_cps: 0.087778
[14:27:03.299] iteration 29361: total_loss: 0.308523, loss_sup: 0.010759, loss_mps: 0.099982, loss_cps: 0.197781
[14:27:03.445] iteration 29362: total_loss: 0.395278, loss_sup: 0.001544, loss_mps: 0.126880, loss_cps: 0.266854
[14:27:03.590] iteration 29363: total_loss: 0.222851, loss_sup: 0.014481, loss_mps: 0.077746, loss_cps: 0.130624
[14:27:03.736] iteration 29364: total_loss: 0.370868, loss_sup: 0.053339, loss_mps: 0.109576, loss_cps: 0.207953
[14:27:03.881] iteration 29365: total_loss: 0.431670, loss_sup: 0.073934, loss_mps: 0.116891, loss_cps: 0.240845
[14:27:04.027] iteration 29366: total_loss: 0.374997, loss_sup: 0.049842, loss_mps: 0.107028, loss_cps: 0.218126
[14:27:04.173] iteration 29367: total_loss: 0.274409, loss_sup: 0.010528, loss_mps: 0.095642, loss_cps: 0.168239
[14:27:04.318] iteration 29368: total_loss: 0.272969, loss_sup: 0.018052, loss_mps: 0.090014, loss_cps: 0.164903
[14:27:04.465] iteration 29369: total_loss: 0.235623, loss_sup: 0.032618, loss_mps: 0.077963, loss_cps: 0.125042
[14:27:04.611] iteration 29370: total_loss: 0.288500, loss_sup: 0.154208, loss_mps: 0.050719, loss_cps: 0.083573
[14:27:04.757] iteration 29371: total_loss: 0.226374, loss_sup: 0.026717, loss_mps: 0.070400, loss_cps: 0.129257
[14:27:04.903] iteration 29372: total_loss: 0.296366, loss_sup: 0.006319, loss_mps: 0.105092, loss_cps: 0.184955
[14:27:05.049] iteration 29373: total_loss: 0.302933, loss_sup: 0.092974, loss_mps: 0.079791, loss_cps: 0.130168
[14:27:05.195] iteration 29374: total_loss: 0.252045, loss_sup: 0.003871, loss_mps: 0.088285, loss_cps: 0.159889
[14:27:05.341] iteration 29375: total_loss: 0.243450, loss_sup: 0.003769, loss_mps: 0.081486, loss_cps: 0.158195
[14:27:05.490] iteration 29376: total_loss: 0.235650, loss_sup: 0.077957, loss_mps: 0.061145, loss_cps: 0.096548
[14:27:05.637] iteration 29377: total_loss: 0.384933, loss_sup: 0.075177, loss_mps: 0.105543, loss_cps: 0.204213
[14:27:05.783] iteration 29378: total_loss: 0.177430, loss_sup: 0.016680, loss_mps: 0.061945, loss_cps: 0.098805
[14:27:05.929] iteration 29379: total_loss: 0.317415, loss_sup: 0.007195, loss_mps: 0.103439, loss_cps: 0.206781
[14:27:06.077] iteration 29380: total_loss: 0.274943, loss_sup: 0.039319, loss_mps: 0.088668, loss_cps: 0.146956
[14:27:06.224] iteration 29381: total_loss: 0.364935, loss_sup: 0.051501, loss_mps: 0.108746, loss_cps: 0.204689
[14:27:06.370] iteration 29382: total_loss: 0.453668, loss_sup: 0.020379, loss_mps: 0.138277, loss_cps: 0.295013
[14:27:06.517] iteration 29383: total_loss: 0.452285, loss_sup: 0.007968, loss_mps: 0.137267, loss_cps: 0.307050
[14:27:06.664] iteration 29384: total_loss: 0.280033, loss_sup: 0.010569, loss_mps: 0.095602, loss_cps: 0.173862
[14:27:06.812] iteration 29385: total_loss: 0.185639, loss_sup: 0.012655, loss_mps: 0.064035, loss_cps: 0.108949
[14:27:06.959] iteration 29386: total_loss: 0.200810, loss_sup: 0.032984, loss_mps: 0.061459, loss_cps: 0.106367
[14:27:07.105] iteration 29387: total_loss: 0.204482, loss_sup: 0.025644, loss_mps: 0.068055, loss_cps: 0.110784
[14:27:07.252] iteration 29388: total_loss: 0.186044, loss_sup: 0.010962, loss_mps: 0.064527, loss_cps: 0.110555
[14:27:07.398] iteration 29389: total_loss: 0.330866, loss_sup: 0.029583, loss_mps: 0.106300, loss_cps: 0.194983
[14:27:07.545] iteration 29390: total_loss: 0.396443, loss_sup: 0.178114, loss_mps: 0.079985, loss_cps: 0.138344
[14:27:07.691] iteration 29391: total_loss: 0.315987, loss_sup: 0.001807, loss_mps: 0.107194, loss_cps: 0.206987
[14:27:07.838] iteration 29392: total_loss: 0.140583, loss_sup: 0.002305, loss_mps: 0.051460, loss_cps: 0.086818
[14:27:07.986] iteration 29393: total_loss: 0.416667, loss_sup: 0.142666, loss_mps: 0.098954, loss_cps: 0.175048
[14:27:08.132] iteration 29394: total_loss: 0.133203, loss_sup: 0.000276, loss_mps: 0.049689, loss_cps: 0.083238
[14:27:08.279] iteration 29395: total_loss: 0.299734, loss_sup: 0.027564, loss_mps: 0.092364, loss_cps: 0.179806
[14:27:08.426] iteration 29396: total_loss: 0.147731, loss_sup: 0.011662, loss_mps: 0.048912, loss_cps: 0.087157
[14:27:08.573] iteration 29397: total_loss: 0.212771, loss_sup: 0.063749, loss_mps: 0.056052, loss_cps: 0.092971
[14:27:08.722] iteration 29398: total_loss: 0.221386, loss_sup: 0.017943, loss_mps: 0.074553, loss_cps: 0.128890
[14:27:08.871] iteration 29399: total_loss: 0.261110, loss_sup: 0.004257, loss_mps: 0.086610, loss_cps: 0.170244
[14:27:09.020] iteration 29400: total_loss: 0.358378, loss_sup: 0.125584, loss_mps: 0.084861, loss_cps: 0.147932
[14:27:09.021] Evaluation Started ==>
[14:27:20.390] ==> valid iteration 29400: unet metrics: {'dc': 0.6740244953694458, 'jc': 0.5605271355013294, 'pre': 0.8092933163113263, 'hd': 5.338830605309275}, ynet metrics: {'dc': 0.6382002460522145, 'jc': 0.5268929582764214, 'pre': 0.7996439566257718, 'hd': 5.360862502214145}.
[14:27:20.392] Evaluation Finished!⏹️
[14:27:20.544] iteration 29401: total_loss: 0.229194, loss_sup: 0.013289, loss_mps: 0.077077, loss_cps: 0.138828
[14:27:20.692] iteration 29402: total_loss: 0.228973, loss_sup: 0.041758, loss_mps: 0.067155, loss_cps: 0.120059
[14:27:20.841] iteration 29403: total_loss: 0.227386, loss_sup: 0.005807, loss_mps: 0.075384, loss_cps: 0.146194
[14:27:20.987] iteration 29404: total_loss: 0.318126, loss_sup: 0.091279, loss_mps: 0.079567, loss_cps: 0.147280
[14:27:21.133] iteration 29405: total_loss: 0.284254, loss_sup: 0.087384, loss_mps: 0.067188, loss_cps: 0.129682
[14:27:21.279] iteration 29406: total_loss: 0.330642, loss_sup: 0.035629, loss_mps: 0.107408, loss_cps: 0.187605
[14:27:21.426] iteration 29407: total_loss: 0.173418, loss_sup: 0.012215, loss_mps: 0.057027, loss_cps: 0.104176
[14:27:21.572] iteration 29408: total_loss: 0.248457, loss_sup: 0.065899, loss_mps: 0.062956, loss_cps: 0.119602
[14:27:21.717] iteration 29409: total_loss: 0.403636, loss_sup: 0.063259, loss_mps: 0.118428, loss_cps: 0.221948
[14:27:21.863] iteration 29410: total_loss: 0.205424, loss_sup: 0.020737, loss_mps: 0.064532, loss_cps: 0.120155
[14:27:22.008] iteration 29411: total_loss: 0.236477, loss_sup: 0.044543, loss_mps: 0.072632, loss_cps: 0.119301
[14:27:22.155] iteration 29412: total_loss: 0.233967, loss_sup: 0.005582, loss_mps: 0.078846, loss_cps: 0.149540
[14:27:22.300] iteration 29413: total_loss: 0.306370, loss_sup: 0.032543, loss_mps: 0.091813, loss_cps: 0.182014
[14:27:22.446] iteration 29414: total_loss: 0.211634, loss_sup: 0.002149, loss_mps: 0.075863, loss_cps: 0.133622
[14:27:22.591] iteration 29415: total_loss: 0.295292, loss_sup: 0.066569, loss_mps: 0.085107, loss_cps: 0.143616
[14:27:22.737] iteration 29416: total_loss: 0.310530, loss_sup: 0.009155, loss_mps: 0.103507, loss_cps: 0.197868
[14:27:22.884] iteration 29417: total_loss: 0.191208, loss_sup: 0.006390, loss_mps: 0.069682, loss_cps: 0.115137
[14:27:23.029] iteration 29418: total_loss: 0.221985, loss_sup: 0.036686, loss_mps: 0.068448, loss_cps: 0.116851
[14:27:23.175] iteration 29419: total_loss: 0.233413, loss_sup: 0.029300, loss_mps: 0.075287, loss_cps: 0.128827
[14:27:23.320] iteration 29420: total_loss: 0.207895, loss_sup: 0.002069, loss_mps: 0.078618, loss_cps: 0.127208
[14:27:23.466] iteration 29421: total_loss: 0.381827, loss_sup: 0.039997, loss_mps: 0.115366, loss_cps: 0.226464
[14:27:23.612] iteration 29422: total_loss: 0.346115, loss_sup: 0.053402, loss_mps: 0.103255, loss_cps: 0.189458
[14:27:23.758] iteration 29423: total_loss: 0.249094, loss_sup: 0.039923, loss_mps: 0.072123, loss_cps: 0.137048
[14:27:23.904] iteration 29424: total_loss: 0.200376, loss_sup: 0.019797, loss_mps: 0.068097, loss_cps: 0.112482
[14:27:24.050] iteration 29425: total_loss: 0.532099, loss_sup: 0.035941, loss_mps: 0.162178, loss_cps: 0.333980
[14:27:24.196] iteration 29426: total_loss: 0.220800, loss_sup: 0.018696, loss_mps: 0.070150, loss_cps: 0.131954
[14:27:24.341] iteration 29427: total_loss: 0.300009, loss_sup: 0.003388, loss_mps: 0.098130, loss_cps: 0.198492
[14:27:24.486] iteration 29428: total_loss: 0.382168, loss_sup: 0.041168, loss_mps: 0.114969, loss_cps: 0.226031
[14:27:24.632] iteration 29429: total_loss: 0.231307, loss_sup: 0.006307, loss_mps: 0.078436, loss_cps: 0.146564
[14:27:24.777] iteration 29430: total_loss: 0.282682, loss_sup: 0.026546, loss_mps: 0.087783, loss_cps: 0.168353
[14:27:24.924] iteration 29431: total_loss: 0.141392, loss_sup: 0.002483, loss_mps: 0.052697, loss_cps: 0.086211
[14:27:25.072] iteration 29432: total_loss: 0.188212, loss_sup: 0.008981, loss_mps: 0.063642, loss_cps: 0.115589
[14:27:25.218] iteration 29433: total_loss: 0.228167, loss_sup: 0.081047, loss_mps: 0.055161, loss_cps: 0.091959
[14:27:25.364] iteration 29434: total_loss: 0.314914, loss_sup: 0.069171, loss_mps: 0.085505, loss_cps: 0.160238
[14:27:25.511] iteration 29435: total_loss: 0.211110, loss_sup: 0.012945, loss_mps: 0.071996, loss_cps: 0.126169
[14:27:25.658] iteration 29436: total_loss: 0.275566, loss_sup: 0.009401, loss_mps: 0.092798, loss_cps: 0.173368
[14:27:25.804] iteration 29437: total_loss: 0.388703, loss_sup: 0.013324, loss_mps: 0.126043, loss_cps: 0.249336
[14:27:25.953] iteration 29438: total_loss: 0.499690, loss_sup: 0.004872, loss_mps: 0.161551, loss_cps: 0.333267
[14:27:26.098] iteration 29439: total_loss: 0.630130, loss_sup: 0.131400, loss_mps: 0.168389, loss_cps: 0.330341
[14:27:26.248] iteration 29440: total_loss: 0.161292, loss_sup: 0.007219, loss_mps: 0.062550, loss_cps: 0.091523
[14:27:26.397] iteration 29441: total_loss: 0.263950, loss_sup: 0.004241, loss_mps: 0.092604, loss_cps: 0.167105
[14:27:26.543] iteration 29442: total_loss: 0.187676, loss_sup: 0.034751, loss_mps: 0.054217, loss_cps: 0.098708
[14:27:26.688] iteration 29443: total_loss: 0.154782, loss_sup: 0.013448, loss_mps: 0.051666, loss_cps: 0.089668
[14:27:26.833] iteration 29444: total_loss: 0.270486, loss_sup: 0.025193, loss_mps: 0.082618, loss_cps: 0.162674
[14:27:26.980] iteration 29445: total_loss: 0.175137, loss_sup: 0.004196, loss_mps: 0.062859, loss_cps: 0.108083
[14:27:27.126] iteration 29446: total_loss: 0.171831, loss_sup: 0.001827, loss_mps: 0.064745, loss_cps: 0.105259
[14:27:27.272] iteration 29447: total_loss: 0.242721, loss_sup: 0.043772, loss_mps: 0.071994, loss_cps: 0.126955
[14:27:27.419] iteration 29448: total_loss: 0.363005, loss_sup: 0.060274, loss_mps: 0.119811, loss_cps: 0.182919
[14:27:27.565] iteration 29449: total_loss: 0.190233, loss_sup: 0.043374, loss_mps: 0.057191, loss_cps: 0.089669
[14:27:27.711] iteration 29450: total_loss: 0.208645, loss_sup: 0.015184, loss_mps: 0.068958, loss_cps: 0.124504
[14:27:27.856] iteration 29451: total_loss: 0.250779, loss_sup: 0.001402, loss_mps: 0.084722, loss_cps: 0.164655
[14:27:28.002] iteration 29452: total_loss: 0.373479, loss_sup: 0.003464, loss_mps: 0.120901, loss_cps: 0.249114
[14:27:28.147] iteration 29453: total_loss: 0.293736, loss_sup: 0.008180, loss_mps: 0.097917, loss_cps: 0.187639
[14:27:28.293] iteration 29454: total_loss: 0.149651, loss_sup: 0.003584, loss_mps: 0.056988, loss_cps: 0.089079
[14:27:28.441] iteration 29455: total_loss: 0.327606, loss_sup: 0.119571, loss_mps: 0.072438, loss_cps: 0.135597
[14:27:28.589] iteration 29456: total_loss: 0.438932, loss_sup: 0.042228, loss_mps: 0.130448, loss_cps: 0.266255
[14:27:28.735] iteration 29457: total_loss: 0.161953, loss_sup: 0.008472, loss_mps: 0.058804, loss_cps: 0.094677
[14:27:28.881] iteration 29458: total_loss: 0.374908, loss_sup: 0.048983, loss_mps: 0.114028, loss_cps: 0.211896
[14:27:29.028] iteration 29459: total_loss: 0.185815, loss_sup: 0.006382, loss_mps: 0.067146, loss_cps: 0.112287
[14:27:29.174] iteration 29460: total_loss: 0.184634, loss_sup: 0.001975, loss_mps: 0.069011, loss_cps: 0.113648
[14:27:29.320] iteration 29461: total_loss: 0.154434, loss_sup: 0.001192, loss_mps: 0.056263, loss_cps: 0.096980
[14:27:29.466] iteration 29462: total_loss: 0.333420, loss_sup: 0.028125, loss_mps: 0.108192, loss_cps: 0.197103
[14:27:29.612] iteration 29463: total_loss: 0.250407, loss_sup: 0.054435, loss_mps: 0.069976, loss_cps: 0.125995
[14:27:29.759] iteration 29464: total_loss: 0.298217, loss_sup: 0.073947, loss_mps: 0.078726, loss_cps: 0.145544
[14:27:29.906] iteration 29465: total_loss: 0.350261, loss_sup: 0.247335, loss_mps: 0.038952, loss_cps: 0.063974
[14:27:30.054] iteration 29466: total_loss: 0.450376, loss_sup: 0.117954, loss_mps: 0.106972, loss_cps: 0.225450
[14:27:30.201] iteration 29467: total_loss: 0.320597, loss_sup: 0.022757, loss_mps: 0.102261, loss_cps: 0.195579
[14:27:30.347] iteration 29468: total_loss: 0.388420, loss_sup: 0.067503, loss_mps: 0.106106, loss_cps: 0.214812
[14:27:30.493] iteration 29469: total_loss: 0.267091, loss_sup: 0.035737, loss_mps: 0.090329, loss_cps: 0.141025
[14:27:30.641] iteration 29470: total_loss: 0.143205, loss_sup: 0.004457, loss_mps: 0.052403, loss_cps: 0.086345
[14:27:30.789] iteration 29471: total_loss: 0.347873, loss_sup: 0.031044, loss_mps: 0.106552, loss_cps: 0.210276
[14:27:30.936] iteration 29472: total_loss: 0.212347, loss_sup: 0.038148, loss_mps: 0.063184, loss_cps: 0.111015
[14:27:31.083] iteration 29473: total_loss: 0.221334, loss_sup: 0.018899, loss_mps: 0.074512, loss_cps: 0.127923
[14:27:31.229] iteration 29474: total_loss: 0.422822, loss_sup: 0.086041, loss_mps: 0.114247, loss_cps: 0.222534
[14:27:31.379] iteration 29475: total_loss: 0.335787, loss_sup: 0.069035, loss_mps: 0.093229, loss_cps: 0.173524
[14:27:31.527] iteration 29476: total_loss: 0.311206, loss_sup: 0.011664, loss_mps: 0.108370, loss_cps: 0.191172
[14:27:31.674] iteration 29477: total_loss: 0.324428, loss_sup: 0.008577, loss_mps: 0.103741, loss_cps: 0.212109
[14:27:31.820] iteration 29478: total_loss: 0.479155, loss_sup: 0.016668, loss_mps: 0.142265, loss_cps: 0.320222
[14:27:31.969] iteration 29479: total_loss: 0.380431, loss_sup: 0.007051, loss_mps: 0.119614, loss_cps: 0.253767
[14:27:32.116] iteration 29480: total_loss: 0.208890, loss_sup: 0.017897, loss_mps: 0.068662, loss_cps: 0.122331
[14:27:32.265] iteration 29481: total_loss: 0.270736, loss_sup: 0.106466, loss_mps: 0.061465, loss_cps: 0.102806
[14:27:32.412] iteration 29482: total_loss: 0.365247, loss_sup: 0.062079, loss_mps: 0.105673, loss_cps: 0.197495
[14:27:32.559] iteration 29483: total_loss: 0.482894, loss_sup: 0.181615, loss_mps: 0.104354, loss_cps: 0.196925
[14:27:32.705] iteration 29484: total_loss: 0.248440, loss_sup: 0.016519, loss_mps: 0.085334, loss_cps: 0.146588
[14:27:32.852] iteration 29485: total_loss: 0.331059, loss_sup: 0.070684, loss_mps: 0.090959, loss_cps: 0.169416
[14:27:32.998] iteration 29486: total_loss: 0.235110, loss_sup: 0.003501, loss_mps: 0.079682, loss_cps: 0.151927
[14:27:33.145] iteration 29487: total_loss: 0.320577, loss_sup: 0.029796, loss_mps: 0.096942, loss_cps: 0.193839
[14:27:33.291] iteration 29488: total_loss: 0.454369, loss_sup: 0.066021, loss_mps: 0.130376, loss_cps: 0.257971
[14:27:33.438] iteration 29489: total_loss: 0.202924, loss_sup: 0.027878, loss_mps: 0.064309, loss_cps: 0.110738
[14:27:33.584] iteration 29490: total_loss: 0.244476, loss_sup: 0.031233, loss_mps: 0.077991, loss_cps: 0.135252
[14:27:33.730] iteration 29491: total_loss: 0.394189, loss_sup: 0.201830, loss_mps: 0.074096, loss_cps: 0.118263
[14:27:33.879] iteration 29492: total_loss: 0.239822, loss_sup: 0.000906, loss_mps: 0.084896, loss_cps: 0.154020
[14:27:34.025] iteration 29493: total_loss: 0.222373, loss_sup: 0.024006, loss_mps: 0.072219, loss_cps: 0.126148
[14:27:34.172] iteration 29494: total_loss: 0.429728, loss_sup: 0.030191, loss_mps: 0.131349, loss_cps: 0.268188
[14:27:34.318] iteration 29495: total_loss: 0.603513, loss_sup: 0.173067, loss_mps: 0.144135, loss_cps: 0.286311
[14:27:34.464] iteration 29496: total_loss: 0.257928, loss_sup: 0.046506, loss_mps: 0.074468, loss_cps: 0.136955
[14:27:34.610] iteration 29497: total_loss: 0.259059, loss_sup: 0.060229, loss_mps: 0.071695, loss_cps: 0.127135
[14:27:34.756] iteration 29498: total_loss: 0.363169, loss_sup: 0.064871, loss_mps: 0.100088, loss_cps: 0.198211
[14:27:34.903] iteration 29499: total_loss: 0.192888, loss_sup: 0.009028, loss_mps: 0.066699, loss_cps: 0.117161
[14:27:35.048] iteration 29500: total_loss: 0.187637, loss_sup: 0.010634, loss_mps: 0.066460, loss_cps: 0.110543
[14:27:35.049] Evaluation Started ==>
[14:27:46.483] ==> valid iteration 29500: unet metrics: {'dc': 0.6714465522555046, 'jc': 0.5566942497576121, 'pre': 0.810629904356653, 'hd': 5.329818336881091}, ynet metrics: {'dc': 0.6176966763353479, 'jc': 0.5065880253532086, 'pre': 0.8052613035910207, 'hd': 5.317750958573974}.
[14:27:46.485] Evaluation Finished!⏹️
[14:27:46.635] iteration 29501: total_loss: 0.244888, loss_sup: 0.053757, loss_mps: 0.069766, loss_cps: 0.121366
[14:27:46.783] iteration 29502: total_loss: 0.398580, loss_sup: 0.058665, loss_mps: 0.115193, loss_cps: 0.224722
[14:27:46.929] iteration 29503: total_loss: 0.218215, loss_sup: 0.069041, loss_mps: 0.059271, loss_cps: 0.089903
[14:27:47.075] iteration 29504: total_loss: 0.369678, loss_sup: 0.037322, loss_mps: 0.106884, loss_cps: 0.225471
[14:27:47.220] iteration 29505: total_loss: 0.377378, loss_sup: 0.132695, loss_mps: 0.082321, loss_cps: 0.162362
[14:27:47.366] iteration 29506: total_loss: 0.186083, loss_sup: 0.012406, loss_mps: 0.067342, loss_cps: 0.106336
[14:27:47.512] iteration 29507: total_loss: 0.267031, loss_sup: 0.010266, loss_mps: 0.091299, loss_cps: 0.165466
[14:27:47.658] iteration 29508: total_loss: 0.160068, loss_sup: 0.005444, loss_mps: 0.057028, loss_cps: 0.097596
[14:27:47.804] iteration 29509: total_loss: 0.184821, loss_sup: 0.001925, loss_mps: 0.065184, loss_cps: 0.117712
[14:27:47.949] iteration 29510: total_loss: 0.204850, loss_sup: 0.003430, loss_mps: 0.071191, loss_cps: 0.130229
[14:27:48.095] iteration 29511: total_loss: 0.210705, loss_sup: 0.001575, loss_mps: 0.071942, loss_cps: 0.137187
[14:27:48.241] iteration 29512: total_loss: 0.341041, loss_sup: 0.070738, loss_mps: 0.093233, loss_cps: 0.177070
[14:27:48.387] iteration 29513: total_loss: 0.168679, loss_sup: 0.006966, loss_mps: 0.057653, loss_cps: 0.104060
[14:27:48.534] iteration 29514: total_loss: 0.352305, loss_sup: 0.031410, loss_mps: 0.111304, loss_cps: 0.209591
[14:27:48.680] iteration 29515: total_loss: 0.498236, loss_sup: 0.024507, loss_mps: 0.153842, loss_cps: 0.319887
[14:27:48.826] iteration 29516: total_loss: 0.228331, loss_sup: 0.036656, loss_mps: 0.069802, loss_cps: 0.121873
[14:27:48.971] iteration 29517: total_loss: 0.195630, loss_sup: 0.012094, loss_mps: 0.070516, loss_cps: 0.113019
[14:27:49.117] iteration 29518: total_loss: 0.288590, loss_sup: 0.005658, loss_mps: 0.096647, loss_cps: 0.186286
[14:27:49.262] iteration 29519: total_loss: 0.275915, loss_sup: 0.035457, loss_mps: 0.086959, loss_cps: 0.153499
[14:27:49.408] iteration 29520: total_loss: 0.162238, loss_sup: 0.013959, loss_mps: 0.056263, loss_cps: 0.092016
[14:27:49.555] iteration 29521: total_loss: 0.245112, loss_sup: 0.063600, loss_mps: 0.066713, loss_cps: 0.114800
[14:27:49.701] iteration 29522: total_loss: 0.257707, loss_sup: 0.086098, loss_mps: 0.063041, loss_cps: 0.108568
[14:27:49.848] iteration 29523: total_loss: 0.264329, loss_sup: 0.063863, loss_mps: 0.072784, loss_cps: 0.127682
[14:27:49.999] iteration 29524: total_loss: 0.282277, loss_sup: 0.026410, loss_mps: 0.096707, loss_cps: 0.159160
[14:27:50.146] iteration 29525: total_loss: 0.212489, loss_sup: 0.012538, loss_mps: 0.072501, loss_cps: 0.127450
[14:27:50.292] iteration 29526: total_loss: 0.315966, loss_sup: 0.057397, loss_mps: 0.089905, loss_cps: 0.168664
[14:27:50.439] iteration 29527: total_loss: 0.210731, loss_sup: 0.012417, loss_mps: 0.069298, loss_cps: 0.129016
[14:27:50.588] iteration 29528: total_loss: 0.153364, loss_sup: 0.005777, loss_mps: 0.056758, loss_cps: 0.090830
[14:27:50.734] iteration 29529: total_loss: 0.164061, loss_sup: 0.027785, loss_mps: 0.052731, loss_cps: 0.083544
[14:27:50.879] iteration 29530: total_loss: 0.543310, loss_sup: 0.097288, loss_mps: 0.143342, loss_cps: 0.302680
[14:27:51.026] iteration 29531: total_loss: 0.176011, loss_sup: 0.001570, loss_mps: 0.064692, loss_cps: 0.109749
[14:27:51.172] iteration 29532: total_loss: 0.190657, loss_sup: 0.013369, loss_mps: 0.063640, loss_cps: 0.113647
[14:27:51.319] iteration 29533: total_loss: 0.433655, loss_sup: 0.258343, loss_mps: 0.067452, loss_cps: 0.107861
[14:27:51.465] iteration 29534: total_loss: 0.208897, loss_sup: 0.011366, loss_mps: 0.069580, loss_cps: 0.127950
[14:27:51.611] iteration 29535: total_loss: 0.265098, loss_sup: 0.000511, loss_mps: 0.089954, loss_cps: 0.174633
[14:27:51.757] iteration 29536: total_loss: 0.285521, loss_sup: 0.007307, loss_mps: 0.096108, loss_cps: 0.182106
[14:27:51.903] iteration 29537: total_loss: 0.327473, loss_sup: 0.114878, loss_mps: 0.077807, loss_cps: 0.134788
[14:27:52.049] iteration 29538: total_loss: 0.375334, loss_sup: 0.023408, loss_mps: 0.123536, loss_cps: 0.228390
[14:27:52.195] iteration 29539: total_loss: 0.255723, loss_sup: 0.000561, loss_mps: 0.092106, loss_cps: 0.163057
[14:27:52.341] iteration 29540: total_loss: 0.256537, loss_sup: 0.055794, loss_mps: 0.073083, loss_cps: 0.127659
[14:27:52.488] iteration 29541: total_loss: 0.256539, loss_sup: 0.026201, loss_mps: 0.081365, loss_cps: 0.148973
[14:27:52.634] iteration 29542: total_loss: 0.191515, loss_sup: 0.006498, loss_mps: 0.064773, loss_cps: 0.120245
[14:27:52.780] iteration 29543: total_loss: 0.349430, loss_sup: 0.069953, loss_mps: 0.095384, loss_cps: 0.184093
[14:27:52.926] iteration 29544: total_loss: 0.395194, loss_sup: 0.127785, loss_mps: 0.090618, loss_cps: 0.176791
[14:27:53.072] iteration 29545: total_loss: 0.173298, loss_sup: 0.002536, loss_mps: 0.063004, loss_cps: 0.107758
[14:27:53.218] iteration 29546: total_loss: 0.220967, loss_sup: 0.020556, loss_mps: 0.069459, loss_cps: 0.130951
[14:27:53.364] iteration 29547: total_loss: 0.200486, loss_sup: 0.039479, loss_mps: 0.059609, loss_cps: 0.101399
[14:27:53.511] iteration 29548: total_loss: 0.194190, loss_sup: 0.005909, loss_mps: 0.067448, loss_cps: 0.120833
[14:27:53.657] iteration 29549: total_loss: 0.200390, loss_sup: 0.003099, loss_mps: 0.068969, loss_cps: 0.128323
[14:27:53.802] iteration 29550: total_loss: 0.274643, loss_sup: 0.021747, loss_mps: 0.090006, loss_cps: 0.162891
[14:27:53.949] iteration 29551: total_loss: 0.362241, loss_sup: 0.031858, loss_mps: 0.110071, loss_cps: 0.220313
[14:27:54.095] iteration 29552: total_loss: 0.319971, loss_sup: 0.105867, loss_mps: 0.074180, loss_cps: 0.139923
[14:27:54.240] iteration 29553: total_loss: 0.168043, loss_sup: 0.002198, loss_mps: 0.061416, loss_cps: 0.104430
[14:27:54.386] iteration 29554: total_loss: 0.289965, loss_sup: 0.068220, loss_mps: 0.077866, loss_cps: 0.143880
[14:27:54.532] iteration 29555: total_loss: 0.170650, loss_sup: 0.002676, loss_mps: 0.064513, loss_cps: 0.103461
[14:27:54.680] iteration 29556: total_loss: 0.213641, loss_sup: 0.006531, loss_mps: 0.075810, loss_cps: 0.131301
[14:27:54.829] iteration 29557: total_loss: 0.454889, loss_sup: 0.007434, loss_mps: 0.153891, loss_cps: 0.293565
[14:27:54.974] iteration 29558: total_loss: 0.300824, loss_sup: 0.039776, loss_mps: 0.092333, loss_cps: 0.168715
[14:27:55.121] iteration 29559: total_loss: 0.287425, loss_sup: 0.003288, loss_mps: 0.094265, loss_cps: 0.189872
[14:27:55.266] iteration 29560: total_loss: 0.308116, loss_sup: 0.086950, loss_mps: 0.078956, loss_cps: 0.142210
[14:27:55.412] iteration 29561: total_loss: 0.324158, loss_sup: 0.060502, loss_mps: 0.088629, loss_cps: 0.175027
[14:27:55.557] iteration 29562: total_loss: 0.228437, loss_sup: 0.050367, loss_mps: 0.065388, loss_cps: 0.112682
[14:27:55.704] iteration 29563: total_loss: 0.370828, loss_sup: 0.172083, loss_mps: 0.071177, loss_cps: 0.127568
[14:27:55.850] iteration 29564: total_loss: 0.232668, loss_sup: 0.034196, loss_mps: 0.072407, loss_cps: 0.126065
[14:27:55.996] iteration 29565: total_loss: 0.332076, loss_sup: 0.021737, loss_mps: 0.103793, loss_cps: 0.206547
[14:27:56.143] iteration 29566: total_loss: 0.153316, loss_sup: 0.003092, loss_mps: 0.052961, loss_cps: 0.097263
[14:27:56.289] iteration 29567: total_loss: 0.541904, loss_sup: 0.016651, loss_mps: 0.161983, loss_cps: 0.363270
[14:27:56.434] iteration 29568: total_loss: 0.194731, loss_sup: 0.008773, loss_mps: 0.069516, loss_cps: 0.116442
[14:27:56.581] iteration 29569: total_loss: 0.304747, loss_sup: 0.024483, loss_mps: 0.096173, loss_cps: 0.184091
[14:27:56.726] iteration 29570: total_loss: 0.515844, loss_sup: 0.056270, loss_mps: 0.143881, loss_cps: 0.315693
[14:27:56.872] iteration 29571: total_loss: 0.266996, loss_sup: 0.028519, loss_mps: 0.089110, loss_cps: 0.149367
[14:27:57.018] iteration 29572: total_loss: 0.263948, loss_sup: 0.095923, loss_mps: 0.060694, loss_cps: 0.107330
[14:27:57.164] iteration 29573: total_loss: 0.208724, loss_sup: 0.029279, loss_mps: 0.063889, loss_cps: 0.115556
[14:27:57.310] iteration 29574: total_loss: 0.403871, loss_sup: 0.072293, loss_mps: 0.110025, loss_cps: 0.221552
[14:27:57.457] iteration 29575: total_loss: 0.252403, loss_sup: 0.025113, loss_mps: 0.077383, loss_cps: 0.149907
[14:27:57.608] iteration 29576: total_loss: 0.129343, loss_sup: 0.001169, loss_mps: 0.048459, loss_cps: 0.079715
[14:27:57.758] iteration 29577: total_loss: 0.387712, loss_sup: 0.227179, loss_mps: 0.059050, loss_cps: 0.101483
[14:27:57.911] iteration 29578: total_loss: 0.391048, loss_sup: 0.044564, loss_mps: 0.120099, loss_cps: 0.226386
[14:27:58.058] iteration 29579: total_loss: 0.240454, loss_sup: 0.037544, loss_mps: 0.076961, loss_cps: 0.125948
[14:27:58.204] iteration 29580: total_loss: 0.208622, loss_sup: 0.006560, loss_mps: 0.071937, loss_cps: 0.130125
[14:27:58.350] iteration 29581: total_loss: 0.165886, loss_sup: 0.014558, loss_mps: 0.056330, loss_cps: 0.094998
[14:27:58.498] iteration 29582: total_loss: 0.313492, loss_sup: 0.067463, loss_mps: 0.084385, loss_cps: 0.161644
[14:27:58.645] iteration 29583: total_loss: 0.295034, loss_sup: 0.016710, loss_mps: 0.092491, loss_cps: 0.185833
[14:27:58.793] iteration 29584: total_loss: 0.143390, loss_sup: 0.005908, loss_mps: 0.051174, loss_cps: 0.086308
[14:27:58.941] iteration 29585: total_loss: 0.291091, loss_sup: 0.005236, loss_mps: 0.097218, loss_cps: 0.188636
[14:27:59.089] iteration 29586: total_loss: 0.235162, loss_sup: 0.040805, loss_mps: 0.071502, loss_cps: 0.122855
[14:27:59.241] iteration 29587: total_loss: 0.249281, loss_sup: 0.032229, loss_mps: 0.083007, loss_cps: 0.134045
[14:27:59.389] iteration 29588: total_loss: 0.158904, loss_sup: 0.002249, loss_mps: 0.059851, loss_cps: 0.096804
[14:27:59.536] iteration 29589: total_loss: 0.273322, loss_sup: 0.068490, loss_mps: 0.072060, loss_cps: 0.132773
[14:27:59.684] iteration 29590: total_loss: 0.256247, loss_sup: 0.018616, loss_mps: 0.085540, loss_cps: 0.152091
[14:27:59.831] iteration 29591: total_loss: 0.318685, loss_sup: 0.021718, loss_mps: 0.098426, loss_cps: 0.198540
[14:27:59.978] iteration 29592: total_loss: 0.221643, loss_sup: 0.034659, loss_mps: 0.065609, loss_cps: 0.121375
[14:28:00.126] iteration 29593: total_loss: 0.522039, loss_sup: 0.095610, loss_mps: 0.132913, loss_cps: 0.293516
[14:28:00.273] iteration 29594: total_loss: 0.233506, loss_sup: 0.012381, loss_mps: 0.077862, loss_cps: 0.143263
[14:28:00.419] iteration 29595: total_loss: 0.231237, loss_sup: 0.040623, loss_mps: 0.066984, loss_cps: 0.123630
[14:28:00.567] iteration 29596: total_loss: 0.210556, loss_sup: 0.004012, loss_mps: 0.075929, loss_cps: 0.130614
[14:28:00.716] iteration 29597: total_loss: 0.612829, loss_sup: 0.198261, loss_mps: 0.125570, loss_cps: 0.288998
[14:28:00.863] iteration 29598: total_loss: 0.380994, loss_sup: 0.095291, loss_mps: 0.098701, loss_cps: 0.187002
[14:28:01.010] iteration 29599: total_loss: 0.184767, loss_sup: 0.027646, loss_mps: 0.060150, loss_cps: 0.096972
[14:28:01.157] iteration 29600: total_loss: 0.268523, loss_sup: 0.014984, loss_mps: 0.088923, loss_cps: 0.164616
[14:28:01.157] Evaluation Started ==>
[14:28:12.525] ==> valid iteration 29600: unet metrics: {'dc': 0.6748434462714611, 'jc': 0.5600872974339642, 'pre': 0.8123722233344459, 'hd': 5.347564384765867}, ynet metrics: {'dc': 0.6219086817791908, 'jc': 0.511165603945856, 'pre': 0.8026127302440714, 'hd': 5.347394047378818}.
[14:28:12.527] Evaluation Finished!⏹️
[14:28:12.680] iteration 29601: total_loss: 0.329635, loss_sup: 0.054264, loss_mps: 0.080223, loss_cps: 0.195148
[14:28:12.831] iteration 29602: total_loss: 0.213957, loss_sup: 0.040011, loss_mps: 0.064382, loss_cps: 0.109564
[14:28:12.977] iteration 29603: total_loss: 0.280958, loss_sup: 0.035426, loss_mps: 0.088887, loss_cps: 0.156645
[14:28:13.122] iteration 29604: total_loss: 0.238146, loss_sup: 0.001121, loss_mps: 0.082225, loss_cps: 0.154800
[14:28:13.268] iteration 29605: total_loss: 0.229101, loss_sup: 0.001267, loss_mps: 0.080344, loss_cps: 0.147490
[14:28:13.416] iteration 29606: total_loss: 0.210530, loss_sup: 0.012049, loss_mps: 0.074005, loss_cps: 0.124476
[14:28:13.562] iteration 29607: total_loss: 0.391918, loss_sup: 0.181034, loss_mps: 0.071949, loss_cps: 0.138935
[14:28:13.707] iteration 29608: total_loss: 0.458313, loss_sup: 0.168174, loss_mps: 0.093421, loss_cps: 0.196718
[14:28:13.852] iteration 29609: total_loss: 0.222497, loss_sup: 0.006033, loss_mps: 0.074821, loss_cps: 0.141643
[14:28:13.999] iteration 29610: total_loss: 0.187506, loss_sup: 0.031585, loss_mps: 0.057882, loss_cps: 0.098039
[14:28:14.146] iteration 29611: total_loss: 0.255944, loss_sup: 0.016077, loss_mps: 0.082385, loss_cps: 0.157482
[14:28:14.294] iteration 29612: total_loss: 0.897519, loss_sup: 0.210272, loss_mps: 0.227345, loss_cps: 0.459902
[14:28:14.439] iteration 29613: total_loss: 0.398819, loss_sup: 0.005495, loss_mps: 0.128811, loss_cps: 0.264514
[14:28:14.589] iteration 29614: total_loss: 0.399989, loss_sup: 0.096219, loss_mps: 0.103021, loss_cps: 0.200749
[14:28:14.735] iteration 29615: total_loss: 0.196456, loss_sup: 0.003129, loss_mps: 0.071316, loss_cps: 0.122011
[14:28:14.881] iteration 29616: total_loss: 0.271854, loss_sup: 0.001344, loss_mps: 0.091077, loss_cps: 0.179433
[14:28:15.029] iteration 29617: total_loss: 0.227533, loss_sup: 0.040590, loss_mps: 0.066754, loss_cps: 0.120189
[14:28:15.174] iteration 29618: total_loss: 0.185669, loss_sup: 0.009556, loss_mps: 0.066716, loss_cps: 0.109397
[14:28:15.321] iteration 29619: total_loss: 0.277607, loss_sup: 0.035359, loss_mps: 0.082067, loss_cps: 0.160182
[14:28:15.467] iteration 29620: total_loss: 0.209827, loss_sup: 0.020491, loss_mps: 0.067842, loss_cps: 0.121494
[14:28:15.615] iteration 29621: total_loss: 0.182801, loss_sup: 0.008875, loss_mps: 0.067894, loss_cps: 0.106032
[14:28:15.760] iteration 29622: total_loss: 0.245883, loss_sup: 0.037010, loss_mps: 0.073308, loss_cps: 0.135565
[14:28:15.906] iteration 29623: total_loss: 0.220104, loss_sup: 0.016260, loss_mps: 0.071403, loss_cps: 0.132440
[14:28:16.051] iteration 29624: total_loss: 0.217637, loss_sup: 0.031951, loss_mps: 0.064671, loss_cps: 0.121016
[14:28:16.200] iteration 29625: total_loss: 0.601189, loss_sup: 0.020440, loss_mps: 0.174227, loss_cps: 0.406523
[14:28:16.346] iteration 29626: total_loss: 0.255305, loss_sup: 0.057851, loss_mps: 0.070893, loss_cps: 0.126561
[14:28:16.491] iteration 29627: total_loss: 0.339610, loss_sup: 0.038302, loss_mps: 0.103135, loss_cps: 0.198173
[14:28:16.637] iteration 29628: total_loss: 0.350075, loss_sup: 0.047758, loss_mps: 0.104644, loss_cps: 0.197672
[14:28:16.783] iteration 29629: total_loss: 0.194670, loss_sup: 0.015796, loss_mps: 0.064544, loss_cps: 0.114331
[14:28:16.928] iteration 29630: total_loss: 0.351874, loss_sup: 0.067158, loss_mps: 0.095873, loss_cps: 0.188842
[14:28:17.073] iteration 29631: total_loss: 0.184887, loss_sup: 0.002521, loss_mps: 0.068662, loss_cps: 0.113704
[14:28:17.219] iteration 29632: total_loss: 0.240345, loss_sup: 0.043481, loss_mps: 0.073757, loss_cps: 0.123108
[14:28:17.366] iteration 29633: total_loss: 0.620799, loss_sup: 0.014782, loss_mps: 0.188830, loss_cps: 0.417188
[14:28:17.512] iteration 29634: total_loss: 0.186728, loss_sup: 0.008464, loss_mps: 0.067172, loss_cps: 0.111093
[14:28:17.659] iteration 29635: total_loss: 0.288894, loss_sup: 0.028723, loss_mps: 0.089848, loss_cps: 0.170323
[14:28:17.805] iteration 29636: total_loss: 0.354449, loss_sup: 0.060973, loss_mps: 0.102382, loss_cps: 0.191095
[14:28:17.951] iteration 29637: total_loss: 0.276516, loss_sup: 0.039785, loss_mps: 0.080612, loss_cps: 0.156119
[14:28:18.098] iteration 29638: total_loss: 0.153226, loss_sup: 0.032426, loss_mps: 0.046367, loss_cps: 0.074434
[14:28:18.245] iteration 29639: total_loss: 0.263562, loss_sup: 0.011654, loss_mps: 0.089807, loss_cps: 0.162101
[14:28:18.391] iteration 29640: total_loss: 0.363275, loss_sup: 0.061896, loss_mps: 0.104479, loss_cps: 0.196900
[14:28:18.538] iteration 29641: total_loss: 0.295735, loss_sup: 0.077459, loss_mps: 0.079063, loss_cps: 0.139214
[14:28:18.685] iteration 29642: total_loss: 0.193444, loss_sup: 0.023885, loss_mps: 0.062764, loss_cps: 0.106796
[14:28:18.831] iteration 29643: total_loss: 0.406631, loss_sup: 0.089003, loss_mps: 0.099989, loss_cps: 0.217639
[14:28:18.981] iteration 29644: total_loss: 0.474377, loss_sup: 0.214093, loss_mps: 0.090553, loss_cps: 0.169731
[14:28:19.127] iteration 29645: total_loss: 0.224418, loss_sup: 0.035248, loss_mps: 0.068236, loss_cps: 0.120934
[14:28:19.274] iteration 29646: total_loss: 0.525360, loss_sup: 0.221438, loss_mps: 0.108980, loss_cps: 0.194942
[14:28:19.425] iteration 29647: total_loss: 0.229383, loss_sup: 0.007325, loss_mps: 0.076945, loss_cps: 0.145114
[14:28:19.571] iteration 29648: total_loss: 0.136278, loss_sup: 0.000706, loss_mps: 0.053453, loss_cps: 0.082119
[14:28:19.719] iteration 29649: total_loss: 0.148095, loss_sup: 0.016814, loss_mps: 0.051538, loss_cps: 0.079742
[14:28:19.867] iteration 29650: total_loss: 0.329494, loss_sup: 0.016186, loss_mps: 0.102906, loss_cps: 0.210401
[14:28:20.015] iteration 29651: total_loss: 0.212501, loss_sup: 0.005481, loss_mps: 0.072948, loss_cps: 0.134072
[14:28:20.163] iteration 29652: total_loss: 0.423410, loss_sup: 0.025636, loss_mps: 0.133240, loss_cps: 0.264535
[14:28:20.310] iteration 29653: total_loss: 0.213798, loss_sup: 0.014397, loss_mps: 0.074616, loss_cps: 0.124785
[14:28:20.457] iteration 29654: total_loss: 0.407771, loss_sup: 0.057390, loss_mps: 0.116818, loss_cps: 0.233563
[14:28:20.603] iteration 29655: total_loss: 0.155961, loss_sup: 0.014901, loss_mps: 0.053535, loss_cps: 0.087525
[14:28:20.749] iteration 29656: total_loss: 0.259398, loss_sup: 0.014353, loss_mps: 0.086063, loss_cps: 0.158982
[14:28:20.895] iteration 29657: total_loss: 0.199444, loss_sup: 0.014282, loss_mps: 0.069677, loss_cps: 0.115485
[14:28:21.041] iteration 29658: total_loss: 0.238889, loss_sup: 0.021169, loss_mps: 0.078542, loss_cps: 0.139178
[14:28:21.187] iteration 29659: total_loss: 0.113900, loss_sup: 0.003190, loss_mps: 0.042731, loss_cps: 0.067978
[14:28:21.334] iteration 29660: total_loss: 0.163891, loss_sup: 0.000769, loss_mps: 0.059365, loss_cps: 0.103756
[14:28:21.480] iteration 29661: total_loss: 0.299838, loss_sup: 0.084369, loss_mps: 0.081778, loss_cps: 0.133691
[14:28:21.625] iteration 29662: total_loss: 0.660823, loss_sup: 0.050709, loss_mps: 0.190828, loss_cps: 0.419286
[14:28:21.773] iteration 29663: total_loss: 0.761530, loss_sup: 0.103659, loss_mps: 0.215260, loss_cps: 0.442611
[14:28:21.921] iteration 29664: total_loss: 0.240037, loss_sup: 0.008093, loss_mps: 0.083514, loss_cps: 0.148430
[14:28:22.066] iteration 29665: total_loss: 0.270295, loss_sup: 0.034747, loss_mps: 0.085296, loss_cps: 0.150252
[14:28:22.212] iteration 29666: total_loss: 0.181971, loss_sup: 0.015580, loss_mps: 0.062105, loss_cps: 0.104285
[14:28:22.358] iteration 29667: total_loss: 0.300961, loss_sup: 0.035276, loss_mps: 0.087639, loss_cps: 0.178046
[14:28:22.505] iteration 29668: total_loss: 0.285119, loss_sup: 0.014711, loss_mps: 0.090571, loss_cps: 0.179838
[14:28:22.650] iteration 29669: total_loss: 0.240513, loss_sup: 0.020545, loss_mps: 0.079461, loss_cps: 0.140506
[14:28:22.796] iteration 29670: total_loss: 0.263359, loss_sup: 0.024431, loss_mps: 0.086688, loss_cps: 0.152241
[14:28:22.942] iteration 29671: total_loss: 0.277141, loss_sup: 0.037566, loss_mps: 0.082085, loss_cps: 0.157490
[14:28:23.087] iteration 29672: total_loss: 0.370773, loss_sup: 0.111452, loss_mps: 0.089955, loss_cps: 0.169366
[14:28:23.233] iteration 29673: total_loss: 0.326691, loss_sup: 0.008806, loss_mps: 0.106592, loss_cps: 0.211293
[14:28:23.381] iteration 29674: total_loss: 0.162915, loss_sup: 0.003449, loss_mps: 0.059919, loss_cps: 0.099547
[14:28:23.527] iteration 29675: total_loss: 0.219249, loss_sup: 0.007924, loss_mps: 0.075963, loss_cps: 0.135362
[14:28:23.673] iteration 29676: total_loss: 0.246402, loss_sup: 0.000627, loss_mps: 0.083635, loss_cps: 0.162139
[14:28:23.819] iteration 29677: total_loss: 0.197031, loss_sup: 0.017187, loss_mps: 0.065868, loss_cps: 0.113976
[14:28:23.880] iteration 29678: total_loss: 0.404799, loss_sup: 0.066425, loss_mps: 0.114268, loss_cps: 0.224107
[14:28:25.092] iteration 29679: total_loss: 0.214509, loss_sup: 0.005370, loss_mps: 0.078240, loss_cps: 0.130899
[14:28:25.240] iteration 29680: total_loss: 0.353254, loss_sup: 0.006366, loss_mps: 0.120289, loss_cps: 0.226598
[14:28:25.387] iteration 29681: total_loss: 0.405500, loss_sup: 0.033890, loss_mps: 0.128572, loss_cps: 0.243038
[14:28:25.533] iteration 29682: total_loss: 0.248166, loss_sup: 0.015472, loss_mps: 0.082025, loss_cps: 0.150669
[14:28:25.680] iteration 29683: total_loss: 0.268004, loss_sup: 0.061715, loss_mps: 0.075906, loss_cps: 0.130383
[14:28:25.825] iteration 29684: total_loss: 0.245566, loss_sup: 0.002431, loss_mps: 0.081305, loss_cps: 0.161829
[14:28:25.972] iteration 29685: total_loss: 0.201951, loss_sup: 0.000777, loss_mps: 0.078955, loss_cps: 0.122219
[14:28:26.124] iteration 29686: total_loss: 0.155865, loss_sup: 0.009919, loss_mps: 0.056133, loss_cps: 0.089813
[14:28:26.270] iteration 29687: total_loss: 0.170735, loss_sup: 0.028533, loss_mps: 0.054140, loss_cps: 0.088063
[14:28:26.415] iteration 29688: total_loss: 0.350792, loss_sup: 0.008179, loss_mps: 0.114434, loss_cps: 0.228180
[14:28:26.561] iteration 29689: total_loss: 0.614208, loss_sup: 0.159610, loss_mps: 0.141229, loss_cps: 0.313369
[14:28:26.707] iteration 29690: total_loss: 0.316615, loss_sup: 0.041566, loss_mps: 0.097560, loss_cps: 0.177489
[14:28:26.853] iteration 29691: total_loss: 0.252056, loss_sup: 0.030157, loss_mps: 0.076935, loss_cps: 0.144964
[14:28:26.998] iteration 29692: total_loss: 0.306435, loss_sup: 0.102761, loss_mps: 0.072689, loss_cps: 0.130985
[14:28:27.144] iteration 29693: total_loss: 0.238334, loss_sup: 0.013047, loss_mps: 0.080778, loss_cps: 0.144509
[14:28:27.290] iteration 29694: total_loss: 0.173019, loss_sup: 0.019879, loss_mps: 0.054827, loss_cps: 0.098312
[14:28:27.437] iteration 29695: total_loss: 0.190371, loss_sup: 0.030508, loss_mps: 0.057854, loss_cps: 0.102009
[14:28:27.582] iteration 29696: total_loss: 0.389046, loss_sup: 0.072826, loss_mps: 0.104369, loss_cps: 0.211851
[14:28:27.731] iteration 29697: total_loss: 0.472513, loss_sup: 0.118865, loss_mps: 0.114492, loss_cps: 0.239157
[14:28:27.877] iteration 29698: total_loss: 0.335670, loss_sup: 0.036465, loss_mps: 0.096844, loss_cps: 0.202361
[14:28:28.023] iteration 29699: total_loss: 0.163498, loss_sup: 0.056356, loss_mps: 0.041195, loss_cps: 0.065947
[14:28:28.169] iteration 29700: total_loss: 0.341512, loss_sup: 0.018411, loss_mps: 0.113841, loss_cps: 0.209260
[14:28:28.169] Evaluation Started ==>
[14:28:39.492] ==> valid iteration 29700: unet metrics: {'dc': 0.6821178264806448, 'jc': 0.5677793228267846, 'pre': 0.8098687920360833, 'hd': 5.315883352690442}, ynet metrics: {'dc': 0.6374802988552432, 'jc': 0.5262973641201815, 'pre': 0.8034798388010944, 'hd': 5.359153852209571}.
[14:28:39.494] Evaluation Finished!⏹️
[14:28:39.647] iteration 29701: total_loss: 0.133076, loss_sup: 0.003049, loss_mps: 0.047970, loss_cps: 0.082056
[14:28:39.794] iteration 29702: total_loss: 0.317961, loss_sup: 0.057793, loss_mps: 0.087445, loss_cps: 0.172723
[14:28:39.939] iteration 29703: total_loss: 0.225303, loss_sup: 0.003575, loss_mps: 0.076179, loss_cps: 0.145549
[14:28:40.084] iteration 29704: total_loss: 0.247633, loss_sup: 0.013210, loss_mps: 0.081250, loss_cps: 0.153173
[14:28:40.231] iteration 29705: total_loss: 0.402039, loss_sup: 0.222630, loss_mps: 0.066854, loss_cps: 0.112555
[14:28:40.376] iteration 29706: total_loss: 0.148361, loss_sup: 0.018574, loss_mps: 0.049909, loss_cps: 0.079878
[14:28:40.521] iteration 29707: total_loss: 0.314994, loss_sup: 0.089074, loss_mps: 0.079825, loss_cps: 0.146095
[14:28:40.668] iteration 29708: total_loss: 0.258823, loss_sup: 0.002433, loss_mps: 0.085773, loss_cps: 0.170617
[14:28:40.816] iteration 29709: total_loss: 0.180009, loss_sup: 0.000262, loss_mps: 0.065006, loss_cps: 0.114741
[14:28:40.962] iteration 29710: total_loss: 0.396655, loss_sup: 0.112267, loss_mps: 0.089544, loss_cps: 0.194844
[14:28:41.107] iteration 29711: total_loss: 0.240619, loss_sup: 0.026110, loss_mps: 0.074149, loss_cps: 0.140360
[14:28:41.254] iteration 29712: total_loss: 0.210357, loss_sup: 0.005955, loss_mps: 0.069803, loss_cps: 0.134599
[14:28:41.399] iteration 29713: total_loss: 0.316691, loss_sup: 0.051528, loss_mps: 0.095596, loss_cps: 0.169567
[14:28:41.544] iteration 29714: total_loss: 0.237400, loss_sup: 0.061550, loss_mps: 0.067714, loss_cps: 0.108136
[14:28:41.697] iteration 29715: total_loss: 0.194716, loss_sup: 0.050762, loss_mps: 0.055760, loss_cps: 0.088195
[14:28:41.842] iteration 29716: total_loss: 0.177775, loss_sup: 0.003647, loss_mps: 0.061447, loss_cps: 0.112681
[14:28:41.988] iteration 29717: total_loss: 0.226058, loss_sup: 0.020846, loss_mps: 0.074366, loss_cps: 0.130846
[14:28:42.134] iteration 29718: total_loss: 0.237877, loss_sup: 0.009049, loss_mps: 0.082003, loss_cps: 0.146825
[14:28:42.281] iteration 29719: total_loss: 0.219774, loss_sup: 0.001336, loss_mps: 0.076971, loss_cps: 0.141467
[14:28:42.427] iteration 29720: total_loss: 0.343297, loss_sup: 0.055487, loss_mps: 0.099321, loss_cps: 0.188489
[14:28:42.576] iteration 29721: total_loss: 0.203556, loss_sup: 0.018830, loss_mps: 0.070355, loss_cps: 0.114370
[14:28:42.721] iteration 29722: total_loss: 0.280769, loss_sup: 0.066519, loss_mps: 0.080689, loss_cps: 0.133561
[14:28:42.867] iteration 29723: total_loss: 0.265224, loss_sup: 0.028238, loss_mps: 0.081730, loss_cps: 0.155256
[14:28:43.015] iteration 29724: total_loss: 0.210271, loss_sup: 0.011385, loss_mps: 0.071987, loss_cps: 0.126898
[14:28:43.161] iteration 29725: total_loss: 0.245458, loss_sup: 0.057083, loss_mps: 0.067924, loss_cps: 0.120451
[14:28:43.308] iteration 29726: total_loss: 0.220280, loss_sup: 0.031319, loss_mps: 0.068939, loss_cps: 0.120023
[14:28:43.455] iteration 29727: total_loss: 0.326372, loss_sup: 0.009635, loss_mps: 0.102222, loss_cps: 0.214516
[14:28:43.601] iteration 29728: total_loss: 0.229246, loss_sup: 0.012926, loss_mps: 0.078500, loss_cps: 0.137819
[14:28:43.749] iteration 29729: total_loss: 0.331078, loss_sup: 0.039519, loss_mps: 0.098311, loss_cps: 0.193248
[14:28:43.897] iteration 29730: total_loss: 0.358853, loss_sup: 0.040701, loss_mps: 0.110969, loss_cps: 0.207183
[14:28:44.043] iteration 29731: total_loss: 0.125803, loss_sup: 0.003141, loss_mps: 0.045572, loss_cps: 0.077090
[14:28:44.189] iteration 29732: total_loss: 0.210920, loss_sup: 0.002974, loss_mps: 0.078677, loss_cps: 0.129269
[14:28:44.335] iteration 29733: total_loss: 0.349385, loss_sup: 0.028858, loss_mps: 0.100299, loss_cps: 0.220228
[14:28:44.481] iteration 29734: total_loss: 0.233064, loss_sup: 0.021936, loss_mps: 0.080490, loss_cps: 0.130639
[14:28:44.627] iteration 29735: total_loss: 0.269331, loss_sup: 0.067735, loss_mps: 0.070797, loss_cps: 0.130799
[14:28:44.773] iteration 29736: total_loss: 0.474064, loss_sup: 0.296333, loss_mps: 0.065320, loss_cps: 0.112410
[14:28:44.918] iteration 29737: total_loss: 0.169433, loss_sup: 0.024555, loss_mps: 0.055644, loss_cps: 0.089234
[14:28:45.063] iteration 29738: total_loss: 0.287991, loss_sup: 0.026973, loss_mps: 0.088846, loss_cps: 0.172171
[14:28:45.209] iteration 29739: total_loss: 0.213744, loss_sup: 0.000531, loss_mps: 0.073549, loss_cps: 0.139664
[14:28:45.355] iteration 29740: total_loss: 0.260432, loss_sup: 0.002561, loss_mps: 0.091810, loss_cps: 0.166061
[14:28:45.501] iteration 29741: total_loss: 0.162378, loss_sup: 0.010990, loss_mps: 0.056362, loss_cps: 0.095026
[14:28:45.646] iteration 29742: total_loss: 0.231380, loss_sup: 0.019915, loss_mps: 0.077454, loss_cps: 0.134011
[14:28:45.792] iteration 29743: total_loss: 0.229382, loss_sup: 0.046917, loss_mps: 0.065096, loss_cps: 0.117369
[14:28:45.940] iteration 29744: total_loss: 0.393743, loss_sup: 0.174328, loss_mps: 0.080915, loss_cps: 0.138501
[14:28:46.085] iteration 29745: total_loss: 0.572522, loss_sup: 0.034838, loss_mps: 0.159933, loss_cps: 0.377751
[14:28:46.231] iteration 29746: total_loss: 0.243198, loss_sup: 0.002209, loss_mps: 0.086205, loss_cps: 0.154784
[14:28:46.376] iteration 29747: total_loss: 0.256429, loss_sup: 0.055617, loss_mps: 0.071635, loss_cps: 0.129177
[14:28:46.521] iteration 29748: total_loss: 0.274590, loss_sup: 0.135595, loss_mps: 0.052971, loss_cps: 0.086024
[14:28:46.667] iteration 29749: total_loss: 0.204222, loss_sup: 0.014394, loss_mps: 0.070239, loss_cps: 0.119589
[14:28:46.814] iteration 29750: total_loss: 0.414360, loss_sup: 0.071656, loss_mps: 0.127208, loss_cps: 0.215496
[14:28:46.959] iteration 29751: total_loss: 0.217304, loss_sup: 0.003614, loss_mps: 0.075059, loss_cps: 0.138631
[14:28:47.105] iteration 29752: total_loss: 0.156825, loss_sup: 0.008669, loss_mps: 0.053778, loss_cps: 0.094379
[14:28:47.251] iteration 29753: total_loss: 0.379036, loss_sup: 0.080570, loss_mps: 0.098061, loss_cps: 0.200405
[14:28:47.397] iteration 29754: total_loss: 0.339270, loss_sup: 0.090427, loss_mps: 0.089809, loss_cps: 0.159035
[14:28:47.543] iteration 29755: total_loss: 0.439989, loss_sup: 0.050655, loss_mps: 0.130520, loss_cps: 0.258814
[14:28:47.689] iteration 29756: total_loss: 0.167100, loss_sup: 0.002828, loss_mps: 0.062858, loss_cps: 0.101414
[14:28:47.836] iteration 29757: total_loss: 0.169165, loss_sup: 0.000446, loss_mps: 0.062642, loss_cps: 0.106077
[14:28:47.982] iteration 29758: total_loss: 0.314994, loss_sup: 0.017268, loss_mps: 0.108716, loss_cps: 0.189010
[14:28:48.128] iteration 29759: total_loss: 0.284778, loss_sup: 0.058988, loss_mps: 0.083947, loss_cps: 0.141843
[14:28:48.274] iteration 29760: total_loss: 0.174592, loss_sup: 0.013755, loss_mps: 0.059263, loss_cps: 0.101573
[14:28:48.423] iteration 29761: total_loss: 0.484136, loss_sup: 0.161286, loss_mps: 0.114604, loss_cps: 0.208246
[14:28:48.569] iteration 29762: total_loss: 0.224576, loss_sup: 0.031107, loss_mps: 0.071559, loss_cps: 0.121910
[14:28:48.716] iteration 29763: total_loss: 0.211112, loss_sup: 0.001928, loss_mps: 0.081420, loss_cps: 0.127764
[14:28:48.861] iteration 29764: total_loss: 0.334395, loss_sup: 0.035283, loss_mps: 0.100667, loss_cps: 0.198445
[14:28:49.007] iteration 29765: total_loss: 0.327277, loss_sup: 0.022524, loss_mps: 0.105537, loss_cps: 0.199216
[14:28:49.153] iteration 29766: total_loss: 0.357549, loss_sup: 0.071447, loss_mps: 0.105244, loss_cps: 0.180858
[14:28:49.301] iteration 29767: total_loss: 0.396165, loss_sup: 0.154119, loss_mps: 0.084504, loss_cps: 0.157541
[14:28:49.447] iteration 29768: total_loss: 0.183530, loss_sup: 0.004694, loss_mps: 0.061995, loss_cps: 0.116842
[14:28:49.593] iteration 29769: total_loss: 0.179311, loss_sup: 0.008330, loss_mps: 0.061806, loss_cps: 0.109174
[14:28:49.739] iteration 29770: total_loss: 0.200310, loss_sup: 0.003354, loss_mps: 0.074643, loss_cps: 0.122313
[14:28:49.885] iteration 29771: total_loss: 0.130961, loss_sup: 0.012438, loss_mps: 0.044823, loss_cps: 0.073699
[14:28:50.030] iteration 29772: total_loss: 0.288227, loss_sup: 0.004412, loss_mps: 0.090466, loss_cps: 0.193349
[14:28:50.176] iteration 29773: total_loss: 0.114036, loss_sup: 0.008459, loss_mps: 0.041586, loss_cps: 0.063991
[14:28:50.323] iteration 29774: total_loss: 0.253549, loss_sup: 0.028562, loss_mps: 0.083052, loss_cps: 0.141935
[14:28:50.469] iteration 29775: total_loss: 0.193162, loss_sup: 0.019069, loss_mps: 0.064651, loss_cps: 0.109442
[14:28:50.615] iteration 29776: total_loss: 0.338774, loss_sup: 0.094772, loss_mps: 0.081818, loss_cps: 0.162184
[14:28:50.763] iteration 29777: total_loss: 0.130869, loss_sup: 0.013377, loss_mps: 0.045710, loss_cps: 0.071782
[14:28:50.910] iteration 29778: total_loss: 0.167112, loss_sup: 0.008002, loss_mps: 0.061783, loss_cps: 0.097328
[14:28:51.056] iteration 29779: total_loss: 0.237914, loss_sup: 0.053996, loss_mps: 0.069563, loss_cps: 0.114355
[14:28:51.202] iteration 29780: total_loss: 0.360246, loss_sup: 0.200857, loss_mps: 0.060137, loss_cps: 0.099252
[14:28:51.350] iteration 29781: total_loss: 0.235599, loss_sup: 0.001399, loss_mps: 0.080463, loss_cps: 0.153738
[14:28:51.496] iteration 29782: total_loss: 0.154107, loss_sup: 0.002272, loss_mps: 0.054626, loss_cps: 0.097209
[14:28:51.642] iteration 29783: total_loss: 0.343033, loss_sup: 0.061501, loss_mps: 0.098008, loss_cps: 0.183524
[14:28:51.788] iteration 29784: total_loss: 0.188104, loss_sup: 0.011700, loss_mps: 0.064401, loss_cps: 0.112003
[14:28:51.939] iteration 29785: total_loss: 0.270737, loss_sup: 0.020184, loss_mps: 0.090451, loss_cps: 0.160102
[14:28:52.084] iteration 29786: total_loss: 0.146132, loss_sup: 0.018839, loss_mps: 0.048630, loss_cps: 0.078664
[14:28:52.230] iteration 29787: total_loss: 0.220329, loss_sup: 0.001140, loss_mps: 0.076385, loss_cps: 0.142804
[14:28:52.381] iteration 29788: total_loss: 0.157171, loss_sup: 0.004514, loss_mps: 0.057225, loss_cps: 0.095433
[14:28:52.526] iteration 29789: total_loss: 0.566290, loss_sup: 0.024715, loss_mps: 0.173683, loss_cps: 0.367892
[14:28:52.672] iteration 29790: total_loss: 0.353481, loss_sup: 0.041126, loss_mps: 0.103870, loss_cps: 0.208485
[14:28:52.818] iteration 29791: total_loss: 0.274615, loss_sup: 0.024507, loss_mps: 0.090107, loss_cps: 0.160001
[14:28:52.966] iteration 29792: total_loss: 0.265247, loss_sup: 0.007816, loss_mps: 0.091946, loss_cps: 0.165484
[14:28:53.113] iteration 29793: total_loss: 0.449020, loss_sup: 0.127188, loss_mps: 0.113731, loss_cps: 0.208102
[14:28:53.259] iteration 29794: total_loss: 0.238721, loss_sup: 0.031897, loss_mps: 0.072918, loss_cps: 0.133906
[14:28:53.406] iteration 29795: total_loss: 0.270733, loss_sup: 0.073333, loss_mps: 0.072793, loss_cps: 0.124607
[14:28:53.552] iteration 29796: total_loss: 0.484364, loss_sup: 0.215386, loss_mps: 0.097146, loss_cps: 0.171832
[14:28:53.698] iteration 29797: total_loss: 0.410204, loss_sup: 0.003433, loss_mps: 0.127483, loss_cps: 0.279289
[14:28:53.845] iteration 29798: total_loss: 0.388442, loss_sup: 0.050798, loss_mps: 0.119328, loss_cps: 0.218315
[14:28:53.991] iteration 29799: total_loss: 0.187466, loss_sup: 0.001705, loss_mps: 0.065945, loss_cps: 0.119816
[14:28:54.138] iteration 29800: total_loss: 0.291793, loss_sup: 0.057804, loss_mps: 0.085410, loss_cps: 0.148579
[14:28:54.138] Evaluation Started ==>
[14:29:05.497] ==> valid iteration 29800: unet metrics: {'dc': 0.6822239895472514, 'jc': 0.5675156343510698, 'pre': 0.8092233918646737, 'hd': 5.344429629695385}, ynet metrics: {'dc': 0.6340888316597434, 'jc': 0.5227073952827872, 'pre': 0.8060930612765211, 'hd': 5.328004685632856}.
[14:29:05.504] Evaluation Finished!⏹️
[14:29:05.657] iteration 29801: total_loss: 0.452880, loss_sup: 0.117036, loss_mps: 0.109758, loss_cps: 0.226085
[14:29:05.807] iteration 29802: total_loss: 0.198213, loss_sup: 0.025553, loss_mps: 0.065579, loss_cps: 0.107081
[14:29:05.953] iteration 29803: total_loss: 0.255966, loss_sup: 0.018709, loss_mps: 0.081126, loss_cps: 0.156131
[14:29:06.099] iteration 29804: total_loss: 0.133928, loss_sup: 0.000487, loss_mps: 0.051161, loss_cps: 0.082281
[14:29:06.245] iteration 29805: total_loss: 0.257640, loss_sup: 0.010074, loss_mps: 0.088008, loss_cps: 0.159557
[14:29:06.391] iteration 29806: total_loss: 0.285634, loss_sup: 0.008011, loss_mps: 0.102137, loss_cps: 0.175486
[14:29:06.537] iteration 29807: total_loss: 0.213875, loss_sup: 0.005220, loss_mps: 0.073235, loss_cps: 0.135419
[14:29:06.682] iteration 29808: total_loss: 0.409281, loss_sup: 0.062929, loss_mps: 0.123933, loss_cps: 0.222419
[14:29:06.827] iteration 29809: total_loss: 0.308837, loss_sup: 0.010564, loss_mps: 0.103937, loss_cps: 0.194336
[14:29:06.972] iteration 29810: total_loss: 0.209246, loss_sup: 0.032693, loss_mps: 0.069946, loss_cps: 0.106607
[14:29:07.118] iteration 29811: total_loss: 0.184399, loss_sup: 0.022960, loss_mps: 0.062700, loss_cps: 0.098739
[14:29:07.263] iteration 29812: total_loss: 0.320443, loss_sup: 0.101109, loss_mps: 0.079003, loss_cps: 0.140331
[14:29:07.409] iteration 29813: total_loss: 0.235034, loss_sup: 0.006254, loss_mps: 0.078783, loss_cps: 0.149997
[14:29:07.554] iteration 29814: total_loss: 0.194147, loss_sup: 0.012267, loss_mps: 0.061536, loss_cps: 0.120344
[14:29:07.704] iteration 29815: total_loss: 0.304510, loss_sup: 0.019196, loss_mps: 0.105420, loss_cps: 0.179894
[14:29:07.849] iteration 29816: total_loss: 0.348082, loss_sup: 0.012783, loss_mps: 0.118328, loss_cps: 0.216972
[14:29:07.997] iteration 29817: total_loss: 0.195773, loss_sup: 0.017856, loss_mps: 0.063749, loss_cps: 0.114168
[14:29:08.142] iteration 29818: total_loss: 0.291314, loss_sup: 0.038167, loss_mps: 0.088055, loss_cps: 0.165091
[14:29:08.289] iteration 29819: total_loss: 0.231990, loss_sup: 0.084024, loss_mps: 0.057373, loss_cps: 0.090593
[14:29:08.434] iteration 29820: total_loss: 0.154175, loss_sup: 0.006982, loss_mps: 0.058376, loss_cps: 0.088818
[14:29:08.581] iteration 29821: total_loss: 0.300069, loss_sup: 0.009311, loss_mps: 0.098544, loss_cps: 0.192214
[14:29:08.727] iteration 29822: total_loss: 0.247274, loss_sup: 0.009366, loss_mps: 0.084070, loss_cps: 0.153837
[14:29:08.875] iteration 29823: total_loss: 0.278200, loss_sup: 0.041296, loss_mps: 0.080432, loss_cps: 0.156472
[14:29:09.022] iteration 29824: total_loss: 0.443941, loss_sup: 0.189296, loss_mps: 0.091081, loss_cps: 0.163564
[14:29:09.171] iteration 29825: total_loss: 0.170465, loss_sup: 0.016293, loss_mps: 0.057606, loss_cps: 0.096566
[14:29:09.317] iteration 29826: total_loss: 0.178467, loss_sup: 0.005434, loss_mps: 0.063659, loss_cps: 0.109373
[14:29:09.462] iteration 29827: total_loss: 0.270876, loss_sup: 0.005706, loss_mps: 0.098503, loss_cps: 0.166666
[14:29:09.608] iteration 29828: total_loss: 0.236176, loss_sup: 0.034908, loss_mps: 0.071567, loss_cps: 0.129701
[14:29:09.754] iteration 29829: total_loss: 0.260835, loss_sup: 0.052688, loss_mps: 0.071513, loss_cps: 0.136634
[14:29:09.900] iteration 29830: total_loss: 0.232419, loss_sup: 0.052312, loss_mps: 0.067936, loss_cps: 0.112172
[14:29:10.049] iteration 29831: total_loss: 0.200867, loss_sup: 0.010212, loss_mps: 0.071442, loss_cps: 0.119212
[14:29:10.196] iteration 29832: total_loss: 0.201984, loss_sup: 0.027573, loss_mps: 0.067557, loss_cps: 0.106853
[14:29:10.341] iteration 29833: total_loss: 0.285843, loss_sup: 0.000449, loss_mps: 0.097913, loss_cps: 0.187480
[14:29:10.487] iteration 29834: total_loss: 0.552273, loss_sup: 0.159089, loss_mps: 0.127715, loss_cps: 0.265468
[14:29:10.633] iteration 29835: total_loss: 0.213864, loss_sup: 0.027086, loss_mps: 0.070594, loss_cps: 0.116183
[14:29:10.780] iteration 29836: total_loss: 0.688141, loss_sup: 0.010771, loss_mps: 0.209144, loss_cps: 0.468226
[14:29:10.929] iteration 29837: total_loss: 0.185172, loss_sup: 0.014365, loss_mps: 0.065043, loss_cps: 0.105764
[14:29:11.074] iteration 29838: total_loss: 0.252907, loss_sup: 0.045920, loss_mps: 0.076092, loss_cps: 0.130896
[14:29:11.224] iteration 29839: total_loss: 0.536307, loss_sup: 0.143787, loss_mps: 0.131990, loss_cps: 0.260530
[14:29:11.372] iteration 29840: total_loss: 0.432315, loss_sup: 0.081155, loss_mps: 0.116486, loss_cps: 0.234674
[14:29:11.520] iteration 29841: total_loss: 0.234447, loss_sup: 0.014262, loss_mps: 0.081073, loss_cps: 0.139111
[14:29:11.670] iteration 29842: total_loss: 0.252700, loss_sup: 0.016737, loss_mps: 0.080204, loss_cps: 0.155759
[14:29:11.816] iteration 29843: total_loss: 0.172905, loss_sup: 0.002834, loss_mps: 0.061621, loss_cps: 0.108450
[14:29:11.962] iteration 29844: total_loss: 0.314917, loss_sup: 0.112726, loss_mps: 0.073203, loss_cps: 0.128988
[14:29:12.108] iteration 29845: total_loss: 0.222159, loss_sup: 0.003363, loss_mps: 0.077182, loss_cps: 0.141614
[14:29:12.254] iteration 29846: total_loss: 0.301378, loss_sup: 0.136461, loss_mps: 0.060063, loss_cps: 0.104853
[14:29:12.399] iteration 29847: total_loss: 0.291583, loss_sup: 0.052463, loss_mps: 0.090341, loss_cps: 0.148779
[14:29:12.545] iteration 29848: total_loss: 0.344215, loss_sup: 0.122768, loss_mps: 0.085725, loss_cps: 0.135723
[14:29:12.693] iteration 29849: total_loss: 0.433030, loss_sup: 0.166590, loss_mps: 0.094536, loss_cps: 0.171904
[14:29:12.839] iteration 29850: total_loss: 0.299407, loss_sup: 0.001805, loss_mps: 0.096858, loss_cps: 0.200745
[14:29:12.985] iteration 29851: total_loss: 0.378661, loss_sup: 0.053718, loss_mps: 0.107166, loss_cps: 0.217776
[14:29:13.131] iteration 29852: total_loss: 0.216595, loss_sup: 0.002677, loss_mps: 0.076213, loss_cps: 0.137705
[14:29:13.277] iteration 29853: total_loss: 0.196440, loss_sup: 0.016065, loss_mps: 0.070586, loss_cps: 0.109789
[14:29:13.422] iteration 29854: total_loss: 0.310780, loss_sup: 0.122079, loss_mps: 0.065996, loss_cps: 0.122704
[14:29:13.568] iteration 29855: total_loss: 0.190428, loss_sup: 0.030752, loss_mps: 0.057786, loss_cps: 0.101890
[14:29:13.714] iteration 29856: total_loss: 0.149980, loss_sup: 0.003736, loss_mps: 0.057886, loss_cps: 0.088358
[14:29:13.860] iteration 29857: total_loss: 0.232730, loss_sup: 0.042797, loss_mps: 0.069814, loss_cps: 0.120119
[14:29:14.006] iteration 29858: total_loss: 0.379465, loss_sup: 0.122027, loss_mps: 0.094751, loss_cps: 0.162686
[14:29:14.152] iteration 29859: total_loss: 0.538634, loss_sup: 0.225428, loss_mps: 0.104693, loss_cps: 0.208512
[14:29:14.300] iteration 29860: total_loss: 0.272650, loss_sup: 0.102867, loss_mps: 0.064858, loss_cps: 0.104925
[14:29:14.445] iteration 29861: total_loss: 0.192198, loss_sup: 0.003912, loss_mps: 0.071559, loss_cps: 0.116727
[14:29:14.592] iteration 29862: total_loss: 0.208151, loss_sup: 0.008513, loss_mps: 0.070344, loss_cps: 0.129294
[14:29:14.738] iteration 29863: total_loss: 0.138971, loss_sup: 0.020756, loss_mps: 0.044387, loss_cps: 0.073828
[14:29:14.884] iteration 29864: total_loss: 0.337363, loss_sup: 0.035658, loss_mps: 0.115820, loss_cps: 0.185885
[14:29:15.030] iteration 29865: total_loss: 0.211876, loss_sup: 0.044245, loss_mps: 0.063544, loss_cps: 0.104087
[14:29:15.176] iteration 29866: total_loss: 0.253450, loss_sup: 0.036206, loss_mps: 0.073432, loss_cps: 0.143812
[14:29:15.321] iteration 29867: total_loss: 0.289169, loss_sup: 0.001374, loss_mps: 0.095456, loss_cps: 0.192339
[14:29:15.467] iteration 29868: total_loss: 0.201606, loss_sup: 0.000590, loss_mps: 0.073206, loss_cps: 0.127811
[14:29:15.613] iteration 29869: total_loss: 0.221548, loss_sup: 0.011990, loss_mps: 0.071894, loss_cps: 0.137663
[14:29:15.761] iteration 29870: total_loss: 0.181478, loss_sup: 0.004871, loss_mps: 0.066433, loss_cps: 0.110173
[14:29:15.907] iteration 29871: total_loss: 0.501707, loss_sup: 0.194987, loss_mps: 0.106743, loss_cps: 0.199976
[14:29:16.053] iteration 29872: total_loss: 0.412274, loss_sup: 0.028961, loss_mps: 0.130202, loss_cps: 0.253110
[14:29:16.199] iteration 29873: total_loss: 0.270527, loss_sup: 0.023610, loss_mps: 0.091410, loss_cps: 0.155508
[14:29:16.345] iteration 29874: total_loss: 0.241684, loss_sup: 0.044815, loss_mps: 0.067365, loss_cps: 0.129505
[14:29:16.491] iteration 29875: total_loss: 0.357549, loss_sup: 0.046750, loss_mps: 0.101156, loss_cps: 0.209643
[14:29:16.637] iteration 29876: total_loss: 0.275309, loss_sup: 0.063316, loss_mps: 0.078790, loss_cps: 0.133203
[14:29:16.783] iteration 29877: total_loss: 0.347248, loss_sup: 0.064510, loss_mps: 0.097315, loss_cps: 0.185423
[14:29:16.928] iteration 29878: total_loss: 0.228093, loss_sup: 0.012076, loss_mps: 0.076578, loss_cps: 0.139438
[14:29:17.074] iteration 29879: total_loss: 0.162230, loss_sup: 0.003823, loss_mps: 0.058599, loss_cps: 0.099808
[14:29:17.221] iteration 29880: total_loss: 0.295986, loss_sup: 0.078561, loss_mps: 0.077097, loss_cps: 0.140329
[14:29:17.366] iteration 29881: total_loss: 0.243030, loss_sup: 0.030492, loss_mps: 0.076884, loss_cps: 0.135655
[14:29:17.513] iteration 29882: total_loss: 0.200660, loss_sup: 0.001262, loss_mps: 0.070046, loss_cps: 0.129352
[14:29:17.660] iteration 29883: total_loss: 0.317388, loss_sup: 0.018986, loss_mps: 0.106129, loss_cps: 0.192273
[14:29:17.806] iteration 29884: total_loss: 0.345067, loss_sup: 0.068606, loss_mps: 0.090894, loss_cps: 0.185567
[14:29:17.952] iteration 29885: total_loss: 0.294077, loss_sup: 0.037593, loss_mps: 0.093028, loss_cps: 0.163455
[14:29:18.098] iteration 29886: total_loss: 0.336543, loss_sup: 0.025033, loss_mps: 0.103145, loss_cps: 0.208365
[14:29:18.245] iteration 29887: total_loss: 0.372738, loss_sup: 0.005781, loss_mps: 0.125903, loss_cps: 0.241054
[14:29:18.392] iteration 29888: total_loss: 0.188415, loss_sup: 0.006639, loss_mps: 0.064537, loss_cps: 0.117238
[14:29:18.538] iteration 29889: total_loss: 0.252277, loss_sup: 0.012218, loss_mps: 0.080961, loss_cps: 0.159098
[14:29:18.684] iteration 29890: total_loss: 0.214373, loss_sup: 0.035190, loss_mps: 0.065429, loss_cps: 0.113754
[14:29:18.831] iteration 29891: total_loss: 0.200515, loss_sup: 0.004871, loss_mps: 0.067052, loss_cps: 0.128592
[14:29:18.981] iteration 29892: total_loss: 0.221170, loss_sup: 0.004008, loss_mps: 0.076354, loss_cps: 0.140809
[14:29:19.128] iteration 29893: total_loss: 0.353361, loss_sup: 0.022305, loss_mps: 0.116931, loss_cps: 0.214125
[14:29:19.275] iteration 29894: total_loss: 0.655524, loss_sup: 0.161478, loss_mps: 0.161759, loss_cps: 0.332287
[14:29:19.422] iteration 29895: total_loss: 0.353213, loss_sup: 0.043861, loss_mps: 0.100161, loss_cps: 0.209192
[14:29:19.568] iteration 29896: total_loss: 0.280130, loss_sup: 0.102502, loss_mps: 0.062401, loss_cps: 0.115228
[14:29:19.717] iteration 29897: total_loss: 0.210368, loss_sup: 0.042218, loss_mps: 0.061931, loss_cps: 0.106219
[14:29:19.863] iteration 29898: total_loss: 0.150933, loss_sup: 0.003113, loss_mps: 0.056424, loss_cps: 0.091397
[14:29:20.010] iteration 29899: total_loss: 0.224927, loss_sup: 0.018419, loss_mps: 0.071497, loss_cps: 0.135010
[14:29:20.156] iteration 29900: total_loss: 0.205079, loss_sup: 0.006739, loss_mps: 0.074126, loss_cps: 0.124214
[14:29:20.156] Evaluation Started ==>
[14:29:31.476] ==> valid iteration 29900: unet metrics: {'dc': 0.6813933889337455, 'jc': 0.5664711995027739, 'pre': 0.8084201696443221, 'hd': 5.34264023263042}, ynet metrics: {'dc': 0.6327690907407293, 'jc': 0.5213184639977688, 'pre': 0.8031925752021304, 'hd': 5.351918221704557}.
[14:29:31.478] Evaluation Finished!⏹️
[14:29:31.633] iteration 29901: total_loss: 0.243800, loss_sup: 0.036260, loss_mps: 0.071185, loss_cps: 0.136354
[14:29:31.781] iteration 29902: total_loss: 0.255363, loss_sup: 0.031999, loss_mps: 0.078052, loss_cps: 0.145312
[14:29:31.930] iteration 29903: total_loss: 0.198530, loss_sup: 0.056293, loss_mps: 0.052753, loss_cps: 0.089484
[14:29:32.077] iteration 29904: total_loss: 0.132996, loss_sup: 0.007091, loss_mps: 0.047302, loss_cps: 0.078603
[14:29:32.222] iteration 29905: total_loss: 0.374037, loss_sup: 0.008654, loss_mps: 0.121101, loss_cps: 0.244282
[14:29:32.367] iteration 29906: total_loss: 0.403968, loss_sup: 0.031785, loss_mps: 0.119176, loss_cps: 0.253008
[14:29:32.513] iteration 29907: total_loss: 0.368832, loss_sup: 0.048507, loss_mps: 0.108357, loss_cps: 0.211968
[14:29:32.659] iteration 29908: total_loss: 0.241399, loss_sup: 0.038724, loss_mps: 0.074211, loss_cps: 0.128464
[14:29:32.807] iteration 29909: total_loss: 0.390251, loss_sup: 0.168906, loss_mps: 0.087825, loss_cps: 0.133520
[14:29:32.953] iteration 29910: total_loss: 0.181104, loss_sup: 0.031853, loss_mps: 0.052420, loss_cps: 0.096832
[14:29:33.099] iteration 29911: total_loss: 0.231909, loss_sup: 0.024089, loss_mps: 0.077250, loss_cps: 0.130571
[14:29:33.246] iteration 29912: total_loss: 0.186749, loss_sup: 0.003896, loss_mps: 0.067248, loss_cps: 0.115605
[14:29:33.391] iteration 29913: total_loss: 0.464968, loss_sup: 0.158481, loss_mps: 0.103738, loss_cps: 0.202749
[14:29:33.536] iteration 29914: total_loss: 0.321677, loss_sup: 0.069002, loss_mps: 0.085774, loss_cps: 0.166902
[14:29:33.683] iteration 29915: total_loss: 0.202211, loss_sup: 0.005783, loss_mps: 0.073606, loss_cps: 0.122822
[14:29:33.829] iteration 29916: total_loss: 0.489825, loss_sup: 0.042304, loss_mps: 0.146927, loss_cps: 0.300594
[14:29:33.978] iteration 29917: total_loss: 0.336103, loss_sup: 0.029101, loss_mps: 0.106190, loss_cps: 0.200812
[14:29:34.125] iteration 29918: total_loss: 0.239919, loss_sup: 0.020248, loss_mps: 0.075970, loss_cps: 0.143702
[14:29:34.271] iteration 29919: total_loss: 0.247270, loss_sup: 0.089625, loss_mps: 0.062759, loss_cps: 0.094887
[14:29:34.416] iteration 29920: total_loss: 0.303704, loss_sup: 0.020257, loss_mps: 0.099526, loss_cps: 0.183920
[14:29:34.562] iteration 29921: total_loss: 0.204431, loss_sup: 0.012325, loss_mps: 0.068353, loss_cps: 0.123753
[14:29:34.707] iteration 29922: total_loss: 0.356003, loss_sup: 0.097467, loss_mps: 0.097163, loss_cps: 0.161372
[14:29:34.853] iteration 29923: total_loss: 0.223345, loss_sup: 0.029484, loss_mps: 0.073411, loss_cps: 0.120450
[14:29:35.002] iteration 29924: total_loss: 0.221212, loss_sup: 0.029064, loss_mps: 0.069255, loss_cps: 0.122894
[14:29:35.148] iteration 29925: total_loss: 0.447427, loss_sup: 0.013683, loss_mps: 0.139636, loss_cps: 0.294108
[14:29:35.297] iteration 29926: total_loss: 0.181208, loss_sup: 0.003852, loss_mps: 0.063432, loss_cps: 0.113923
[14:29:35.442] iteration 29927: total_loss: 0.262457, loss_sup: 0.045454, loss_mps: 0.082585, loss_cps: 0.134418
[14:29:35.590] iteration 29928: total_loss: 0.189249, loss_sup: 0.027613, loss_mps: 0.061262, loss_cps: 0.100374
[14:29:35.736] iteration 29929: total_loss: 0.171824, loss_sup: 0.004820, loss_mps: 0.065130, loss_cps: 0.101874
[14:29:35.883] iteration 29930: total_loss: 0.277426, loss_sup: 0.027844, loss_mps: 0.092194, loss_cps: 0.157387
[14:29:36.031] iteration 29931: total_loss: 0.222216, loss_sup: 0.002416, loss_mps: 0.078906, loss_cps: 0.140895
[14:29:36.178] iteration 29932: total_loss: 0.290583, loss_sup: 0.035673, loss_mps: 0.099776, loss_cps: 0.155134
[14:29:36.324] iteration 29933: total_loss: 0.153602, loss_sup: 0.000459, loss_mps: 0.056843, loss_cps: 0.096300
[14:29:36.470] iteration 29934: total_loss: 0.318008, loss_sup: 0.125216, loss_mps: 0.071115, loss_cps: 0.121677
[14:29:36.617] iteration 29935: total_loss: 0.277981, loss_sup: 0.051604, loss_mps: 0.081803, loss_cps: 0.144574
[14:29:36.765] iteration 29936: total_loss: 0.308599, loss_sup: 0.061574, loss_mps: 0.087819, loss_cps: 0.159206
[14:29:36.912] iteration 29937: total_loss: 0.223965, loss_sup: 0.017987, loss_mps: 0.077486, loss_cps: 0.128492
[14:29:37.059] iteration 29938: total_loss: 0.265305, loss_sup: 0.045646, loss_mps: 0.080434, loss_cps: 0.139225
[14:29:37.206] iteration 29939: total_loss: 0.230246, loss_sup: 0.013959, loss_mps: 0.074528, loss_cps: 0.141758
[14:29:37.352] iteration 29940: total_loss: 0.187638, loss_sup: 0.011104, loss_mps: 0.064989, loss_cps: 0.111545
[14:29:37.498] iteration 29941: total_loss: 0.402640, loss_sup: 0.018707, loss_mps: 0.129383, loss_cps: 0.254550
[14:29:37.645] iteration 29942: total_loss: 0.175712, loss_sup: 0.001357, loss_mps: 0.063712, loss_cps: 0.110644
[14:29:37.791] iteration 29943: total_loss: 0.470606, loss_sup: 0.101854, loss_mps: 0.129470, loss_cps: 0.239282
[14:29:37.937] iteration 29944: total_loss: 0.199409, loss_sup: 0.013922, loss_mps: 0.066959, loss_cps: 0.118529
[14:29:38.085] iteration 29945: total_loss: 0.322113, loss_sup: 0.043780, loss_mps: 0.097437, loss_cps: 0.180895
[14:29:38.232] iteration 29946: total_loss: 0.187628, loss_sup: 0.005218, loss_mps: 0.069874, loss_cps: 0.112536
[14:29:38.378] iteration 29947: total_loss: 0.420883, loss_sup: 0.157080, loss_mps: 0.093710, loss_cps: 0.170093
[14:29:38.524] iteration 29948: total_loss: 0.336089, loss_sup: 0.022829, loss_mps: 0.102383, loss_cps: 0.210878
[14:29:38.670] iteration 29949: total_loss: 0.310605, loss_sup: 0.025977, loss_mps: 0.091312, loss_cps: 0.193316
[14:29:38.816] iteration 29950: total_loss: 0.150897, loss_sup: 0.010243, loss_mps: 0.055934, loss_cps: 0.084720
[14:29:38.962] iteration 29951: total_loss: 0.264853, loss_sup: 0.068580, loss_mps: 0.073571, loss_cps: 0.122701
[14:29:39.111] iteration 29952: total_loss: 0.358322, loss_sup: 0.090893, loss_mps: 0.087687, loss_cps: 0.179742
[14:29:39.261] iteration 29953: total_loss: 0.222368, loss_sup: 0.003504, loss_mps: 0.077640, loss_cps: 0.141224
[14:29:39.407] iteration 29954: total_loss: 0.266524, loss_sup: 0.021681, loss_mps: 0.087386, loss_cps: 0.157458
[14:29:39.554] iteration 29955: total_loss: 0.201877, loss_sup: 0.030610, loss_mps: 0.062038, loss_cps: 0.109229
[14:29:39.700] iteration 29956: total_loss: 0.254345, loss_sup: 0.009454, loss_mps: 0.094571, loss_cps: 0.150320
[14:29:39.846] iteration 29957: total_loss: 0.395430, loss_sup: 0.041426, loss_mps: 0.121312, loss_cps: 0.232692
[14:29:39.992] iteration 29958: total_loss: 0.271279, loss_sup: 0.104724, loss_mps: 0.062898, loss_cps: 0.103657
[14:29:40.139] iteration 29959: total_loss: 0.377848, loss_sup: 0.032881, loss_mps: 0.114493, loss_cps: 0.230473
[14:29:40.285] iteration 29960: total_loss: 0.303515, loss_sup: 0.060969, loss_mps: 0.084357, loss_cps: 0.158189
[14:29:40.431] iteration 29961: total_loss: 0.461837, loss_sup: 0.063388, loss_mps: 0.144234, loss_cps: 0.254215
[14:29:40.577] iteration 29962: total_loss: 0.244076, loss_sup: 0.109539, loss_mps: 0.048306, loss_cps: 0.086231
[14:29:40.723] iteration 29963: total_loss: 0.275010, loss_sup: 0.018540, loss_mps: 0.091227, loss_cps: 0.165244
[14:29:40.870] iteration 29964: total_loss: 0.192700, loss_sup: 0.011656, loss_mps: 0.066876, loss_cps: 0.114169
[14:29:41.015] iteration 29965: total_loss: 0.185075, loss_sup: 0.016236, loss_mps: 0.061666, loss_cps: 0.107173
[14:29:41.162] iteration 29966: total_loss: 0.600608, loss_sup: 0.128223, loss_mps: 0.149374, loss_cps: 0.323011
[14:29:41.307] iteration 29967: total_loss: 0.192135, loss_sup: 0.005313, loss_mps: 0.065468, loss_cps: 0.121355
[14:29:41.453] iteration 29968: total_loss: 0.171613, loss_sup: 0.030641, loss_mps: 0.052360, loss_cps: 0.088611
[14:29:41.599] iteration 29969: total_loss: 0.227838, loss_sup: 0.000358, loss_mps: 0.077004, loss_cps: 0.150477
[14:29:41.745] iteration 29970: total_loss: 0.226300, loss_sup: 0.011432, loss_mps: 0.078571, loss_cps: 0.136297
[14:29:41.894] iteration 29971: total_loss: 0.367882, loss_sup: 0.118909, loss_mps: 0.087517, loss_cps: 0.161456
[14:29:42.045] iteration 29972: total_loss: 0.242391, loss_sup: 0.033683, loss_mps: 0.073614, loss_cps: 0.135094
[14:29:42.192] iteration 29973: total_loss: 0.576357, loss_sup: 0.046393, loss_mps: 0.168361, loss_cps: 0.361603
[14:29:42.338] iteration 29974: total_loss: 0.291543, loss_sup: 0.098150, loss_mps: 0.071164, loss_cps: 0.122229
[14:29:42.483] iteration 29975: total_loss: 0.243978, loss_sup: 0.047565, loss_mps: 0.070189, loss_cps: 0.126224
[14:29:42.629] iteration 29976: total_loss: 0.153245, loss_sup: 0.003945, loss_mps: 0.056590, loss_cps: 0.092711
[14:29:42.775] iteration 29977: total_loss: 0.173694, loss_sup: 0.006156, loss_mps: 0.065322, loss_cps: 0.102216
[14:29:42.922] iteration 29978: total_loss: 0.364028, loss_sup: 0.077930, loss_mps: 0.098500, loss_cps: 0.187597
[14:29:43.073] iteration 29979: total_loss: 0.253240, loss_sup: 0.014831, loss_mps: 0.084416, loss_cps: 0.153993
[14:29:43.222] iteration 29980: total_loss: 0.233753, loss_sup: 0.061265, loss_mps: 0.061884, loss_cps: 0.110603
[14:29:43.368] iteration 29981: total_loss: 0.401548, loss_sup: 0.128098, loss_mps: 0.102480, loss_cps: 0.170969
[14:29:43.514] iteration 29982: total_loss: 0.325808, loss_sup: 0.009585, loss_mps: 0.103928, loss_cps: 0.212295
[14:29:43.660] iteration 29983: total_loss: 0.451167, loss_sup: 0.107073, loss_mps: 0.111357, loss_cps: 0.232736
[14:29:43.810] iteration 29984: total_loss: 0.397710, loss_sup: 0.032265, loss_mps: 0.120084, loss_cps: 0.245361
[14:29:43.956] iteration 29985: total_loss: 0.136610, loss_sup: 0.007334, loss_mps: 0.048631, loss_cps: 0.080646
[14:29:44.102] iteration 29986: total_loss: 0.453314, loss_sup: 0.090064, loss_mps: 0.124730, loss_cps: 0.238520
[14:29:44.251] iteration 29987: total_loss: 0.338044, loss_sup: 0.026270, loss_mps: 0.106994, loss_cps: 0.204779
[14:29:44.398] iteration 29988: total_loss: 0.247179, loss_sup: 0.036640, loss_mps: 0.075086, loss_cps: 0.135453
[14:29:44.544] iteration 29989: total_loss: 0.221797, loss_sup: 0.015343, loss_mps: 0.078516, loss_cps: 0.127938
[14:29:44.691] iteration 29990: total_loss: 0.158254, loss_sup: 0.029298, loss_mps: 0.050164, loss_cps: 0.078792
[14:29:44.837] iteration 29991: total_loss: 0.220947, loss_sup: 0.045468, loss_mps: 0.064331, loss_cps: 0.111148
[14:29:44.989] iteration 29992: total_loss: 0.525012, loss_sup: 0.110998, loss_mps: 0.142204, loss_cps: 0.271811
[14:29:45.136] iteration 29993: total_loss: 0.223040, loss_sup: 0.051276, loss_mps: 0.061666, loss_cps: 0.110099
[14:29:45.282] iteration 29994: total_loss: 0.194791, loss_sup: 0.060668, loss_mps: 0.050440, loss_cps: 0.083684
[14:29:45.432] iteration 29995: total_loss: 0.329350, loss_sup: 0.032741, loss_mps: 0.100697, loss_cps: 0.195912
[14:29:45.579] iteration 29996: total_loss: 0.202199, loss_sup: 0.036892, loss_mps: 0.060788, loss_cps: 0.104519
[14:29:45.726] iteration 29997: total_loss: 0.221620, loss_sup: 0.022148, loss_mps: 0.072350, loss_cps: 0.127122
[14:29:45.873] iteration 29998: total_loss: 0.407760, loss_sup: 0.110422, loss_mps: 0.100088, loss_cps: 0.197251
[14:29:46.019] iteration 29999: total_loss: 0.383269, loss_sup: 0.008023, loss_mps: 0.125555, loss_cps: 0.249691
[14:29:46.166] iteration 30000: total_loss: 0.309272, loss_sup: 0.002828, loss_mps: 0.110955, loss_cps: 0.195490
[14:29:46.166] Evaluation Started ==>
[14:29:57.630] ==> valid iteration 30000: unet metrics: {'dc': 0.6831352306538303, 'jc': 0.5684660779380644, 'pre': 0.8073761075183356, 'hd': 5.34057598392399}, ynet metrics: {'dc': 0.6408794801745241, 'jc': 0.5295600256906559, 'pre': 0.8017495342721302, 'hd': 5.351300202704046}.
[14:29:57.632] Evaluation Finished!⏹️
[14:29:57.654] save last unet model to ../model/s2me-ent-css_5.0_25k/Scribble/unet_last_model.pth
[14:29:57.699] save last ynet model to ../model/s2me-ent-css_5.0_25k/Scribble/ynet_ffc_last_model.pth
[14:29:57.832] Training Finished!⏹️
[14:29:57.832] Unet Best Validation dice: 0.6891 in iter: 20700.
[14:29:57.832] Ynet Best Validation dice: 0.6513 in iter: 11800.
[14:29:57.832] ==> Test with best models:
[14:30:15.962] ==> Unet Test metrics: {'unet_dc': 0.678527753619008, 'unet_jc': 0.5658715652474762, 'unet_pre': 0.7150685917841363, 'unet_hd': 4.303911956609274}
[14:30:15.962] ==> Ynet Test metrics: {'ynet_dc': 0.6585697118526138, 'ynet_jc': 0.5430091088037216, 'ynet_pre': 0.7021870606232615, 'ynet_hd': 4.449624757635085}
